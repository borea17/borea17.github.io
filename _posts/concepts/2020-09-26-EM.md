---
title: "Expectation Maximization"
permalink: "/ML_concepts/probabilistic_models/expectation_maximization"
author: "Markus Borea"
tags: [interaction network, graph networks, generalization]
published: true
toc: true
toc_sticky: true
toc_label: "Table of Contents"
type: "concepts probability"
---

NOTE: THIS IS CURRENTLY WIP

[Burgess et al. (2019)](https://arxiv.org/abs/1901.11390) developed
the **Multi-Object Network (MONet)** as an end-to-end trainable model to
decompose images into meaningful entities such as objects. Notably,
the whole training process is unsupervised, i.e., there are no labeled
segmentations, handcrafted bounding boxes or whatsoever. In essence,
their model combines a Variational Auto-Encoder
([VAE](https://borea17.github.io/paper_summaries/auto-encoding_variational_bayes))
with a recurrent attention network
([U-Net](https://borea17.github.io/paper_summaries/u_net)
*segmentation network*) to spatially decompose
scenes into attention masks (over which the VAE needs to
reconstruct masked regions) and latent representations of each masked
region. As a proof of concept, they show that their model could learn
disentangled representations in a common latent code (i.e.,
representations of object features in latent space) and object
segmentations (i.e., attention masks on the original image) on
non-trivial 3D scenes. 

## Model Description
