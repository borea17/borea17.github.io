---
title: "Expectation Maximization"
permalink: "/ML_concepts/probabilistic_models/expectation_maximization"
author: "Markus Borea"
published: false
toc: true
toc_sticky: true
toc_label: "Table of Contents"
type: "concepts probability"
---

NOTE: THIS IS CURRENTLY WIP

- simple structure consisting of two steps
- no guarantee about global optimality, but at least local optimality


## EM in Theory


## Practice Example 



PAPERS TO SUMMARIZE:

- CURL: Contrastive Unsupervised Representations for RL

Problems:
- noisy TV
- bullet problem

Bengio: Microsoft research talk
"we should not be working on unsupervised learning from the
reconstruction side"
=> trying to encode noise

- causality in ML (schÃ¶lkopf)

- Attend, Infer, Repeat 

- dreamer (schmidhuber)
- contrastive learning of structured world models

- learning object-centric representations for high-level planning in minecraft

- CATEGORICAL REPARAMETERIZATION WITH GUMBEL-SOFTMAX
https://arxiv.org/pdf/2003.11830.pdf


Current Direction:
- Representation Learning
- Learning without Labels
- Unsupervised Pretraining may drastically speed up training time in
  RL


## Problem Statement

Representation Learning in combination with RL 
- what is a good representation 
- how can it be learned
- somehow representation is dependent on down-stream tasks
- how to generalize
- improve sample efficiency

Humans have some kind of object-centric representation 
- reconstruction based approaches suffer from 
   - additional complexity of balancing various auxiliary losses
    (reconstruction, regularization, masked reconstruction)
   - noisy tv problem 
        => encoding noise is useless
   - bullet problem
        => down-stream task is important for representation
   - how to learn without couple back to pixel-space?
   - how can we interpret such representation

What are key desiderate for object-centric representations?

- objects need have common format (e.g., across latent space)
  such that comparison between objects is posible
- objects should be self-contained

