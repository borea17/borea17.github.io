---
title: "What is the ELBO?"
permalink: "/ML_101/probability_theory/evidence_lower_bound"
author: "Markus Borea"
published: false
type: "101 probability"
---


Consider the following case:  
Let $\textbf{X} = \\{\textbf{x}^{(i)}\\}\_{i=1}^N$ denote a dataset
consisting of $N$ i.i.d. samples where each observed datapoint
$\textbf{x}^{(i)}$ is generated from a process in which firstly a
latent (hidden) variable $\textbf{z}^{(i)}$ is sampled from a prior
distribution $p\_{\boldsymbol{\theta}} (\textbf{z})$ and then
$\textbf{x}^{(i)}$ is sampled from a conditional distribution
$p\_{\boldsymbol{\theta}} \left(\textbf{x} | \textbf{z}^{(i)}\right)$.

The **evidence lower bound (ELBO)** $\mathcal{L}$ (or **variational lower
bound**) **defines a lower bound on the log-evidence** $\log
p\_{\boldsymbol{\theta}}(\textbf{x}^{(i)})$ **given an variational
approximation** $q\_{\boldsymbol{\phi}} \left(\textbf{z} |
\textbf{x}^{(i)} \right)$ **of the true posterior**
$p\_{\boldsymbol{\theta}} \left( \textbf{z} |
\textbf{x}^{(i)}\right)$, i.e., 

$$
  \log p_{\boldsymbol{\theta}} (\textbf{x}^{(i)}) = \underbrace{D_{KL} \left(
  q_{\boldsymbol{\phi}} \left( \textbf{z} | \textbf{x}^{(i)} \right)
  || 
  p_{\boldsymbol{\theta}} \left( \textbf{z} |
  \textbf{x}^{(i)}\right)\right)}_{\ge 0} + \mathcal{L} \left(
  \boldsymbol{\theta}, \boldsymbol{\phi}; \textbf{x}^{(i)}\right) 
$$

with 

$$
  \log p_{\boldsymbol{\theta}} \ge 
  \mathcal{L} \left( \boldsymbol{\theta}, \boldsymbol{\phi};
  \textbf{x}^{(i)} \right) = - D_{KL} \left( q_{\boldsymbol{\phi}}
  \left( \textbf{z} | \textbf{x}^{(i)} \right) ||
  p_{\boldsymbol{\theta}} (\textbf{z})\right) +
  \mathbb{E}_{q_{\boldsymbol{\phi}}
  \left(\textbf{z}|\textbf{x}^{(i)}\right)} \left[ \log
  p_{\boldsymbol{\theta}} \left( \textbf{x}^{(i)} | \textbf{z}\right) \right],
$$

where $p\_{\boldsymbol{\theta}} (\textbf{z})$ is a prior on the latent
distribution. 

Variational Bayesian methods are a very popular framework in machine 
learning, since they allow to cast statistical inference problems into
optimization problems. One crucial quantity for many derivations is the
on the variational


approximation :  

Consider the case of 


$$
  \log p_{\boldsymbol{\theta}} (\textbf{x}^{(i)}) = D_{KL} \left(
  q_{\boldsymbol{\phi}} \left( \textbf{z} | \textbf{x}^{(i)} \right)
  || 
  p_{\boldsymbol{\theta}} \left( \textbf{z} |
  \textbf{x}^{(i)}\right)\right) + \mathcal{L} \left(
  \boldsymbol{\theta}, \boldsymbol{\phi}; \textbf{x}^{(i)}\right) 
$$

**Derivation:**  

$$
\begin{align}
  D_{KL} \big[ Q || P \big] 
\end{align}
$$

$$
  \mathcal{L} \left( \boldsymbol{\theta}, \boldsymbol{\phi};
  \textbf{x}^{(i)} \right) = - D_{KL} \left( q_{\boldsymbol{\phi}}
  \left( \textbf{z} | \textbf{x}^{(i)} \right) ||
  p_{\boldsymbol{\theta}} (\textbf{z})\right) +
  \mathbb{E}_{q_{\boldsymbol{\phi}}
  \left(\textbf{z}|\textbf{x}^{(i)}\right)} \left[ \log
  p_{\boldsymbol{\theta}} \left( \textbf{x}^{(i)} | \textbf{z}\right) \right]
$$

## Resources

1. [Understanding the Variational Lower Bound by Xitong Yang](http://legacydirs.umiacs.umd.edu/~xyang35/files/understanding-variational-lower.pdf)
2. 
