{
  "hash": "942a346f718dc7716f7640bfb189c79a",
  "result": {
    "markdown": "---\ntitle: \"How does the attention mechanism ([Attention is all you need](https://arxiv.org/abs/1706.03762)) work?\"\ndraft: \n---\n\nIn essence, an **attention mechanism** can be intuitively understood as a means to assign individual\nimportance (or rather *attention*) to each entity in a collection of entities (e.g., words in a\nsentence or pixels in an image) using some cues as input. Mathmatically, this translates into\n**computing a weighted average over all entities in which the attention weights are obtained from\nthe attention cues**. \n\nMore abstractly, the attention mechanism can be used to answer the following questions\n\n* What entities (e.g., pixels or words) should we attend to or focus on?\n* What entities (e.g., pixels or words) are relevant for the task at hand?\n\n[Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762) call their particular attention mechanism\n**Scaled Dot-Product Attention**. Therein, the collection of entities is termed **values** and the\nattention cues are termed **queries** and **keys**. Attention to particular values (entities) is\nobtained by computing the weighted average over all values (entities) in which the **attention\nweights** are obtained by combining the attention cues.\n\nThe attention cues (queries and keys) are vectors of length $d_k$ defined per value and can\nbe seen as two different answers to the same question: *How much attention should we put to this\nentity?* The **alignment** between the attention cues is computed via the dot-product (hence the\nname), additionally the **alignment scores** are passed through a *Softmax*-layer to obtain\nnormalized **attention weights**. Finally, these attention weights are used to compute the weighted\naverage. \n\nTo speed things up, queries, keys and values are packed into matrices $\\textbf{Q}, \\textbf{K}\n\\in \\mathbb{R}^{N_v \\times d_k}$ and $\\textbf{V} \\in \\mathbb{R}^{N_v \\times d_v}$,\nrespectively. As a result, the concise formulation of Scaled Dot-Product Attention is given by \n\n$$\n\\text{Attention}(\\textbf{Q}, \\textbf{K}, \\textbf{V}) = \n\\underbrace{\\text{softmax} \n\t\\left(\n\t%\\overbrace{\n\t\\frac {\\textbf{Q} \\textbf{K}^{\\text{T}}} {\\sqrt{d_k}}\n\t%}^{\\text{attention alignment }  \\textbf{L} \\in }\n\t\\right)\n}_{\\text{attention weight }\\textbf{W} \\in \\mathbb{R}^{N_v \\times N_v}} \\textbf{V}\n$$\n\nin which $\\frac{1}{\\sqrt{d_k}}$ is an additional scalar which [Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762) \nadded to counter vanishing gradients (they hypothesize that for a higher cue dimension $d_k$ the \ndot-product might grow large in magnitude). \n\nThe figure highlights how the corresponding vectors are packed into matrices, respectively.\n\n| ![Matrix Packing for Scaled Dot-Product Attention](./img/matrix_packing.png \"Matrix Packing for Dot-Product Attention\") |\n| :--         |\n| **Matrix Packing for Scaled Dot-Product Attention** |\n\nKeeping in mind that attention can be seen as computing a weighted average (on each dimension) over all \nentities $\\textbf{V}$, it is obvious that the result has the same dimension as $\\textbf{V}$. Below is \na simple implementation of the attention mechanism. Note that in practice commonly $\\textbf{Q}$, $\\textbf{K}$ \nand $\\textbf{V}$ are learnable matrices.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"false\"}\nimport math\nimport torch \n\n\ndef attention(\n        query_matrix: torch.Tensor, \n        key_matrix: torch.Tensor, \n        value_matrix: torch.Tensor\n    ) -> torch.Tensor:\n    \"\"\"Simplistic implementation of scaled dot-product attention.\n\n    Args:\n        query_matrix (torch.Tensor): shape [batch_size, N_v, d_k]\n        key_matrix (torch.Tensor):   shape [batch_size, N_v, d_k]\n        value_matrix (torch.Tensor): shape [batch_size, N_v, d_v]\n\n    Returns\n        torch.Tensor:                shape [batch_size, N_v, d_v]\n    \"\"\"\n    scale_factor = 1 / math.sqrt(query_matrix.size(-1))\n    # compute unnormalized attention weights of shape [batch_size, N_v, N_v] \n    attn_weights = scale_factor * query_matrix @ key_matrix.transpose(-2, -1)\n    # normalize attention weights (i.e., sum over last dimension equal to one)\n    normalized_attn_weights = torch.softmax(attn_weights, dim=-1)\n    # compute result of shape [batch_size, N_v, d_v] \n    return normalized_attn_weights @ value_matrix\n```\n:::\n\n\n::: {.callout-tip collapse=true}\n## Key, Query, Value - Naming \n\nInstead of the weighted average formulation above, the attention mechanism could also be \nviewed as a **retrieval process** in which we have a storage of **key-value** pairs \n(e.g., [dictionaries](https://docs.python.org/3/tutorial/datastructures.html#dictionaries)) \nand a corresponding **query** input. \n\nIn this view, we are trying to retrieve the optimal value (representation) to the given query \nby computing the alignment between query and keys.\n:::\n\n\n::: {.callout-tip collapse=true}\n##  Self-Attention\n\nSelf-Attention simply means that the matrices $\\textbf{Q}$, $\\textbf{K}$ and $\\textbf{V}$ are derived \nfrom the same input embedding, see code below.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport torch.nn as nn\n\n\nclass SelfAttentionLayer(nn.Module):\n    def __init__(self, x_in: int, d_v: int, d_k: int) -> None:\n        super().__init__()\n\n        self.key_transformation = nn.Linear(x_in, d_k)\n        self.query_transformation = nn.Linear(x_in, d_k)\n        self.value_transformation = nn.Linear(x_in, d_v)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        key_matrix = self.key_transformation(x)\n        query_matrix = self.query_transformation(x)\n        value_matrix = self.value_transformation(x)\n\n        result = attention(\n            query_matrix=query_matrix, \n            value_matrix=value_matrix,\n            key_matrix=key_matrix,\n        )\n\n        return result\n```\n:::\n\n\n:::\n\n\n<!-- ::: {.callout-tip collapse=true} -->\n<!-- ##  Multihead-Attention -->\n<!---->\n<!-- ::: -->\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}