{
  "hash": "05f4860b65a59b7cffd54051a71be063",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"How does the attention mechanism ([Attention is all you need](https://arxiv.org/abs/1706.03762)) work?\"\ndraft: false\n---\n\nIn essence, an **attention mechanism** can be intuitively understood as a means to assign individual\nimportance (or rather *attention*) to each entity in a collection of entities (e.g., words in a\nsentence or pixels in an image) using some cues as input. Mathmatically, this translates into\n**computing a weighted average over all entities**. In the attention mechansim from the \n[Attention is all you need](https://arxiv.org/abs/1706.03762), the **attention weights are \nobtained from the attention cues**. \n\nMore abstractly, the attention mechanism can be used to answer the following questions\n\n* What entities (e.g., pixels or words) should we attend to or focus on?\n* What entities (e.g., pixels or words) are relevant for the task at hand?\n\n[Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762) call their particular attention mechanism\n**Scaled Dot-Product Attention**. Therein, the collection of entities is termed **values** and the\nattention cues are termed **queries** and **keys**. Attention to particular values (entities) is\nobtained by computing the weighted average over all values (entities) in which the **attention\nweights** are obtained by combining the attention cues.\n\nThe attention cues (queries and keys) are vectors of length $d_k$ defined per value and can\nbe seen as representations of questions (queries) and facts (keys): E.g., we could imagine a \nquery representing the question `Are you a noun?` and a corresponding key representing the facts \n`Noun, positive connotation, important, female.` The **alignment** between the attention cues\nis computed via the dot-product (hence the\nname), additionally the **alignment scores** are passed through a *Softmax*-layer to obtain\nnormalized **attention weights**. Finally, these attention weights are used to compute the weighted\naverage. \n\nTo speed things up, queries, keys and values are packed into matrices $\\textbf{Q}, \\textbf{K}\n\\in \\mathbb{R}^{N_v \\times d_k}$ and $\\textbf{V} \\in \\mathbb{R}^{N_v \\times d_v}$,\nrespectively. As a result, the concise formulation of Scaled Dot-Product Attention is given by \n\n$$\n\\text{Attention}(\\textbf{Q}, \\textbf{K}, \\textbf{V}) = \n\\underbrace{\\text{softmax} \n\t\\left(\n\t%\\overbrace{\n\t\\frac {\\textbf{Q} \\textbf{K}^{\\text{T}}} {\\sqrt{d_k}}\n\t%}^{\\text{attention alignment }  \\textbf{L} \\in }\n\t\\right)\n}_{\\text{attention weight }\\textbf{W} \\in \\mathbb{R}^{N_v \\times N_v}} \\textbf{V}\n= \\textbf{A}\n$$\n\nin which $\\frac{1}{\\sqrt{d_k}}$ is an additional scalar which [Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762) \nadded to counter vanishing gradients (they hypothesize that for a higher cue dimension $d_k$ the \ndot-product might grow large in magnitude). \n\nThe figure below highlights how the corresponding vectors are packed into matrices and which vectors \nare used to obtain the first attention vector $\\textbf{a}_1$.\n\n| ![Scaled Dot-Product Attention](./img/scaled_dot_attention.png \"Scaled Dot-Product Attention\") |\n| :--         |\n| **Scaled Dot-Product Attention: Matrix Packing and Computation Schematic** |\n\nThe result matrix $\\textbf{A}$ has the same dimensionality as $\\textbf{V}$ and in fact each entry $\\textbf{a}_i$ \nis basically a (normalized) linear combination of the vectors $\\{\\textbf{v}_j\\}_{j=1}^{N_v}$\n\n$$\n\\textbf{a}_i = \\sum_j w_{ij} (\\textbf{q}_i, \\textbf{k}_j) \\textbf{v}_j \\quad \\text{with} \\quad \\sum_j w_{ij} (\\textbf{q}_i, \\textbf{k}_j) = 1.\n$$\n\nIn this formulation, it is obvious that scaled-dot product attention means basically **computing a weighted\naverage over all entities**. Furthermore, each attention vector $\\textbf{a}_i$ has a fixed query\nvector $\\textbf{q}_i$ which explains the name **query**.\n\n::: {.callout-tip collapse=true}\n## Difference between Attention and Fully Connected Layer\n\nThe equation above looks surprisingly similar to a fully connected layer with no bias and \nsame dimensionality between input $\\textbf{v}_i \\in\\mathbb{R}^N$ and output $\\textbf{a}_i \\in \\mathbb{R}^N$. \nIn this case, we could describe the output of a fully connected layer as \n\n$$\n\\text{a}_i = \\sum_j w_{ij} \\text{v}_j.\n$$\n\nIf we would pass the weight matrix $\\textbf{W} \\in \\mathbb{R}^{N\\times N}$ through a softmax layer, \nwe could even achieve the following \n\n$$\n\\text{a}_i = \\sum_j w_{ij} \\text{v}_j \\quad \\text{with} \\quad \\sum_{j} w_{ij} = 1.\n$$\n\n*So what's the difference between an attention and a fully connected layer?*\n\nIn fact, the only difference is the value dimensionality $d_v$, i.e., \nin case of $d_v = 1$ there is no difference.\n\n\n| ![Linear Layer](./img/linear_layer.png \"Linear Layer\") | ![Attention Layer](./img/att_layer.png \"Attention Layer\")\n| :--         | :-- |\n| **Linear Layer**  | **Attention Layer** |\n\n:::\n\n\n::: {.callout-tip collapse=true} \n# Implementation \n\n::: {#ef2e5d6c .cell execution_count=2}\n``` {.python .cell-code code-fold=\"false\"}\nimport math \nimport torch  \n\n\ndef attention( \n       query_matrix: torch.Tensor,  \n       key_matrix: torch.Tensor,  \n       value_matrix: torch.Tensor \n   ) -> torch.Tensor: \n   \"\"\"Simplistic implementation of scaled dot-product attention. \n\n   Args: \n       query_matrix (torch.Tensor): shape [batch_size, N_v, d_k] \n       key_matrix (torch.Tensor):   shape [batch_size, N_v, d_k] \n       value_matrix (torch.Tensor): shape [batch_size, N_v, d_v] \n\n   Returns \n       torch.Tensor:                shape [batch_size, N_v, d_v] \n   \"\"\" \n   scale_factor = 1 / math.sqrt(query_matrix.size(-1)) \n   # compute unnormalized attention weights of shape [batch_size, N_v, N_v]  \n   attn_weights = scale_factor * query_matrix @ key_matrix.transpose(-2, -1) \n   # normalize attention weights (i.e., sum over last dimension equal to one) \n   normalized_attn_weights = torch.softmax(attn_weights, dim=-1) \n   # compute result of shape [batch_size, N_v, d_v]  \n   return normalized_attn_weights @ value_matrix \n```\n:::\n\n\n::: \n\n\n::: {.callout-tip collapse=true} \n## Causal / Masked Attention  \n\nIn fact, the attention mechanism defined above allows access to all future values in the sequence, \ne.g., the first entry $\\textbf{a}_1$ is a (normalized) linear combination of all vectors \n$\\{\\textbf{v}_j\\}_{j=1}^{N_v}$. This may be problematic when the values are generated on the fly \n(e.g., next-token prediction). \n\nTo address this, we can rewrite the attention mechanism as follows \n\n$$\n\\textbf{a}_i = \\sum_{j=1}^{i} w_{ij} (\\textbf{q}_i, \\textbf{k}_j) \\textbf{v}_j \\quad \n  \\text{with} \\quad \\sum_{j=1}^{i} w_{ij} (\\textbf{q}_i, \\textbf{k}_j) = 1.\n$$\n\nThis can be done by using the standard attention mechansim and **masking out** future values\n\n$$\nw_{ij} (\\textbf{q}_i, \\textbf{k}_j) = 0 \\quad \\text{for} \\quad j \\ge i.\n$$\n\nThe weights can be concisely written into a weight matrix $\\textbf{W} \\in \\mathbb{R}^{N_v \\times N_v}$ \nand the above equation basically sets the upper triangular part to zero.\n\nThus, it is fairly simple to make the attention mechanism causal, see the implementation below.\n\n::: {#a5512918 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"false\"}\nimport math \nimport torch  \n\ndef attention( \n       query_matrix: torch.Tensor,  \n       key_matrix: torch.Tensor,  \n       value_matrix: torch.Tensor,\n       is_causal: bool = False,\n   ) -> torch.Tensor: \n   \"\"\"Simplistic implementation of scaled dot-product attention allowing for causal masking. \n\n   Args: \n       query_matrix (torch.Tensor): shape [batch_size, N_v, d_k] \n       key_matrix (torch.Tensor):   shape [batch_size, N_v, d_k] \n       value_matrix (torch.Tensor): shape [batch_size, N_v, d_v] \n       is_causal (bool):            whether to mask out future values\n\n   Returns \n       torch.Tensor:                shape [batch_size, N_v, d_v] \n   \"\"\" \n   scale_factor = 1 / math.sqrt(query_matrix.size(-1)) \n   # compute unnormalized attention weights of shape [batch_size, N_v, N_v]  \n   attn_weights = scale_factor * query_matrix @ key_matrix.transpose(-2, -1) \n   # normalize attention weights (i.e., sum over last dimension equal to one) \n   normalized_attn_weights = torch.softmax(attn_weights, dim=-1) \n   if is_causal:\n      causal_mask = torch.ones_like(attn_weights).tril(diagonal=0)\n      normalized_attn_weights = causal_mask.mul(normalized_attn_weights)\n   # compute result of shape [batch_size, N_v, d_v]  \n   return normalized_attn_weights @ value_matrix \n```\n:::\n\n\n::: \n\n::: {.callout-tip collapse=true}  \n## Multi-Head Attention\n\nIn the transformer architecture introduced by [Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762),\nthey use **multi-head attention layers** which are essentially adapted \nparallelized scaled-dot product attention layers defined as follows \n\n$$\n\\begin{align*}\n&\\text{Multi-Head}(\\textbf{Q}, \\textbf{K}, \\textbf{V}) = \\text{Concat} (\\text{head}_1\\, \\dots, \\text{head}_h) \\textbf{W}^O \\\\\n&\\quad \\text{with} \\quad \\text{head}_i = \\text{Attention}(\\textbf{Q} \\textbf{W}_i^Q, \\textbf{K} \\textbf{W}_i^K, \\textbf{V} \\textbf{W}_i^V),\n\\end{align*}\n$$\n\nwhere linear projections parametrized by $\\textbf{W}_i^Q \\in \\mathbb{R}^{ d_{\\text{model}} \\times d_k}, \n\\textbf{W}_i^K \\in \\mathbb{R}^{ d_{\\text{model}} \\times d_k}, \\textbf{W}_i^V \\in \\mathbb{R}^{ d_{\\text{model}} \\times d_v}$ and $\\textbf{W}^O \\in \\mathbb{R}^{ d_v \\times hd_{\\text{model}}}$ are learnable parameters (i.e., linear layers without bias).\n\nNote that without $\\textbf{W}^O$ and adapted linear projections using \nthe following dimensions $\\textbf{W}_i^Q \\in \\mathbb{R}^{ d_{k} \\times d_k}, \n\\textbf{W}_i^K \\in \\mathbb{R}^{ d_{k} \\times d_k}, \\textbf{W}_i^V \\in \\mathbb{R}^{ d_{v} \\times d_v}$, multi-head \nattention would just be parallelized scaled dot-product attention. The projections are simply used to create \ndifferent representations of the query, key and value matrices (otherwise each head would look exactly the same, e.g., assume \nthat each $\\textbf{W}_i^{Q / K / V}$ is an identity matrix).\n\nUsing $d_{\\text{model}}$ instead of $d_k / d_v$ for the linear projections is an architectural decision \nto reduce the number of trainable paramters whithout losing too much expressiveness. The concatenation plus linear \nlayer increases the expressiveness of the model, since it allows the model to jointly combine attention representations of \ndifferent subspaces.\n\n| ![Multi-Head Attention](./img/multi_head_att.png \"Multi-Head Attention\") |\n| :--         |\n| **Multi-Head Attention Layer**: Linearly project values, queries and keys $h$-times into model dimension, apply scaled-dot-product attention, concatenate result (attention in different subspaces) and linearly project once again (simply increase the expressiveness). Image taken from [Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762). |\n\nBelow is a simplistic implementation of multi-head attention using the masked attention implementation \nabove.\n\n::: {#48bd0a93 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"false\"}\nimport math\nimport torch\n\n\nclass MultiheadAttention(torch.nn.Module):\n    def __init__(self, d_model: int, h: int, is_causal: bool, d_k: int, d_v: int) -> None:\n        super().__init__()\n        self.is_causal = is_causal\n        self.h = h\n        # linear projections for Q, K and V (h times)\n        self.W_Q_i = [torch.nn.Linear(d_k, d_model, bias=False) for _ in range(h)]\n        self.W_K_i = [torch.nn.Linear(d_k, d_model, bias=False) for _ in range(h)]\n        self.W_V_i = [torch.nn.Linear(d_v, d_model, bias=False) for _ in range(h)]\n        # linear projection for concatenated result\n        self.W_o = torch.nn.Linear(d_model*h, d_v, bias=False)\n\n\n    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.tensor) -> torch.tensor:\n        \"\"\"Forward method for MultiHeadAttention\n\n        Args: \n           Q (torch.Tensor): shape [batch_size, N_v, d_k] \n           K (torch.Tensor): shape [batch_size, N_v, d_k] \n           V (torch.Tensor): shape [batch_size, N_v, d_v] \n\n        Returns \n           torch.Tensor:     shape [batch_size, N_v, d_v] \n        \"\"\"\n        # compute attention heads [head_1, ..., head_h]\n        # head_i has shape: [batch_size, N_v, d_model]\n        att_heads = [attention(query_matrix=self.W_Q_i[i](Q), \n                               key_matrix=self.W_K_i[i](K), \n                               value_matrix=self.W_V_i[i](V), \n                               is_causal=self.is_causal) \n                     for i in range(self.h)]\n        # concatenate result shape: [batch_size, N_v, d_model*h\n        concat_heads = torch.concatenate(att_heads, dim=-1)\n        # feed through last linear layer\n        return self.W_o(concat_heads)\n```\n:::\n\n\n::: \n\n<!-- ::: {.callout-tip collapse=true}  -->\n<!-- ##  Self-Attention  -->\n<!---->\n<!-- Self-Attention simply means that the matrices $\\textbf{Q}$, $\\textbf{K}$ and $\\textbf{V}$ are derived   -->\n<!-- from the same input embedding, see code below.  -->\n<!---->\n<!-- ```{python}  -->\n<!---->\n<!-- import torch.nn as nn  -->\n<!---->\n<!---->\n<!-- class SelfAttentionLayer(nn.Module):  -->\n<!--    def __init__(self, x_in: int, d_v: int, d_k: int) -> None:  -->\n<!--        super().__init__()  -->\n<!---->\n<!--        self.key_transformation = nn.Linear(x_in, d_k)  -->\n<!--        self.query_transformation = nn.Linear(x_in, d_k)  -->\n<!--        self.value_transformation = nn.Linear(x_in, d_v)  -->\n<!---->\n<!--    def forward(self, x: torch.Tensor) -> torch.Tensor:  -->\n<!--        key_matrix = self.key_transformation(x)  -->\n<!--        query_matrix = self.query_transformation(x)  -->\n<!--        value_matrix = self.value_transformation(x)  -->\n<!---->\n<!--        result = attention(  -->\n<!--            query_matrix=query_matrix,   -->\n<!--            value_matrix=value_matrix,  -->\n<!--            key_matrix=key_matrix,  -->\n<!--        )  -->\n<!---->\n<!--        return result  -->\n<!-- ```  -->\n<!---->\n<!-- :::  -->\n<!---->\n<!---->\n<!-- ::: {.callout-tip collapse=true}  -->\n<!-- ##  Multihead-Attention  -->\n<!---->\n<!-- :::  -->\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}