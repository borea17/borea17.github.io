{
  "hash": "da0dea204a50b88847b6e35211306fd1",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"How does the attention mechanism ([Attention is all you need](https://arxiv.org/abs/1706.03762)) work?\"\ndraft: false\n---\n\nIn essence, an **attention mechanism** can be intuitively understood as a means to assign individual\nimportance (or rather *attention*) to each entity in a collection of entities (e.g., words in a\nsentence or pixels in an image) using some cues as input. Mathmatically, this translates into\n**computing a weighted average over all entities**. In the attention mechansim from the \n[Attention is all you need](https://arxiv.org/abs/1706.03762), the **attention weights are \nobtained from the attention cues**. \n\nMore abstractly, the attention mechanism can be used to answer the following questions\n\n* What entities (e.g., pixels or words) should we attend to or focus on?\n* What entities (e.g., pixels or words) are relevant for the task at hand?\n\n[Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762) call their particular attention mechanism\n**Scaled Dot-Product Attention**. Therein, the collection of entities is termed **values** and the\nattention cues are termed **queries** and **keys**. Attention to particular values (entities) is\nobtained by computing the weighted average over all values (entities) in which the **attention\nweights** are obtained by combining the attention cues.\n\nThe attention cues (queries and keys) are vectors of length $d_k$ defined per value and can\nbe seen as representations of questions (queries) and facts (keys): E.g., we could imagine a \nquery representing the question `Are you a noun?` and a corresponding key representing the facts \n`Noun, positive connotation, important, female.` The **alignment** between the attention cues\nis computed via the dot-product (hence the\nname), additionally the **alignment scores** are passed through a *Softmax*-layer to obtain\nnormalized **attention weights**. Finally, these attention weights are used to compute the weighted\naverage. \n\nTo speed things up, queries, keys and values are packed into matrices $\\textbf{Q}, \\textbf{K}\n\\in \\mathbb{R}^{N_v \\times d_k}$ and $\\textbf{V} \\in \\mathbb{R}^{N_v \\times d_v}$,\nrespectively. As a result, the concise formulation of Scaled Dot-Product Attention is given by \n\n$$\n\\text{Attention}(\\textbf{Q}, \\textbf{K}, \\textbf{V}) = \n\\underbrace{\\text{softmax} \n\t\\left(\n\t%\\overbrace{\n\t\\frac {\\textbf{Q} \\textbf{K}^{\\text{T}}} {\\sqrt{d_k}}\n\t%}^{\\text{attention alignment }  \\textbf{L} \\in }\n\t\\right)\n}_{\\text{attention weight }\\textbf{W} \\in \\mathbb{R}^{N_v \\times N_v}} \\textbf{V}\n= \\textbf{A}\n$$\n\nin which $\\frac{1}{\\sqrt{d_k}}$ is an additional scalar which [Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762) \nadded to counter vanishing gradients (they hypothesize that for a higher cue dimension $d_k$ the \ndot-product might grow large in magnitude). \n\nThe figure below highlights how the corresponding vectors are packed into matrices and which vectors \nare used to obtain the first attention vector $\\textbf{a}_1$.\n\n| ![Scaled Dot-Product Attention](./img/scaled_dot_attention.png \"Scaled Dot-Product Attention\") |\n| :--         |\n| **Scaled Dot-Product Attention: Matrix Packing and Computation Schematic** |\n\nThe result matrix $\\textbf{A}$ has the same dimensionality as $\\textbf{V}$ and in fact each entry $\\textbf{a}_i$ \nis basically a (normalized) linear combination of the vectors $\\{\\textbf{v}_j\\}_{j=1}^{N_v}$\n\n$$\n\\textbf{a}_i = \\sum_j w_{ij} (\\textbf{q}_i, \\textbf{k}_j) \\textbf{v}_j \\quad \\text{with} \\quad \\sum_j w_{ij} (\\textbf{q}_i, \\textbf{k}_j) = 1.\n$$\n\nIn this formulation, it is obvious that scaled-dot product attention means basically **computing a weighted\naverage over all entities**. Furthermore, each attention vector $\\textbf{a}_i$ has a fixed query\nvector $\\textbf{q}_i$ which explains the name **query**.\n\n::: {.callout-tip collapse=true}\n## Difference between Attention and Fully Connected Layer\n\nThe equation above looks surprisingly similar to a fully connected layer with no bias and \nsame dimensionality between input $\\textbf{v}_i \\in\\mathbb{R}^N$ and output $\\textbf{a}_i \\in \\mathbb{R}^N$. \nIn this case, we could describe the output of a fully connected layer as \n\n$$\n\\text{a}_i = \\sum_j w_{ij} \\text{v}_j.\n$$\n\nIf we would pass the weight matrix $\\textbf{W} \\in \\mathbb{R}^{N\\times N}$ through a softmax layer, \nwe could even achieve the following \n\n$$\n\\text{a}_i = \\sum_j w_{ij} \\text{v}_j \\quad \\text{with} \\quad \\sum_{j} w_{ij} = 1.\n$$\n\n*So what's the difference between an attention and a fully connected layer?*\n\nIn fact, the only difference is the value dimensionality $d_v$, i.e., \nin case of $d_v = 1$ there is no difference.\n\n\n| ![Linear Layer](./img/linear_layer.png \"Linear Layer\") | ![Attention Layer](./img/att_layer.png \"Attention Layer\")\n| :--         | :-- |\n| **Linear Layer**  | **Attention Layer** |\n\n:::\n\n\n::: {.callout-tip collapse=true} \n# Implementation \n\n::: {#81659244 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"false\"}\nimport math \nimport torch  \n\n\ndef attention( \n       query_matrix: torch.Tensor,  \n       key_matrix: torch.Tensor,  \n       value_matrix: torch.Tensor \n   ) -> torch.Tensor: \n   \"\"\"Simplistic implementation of scaled dot-product attention. \n\n   Args: \n       query_matrix (torch.Tensor): shape [batch_size, N_v, d_k] \n       key_matrix (torch.Tensor):   shape [batch_size, N_v, d_k] \n       value_matrix (torch.Tensor): shape [batch_size, N_v, d_v] \n\n   Returns \n       torch.Tensor:                shape [batch_size, N_v, d_v] \n   \"\"\" \n   scale_factor = 1 / math.sqrt(query_matrix.size(-1)) \n   # compute unnormalized attention weights of shape [batch_size, N_v, N_v]  \n   attn_weights = scale_factor * query_matrix @ key_matrix.transpose(-2, -1) \n   # normalize attention weights (i.e., sum over last dimension equal to one) \n   normalized_attn_weights = torch.softmax(attn_weights, dim=-1) \n   # compute result of shape [batch_size, N_v, d_v]  \n   return normalized_attn_weights @ value_matrix \n```\n:::\n\n\n::: \n\n\n::: {.callout-tip collapse=true} \n## Causal / Masked Attention  \n\nIn fact, the attention mechanism defined above allows access to all future values in the sequence, \ne.g., the first entry $\\textbf{a}_1$ is a (normalized) linear combination of all vectors \n$\\{\\textbf{v}_j\\}_{j=1}^{N_v}$. This may be problematic when the values are generated on the fly \n(e.g., next-token prediction). \n\nTo address this, we can rewrite the attention mechanism as follows \n\n$$\n\\textbf{a}_i = \\sum_{j=1}^{i} w_{ij} (\\textbf{q}_i, \\textbf{k}_j) \\textbf{v}_j \\quad \n  \\text{with} \\quad \\sum_{j=1}^{i} w_{ij} (\\textbf{q}_i, \\textbf{k}_j) = 1.\n$$\n\nThis can be done by using the standard attention mechansim and **masking out** future values\n\n$$\nw_{ij} (\\textbf{q}_i, \\textbf{k}_j) = 0 \\quad \\text{for} \\quad j \\ge i.\n$$\n\nThe weights can be concisely written into a weight matrix $\\textbf{W} \\in \\mathbb{R}^{N_v \\times N_v}$ \nand the above equation basically sets the upper triangular part to zero.\n\nThus, it is fairly simple to make the attention mechanism causal, see the implementation below.\n\n::: {#3083eda3 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"false\"}\nimport math \nimport torch  \n\ndef attention( \n       query_matrix: torch.Tensor,  \n       key_matrix: torch.Tensor,  \n       value_matrix: torch.Tensor,\n       is_causal: bool = False,\n   ) -> torch.Tensor: \n   \"\"\"Simplistic implementation of scaled dot-product attention allowing for causal masking. \n\n   Args: \n       query_matrix (torch.Tensor): shape [batch_size, N_v, d_k] \n       key_matrix (torch.Tensor):   shape [batch_size, N_v, d_k] \n       value_matrix (torch.Tensor): shape [batch_size, N_v, d_v] \n       is_causal (bool):            whether to mask out future values\n\n   Returns \n       torch.Tensor:                shape [batch_size, N_v, d_v] \n   \"\"\" \n   scale_factor = 1 / math.sqrt(query_matrix.size(-1)) \n   # compute unnormalized attention weights of shape [batch_size, N_v, N_v]  \n   attn_weights = scale_factor * query_matrix @ key_matrix.transpose(-2, -1) \n   # normalize attention weights (i.e., sum over last dimension equal to one) \n   normalized_attn_weights = torch.softmax(attn_weights, dim=-1) \n   if is_causal:\n      causal_mask = torch.ones_like(attn_weights).tril(diagonal=0)\n      normalized_attn_weights = causal_mask.mul(normalized_attn_weights)\n   # compute result of shape [batch_size, N_v, d_v]  \n   return normalized_attn_weights @ value_matrix \n```\n:::\n\n\n::: \n\n::: {.callout-tip collapse=true} \n##  Self-Attention \n\nSelf-Attention simply means that the matrices $\\textbf{Q}$, $\\textbf{K}$ and $\\textbf{V}$ are derived  \nfrom the same input embedding, see code below. \n\n::: {#e8361bf6 .cell execution_count=4}\n``` {.python .cell-code}\nimport torch.nn as nn \n\n\nclass SelfAttentionLayer(nn.Module): \n   def __init__(self, x_in: int, d_v: int, d_k: int) -> None: \n       super().__init__() \n\n       self.key_transformation = nn.Linear(x_in, d_k) \n       self.query_transformation = nn.Linear(x_in, d_k) \n       self.value_transformation = nn.Linear(x_in, d_v) \n\n   def forward(self, x: torch.Tensor) -> torch.Tensor: \n       key_matrix = self.key_transformation(x) \n       query_matrix = self.query_transformation(x) \n       value_matrix = self.value_transformation(x) \n\n       result = attention( \n           query_matrix=query_matrix,  \n           value_matrix=value_matrix, \n           key_matrix=key_matrix, \n       ) \n\n       return result \n```\n:::\n\n\n::: \n\n\n::: {.callout-tip collapse=true} \n##  Multihead-Attention \n\n::: \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}