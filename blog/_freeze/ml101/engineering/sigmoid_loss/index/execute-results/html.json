{
  "hash": "c40ddfaf7078ac3502827e568131de32",
  "result": {
    "markdown": "---\ntitle: \"Why should a MSE loss be avoided after a sigmoid layer?\"\n---\n\nMSE loss after a sigmoid layer leads to the **vanishing gradients problem in cases\nwhere the outputs of the sigmoid layer are close to** $0$ **or** $1$ irrespective of\nthe true probability/label. I.e., in the extreme case where the network\noutput is something close to zero while the true label is 1, gradients w.r.t.\nnetwork parameters are close to zero.\n\n<!-- DL-book by Bengio, 'You must have some log form loss to cancel the exponential part when your output is sigmoid' -->\n\n\n::: {.callout-tip collapse=true}\n## Derivation\n\nLet's denote the output of the network by $\\widetilde{\\textbf{p}} \\in [0, 1]^{N}$ (for estimated\nprobabilities) and the input to the sigmoid $\\textbf{x}\\in \\mathbb{R}^{N}$ (i.e., layer output\nbefore sigmoid is applied)\n\n$$\n\\textbf{p} = \\text{Sigmoid} (\\textbf{x}) = \\frac {1}{1 + \\exp \\left({-\\textbf{x}}\\right)}\n$$\n\nNow suppose that $\\textbf{p}\\in [0, 1]^{N}$ denotes the true probabilities, then applying the\nMSE loss gives\n\n$$\n\\text{MSE loss} \\left(\\widetilde{\\textbf{p}}, \\textbf{p}\\right)\n=J\\left(\\widetilde{\\textbf{p}}, \\textbf{p}\\right) = \\sum_{i=1}^N\n\\left(\\widetilde{p}_i - p_i \\right)^2,\n$$\n\nThe gradient w.r.t. network parameters will be proportional to the gradient w.r.t.\n$\\textbf{x}$ (backpropagation rule) which is as follows\n\n$$\n\\frac {\\partial J \\left(\\widetilde{\\textbf{p}}, \\textbf{p}\\right)}{\\partial\nx_i} = \\sum_{j=1}^N \\frac {\\partial J}{\\partial p_j} \\cdot \\frac {\\partial p_j}{\\partial x_j} = 2\n\\left(\\widetilde{p}_i - p_i \\right) \\cdot \\frac {\\partial p_i}{\\partial x_i} = 2\n\\left(\\widetilde{p}_i - p_i \\right) \\cdot p_i (1 - p_i)\n$$\n\nHere we can directly see that even if the absolute error approaches 1, i.e.,\n$\\left(\\widetilde{p}_i - p_i \\right)\\rightarrow 1$, the gradient vanishes for\n$p_i\\rightarrow 1$ and $p_i\\rightarrow 0$.\n\nLet's derive the gradient of the sigmoid using substitution and the chain rule\n\n$$\n\\begin{align}\n\\frac {\\partial p_i} {\\partial x_i} &= \\frac {\\partial u}{\\partial t} \\cdot \\frac\n{\\partial t}{\\partial x} \\quad \\text{with} \\quad u(t) = t^{-1}, \\quad t(x_i) = 1 +\n\\exp{(-x_i)}\\\\\n&= -t^{-2} \\cdot \\left(-\\exp{x_i}\\right) = \\frac {exp\\left(-x_i\\right)}{\\left(1 +\n\\exp\\left(-x_i\\right)\\right)^2}\\\\\n&= \\underbrace{\\frac {1}{1 + \\exp \\left( -x \\right)}}_{p_i} \\underbrace{\\frac {\\exp\n(-x_i)}{1+exp(-x_i)}}_{1-p_i} = p_i \\left(1 - p_i \\right)\n\\end{align}\n$$\n:::\n\n\n::: {.callout-tip collapse=true}\n## Visualization\n\nLet's write a nice visualization using pytorch's autograd.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\"}\nimport torch\nfrom torch import nn\nimport matplotlib.pyplot as plt\n\n\ndef make_visualization():\n    p0, p1 = 0, 1  # true probability\n\n    points = torch.arange(-10, 10, step=0.01)\n    # create variables to track gradients\n    grad_points = nn.Parameter(points, requires_grad=True)\n    p_tilde_0 = nn.Parameter(torch.sigmoid(points), requires_grad=True)\n    p_tilde_1 = nn.Parameter(torch.sigmoid(points), requires_grad=True)\n    grad_points_p0_x = nn.Parameter(points, requires_grad=True)\n    grad_points_p1_x = nn.Parameter(points, requires_grad=True)\n\n    # computations over which gradients are calculated\n    out_sigmoid = torch.sigmoid(grad_points)\n    MSE_loss_p0_p = (p_tilde_0 - p0)**2\n    MSE_loss_p1_p = (p_tilde_1 - p1)**2\n    MSE_loss_p0_x = (torch.sigmoid(grad_points_p0_x) - p0)**2\n    MSE_loss_p1_x = (torch.sigmoid(grad_points_p1_x) - p1)**2\n    # calculate gradients\n    out_sigmoid.sum().backward()\n    MSE_loss_p0_p.sum().backward()\n    MSE_loss_p1_p.sum().backward()\n    MSE_loss_p0_x.sum().backward()\n    MSE_loss_p1_x.sum().backward()\n\n    fig = plt.figure(figsize=(9,8))\n    # sigmoid\n    plt.subplot(3, 3, 1)\n    plt.plot(grad_points.detach().numpy(), out_sigmoid.detach().numpy())\n    plt.title(r'$\\widetilde{p}(x)$ = Sigmoid $(x)$')\n    # MSE loss w.r.t. \\widetilde{p}\n    plt.subplot(3, 3, 2)\n    plt.plot(p_tilde_0.detach().numpy(), MSE_loss_p0_p.detach().numpy(),\n             color='blue', label=r'$p=$' + f'{p0}')\n    plt.plot(p_tilde_1.detach().numpy(), MSE_loss_p1_p.detach().numpy(),\n             color='red', label=r'$p=$' + f'{p1}')\n    plt.legend()\n    plt.title(r'MSE Loss $(\\widetilde{p}, p) = (\\widetilde{p} - p)^2$')\n    plt.subplots_adjust(bottom=-0.2)\n    # MSE loss w.r.t. x\n    plt.subplot(3, 3, 3)\n    plt.plot(grad_points_p0_x.detach().numpy(), MSE_loss_p0_x.detach().numpy(),\n             color='blue', label=r'$p=$' + f'{p0}')\n    plt.plot(grad_points_p1_x.detach().numpy(), MSE_loss_p1_x.detach().numpy(),\n             color='red', label=r'$p=$' + f'{p1}')\n    plt.legend()\n    plt.title(r'MSE Loss $(x, p) = (\\widetilde{p}(x) - p)^2$')\n    # derivative of sigmoid\n    plt.subplot(3, 3, 4)\n    plt.plot(grad_points.detach().numpy(), grad_points.grad)\n    plt.title(r'Derivative Sigmoid w.r.t. x')\n    plt.xlabel('x')\n    # derivative of MSE loss w.r.t. \\widetilde{p}\n    plt.subplot(3, 3, 5)\n    plt.plot(p_tilde_0.detach().numpy(), p_tilde_0.grad, color='blue',\n             label=r'$p=$' + f'{p0}')\n    plt.plot(p_tilde_1.detach().numpy(), p_tilde_1.grad, color='red',\n             label=r'$p=$' + f'{p1}')\n    plt.xlabel(r'$\\widetilde{p}$')\n    plt.title(r'Derivative MSE loss w.r.t. $\\widetilde{p}$')\n    plt.legend()\n    # derivative of MSE Loss w.r.t x\n    plt.subplot(3, 3, 6)\n    plt.plot(grad_points_p0_x.detach().numpy(), grad_points_p0_x.grad,\n             color='blue', label=r'$p=$' + f'{p0}')\n    plt.plot(grad_points_p1_x.detach().numpy(), grad_points_p1_x.grad,\n             color='red', label=r'$p=$' + f'{p1}')\n    plt.xlabel(r'$x$')\n    plt.title(r'Derivative MSE loss w.r.t. x')\n    plt.legend()\n    return\n\nmake_visualization()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=740 height=623}\n:::\n:::\n\n\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}