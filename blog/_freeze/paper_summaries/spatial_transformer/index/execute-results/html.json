{
  "hash": "c88bda498fbe465f869faf8a0f5e4f02",
  "result": {
    "markdown": "---\ntitle: \"Spatial Transformer Networks\"\ncategories: [\"reimplementation\"]\ndate: \"2020-08-30\"\nexecute:\n  eval: false # true\nengine: jupyter\nformat:\n  html: \n    code-fold: show \n    highlight-style: github \n    code-block-bg: true\n    code-tools: \n      toggle: true\n      source: \"https://github.com/borea17/Notebooks/blob/master/04_Spatial_Transformer_Networks.ipynb\"\n---\n\n<!-- nextjournal_link: \"https://nextjournal.com/borea17/spatial_transformer_networks/\" -->\n\n[Jaderberg et al. (2015)](https://arxiv.org/abs/1506.02025) introduced\nthe learnable **Spatial Transformer (ST)** module that can be used to\nempower standard neural networks to actively spatially\ntransform feature maps or input data. In essence, the ST can\nbe understood as a black box that applies some spatial transformation\n(e.g., crop, scale, rotate) to a given input (or part of it)\nconditioned on the particular input during a single forward path. In\ngeneral, STs can also be seen as a learnable attention mechanism\n(including spatial transformation on the region of interest). Notably,\nSTs can be easily integrated in existing neural network architectures\nwithout any supervision or modification to the optimization, i.e., STs\nare differentiable plug-in modules. The authors could show that STs\nhelp the models to learn invariances to translation, scale, rotation\nand more generic warping which resulted in state-of-the-art\nperformance on several benchmarks, see image below.\n\n| ![Spatial Transformer in Practice](./img/ST_inpractice.gif \"Spatial Transformer in Practice\") |\n| :--  |\n|  **ST Example**: Results (after training) of using a ST as the first layer of a fully-connected network (`ST-FCN Affine`, left) or a convolutional neural network (`ST-CNN Affine`, right) trained for cluttered MNIST digit recognition are shown. Clearly, the output of the ST exhibits much less translation variance and attends to the digit. Taken from [Jaderberg et al. (2015)](https://arxiv.org/abs/1506.02025) linked [video](https://goo.gl/qdEhUu).|\n\n## Model Description\n\nThe aim of STs is to provide neural networks with spatial\ntransformation and attention capabilities in a reasonable and\nefficient way. Note that standard neural network architectures (e.g.,\nCNNs) are limited in this regard[^1]. Therefore, the ST constitutes\nparametrized transformations $\\mathcal{T}_{\\boldsymbol{\\theta}}$ that\ntransform the regular input grid to a new sampling grid, see image\nbelow. Then, some form of interpolation is used to compute the pixel\nvalues in the new sampling grid (i.e., interpolation between values of\nthe old grid).\n\n| ![Parametrized Sampling Grids](./img/parametrised_sampling_grid.png \"Parametrized Sampling Grids\") |\n| :--  |\n| Two examples of applying the parametrised sampling grid to an image $\\textbf{U}$ producing the output $\\textbf{V}$. The green dots represent the new sampling grid which is obtained by transforming the regular grid $\\textbf{G}$ (defined on $\\textbf{V}$) using the transformation $\\mathcal{T}$. <br> (a) The sampling grid is the regular grid $\\textbf{G} = \\mathcal{T}_{\\textbf{I}} (\\textbf{G})$, where $\\textbf{I}$ is the identity transformation matrix. <br> (b) The sampling grid is the result of warping the regular grid with an affine transformation $\\mathcal{T}_{\\boldsymbol{\\theta}} (\\textbf{G})$. <br> Taken from [Jaderberg et al. (2015)](https://arxiv.org/abs/1506.02025). |\n\nTo this end, the ST is divided into three consecutive parts:\n\n* **Localisation Network**: Its purpose is to retrieve the parameters\n  $\\boldsymbol{\\theta}$ of the spatial transformation\n  $\\mathcal{T}_{\\boldsymbol{\\theta}}$ taking the current feature map\n  $\\textbf{U}$ as input, i.e., $\\boldsymbol{\\theta} = f_{\\text{loc}}\n  \\left(\\textbf{U} \\right)$. Thereby, the spatial transformation is\n  conditioned on the input. Note that dimensionality of\n  $\\boldsymbol{\\theta}$ depends on the transformation type which needs\n  to be defined beforehand, see some examples below. Furthermore, the\n  localisation network can take any differentiable form, e.g., a CNN\n  or FCN.\n\n  <ins>*Examples of Spatial Transformations*</ins>\n  The following examples highlight how a regular grid\n\n  $$\n  \\textbf{G} = \\left\\{ \\begin{bmatrix} x_i^t \\\\ y_i^t \\end{bmatrix}\n  \\right\\}_{i=1}^{H^t \\cdot W^t}\n  $$\n\n  defined on the output/target map $\\textbf{V}$ (i.e., $H^t$ and $W^t$ denote\n  height and width of $\\textbf{V}$) can be transformed into a new sampling grid\n\n  $$\n  \\widetilde{\\textbf{G}} = \\left\\{ \\begin{bmatrix} x_i^s \\\\ y_i^s \\end{bmatrix}\n  \\right\\}_{i=1}^{H^s \\cdot W^s}\n  $$\n\n  defined on the input/source feature map $\\textbf{U}$ using a parametrized\n  transformation $\\mathcal{T}_{\\boldsymbol{\\theta}}$, i.e., $\\widetilde{G} =\n  T_{\\boldsymbol{\\theta}} (G)$. Visualizations have bee created by\n  me, interactive versions can be found [here](https://github.com/borea17/InteractiveTransformations).\n\n  <!-- * <ins>Affine Transformations</ins> -->\n\n    | ![Affine Transform](./img/affine_transform.gif) |\n    | :--         |\n    | This transformation allows cropping, translation, rotation, scale and skew to be applied to the input feature map. It has 6 degrees of freedom (DoF). |\n\n\n  <!-- * <ins>(Standard) Attention</ins> -->\n\n    | ![Attention Transform](./img/attention_transform.gif) |\n    | :--  |\n    |  This transformation is more constrained with only 3-DoF. Therefore it only allows cropping, translation and isotropic scaling to be applied to the input feature map.|\n\n\n    | ![Projective Transform](./img/projective_transform.gif) |\n    | :--  |\n    |  This transformation has 8-DoF and can be seen as an extension to the affine transformation. The main difference is that affine transformations are constrained to preserve parallelism.  |\n\n  <!-- * <ins>Thin Plate Spline (TPS) Transformations</ins> -->\n\n* **Grid Generator**: Its purpose to create the new sampling grid\n  $\\widetilde{\\textbf{G}}$ on the input feature map $\\textbf{U}$ by applying\n  the predefined parametrized transformation using the parameters\n  $\\boldsymbol{\\theta}$ obtained from the localisation network, see\n  examples above.\n  <!-- Note that [Jaderberg et al. -->\n  <!-- (2015)](https://arxiv.org/abs/1506.02025) define normalized -->\n  <!-- coordinates for the target feature map, i.e., $-1 \\le x_i^t, y_i^t \\le 1$.  -->\n\n* **Sampler**: Its purpose is to compute the warped version of the\n  input feature map $\\textbf{U}$ by computing the pixel values in the\n  new sampling grid $\\widetilde{\\textbf{G}}$ obtained from the grid\n  generator. Note that the new sampling grid does not necessarily\n  align with the input feature map grid, therefore some kind of\n  interpolation is needed. [Jaderberg et al.\n  (2015)](https://arxiv.org/abs/1506.02025) formulate this\n  interpolation as the application of a sampling kernel centered at a\n  particular location in the input feature map, i.e.,\n\n  $$\n    V_i^c = \\sum_{n=1}^{H^s} \\sum_{m=1}^{W^s} U_{n,m}^c \\cdot \\underbrace{k(x_i^s - x_m^t;\n    \\boldsymbol{\\Phi}_x)}_{k_{\\boldsymbol{\\Phi}_x}} \\cdot \\underbrace{k(y_i^s - y_n^t; \\boldsymbol{\\Phi}_y)}_{k_{\\boldsymbol{\\Phi}_y}},\n  $$\n\n  where $V_i^c \\in \\mathbb{R}^{W^t \\times H^t}$ denotes the new pixel\n  value of the $c$-th channel at the $i$-th position of the new\n  sampling grid coordinates[^2] $\\begin{bmatrix} x_i^s &\n  y_i^s\\end{bmatrix}^{T}$ and $\\boldsymbol{\\Phi}_x,\n  \\boldsymbol{\\Phi}_y$ are the parameters of a generic sampling kernel\n  $k()$ which defines the image interpolation. As the sampling grid\n  coordinates are not channel-dependent, each channel is transformed\n  in the same way resulting in spatial consistency between channels.\n  Note that although in theory we need to sum over all input\n  locations, in practice we can ignore this sum by just looking at the\n  kernel support region for each $V_i^c$ (similar to CNNs).\n\n  The sampling kernel can be chosen freely as long as (sub-)gradients\n  can be defined with respect to $x_i^s$ and $y_i^s$. Some possible\n  choices are shown below.\n\n  $$\n    \\begin{array}{lcc}\n    \\hline\n      \\textbf{Interpolation Method} & k_{\\boldsymbol{\\Phi}_x} &\n    k_{\\boldsymbol{\\Phi}_x} \\\\ \\hline\n      \\text{Nearest Neightbor} &  \\delta( \\lfloor x_i^s + 0.5\\rfloor -\n    x_m^t) &  \\delta( \\lfloor y_i^s + 0.5\\rfloor - y_n^t) \\\\\n      \\text{Bilinear} &  \\max \\left(0, 1 -  \\mid x_i^s - x_m^t \\mid\n    \\right) &  \\max (0, 1 - \\mid y_i^s - y_m^t\\mid ) \\\\ \\hline\n    \\end{array}\n  $$\n\nThe figure below summarizes the ST architecture and shows how the\nindividual parts interact with each other.\n\n| ![Architecture of Spatial Transformer](./img/spatial_transformer.png \"Architecture of Spatial Transformer\") |\n| :--  |\n|  **Architecture of ST Module**. Taken from [Jaderberg et al. (2015)](https://arxiv.org/abs/1506.02025).  |\n\n\n[^1]: Clearly, convolutional layers are not rotation or scale\n    invariant. Even the translation-equivariance property does not\n    necessarily make CNNs translation-invariant as typically some\n    fully connected layers are added at the end. Max-pooling layers\n    can introduce some translation invariance, however are limited by\n    their size such that often large translation are not captured.\n\n\n[^2]: [Jaderberg et al. (2015)](https://arxiv.org/abs/1506.02025)\n    define the transformation with normalized coordinates, i.e., $-1\n    \\le x_i^s, y_i^s \\le 1$. However, in the sampling kernel equations\n    it seems more likely that they assume unnormalized/absolute coordinates, e.g.,\n    in equation 4 of the paper normalized coordinates would be nonsensical.\n\n\n**Motivation**: With the introduction of GPUs, convolutional layers\nenabled computationally efficient training of feature detectors on\npatches due to their weight sharing and local connectivity concepts.\nSince then, CNNs have proven to be the most powerful framework when it\ncomes to computer vision tasks such as image classification or\nsegmentation.\n\nDespite their success, [Jaderberg et al.\n(2015)](https://arxiv.org/abs/1506.02025) note that CNNs are still\nlacking mechanisms to be spatially invariant to the input data in a\ncomputationally and parameter efficient manner. While convolutional\nlayers are translation-equivariant to the input data and the use of\nmax-pooling layers has helped to allow the network to be somewhat\nspatially invariant to the position of features, this invariance is\nlimited to the (typically) small spatial support of max-pooling (e.g.,\n$2\\times 2$). As a result, CNNs are typically not invariant to larger\ntransformations, thus need to learn complicated functions to\napproximate these invariances.\n\n<!-- Data augmentation is a standard trick -->\n<!-- to increase the performance of CNNs by  -->\n\nWhat if we could enable the network to learn transformations of the\ninput data? This is the main idea of STs! Learning spatial invariances\nis much easier when you have spatial transformation capabilities. The\nsecond aim of STs is to be computationally and parameter efficient.\nThis is done by using structured, parameterized transformations which\ncan be seen as a weight sharing scheme.\n\n## Implementation\n\n[Jaderberg et al. (2015)](https://arxiv.org/abs/1506.02025) performed\nseveral supervised learning tasks (distorted MNIST, Street View House\nNumbers, fine-grained bird classification) to test the performance of\na standard architecture (FCN or CNN) against an architecture that\nincludes one or several ST modules. They could emperically validate\nthat including STs results in performance gains, i.e., higher\naccuracies across multiple tasks.\n\nThe following reimplementation aims to reproduce a subset of the\ndistored MNIST experiment (RTS distorted MNIST) comparing a standard\nCNN with a ST-CNN architecture. A starting point for the\nimplementation was [this pytorch tutorial by Ghassen\nHamrouni](https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html).\n\n### RTS Distorted MNIST\n\nWhile [Jaderberg et al. (2015)](https://arxiv.org/abs/1506.02025)\nexplored multiple distortions on the MNIST handwriting dataset, this\nreimplementation focuses on the rotation-translation-scale (RTS)\ndistorted MNIST, see image below. As described in appendix A.4 of\n[Jaderberg et al. (2015)](https://arxiv.org/abs/1506.02025) this\ndataset can easily be generated by augmenting the standard MNIST\ndataset as follows:\n* randomly rotate by sampling the angle uniformly in\n  $[+45^{\\circ}, 45^{\\circ}]$,\n* randomly scale by sampling the factor uniformly in $[0.7, 1.2]$,\n* translate by picking a random location on a $42\\times 42$ image\n  (MNIST digits are $28 \\times 28$).\n\n| ![RTS Distorted MNIST Examples](./img/distortedMNIST.png \"RTS Distorted MNIST Examples\") |\n| :---: |\n| **RTS Distorted MNIST Examples** |\n\nNote that this transformation could also be used as a data\naugmentation technique, as the resulting images remain (mostly) valid\ndigit representations (humans could still assign correct labels).\n\nThe code below can be used to create this dataset:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nfrom torchvision import datasets, transforms\n\n\ndef load_data():\n    \"\"\"loads MNIST datasets with 'RTS' (rotation, translation, scale)\n    transformation\n\n    Returns:\n        train_dataset (torch dataset): training dataset\n        test_dataset (torch dataset): test dataset\n    \"\"\"\n    def place_digit_randomly(img):\n        new_img = torch.zeros([42, 42])\n        x_pos, y_pos = torch.randint(0, 42-28, (2,))\n        new_img[y_pos:y_pos+28, x_pos:x_pos+28] = img\n        return new_img\n\n    transform = transforms.Compose([\n        transforms.RandomAffine(degrees=(-45, 45),\n                                scale=(0.7, 1.2)),\n        transforms.ToTensor(),\n        transforms.Lambda(lambda img: place_digit_randomly(img)),\n        transforms.Lambda(lambda img: img.unsqueeze(0))\n    ])\n    train_dataset = datasets.MNIST('./data', transform=transform,\n                                   train=True, download=True)\n    test_dataset = datasets.MNIST('./data', transform=transform,\n                                   train=True, download=True)\n    return train_dataset, test_dataset\n\n\ntrain_dataset, test_dataset = load_data()\n```\n:::\n\n\n### Model Implementation\n\nThe model implementation can be divided into three tasks:\n\n* **Network Architectures**: The network architectures are based upon\n  the description in appendix A.4 of [Jaderberg et al.\n  (2015)](https://arxiv.org/abs/1506.02025). Note that there is only\n  one ST at the beginning of the network such that the resulting\n  transformation is only applied over one channel (input channel). For\n  the sake of simplicity, we only implement an affine transformation\n  matrix. Clearly, including an ST increases the networks capacity\n  due to the number of added trainable parameters. To allow for a fair\n  comparison, we therefore increase the capacity of the convolutional\n  and linear layers in the standard CNN.\n\n  The code below creates both architectures and counts their trainable\n  parameters.\n\n\n  ::: {.cell execution_count=2}\n  ``` {.python .cell-code}\n  import torch.nn as nn\n  import numpy as np\n  import torch.nn.functional as F\n  \n  \n  def get_number_of_trainable_parameters(model):\n    \"\"\"taken from\n    discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325\n    \"\"\"\n    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n    params = sum([np.prod(p.size()) for p in model_parameters])\n    return params\n  \n  \n  class CNN(nn.Module):\n  \n    def __init__(self, img_size=42, include_ST=False):\n        super(CNN, self).__init__()\n        self.ST = include_ST\n        self.name = 'ST-CNN Affine' if include_ST else 'CNN'\n        c_dim = 32 if include_ST else 36\n        self.convs = nn.Sequential(\n            nn.Conv2d(1, c_dim, kernel_size=9, stride=1, padding=0),\n            nn.MaxPool2d(kernel_size=(2,2), stride=2),\n            nn.ReLU(True),\n            nn.Conv2d(c_dim, c_dim, kernel_size=7, stride=1, padding=0),\n            nn.MaxPool2d(kernel_size=(2,2), stride=2),\n            nn.ReLU(True),\n        )\n        out_conv = int((int((img_size - 8)/2) - 6)/2)\n        self.classification = nn.Sequential(\n            nn.Linear(out_conv**2*c_dim, 50),\n            nn.ReLU(True),\n            nn.Linear(50, 10),\n            nn.LogSoftmax(dim=1),\n        )\n        if include_ST:\n            loc_conv_out_dim = int((int(img_size/2) - 4)/2) - 4\n            loc_regression_layer = nn.Linear(20, 6)\n            # initalize final regression layer to identity transform\n            loc_regression_layer.weight.data.fill_(0)\n            loc_regression_layer.bias = nn.Parameter(\n                torch.tensor([1., 0., 0., 0., 1., 0.]))\n            self.localisation_net = nn.Sequential(\n                nn.Conv2d(1, 20, kernel_size=5, stride=1, padding=0),\n                nn.MaxPool2d(kernel_size=(2,2), stride=2),\n                nn.ReLU(True),\n                nn.Conv2d(20, 20, kernel_size=5, stride=1, padding=0),\n                nn.ReLU(True),\n                nn.Flatten(),\n                nn.Linear(loc_conv_out_dim**2*20, 20),\n                nn.ReLU(True),\n                loc_regression_layer\n            )\n        return\n  \n    def forward(self, img):\n        batch_size = img.shape[0]\n        if self.ST:\n            out_ST = self.ST_module(img)\n            img = out_ST\n        out_conv = self.convs(img)\n        out_classification = self.classification(out_conv.view(batch_size, -1))\n        return out_classification\n  \n    def ST_module(self, inp):\n        # act on twice downsampled inp\n        down_inp = F.interpolate(inp, scale_factor=0.5, mode='bilinear',\n                                  recompute_scale_factor=False, align_corners=False)\n        theta_vector = self.localisation_net(down_inp)\n        # affine transformation\n        theta_matrix = theta_vector.view(-1, 2, 3)\n        # grid generator\n        grid = F.affine_grid(theta_matrix, inp.size(), align_corners=False)\n        # sampler\n        out = F.grid_sample(inp, grid, align_corners=False)\n        return out\n  \n    def get_attention_rectangle(self, inp):\n        assert inp.shape[0] == 1, 'batch size has to be one'\n        # act on twice downsampled inp\n        down_inp = F.interpolate(inp, scale_factor=0.5, mode='bilinear',\n                                 recompute_scale_factor=False, align_corners=False)\n        theta_vector = self.localisation_net(down_inp)\n        # affine transformation matrix\n        theta_matrix = theta_vector.view(2, 3).detach()\n        # create normalized target rectangle input image\n        target_rectangle = torch.tensor([\n            [-1., -1., 1., 1., -1.],\n            [-1., 1., 1., -1, -1.],\n            [1., 1., 1., 1., 1.]]\n        ).to(inp.device)\n        # get source rectangle by transformation\n        source_rectangle = torch.matmul(theta_matrix, target_rectangle)\n        return source_rectangle\n  \n  \n  # instantiate models\n  cnn = CNN(img_size=42, include_ST=False)\n  st_cnn = CNN(img_size=42, include_ST=True)\n  # print trainable parameters\n  for model in [cnn, st_cnn]:\n    num_trainable_params = get_number_of_trainable_parameters(model)\n    print(f'{model.name} has {num_trainable_params} trainable parameters')\n  ```\n  :::\n  \n  \n  ![Trainable Parameters](./img/trainable_params.png \"Trainable Paramas\")\n\n* **Training Procedure**: As described in appendix A.4 of [Jaderberg et al.\n  (2015)](https://arxiv.org/abs/1506.02025), the networks are trained\n  with standard SGD, batch size of $256$ and base learning rate of\n  $0.01$. To reduce computation time, the number of epochs is limited\n  to $50$.\n\n  The loss function is the multinomial cross entropy loss, i.e.,\n\n  $$\n    \\text{Loss} = - \\sum_{i=1}^N \\sum_{k=1}^C p_i^{(k)} \\cdot \\log\n    \\left( \\widehat{p}_i^{(k)} \\right),\n  $$\n\n  where $k$ enumerates the number of classes, $i$ enumerates the\n  number of images, $p_i^{(k)} \\in \\{0, 1\\}$ denotes the true probability of image\n  $i$ and class $k$ and $\\widehat{p}_i^{(k)} \\in [0, 1]$ is the\n  probability predicted by the network. Note that the true probability\n  distribution is categorical (hard labels), i.e.,\n\n  $$\n    p_i^{(k)} = 1_{k = y_i} = \\begin{cases}1 & \\text{if } k = y_i \\\\ 0\n    & \\text{else}\\end{cases}\n  $$\n\n  where $y_i \\in \\{0, 1, \\cdots, 9 \\}$ is the label assigned to the\n  $i$-th image $\\textbf{x}_i$. Thus, we can rewrite the loss as follows\n\n  $$\n    \\text{Loss} = - \\sum_{i=1}^N \\log \\left( \\widehat{p}_{i, y_i}\n    \\right),\n  $$\n\n  which is the definition of the negative log likelihood loss\n  ([NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html))\n  in Pytorch, when the logarithmized predictions $\\log \\left(\n  \\widehat{p}_{i, y_i} \\right)$ (matrix of size $N\\times C$) and\n  class labels $y_i$ (vector of size $N$) are given as input.\n\n  The code below summarizes the whole training procedure.\n\n\n  ::: {.cell execution_count=3}\n  ``` {.python .cell-code}\n  from livelossplot import PlotLosses\n  from torch.utils.data import DataLoader\n  \n  \n  def train(model, dataset):\n      # fix hyperparameters\n      epochs = 50\n      learning_rate = 0.01\n      batch_size = 256\n      step_size_scheduler = 50000\n      gamma_scheduler = 0.1\n      # set device\n      device = 'cuda' if torch.cuda.is_available() else 'cpu'\n      print(f'Device: {device}')\n  \n      data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True,\n                              num_workers=4)\n  \n      model.to(device)\n      optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n      scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=gamma_scheduler,\n                                                  step_size=step_size_scheduler)\n  \n      losses_plot = PlotLosses()\n      print(f'Start training with {model.name}')\n      for epoch in range(1, epochs+1):\n          avg_loss = 0\n          for data, label in data_loader:\n              model.zero_grad()\n  \n              log_prop_pred = model(data.to(device))\n              # multinomial cross entropy loss\n              loss = F.nll_loss(log_prop_pred, label.to(device))\n  \n              loss.backward()\n              optimizer.step()\n              scheduler.step()\n  \n              avg_loss += loss.item() / len(data_loader)\n  \n          losses_plot.update({'log loss': np.log(avg_loss)})\n          losses_plot.send()\n      trained_model = model\n      return trained_model\n  ```\n  :::\n  \n  \n* **Test Procedure**: A very simple test procedure to evaluate both\n  models is shown below. It is basically the same as in [the pytorch\n  tutorial](https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html).\n\n\n  ::: {.cell execution_count=4}\n  ``` {.python .cell-code}\n  def test(trained_model, test_dataset):\n      device = 'cuda' if torch.cuda.is_available() else 'cpu'\n      test_loader = DataLoader(test_dataset, batch_size=256, shuffle=True,\n                              num_workers=4)\n      with torch.no_grad():\n          trained_model.eval()\n          test_loss = 0\n          correct = 0\n          for data, label in test_loader:\n              data, label = data.to(device), label.to(device)\n  \n              log_prop_pred = trained_model(data)\n              class_pred = log_prop_pred.max(1, keepdim=True)[1]\n  \n              test_loss += F.nll_loss(log_prop_pred, label).item()/len(test_loader)\n              correct += class_pred.eq(label.view_as(class_pred)).sum().item()\n  \n          print(f'{trained_model.name}: avg loss: {np.round(test_loss, 2)},  ' +\n                f'avg acc {np.round(100*correct/len(test_dataset), 2)}%')\n      return\n  ```\n  :::\n  \n  \n### Results\n\nLastly, the results can also divided into three sections:\n\n* **Training Results**: Firstly, we train our models on the training dataset and compare the logarithmized losses:\n\n\n  ::: {.cell execution_count=5}\n  ``` {.python .cell-code}\n  trained_cnn = train(cnn, train_dataset)\n  ```\n  :::\n  \n  \n  ![Training Results CNN](./img/train_cnn_results.png \"Training Results CNN\")\n\n\n  ::: {.cell execution_count=6}\n  ``` {.python .cell-code}\n  trained_st_cnn = train(st_cnn, train_dataset)\n  ```\n  :::\n  \n  \n  ![Training Results ST-CNN](./img/train_st_cnn_results.png \"Training Results ST-CNN\")\n\n  The logarithmized losses already indicate that the ST-CNN performs\n  better than the standard CNN (at least, it decreases the loss\n  faster). However, it can also be noted that training the ST-CNN\n  seems less stable.\n\n* **Test Performance**: While the performance on the training dataset\n  may be a good indicator, test set performance is much more\n  meaningful. Let's compare the losses and accuracies between both\n  trained models:\n\n\n  ::: {.cell execution_count=7}\n  ``` {.python .cell-code}\n  for trained_model in [trained_cnn, trained_st_cnn]:\n      test(trained_model, test_dataset)\n  ```\n  :::\n  \n  \n  ![Test Results](./img/test_results.png \"Test Results\")\n\n  Clearly, the ST-CNN performs much better than the standard CNN. Note\n  that training for more epochs would probably result in even better\n  accuracies in both models.\n\n* **Visualization of Learned Transformations**: Lastly, it might be\n  interesting to see what the ST module actually does after training:\n\n\n  ::: {.cell execution_count=8}\n  ``` {.python .cell-code}\n  import matplotlib.pyplot as plt\n  from matplotlib.patches import ConnectionPatch\n  \n  \n  def visualize_learned_transformations(trained_st_cnn, test_dataset, digit_class=8):\n      device = 'cuda' if torch.cuda.is_available() else 'cpu'\n      trained_st_cnn.to(device)\n      n_samples = 5\n  \n      data_loader = DataLoader(test_dataset, batch_size=256, shuffle=True)\n      batch_img, batch_label = next(iter(data_loader))\n      i_samples = np.where(batch_label.numpy() == digit_class)[0][0:n_samples]\n  \n      fig = plt.figure(figsize=(n_samples*2.5, 2.5*4))\n      for counter, i_sample in enumerate(i_samples):\n          img = batch_img[i_sample]\n          label = batch_label[i_sample]\n  \n          # input image\n          ax1 = plt.subplot(4, n_samples, 1 + counter)\n          plt.imshow(transforms.ToPILImage()(img), cmap='gray')\n          plt.axis('off')\n          if counter == 0:\n              ax1.annotate('Input', xy=(-0.3, 0.5), xycoords='axes fraction',\n                          fontsize=14, va='center', ha='right')\n  \n          # image including border of affine transformation\n          img_inp = img.unsqueeze(0).to(device)\n          source_normalized = trained_st_cnn.get_attention_rectangle(img_inp)\n          # remap into absolute values\n          source_absolute = 0 + 20.5*(source_normalized.cpu() + 1)\n          ax2 = plt.subplot(4, n_samples, 1 + counter + n_samples)\n          x = np.arange(42)\n          y = np.arange(42)\n          X, Y = np.meshgrid(x, y)\n          plt.pcolor(X, Y, img.squeeze(0), cmap='gray')\n          plt.plot(source_absolute[0], source_absolute[1], color='red')\n          plt.axis('off')\n          ax2.axes.set_aspect('equal')\n          ax2.set_ylim(41, 0)\n          ax2.set_xlim(0, 41)\n          if counter == 0:\n              ax2.annotate('ST', xy=(-0.3, 0.5), xycoords='axes fraction',\n                          fontsize=14, va='center', ha='right')\n          # add arrow between\n          con = ConnectionPatch(xyA=(21, 41), xyB=(21, 0), coordsA='data',\n                                coordsB='data', axesA=ax1, axesB=ax2,\n                                arrowstyle=\"-|>\", shrinkB=5)\n          ax2.add_artist(con)\n  \n          # ST module output\n          st_img = trained_st_cnn.ST_module(img.unsqueeze(0).to(device))\n  \n          ax3 = plt.subplot(4, n_samples, 1 + counter + 2*n_samples)\n          plt.imshow(transforms.ToPILImage()(st_img.squeeze(0).cpu()), cmap='gray')\n          plt.axis('off')\n          if counter == 0:\n              ax3.annotate('ST Output', xy=(-0.3, 0.5), xycoords='axes fraction',\n                          fontsize=14, va='center', ha='right')\n          # add arrow between\n          con = ConnectionPatch(xyA=(21, 41), xyB=(21, 0), coordsA='data',\n                                coordsB='data', axesA=ax2, axesB=ax3,\n                                arrowstyle=\"-|>\", shrinkB=5)\n          ax3.add_artist(con)\n  \n          # predicted label\n          log_pred = trained_st_cnn(img.unsqueeze(0).to(device))\n          pred_label = log_pred.max(1)[1].item()\n  \n          ax4 = plt.subplot(4, n_samples, 1 + counter + 3*n_samples)\n          plt.text(0.45, 0.43, str(pred_label), fontsize=22)\n          plt.axis('off')\n          #plt.title(f'Ground Truth {label.item()}', y=-0.1, fontsize=14)\n          if counter == 0:\n              ax4.annotate('Prediction', xy=(-0.3, 0.5), xycoords='axes fraction',\n                          fontsize=14, va='center', ha='right')\n          # add arrow between\n          con = ConnectionPatch(xyA=(21, 41), xyB=(0.5, 0.65), coordsA='data',\n                                coordsB='data', axesA=ax3, axesB=ax4,\n                                arrowstyle=\"-|>\", shrinkB=5)\n          ax4.add_artist(con)\n      return\n  \n  \n  visualize_learned_transformations(st_cnn, test_dataset, 2)\n  ```\n  :::\n  \n  \n  ![Transformation Visualization](./img/transformation_visualization.png \"Transformation Visualization\")\n\n  Clearly, the ST module attends to the digits such that the\n  ST output has much less variation in terms of rotation, translation\n  and scale making the classification task for the follow up CNN easier.\n\n  Pretty cool, hugh?\n\n\n\n\n\n\n\n---------------------------------------------------------------------------\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}