{
  "hash": "626fcddf63e123d68a05fb619fc831bf",
  "result": {
    "markdown": "---\ntitle: \"MONet: Unsupervised Scene Decomposition and Representation\"\ncategories: [\"reimplementation\", \"unsupervised\", \"VAE\"]\ndate: \"2021-03-27\"\nexecute:\n  eval: false # true\nengine: jupyter\nformat:\n  html: \n    code-fold: show \n    highlight-style: github-dark \n    code-block-bg: \"#000000\"\n    code-tools: \n      toggle: true\n      # TODO UPDATE\n      source: \"https://github.com/borea17/Notebooks/blob/master/07_Concrete_Distribution.ipynb\"\n---\n\n[Burgess et al. (2019)](https://arxiv.org/abs/1901.11390) developed\nthe **Multi-Object Network (MONet)** as an end-to-end trainable model to\ndecompose images into meaningful entities such as objects. Similar to\n[AIR](https://borea17.github.io/paper_summaries/AIR), the whole training process\nis unsupervised, i.e., there are no labeled segmentations, handcrafted bounding\nboxes or whatsoever. In essence, their model combines a Variational Auto-Encoder\n([VAE](https://borea17.github.io/paper_summaries/auto-encoding_variational_bayes))\nwith a recurrent attention network\n([U-Net](https://borea17.github.io/paper_summaries/u_net) *segmentation\nnetwork*) to spatially decompose scenes into attention masks (over which the VAE\nneeds to reconstruct masked regions) and latent representations of each masked\nregion. In contrast to AIR, MONet does not contain a fully generative model and\nits latent space is less structured. As a proof of concept, they show that their\nmodel could learn disentangled representations in a common latent code (i.e.,\nrepresentations of object features in latent space) and object segmentations\n(i.e., attention masks on the original image) on non-trivial 3D scenes.\n\n## Model Description\n\nMONet builds upon the inductive bias that the world (or rather\n*simple images* of the world) can often be approximated as a composition of\nindividual objects with the same underlying structure (i.e., different\ninstantiations of the same class). To put this into practice, [Burgess\net al. (2019)](https://arxiv.org/abs/1901.11390) developed a\nconditional generative sampling scheme in which scenes are\nspatially decomposed into parts that have to be individually modelled\nthrough a common representation code. The architecture incorporates two\nkind of neural networks that are trained in tandem:\n\n* **Attention Network**: Its purpose is to deliver attention\n  masks $\\textbf{m}_k$ for the image such that the whole image is\n  completely spatially decomposed into $K$ parts, i.e., $\\sum_{k=1}^K\n  \\textbf{m}_k = \\textbf{1}$. Ideally, after training each mask focuses on a\n  semantically meaningful element/segment of the image.\n  Thus, it may also be understood as a *segmentation network*.\n\n  To allow for a variable number of attention masks, [Burgess et al.\n  (2019)](https://arxiv.org/abs/1901.11390) use a\n  recurrent neural network $\\alpha_{\\boldsymbol{\\psi}}$ for the\n  decomposition. Therein, an auto-regressive process is defined for the\n  ongoing state.\n  This state is called *scope* $\\textbf{s}_k \\in [0, 1]^{W\\times\n  H}$ (image width $W$ and height $H$) as it is\n  used to track the image parts that remain to be explained, i.e., the\n  scope for the next state is given by\n\n  $$\n     \\textbf{s}_{k+1} = \\textbf{s}_k \\odot \\left(\\textbf{1} -\n  \\underbrace{\\alpha_{\\boldsymbol{\\psi}} \\left( \\textbf{x};\n  \\textbf{s}_{k} \\right)}_{[0,1]^{W \\times H}} \\right)\n  $$\n\n  with the first scope $\\textbf{s}_0 = \\textbf{1}$ ($\\odot$ denotes\n  element-wise multiplication). The attention\n  masks are given by\n\n  $$\n    \\textbf{m}_k  = \\begin{cases} \\textbf{s}_{k-1} \\odot\n    \\alpha_{\\boldsymbol{\\psi}} \\left( \\textbf{x}; \\textbf{s}_{k-1}\n    \\right) & \\forall k < K \\\\\n    \\textbf{s}_{k-1} & k=K \\end{cases}\n  $$\n\n  By construction, we get that\n\n  $$\n  \\begin{align}\n    &\\textbf{s}_{k} = \\textbf{s}_{k+1} + \\textbf{m}_{k + 1} =\n    \\textbf{s}_{k+2} + \\textbf{m}_{k+2} + \\textbf{m}_{k+1} \\\\\n    \\textbf{1}=&\\textbf{s}_0 =\n    \\textbf{s}_{K-1} + \\sum_{k=1}^{K-1} \\textbf{m}_{k} = \\sum_{k=1}^K \\textbf{m}_k,\n  \\end{align}\n  $$\n\n  i.e., at each recursion the remaining part to be explained\n  $\\textbf{s}_{k}$ is divided into a segmentation mask\n  $\\textbf{m}_{k+1}$ and a new scope $\\textbf{s}_{k+1}$ such that\n  with $\\textbf{s}_0=\\textbf{1}$ the entire image is explained by the\n  resulting segmentation masks, i.e., $\\sum_{k=1}^K \\textbf{m}_k = \\textbf{1}$.\n\n\n* **Component VAE**: Its purpose is to represent each masked region in a\n  common latent code, i.e., each segment is encoded by the same\n  VAE[^1]. The encoder distribution $q_{\\boldsymbol{\\phi}}\n  \\left(\\textbf{z}_k | \\textbf{x}, \\textbf{m}_k\\right)$\n  is conditioned both on the input image $\\textbf{x}$ and the corresponding attention mask\n  $\\textbf{m}_k$. I.e., instead of feeding each masked region into the\n  network, [Burgess et al. (2019)](https://arxiv.org/abs/1901.11390)\n  use the whole image $\\textbf{x}$ concatenated with the corresponding\n  attention mask $\\textbf{m}_k$. As a result, we get $K$ different\n  latent codes $\\textbf{z}_k$ (termed \"slots\") which represent the\n  features of each object (masked region) in a common latent/feature\n  space across all objects.\n\n  The decoder distribution $p_{\\boldsymbol{\\theta}}$ is required to\n  reconstruct the image component $\\widetilde{\\textbf{x}}_k \\sim p_{\\boldsymbol{\\theta}} \\left( \\textbf{x} | \\textbf{z}_k \\right)$\n  and the attention masks[^2] $\\widetilde{\\textbf{m}}_k \\sim p_{\\boldsymbol{\\theta}}\n  \\left(\\textbf{c} | \\textbf{z}_k \\right)$ from these latent codes.\n  Note that $p_{\\boldsymbol{\\theta}} \\left(\\textbf{c} | \\textbf{z}_k\n  \\right)$ defines the mask distribution of the Component VAE, whereas\n  $q_{\\boldsymbol{\\psi}} \\left(\\textbf{c} | \\textbf{x}\\right)$\n  denotes the mask distribution of the attention network[^3].\n\n  Importantly, each of the $k$ component reconstruction distributions is\n  multiplied with the corresponding attention mask\n  $\\textbf{m}_k$, i.e.,\n\n  $$\n  \\text{Reconstruction Distribution}_k = \\textbf{m}_k \\odot\n     p_{\\boldsymbol{\\theta}} \\left(\\textbf{x} | \\textbf{z}_k \\right).\n  $$\n\n  The negative (decoder) log likelihood *NLL* (can be interpreted as\n  the *reconstruction error*, see my post on\n  [VAEs](https://borea17.github.io/paper_summaries/auto-encoding_variational_bayes#model-description))\n  of the whole image is given by\n\n  $$\n  \\text{NLL} = - \\log \\left( \\sum_{k=1}^K \\textbf{m}_k \\odot p_{\\boldsymbol{\\theta}} \\left(\\textbf{x} | \\textbf{z}_k \\right)\\right),\n  $$\n\n  where the sum can be understood as the reconstruction distribution\n  of the whole image (mixture of components) conditioned on the latent\n  codes $\\textbf{z}_k$ and the attention masks $\\textbf{m}_k$. Clearly, the\n  reconstructions $\\widetilde{\\textbf{x}}_k \\sim p_{\\boldsymbol{\\theta}} \\left(\n  \\textbf{x} | \\textbf{z}_k\\right)$ are unconstrained outside of the masked\n  regions (i.e., where $m_{k,i} = 0$).\n\n  Note that they use a prior for the latent codes $\\textbf{z}_k$, but not for\n  the attentions masks $\\textbf{m}_k$. Thus, the model is not fully generative,\n  but rather a conditional generative model.\n\n\nThe figure below summarizes the whole architecture of the model by\nshowing the individual components (attention network, component VAE)\nand their interaction.\n\n| ![Schematic of MONet](./img/MONet_schematic.png \"Schematic of MONet\") |\n| :--         |\n| **Schematic of MONet**. A recurrent attention network is used to obtain the attention masks $\\textbf{m}^{(i)}$. Afterwards, a group structured representation is retrieved by feeding each concatenation of $\\textbf{m}^{(i)}, \\textbf{x}$ through the same VAE with encoder parameters $\\boldsymbol{\\phi}$ and decoder parameters $\\boldsymbol{\\theta}$. The outputs of the VAE are the unmasked image reconstructions $\\widetilde{\\textbf{x}}^{(i)}$ and mask reconstructions $\\widetilde{\\textbf{m}}^{(i)}$. Lastly, the reconstructed image is assembled using the deterministic attention masks $\\textbf{m}^{(i)}$ and the sampled unmasked image reconstructions $\\widetilde{\\textbf{x}}^{(i)}$. |\n\nThe whole model is end-to-end trainable with the following loss\nfunction\n\n$$\n\\begin{align}\n\\mathcal{L}\\left(\\boldsymbol{\\phi}; \\boldsymbol{\\theta};\n\\boldsymbol{\\psi}; \\textbf{x} \\right) &= \\underbrace{- \\log \\left( \\sum_{k=1}^K \\textbf{m}_k \\odot p_{\\boldsymbol{\\theta}} \\left(\\textbf{x} |\n\\textbf{z}_k \\right)\\right)}_{\\text{Reconstruction Error between }\n\\widetilde{\\textbf{x}} \\text{ and } \\textbf{x}} + \\beta\n\\underbrace{D_{KL} \\left( \\prod_{k=1}^K q_{\\boldsymbol{\\phi}} \\left(\\textbf{z}_k |\n\\textbf{x}, \\textbf{m}_k\\right) || p(\\textbf{z})\n\\right)}_{\\text{Regularization Term for Distribution of }\\textbf{z}_k}\\\\\n&+ \\gamma \\underbrace{D_{KL} \\left( q_{\\boldsymbol{\\psi}} \\left( \\textbf{c} |\n\\textbf{x} \\right) || p_{\\boldsymbol{\\theta}} \\left( \\textbf{c} | \\{\n\\textbf{z}_k \\} \\right) \\right)}_{\\text{Reconstruction Error between }\n\\widetilde{\\textbf{m}}_k \\text{ and } \\textbf{m}_k},\n\\end{align}\n$$\n\nwhere the first term measures the reconstruction error of the\nfully reconstructed image (sum) as mentioned above. The second term is\nthe KL divergence between the variational posterior\napproximation factorized across slots, i.e., $q_{\\boldsymbol{\\phi}}\n\\left( \\textbf{z} | \\textbf{x} \\right) = \\prod_{k=1}^K\nq_{\\boldsymbol{\\phi}} \\left(\\textbf{z}_k| \\textbf{x},\n\\textbf{m}_k\\right)$, and the prior of the latent distribution\n$p(\\textbf{z})$. As this term pushes the encoder distribution to be\nclose to the prior distribution, it is commonly referred to as\n*regularization term*. It is weighted by the tuneable\nhyperparameter $\\beta$ to encourage learning of disentanglement latent\nrepresentions, see [Higgins et al.\n(2017)](https://deepmind.com/research/publications/beta-VAE-Learning-Basic-Visual-Concepts-with-a-Constrained-Variational-Framework).\nNote that the first two terms are derived from the standard VAE loss.\nThe third term is the KL divergence between the attention mask\ndistribution generated by the attention network\n$q_{\\boldsymbol{\\psi}} \\left( \\textbf{c} | \\textbf{x} \\right)$ and\nthe component VAE $p_{\\boldsymbol{\\theta}}\n\\left(\\textbf{c} |\\{\\textbf{z}_k\\} \\right)$, i.e., it forces these\ndistributions to lie close to each other. It could be understood as\nthe reconstructions error of the VAE's attention masks\n$\\widetilde{\\textbf{m}}_k$, as it forces them to lie close to the\nattention masks $\\textbf{m}_k$ of the attention network. Note however\nthat the attention network itself is trainable, thus the network could\nalso react by pushing the attention mask distribution towards the\nreconstructed mask distribution of the VAE. $\\gamma$ is a tuneable\nhypeterparameter to modulate the importance of this term, i.e.,\nincreasing $\\gamma$ results in close distributions.\n\n\n[^1]: Encoding each segment through the same VAE can be understood as\n    an architectural prior on common structure within individual\n    objects.\n\n[^2]: [Burgess et al. (2019)](https://arxiv.org/abs/1901.11390) do not\n    explain why the Component VAE should also model the attention\n    masks. Note however that this allows for better generalization,\n    e.g., shape/class variation depends on attention mask.\n\n\n[^3]: For completeness $\\textbf{c} \\in \\{1, \\dots, K\\}$ denotes a\n    categorical variable to indicate the probability that pixels\n    belong to a particular component $k$, i.e., $\\textbf{m}_k =\n    p(\\textbf{c} = k)$.\n\n**Motivation**: The model aims to produce semantically meaningful\ndecompositions in terms of segmentation and latent space attributes.\nPrevious work such as the [Spatial Broadcast\ndecoder](https://borea17.github.io/paper_summaries/spatial_broadcast_decoder)\nhas shown that VAEs are extensively capable of decomposing *simple*\nsingle-object scenes into disentangled latent space representations.\nHowever, even *simple* multi-object scenes are far more challenging to\nencode due to their complexity. [Burgess et al.\n(2019)](https://arxiv.org/abs/1901.11390) hypothesize that exploiting\nthe compositional structure of scenes (inductive bias) may help to\nreduce this complexity. Instead of decomposing the entire multi-object\nscene in one sweep, MONet breaks the image in multiple ($K$) tasks which it\ndecomposes with the same VAE[^4]. Restricting the model complexity of\nthe decoder (e.g., by using few layers), forces the model to produce\nsegmentation with similar tasks, i.e., segmentations over structurally\nsimilar scene elements such that the VAE is capable of solving all\ntasks (note that this is a hypothesis). The authors argue that\noptimization should push towards a meaningful decomposition.\nFurthermore, they empirically validate their hypothesis by showing\nthat for the *Objects Room* dataset the reconstruction error is much\nlower when the ground truth attention masks are given compared to an\n*all-in-one* (single sweep) or a *wrong* masks situation.\n\nAdding some more motivation: It might be helpful to think about the\ndata-generating process: Commonly, *artificial* multi-object scenes are\ncreated by adding each object successively to the image. Assuming that\neach of these objects is generated from the same class with different\ninstantiations (i.e., different color/shape/size/...), it seems most\nnatural to recover this process by decomposing the image and then\ndecoding each part.\n\n[^4]: Philosophical note: Humans also tend to work better when focusing on one task at a time.\n\n\n## Implementation\n\n[Burgess et al. (2019)](https://arxiv.org/abs/1901.11390) tested MONet\non three different multi-object scene datasets (*Objects Room*, *CLEVR*,\n*Multi-dSprites*) and showed that their model could successively learn to\n<!-- decompose scenes into semantically meaningful parts (i.e., produce -->\n<!-- meaningful segmentation masks), to represent each segmented object in a -->\n<!-- common (nearly disentangled) latent code, and to generalize to unseen -->\n<!-- scene configurations without any supervision.  -->\n\n* decompose scenes into semantically meaningful parts, i.e.,\n  produce meaningful segmentation masks,\n* represent each segmented object in a common (nearly disentangled) latent\n  code, and\n* generalize to unseen scene configurations\n\nwithout any supervision. Notably, MONet can handle a variable number\nof objects by producing latent codes that map to an empty scene,\nsee image below. Furthermore, it turned out that MONet is also able to\ndeal with occlusions: In the CLEVR dataset the unmasked\nreconstructions could even recover occluded objects, see image below.\n[Burgess et al. (2019)](https://arxiv.org/abs/1901.11390) argue that\nthis indicates how `MONet is learning from and constrained by the\nstructure of the data`.\n\n| ![MONet Paper Results](./img/paper_results.png \"MONet Paper Results\") |\n| :--         |\n| **MONet Paper Results**: Decomposition on *Multi-dSprties* and *CLEVR* images. First row shows the input image, second and third row the corresponding reconstructions and segmentations by MONet (trained for 1,000,000 iterations). The last three rows show the unmasked component reconstructions from some chosen slots (indicated by $S$). Red arrows highlight occluded regions of shapes that are completed as full shapes. Taken from [Burgess et al. (2019)](https://arxiv.org/abs/1901.11390).  |\n\nThe following reimplementation aims to reproduce some of these results\nwhile providing an in-depth understanding of the model architecture.\nTherefore, a dataset that is similar to the *Multi-dSprites* dataset\nis created, then the whole model (as faithfully as possible close to\nthe original architecture) is reimplemented and trained in Pytorch and\nlastly, some useful visualizations of the trained model are created.\n\n### Data Generation\n\nA dataset that is similar in spirit to the *Multi-dSprites* will be\ngenerated. [Burgess et al. (2019)](https://arxiv.org/abs/1901.11390)\ngenerated this dataset by sampling $1-4$ images randomly from the\nbinary [dsprites dataset](https://github.com/deepmind/dsprites-dataset)(consisting of\n$737,280$ images), colorizing these by sampling from a uniform random\nRGB color and compositing those (with occlusion) onto a uniform random\nRGB background.\n\nTo reduce training time, we are going to generate a much simpler dataset\nof $x$ images with two non-overlaping objects (`square` or `circle`) and a\nfixed color space (`red`, `green` or `aqua`) for these objects, see\nimage below. The dataset is generated by sampling uniformly random\nfrom possible latent factors, i.e., random non-overlaping\npositions for the two objects, random object constellations and random\ncolors from color space, see code below image.\n\n\n| ![Examples of Dataset](./img/self_dataset.png \"Examples of Dataset\") |\n| :--:        |\n| Visualization of self-written dataset. |\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom PIL import Image, ImageDraw\nimport torchvision.transforms as transforms\nimport numpy as np\nfrom torch.utils.data import TensorDataset\nimport torch\n\n\ndef generate_dataset(n_samples, SEED=1):\n    ############### CONFIGURATION ###############\n    canvas_size=64\n    min_obj_size, max_obj_size = 12, 20\n    min_num_obj, max_num_obj = 0, 2\n    shapes = [\"circle\", \"square\"]\n    colors = [\"red\", \"green\", \"aqua\"]\n    #############################################\n    data = torch.empty([n_samples, 3, canvas_size, canvas_size])\n    labels = torch.empty([n_samples, 1, canvas_size, canvas_size])\n\n    pos_positions = np.arange(canvas_size - max_obj_size - 1)\n\n    np.random.seed(SEED)\n    rnd_positions = np.random.choice(pos_positions, size=(n_samples, 2), replace=True)\n    rnd_num_objs = np.random.randint(min_num_obj, max_num_obj + 1, size=(n_samples))\n    rnd_obj_sizes = np.random.randint(min_obj_size, max_obj_size + 1, \n                                      size=(n_samples, max_num_obj))\n    rnd_shapes = np.random.choice(shapes, size=(n_samples, max_num_obj), replace=True)\n    rnd_colors = np.random.choice(colors, size=(n_samples, max_num_obj), replace=True)\n    for i_data in range(n_samples):\n        x_0, y_0 = rnd_positions[i_data]\n        num_objs = rnd_num_objs[i_data]\n        if num_objs > 1:\n            # make sure that there is no overlap\n            max_obj_size = max(rnd_obj_sizes[i_data])\n            impos_x_pos = np.arange(x_0 - max_obj_size, x_0 + max_obj_size + 1)\n            impos_y_pos = np.arange(y_0 - max_obj_size, y_0 + max_obj_size + 1)\n            x_1 = np.random.choice(np.setdiff1d(pos_positions, impos_x_pos), size=1)\n            y_1 = np.random.choice(np.setdiff1d(pos_positions, impos_y_pos), size=1)\n        else:\n            x_1 = 0\n            y_1 = 0\n\n        # current latent factors\n        num_objs = rnd_num_objs[i_data]\n        x_positions, y_positions = [x_0, x_1], [y_0, y_1]\n        obj_sizes = rnd_obj_sizes[i_data]\n        shapes = rnd_shapes[i_data]\n        colors = rnd_colors[i_data]\n\n        # create img and label tensors\n        img, label = generate_img_and_label(\n            x_pos=x_positions[:num_objs],\n            y_pos=y_positions[:num_objs],\n            shapes=shapes[:num_objs],\n            colors=colors[:num_objs],\n            sizes=obj_sizes[:num_objs],\n            img_size=canvas_size\n        )\n        data[i_data] = img\n        labels[i_data] = label\n    dataset = TensorDataset(data, labels)\n    return dataset\n\n\ndef generate_img_and_label(x_pos, y_pos, shapes, colors, sizes, img_size):\n    \"\"\"generates a img and corresponding segmentation label mask\n    from the provided latent factors\n\n    Args:\n        x_pos (list): x positions of objects\n        y_post (list): y positions of objects\n        shapes (list): shape can only be `circle` or `square`\n        colors (list): colors of object\n        sizes (list): object sizes\n\n    Returns:\n        img (torch tensor): generated image represented as tensor\n        label (torch tensor): corresponding semantic segmentation mask\n    \"\"\"\n    out_img = Image.new(\"RGB\", (img_size, img_size), color=\"black\")\n    labels = []\n    # add objects\n    for x, y, shape, color, size in zip(x_pos, y_pos, shapes, colors, sizes):\n        img = Image.new(\"RGB\", (img_size, img_size), color=\"black\")\n        # define end coordinates\n        x_1, y_1 = x + size, y + size\n        # draw new image onto black image\n        img1 = ImageDraw.Draw(img)\n        img2 = ImageDraw.Draw(out_img)\n        if shape == \"square\":\n            img1.rectangle([(x, y), (x_1, y_1)], fill=color)\n            img2.rectangle([(x, y), (x_1, y_1)], fill=color)\n        elif shape == \"circle\":\n            img1.ellipse([(x, y), (x_1, y_1)], fill=color)\n            img2.ellipse([(x, y), (x_1, y_1)], fill=color)\n        labels.append((transforms.ToTensor()(img).sum(0) > 0).unsqueeze(0))\n    out_image = transforms.ToTensor()(out_img).type(torch.float32)\n    out_label = torch.zeros(1, img_size, img_size)\n    for i_object in range(len(labels)):\n        out_label[labels[i_object]] = i_object + 1\n    return out_image, out_label\n```\n:::\n\n\n### Model Implementation\n\nMONet is a rather sophisticated model composing two powerful neural network\narchitectures in a reasonable way. One major downside of such complex\nmodels is that they comprise lots of hyperparamters from which much\nremains unknown such as sensitivity to small pertubations (e.g.,\nchanging layers within network architectures or parameters $\\beta$,\n$\\gamma$). Therefore, the model\nimplementation aims to be as close as possible to the original model.\nNote that [Burgess et al. (2019)](https://arxiv.org/abs/1901.11390)\ndid not publish their implementation.\n\nFor the sake of simplicity, this section is divided into four parts:\n\n* **Attention Network**: The architecture of the recurrent neural\n  network $\\boldsymbol{\\alpha}_{\\psi}$ is described in appendix B.2 of\n  [Burgess et al. (2019)](https://arxiv.org/abs/1901.11390).\n  Basically, it consists of a slightly modified\n  [U-Net](https://borea17.github.io/paper_summaries/u_net)\n  architecture that (at the $k$th step) takes as input the\n  concatenation of the image $\\textbf{x}$ and the current scope mask\n  in log units $\\log \\textbf{s}_k$. The output of the modified U-Net\n  is a one channel image $\\textbf{o} \\in ]-\\infty, + \\infty[^{W\\times\n  H}$ in which each entry can be interpreted as the logits probability\n  $\\text{logits }\\boldsymbol{\\alpha}_k$. A sigmoid layer can be used to\n  transform these logits into probabilities, i.e.,\n\n  $$\n  \\begin{align}\n    \\boldsymbol{\\alpha}_k &= \\text{Sigmoid} \\left(\\text{logits }\n    \\boldsymbol{\\alpha}_k \\right) = \\frac {1} {1 + \\exp\\left(- \\text{logits }\n    \\boldsymbol{\\alpha}_k \\right)}\\\\\n    1 - \\boldsymbol{\\alpha}_k &= 1 - \\text{Sigmoid} \\left(\\text{logits }\n    \\boldsymbol{\\alpha}_k \\right) = \\frac {\\exp\\left(- \\text{logits }\n    \\boldsymbol{\\alpha}_k \\right) } { 1 + \\exp\\left(- \\text{logits }\n    \\boldsymbol{\\alpha}_k \\right)}\n  \\end{align}\n  $$\n\n  Additionally, [Burgess et al.\n  (2019)](https://arxiv.org/abs/1901.11390) transform these\n  probabilties into logaritmic units, i.e.,\n\n  $$\n  \\begin{align}\n     \\log \\boldsymbol{\\alpha}_k &= - \\log \\left( 1 + \\exp\\left(- \\text{logits }\n    \\boldsymbol{\\alpha}_k \\right)\\right)=\\text{LogSigmoid }\\left(\n  \\text{logits } \\boldsymbol{\\alpha}_k \\right)\\\\\n     \\log \\left(1 - \\boldsymbol{\\alpha}_k\\right) &= - \\text{logits }\n  \\boldsymbol{\\alpha}_k + \\log \\boldsymbol{\\alpha}_k,\n  \\end{align}\n  $$\n\n  i.e., a LogSigmoid layer can be used (instead of a sigmoid\n  layer with applying logarithm to both outputs) to speed up the computations.\n  From the model description above, it follows\n\n  $$\n  \\begin{align}\n    \\textbf{s}_{k+1} &= \\textbf{s}_k \\odot \\left( 1 -\n    \\boldsymbol{\\alpha}_k \\right) \\quad &&\\Leftrightarrow  \\quad \\log\n    \\textbf{s}_{k+1} = \\log \\textbf{s}_k + \\log \\left(1 - \\boldsymbol{\\alpha}_k \\right)\\\\\n    \\textbf{m}_{k+1} &= \\textbf{s}_{k} \\odot \\boldsymbol{\\alpha}_k \\quad\n  &&\\Leftrightarrow \\quad \\log \\textbf{m}_{k+1} = \\log \\textbf{s}_{k} + \\log \\boldsymbol{\\alpha}_k,\n  \\end{align}\n  $$\n\n  i.e., the output of the $k$th step can be computed by simply adding the\n  log current scope $\\log \\textbf{s}_k$ to each log probability. As a\n  result, the next log attention mask $\\log \\textbf{m}_{k+1}$ and next\n  log scope $\\log \\textbf{s}_{k+1}$ can be retrieved. Note that using\n  log units instead of standard units is beneficial as it ensures\n  numerical stability while simplifying the optimization due\n  to an increased learning signal. <!-- or simpliyfing the loss function computation -->\n\n  The code below summarizes the network architecture, [Burgess et al.\n  (2019)](https://arxiv.org/abs/1901.11390) did not state the channel\n  dimensionality within the U-Net blocks explicitely. However, as they\n  mentioned to use a `U-Net blueprint`, it is assumed that they use\n  the same dimensionality as in the original [U-Net\n  paper](https://borea17.github.io/paper_summaries/u_net). To reduce\n  training time and memory capacity, the following implementation caps\n  the channel dimensionality in the encoder to 64 output channels.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass UNet(nn.Module):\n    \"\"\"U-Net architecture with blocks proposed by Burgess et al. (2019)\n\n    Attributes:\n        encoder_blocks (list): u_net blocks of encoder path\n        decoder_blocks (list): u_net blocks of decoder path\n        bottleneck_MLP (list): bottleneck is a 3-layer MLP with ReLUs\n        out_conv (nn.Conv2d): convolutional classification layer\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.encoder_blocks = nn.ModuleList(\n            [\n                UNet._block(4, 16),              # [batch_size, 16, 64, 64]\n                UNet._block(16, 32),             # [batch_size, 32, 32, 32]\n                UNet._block(32, 64),             # [batch_size, 64, 16, 16]\n                UNet._block(64, 64),             # [batch_size, 64, 8, 8]\n                UNet._block(64, 64),             # [batch_size, 75, 4, 4]\n            ]\n        )\n        self.bottleneck_MLP = nn.Sequential(\n            nn.Flatten(),                        # [batch_size, 512*4*4]\n            nn.Linear(64*4*4, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),                 # [batch_size, 512*4*4]\n            nn.ReLU(),\n            nn.Linear(128, 64*4*4),              # [batch_size, 512*4*4]\n            nn.ReLU(),             # reshaped into [batch_size, 512, 4, 4]\n        )\n        self.decoder_blocks = nn.ModuleList(\n            [\n                UNet._block(128, 64),             # [batch_size, 64, 4, 4]\n                UNet._block(128, 64),             # [batch_size, 64, 8, 8]\n                UNet._block(128, 32),             # [batch_size, 32, 16, 16]\n                UNet._block(64, 16),              # [batch_size, 32, 32, 32]\n                UNet._block(32, 16),              # [batch_size, 64, 64, 64]\n            ]\n        )\n\n        self.out_conv = nn.Conv2d(16, 1, kernel_size=(1,1), stride=1)\n        return\n\n    def forward(self, x):\n        # go through encoder path and store intermediate results\n        skip_tensors = []\n        for index, encoder_block in enumerate(self.encoder_blocks):\n            out = encoder_block(x)\n            skip_tensors.append(out)\n            # no resizing in the last block\n            if index < len(self.encoder_blocks) - 1:  # downsample\n                x = F.interpolate(\n                    out, scale_factor=0.5, mode=\"nearest\", \n\t\t\t\t\trecompute_scale_factor=False\n                )\n        last_skip = out\n        # feed last skip tensor through bottleneck\n        out_MLP = self.bottleneck_MLP(last_skip)\n        # reshape output to match last skip tensor\n        out = out_MLP.view(last_skip.shape)\n        # go through decoder path and use skip tensors\n        for index, decoder_block in enumerate(self.decoder_blocks):\n            inp = torch.cat((skip_tensors[-1 - index], out), 1)\n            out = decoder_block(inp)\n            # no resizing in the last block\n            if index < len(self.decoder_blocks) - 1:  # upsample\n                out = F.interpolate(out, scale_factor=2, mode=\"nearest\")\n        prediction = self.out_conv(out)\n        return prediction\n\n    @staticmethod\n    def _block(in_channels, out_channels):\n        \"\"\"U-Net block as described by Burgess et al. (2019)\"\"\"\n        u_net_block = nn.Sequential(\n            nn.Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size=(3, 3),\n                stride=1,\n                padding=1,\n                bias=False,\n            ),\n            nn.InstanceNorm2d(num_features=out_channels, affine=True),\n            nn.ReLU(),\n        )\n        return u_net_block\n\n\nclass AttentionNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.unet = UNet()\n        return\n\n    def forward(self, x, num_slots):\n        log_s_k = torch.zeros_like(x[:, 0:1, :, :])\n        # initialize list to store intermediate results\n        log_m = []\n        for slot in range(num_slots - 1):\n            inp = torch.cat((x, log_s_k), 1)\n            alpha_logits = self.unet(inp)  # [batch_size, 1, image_dim, image_dim]\n            # transform into log probabilties log (alpha_k) and log (1 - alpha_k)\n            log_alpha_k = F.logsigmoid(alpha_logits)\n            log_1_m_alpha_k = -alpha_logits + log_alpha_k\n            # compute log_new_mask, log_new_scope\n            log_new_mask = log_s_k + log_alpha_k\n            log_new_scope = log_s_k + log_1_m_alpha_k\n            # store intermediate results in list\n            log_m.append(log_new_mask)\n            # update log scope\n            log_s_k = log_new_scope\n        log_m.append(log_s_k)\n        # convert list to tensor [batch_size, num_slots, 1, image_dim, image_dim]\n        log_m = torch.cat(log_m, dim=1).unsqueeze(2)\n        return log_m\n```\n:::\n\n\n<!-- [^5]: [Burgess et al. (2019)](https://arxiv.org/abs/1901.11390) state -->\n<!--     that they use a log softmax layer, however this would only be -->\n<!--     possible if there were two channels at the output. Note that in -->\n<!--     binary classification, a pixel-wise softmax layer (two channel) can be -->\n<!--     transformed into a sigmoid layer (one channel) by using the -->\n<!--     difference between the two channels as input:  -->\n<!--     $$ -->\n<!--     \\begin{align} -->\n<!--       \\text{Softmax} (x_1) &= \\frac {1} {1 + \\exp(x_2 - x_1)} = \\frac -->\n<!--     {1} {1 + \\exp(t)} = \\text{Sigmoid} (t), \\\\ -->\n<!--       \\text{Softmax} (x_2) &= \\frac {1}{1 + \\exp(x_1 - x_2)} = \\frac -->\n<!--     {1}{1+\\exp(-t)} = 1 - \\text{Sigmoid}(t). -->\n<!--     \\end{align} -->\n<!--     $$ -->\n\n* **Component VAE**: The architectures for the encoder\n  $q_{\\boldsymbol{\\phi}}$ and decoder $p_{\\boldsymbol{\\theta}}$\n  neural networks are described in appendix B.1 of [Burgess et al.\n  (2019)](https://arxiv.org/abs/1901.11390). Basically, the encoder\n  $q_{\\boldsymbol{\\phi}}(\\textbf{z}_k | \\textbf{x}, \\textbf{m}_k)$ is a typical CNN that takes the\n  concatentation of an image $\\textbf{x}$ and a segmentation mask in\n  logaritmic units $\\log \\textbf{m}_k$ as input to compute the mean\n  $\\boldsymbol{\\mu}_{E, k}$ and\n  logarithmed variance $\\boldsymbol{\\sigma}^2_{E,k}$ of the Gaussian latent\n  space distribution $\\mathcal{N} \\left(\n  \\boldsymbol{\\mu}_{E, k}, \\text{diag}\\left(\\boldsymbol{\\sigma}^2_{E,k} \\right)\n  \\right)$. Sampling from this distribution is avoided by using\n  the reparametrization trick, i.e., the latent variable\n  $\\textbf{z}_k$ is expressed as a deterministic variable[^5]\n\n  $$\n    \\textbf{z}_k = \\boldsymbol{\\mu}_{E, k} +\n    \\boldsymbol{\\sigma}^2_{E,k} \\odot \\boldsymbol{\\epsilon} \\quad\n    \\text{where} \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}\\left(\n    \\textbf{0}, \\textbf{I}\n    \\right).\n  $$\n\n  The component VAE uses a [Spatial Broadcast\n  decoder](https://borea17.github.io/paper_summaries/spatial_broadcast_decoder) $p_{\\boldsymbol{\\theta}}$\n  to transform the latent vector $\\textbf{z}_k$ into the reconstructed\n  image component $\\widetilde{\\textbf{x}}_k \\sim p_{\\boldsymbol{\\theta}}\n  \\left(\\textbf{x} | \\textbf{z}_k \\right)$ and mask\n  $\\widetilde{\\textbf{m}}_k \\sim p_{\\boldsymbol{\\theta}} \\left(\n  \\textbf{c}|\\textbf{z}_k \\right)$. [Burgess et al.\n  (2019)](https://arxiv.org/abs/1901.11390) chose independent Gaussian\n  distributions with fixed variances for each pixel as the\n  reconstructed image component distributions\n  $p_{\\boldsymbol{\\theta}} \\left( x_i | \\textbf{z}_k \\right) \\sim\n  \\mathcal{N} \\left(\\mu_{k,i} (\\boldsymbol{\\theta}), \\sigma_k^2\n  \\right)$ and independent Bernoulli distributions for each pixel as\n  the reconstructed mask distributions $p\\left(c_{k, i}| \\textbf{z}_k\n  \\right) \\sim \\text{Bern} \\left( p_{k,i}\n  (\\boldsymbol{\\theta})\\right)$. I.e., the decoder output is a 4 channel\n  image from which the first three channels correspond to the 3 RGB\n  channels for the means of the image components\n  $\\boldsymbol{\\mu}_k$ and the last channel corresponds to the logits probabilities\n  of the Bernoulli distribution $\\text{logits }\\textbf{p}_k$.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nclass CNN_VAE(nn.Module):\n    \"\"\"simple CNN-VAE class with a Gaussian encoder (mean and diagonal variance\n    structure) and a Gaussian decoder with fixed variance \n    (decoder is implemented as a Spatial Broadcast decoder) \n\n    Attributes\n        latent_dim (int): dimension of latent space\n        encoder (nn.Sequential): encoder network for mean and log_var\n        decoder (nn.Sequential): spatial broadcast decoder  for mean (fixed var)\n        x_grid (torch tensor): appended x coordinates for spatial broadcast decoder\n        y_grid (torch tensor): appended x coordinates for spatial broadcast decoder\n    \"\"\"\n\n    def __init__(self):\n        super(CNN_VAE, self).__init__()\n        self.latent_dim = 8\n        self.encoder = nn.Sequential(\n            # shape: [batch_size, 4, 64, 64]\n            nn.Conv2d(4, 32, kernel_size=(3,3), stride=2, padding=1),\n            nn.ReLU(),\n            # shape: [batch_size, 32, 32, 32]\n            nn.Conv2d(32, 32, kernel_size=(3,3), stride=2, padding=1),\n            nn.ReLU(),\n            # shape: [batch_size, 32, 16, 16]\n            nn.Conv2d(32, 64, kernel_size=(3,3), stride=2, padding=1),\n            nn.ReLU(),\n            # shape: [batch_size, 64, 8, 8]\n            nn.Conv2d(64, 64, kernel_size=(3,3), stride=2, padding=1),\n            nn.ReLU(),\n            # shape: [batch_size, 64, 4, 4],\n            nn.Flatten(),\n            # shape: [batch_size, 1024]\n        )\n        self.MLP = nn.Sequential(\n            nn.Linear(64*4*4, 256),\n            nn.ReLU(),\n            nn.Linear(256, 2*self.latent_dim),\n        )\n        # spatial broadcast decoder configuration\n        img_size = 64\n        # \"input width and height of CNN both 8 larger than target output\"\n        x = torch.linspace(-1, 1, img_size + 8)\n        y = torch.linspace(-1, 1, img_size + 8)\n        x_grid, y_grid = torch.meshgrid(x, y)\n        # reshape into [1, 1, img_size, img_size] and save in state_dict\n        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape).clone())\n        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape).clone())\n        self.decoder = nn.Sequential(\n             # shape [batch_size, latent_dim + 2, 72, 72]\n            nn.Conv2d(in_channels=self.latent_dim+2, out_channels=16,\n                      stride=(1, 1), kernel_size=(3,3)),\n            nn.ReLU(),\n            # shape [batch_size, 16, 70, 70]\n            nn.Conv2d(in_channels=16, out_channels=16, stride=(1,1),\n                      kernel_size=(3, 3)),\n            nn.ReLU(),\n            # shape [batch_size, 16, 68, 68]\n            nn.Conv2d(in_channels=16, out_channels=16, stride=(1,1),\n                      kernel_size=(3, 3)),\n            nn.ReLU(),\n            # shape [batch_size, 16, 66, 66]\n            nn.Conv2d(in_channels=16, out_channels=16, stride=(1,1),\n                      kernel_size=(3, 3)),\n            # shape [batch_size, 4, 64, 64]\n            nn.ReLU(),\n            nn.Conv2d(in_channels=16, out_channels=4, stride=(1,1),\n                      kernel_size=(1, 1)),\n        )\n        return\n\n    def forward(self, x):\n        [z, mu_E, log_var_E] = self.encode(x)\n        x_rec = self.decode(z)\n        return x_rec, z, mu_E, log_var_E\n\n    def encode(self, x):\n        out_encoder = self.MLP(self.encoder(x))\n        mu_E, log_var_E = torch.chunk(out_encoder, 2, dim=1)\n        # sample noise variable for each batch\n        epsilon = torch.randn_like(log_var_E)\n        # get latent variable by reparametrization trick\n        z = mu_E + torch.exp(0.5 * log_var_E) * epsilon\n        return [z, mu_E, log_var_E]\n\n    def decode(self, z):\n        batch_size = z.shape[0]\n        # reshape z into [batch_size, latent_dim, 1, 1]\n        z = z.view(z.shape + (1, 1))\n        # tile across image [batch_size, latent_im, 64+8, 64+8]\n        z_b = z.repeat(1, 1, 64 + 8, 64 + 8)\n        # upsample x_grid and y_grid to [batch_size, 1, 64+8, 64+8]\n        x_b = self.x_grid.repeat(batch_size, 1, 1, 1)\n        y_b = self.y_grid.repeat(batch_size, 1, 1, 1)\n        # concatenate vectors [batch_size, latent_dim+2, 64+8, 64+8]\n        z_sb = torch.cat((z_b, x_b, y_b), dim=1)\n        # apply convolutional layers mu_D\n        mu_D = self.decoder(z_sb)\n        return mu_D\n\n\t\nclass ComponentVAE(CNN_VAE):\n    \"\"\"Component VAE class for use in MONet as proposed by Burgess et al. (2019)\n\n    Attributes:\n        #################### CNN_VAE ########################\n        encoder (nn.Sequential): encoder network for mean and log_var\n        decoder (nn.Sequential): decoder network for mean (fixed var)\n        img_dim (int): image dimension along one axis\n        expand_dim (int): expansion of latent image to accomodate for lack of padding\n        x_grid (torch tensor): appended x coordinates for spatial broadcast decoder\n        y_grid (torch tensor): appended x coordinates for spatial broadcast decoder\n        #####################################################\n        img_channels (int): number of channels in image\n    \"\"\"\n\n    def __init__(self,):\n        super().__init__()\n        self.img_channels = 3\n        return\n\n    def forward(self, image, log_mask, deterministic=False):\n        \"\"\"\n        parellize computation of reconstructions\n\n        Args:\n            image (torch.tensor): input image [batch, img_channels, img_dim, img_dim]\n            log_mask (torch.tensor): all seg masks [batch, slots, 1, img_dim, img_dim]\n\n        Returns:\n            mu_z_k (torch.tensor): latent mean [batch, slot, latent_dim]\n            log_var_z_k (torch.tensor): latent log_var [batch, slot, latent_dim]\n            z_k (torch.tensor): latent log_var [batch, slot, latent_dim]\n            x_r_k (torch.tensor): img reconstruction \n\t\t\t\t[batch, slot, img_chan, img_dim, img_dim]\n            logits_m_r_k (torch.tensor): mask recons. [batch, slot, 1, img_dim, img_dim]\n        \"\"\"\n        num_slots = log_mask.shape[1]\n        # create input [batch_size*num_slots, image_channels+1, img_dim, img_dim]\n        x = ComponentVAE._prepare_input(image, log_mask, num_slots)\n        # get encoder distribution parameters [batch*slots, latent_dim]\n        [z_k, mu_z_k, log_var_z_k] = self.encode(x)\n        if deterministic:\n            z_k = mu_z_k\n        # get decoder dist. parameters [batch*slots, image_channels, img_dim, img_dim]\n        [x_r_k, logits_m_r_k] = self.decode(z_k)\n        # convert outputs into easier understandable shapes\n        [mu_z_k, log_var_z_k, z_k, x_r_k, logits_m_r_k] = ComponentVAE._prepare_output(\n            mu_z_k, log_var_z_k, z_k, x_r_k, logits_m_r_k, num_slots\n        )\n        return [mu_z_k, log_var_z_k, z_k, x_r_k, logits_m_r_k]\n\n    def decode(self, z):\n        \"\"\"\n        Args:\n            z (torch.tensor): [batch_size*num_slots, latent_dim]\n\n        Returns:\n            mu_x (torch.tensor): [batch*slots, img_channels, img_dim, img_dim]\n            logits_m (torch.tensor): [batch*slots, 1, img_dim, img_dim]\n\n        \"\"\"\n        mu_D = super().decode(z)\n        # split into means of x and logits of m\n        mu_x, logits_m = torch.split(mu_D, self.img_channels, dim=1)\n        # enforce positivity of mu_x\n        mu_x = mu_x.abs()\n        return [mu_x, logits_m]\n\n    @staticmethod\n    def _prepare_input(image, log_mask, num_slots):\n        \"\"\"\n        Args:\n            image (torch.tensor): input image [batch, img_channels, img_dim, img_dim]\n            log_mask (torch.tensor): all seg masks [batch, slots, 1, img_dim, img_dim]\n            num_slots (int): number of slots (log_mask.shape[1])\n\n        Returns:\n            x (torch.tensor): input image [batch*slots, img_channels+1, img_dim, img_dim]\n        \"\"\"\n        # prepare image [batch_size*num_slots, image_channels, img_dim, img_dim]\n        image = image.repeat(num_slots, 1, 1, 1)\n        # prepare log_mask [batch_size*num_slots, 1, img_dim, img_dim]\n        log_mask = torch.cat(log_mask.squeeze(2).chunk(num_slots, 1), 0)\n        # concatenate along color channel\n        x = torch.cat((image, log_mask), dim=1)\n        return x\n\n    @staticmethod\n    def _prepare_output(mu_z_k, log_var_z_k, z_k, x_r_k, logits_m_r_k, num_slots):\n        \"\"\"\n        convert output into an easier understandable format\n\n        Args:\n            mu_z_k (torch.tensor): [batch_size*num_slots, latent_dim]\n            log_var_z_k (torch.tensor): [batch_size*num_slots, latent_dim]\n            z_k (torch.tensor): [batch_size*num_slots, latent_dim]\n            x_r_k (torch.tensor): [batch_size*num_slots, img_channels, img_dim, img_dim]\n            logits_m_r_k (torch.tensor): [batch_size*num_slots, 1, img_dim, img_dim]\n            num_slots (int): number of slots (log_mask.shape[1])\n\n        Returns:\n            mu_z_k (torch.tensor): [batch, slot, latent_dim]\n            log_var_z_k (torch.tensor): [batch, slot, latent_dim]\n            z_k (torch.tensor): [batch, slot, latent_dim]\n            x_r_k (torch.tensor): [batch, slots, img_channels, img_dim, img_dim]\n            logits_m_r_k (torch.tensor): [batch, slots, 1, img_dim, img_dim]\n        \"\"\"\n        mu_z_k = torch.stack(mu_z_k.chunk(num_slots, dim=0), 1)\n        log_var_z_k = torch.stack(log_var_z_k.chunk(num_slots, dim=0), 1)\n        z_k = torch.stack(z_k.chunk(num_slots, dim=0), 1)\n        x_r_k = torch.stack(x_r_k.chunk(num_slots, dim=0), 1)\n        logits_m_r_k = torch.stack(logits_m_r_k.chunk(num_slots, dim=0), 1)\n        return [mu_z_k, log_var_z_k, z_k, x_r_k, logits_m_r_k]\n```\n:::\n\n\n[^5]: This is explained in more detail in my [VAE](https://borea17.github.io/paper_summaries/auto-encoding_variational_bayes) post. For simplicity, we are setting the number of (noise variable) samples $L$ per datapoint to 1 (see equation $\\displaystyle \\widetilde{\\mathcal{L}}$ in [*Reparametrization Trick*](https://borea17.github.io/paper_summaries/auto-encoding_variational_bayes#model-description) paragraph). Note that [Kingma and Welling (2013)](https://arxiv.org/abs/1312.6114) stated that in their experiments setting $L=1$ sufficed as long as the minibatch size was large enough.\n\n\n * **MONet Implementation**: The compositional structure is achieved\n    by looping for $K$ steps over the image and combining the attention\n    network with the component VAE. While attention masks and latent\n    codes can be generated easily (during test time), computing the\n    loss $\\mathcal{L}$ is more complicated. Remind that the loss\n    function is given by\n\n    $$\n    \\begin{align}\n    \\mathcal{L}\\left(\\boldsymbol{\\phi}; \\boldsymbol{\\theta};\n    \\boldsymbol{\\psi}; \\textbf{x} \\right) &= \\underbrace{- \\log \\left( \\sum_{k=1}^K \\textbf{m}_k \\odot p_{\\boldsymbol{\\theta}} \\left(\\textbf{x} |\n    \\textbf{z}_k \\right)\\right)}_{\\text{Reconstruction Error between }\n    \\widetilde{\\textbf{x}} \\text{ and } \\textbf{x}} + \\beta\n    \\underbrace{D_{KL} \\left( \\prod_{k=1}^K q_{\\boldsymbol{\\phi}} \\left(\\textbf{z}_k |\n    \\textbf{x}, \\textbf{m}_k\\right) || p(\\textbf{z})\n    \\right)}_{\\text{Regularization Term for Distribution of }\\textbf{z}_k}\\\\\n    &+ \\gamma \\underbrace{D_{KL} \\left( q_{\\boldsymbol{\\psi}} \\left( \\textbf{c} |\n    \\textbf{x} \\right) || p_{\\boldsymbol{\\theta}} \\left( \\textbf{c} | \\{\n    \\textbf{z}_k \\} \\right) \\right)}_{\\text{Reconstruction Error between }\n    \\widetilde{\\textbf{m}}_k \\text{ and } \\textbf{m}_k}.\n    \\end{align}\n    $$\n\n    Each of these three terms can be written in a more explicit form such that\n    the implementation becomes trivial:\n\n    1. *Reconstruction Error between $\\widetilde{\\textbf{x}}$ and\n       $\\textbf{x}$*: This term is also known as the negative log\n       likelihood (NLL) of the whole reconstructed image. [Burgess et\n       al. (2019)](https://arxiv.org/abs/1901.11390) chose independent\n       Gaussian distributions with fixed variance for each pixel as\n       the decoder distribution $p_{\\boldsymbol{\\theta}} \\left(\n       x_{i} | \\textbf{z}_k \\right) \\sim \\mathcal{N} \\left(\\mu_{k,\n       i}(\\boldsymbol{\\theta}), \\sigma_k^2 \\right)$. \n\n\n    2. *Regularization Term for Distribution of $\\textbf{z}_k$*: The\n       coding space is regularized using the KL divergence between the\n       latent (posterior) distribution $q_{\\boldsymbol{\\phi}} \\left( \\textbf{z}_k \\right) \\sim\n       \\mathcal{N} \\left( \\boldsymbol{\\mu}_k, \\left(\\boldsymbol{\\sigma}_k^2\\right)^{\\text{T}}\n       \\textbf{I} \\right)$ factorized across slots and the latent prior distribution weighted with the hyperparameter\n       $\\beta$. The product of multiple Gaussians is itself a\n       Gaussian, however it is rather complicated to compute the new\n       mean and covariance matrix of this Gaussian. Fortunately, each\n       $\\textbf{z}_k$ is sampled independently from the corresponding\n       latent distribution $q_{\\boldsymbol{\\phi}}(\\textbf{z}_k)$,\n       thus we can generate the new mean and covariance by\n       concatenation (see [this\n       post](https://stats.stackexchange.com/a/308137)), i.e.,\n\n       $$\n         q(\\textbf{z}_1, \\dots, \\textbf{z}_K) = \\prod_{k=1}^K q_{\\boldsymbol{\\phi}} \\left(\\textbf{z}_k\n       \\right)  = q\\left( \\begin{bmatrix} \\textbf{z}_1 \\\\ \\vdots\n         \\\\ \\textbf{z}_K  \\end{bmatrix}\\right) = \\mathcal{N} \\left(\n         \\underbrace{\n         \\begin{bmatrix} \\boldsymbol{\\mu}_1 \\\\ \\vdots\n          \\\\ \\boldsymbol{\\mu}_K \\end{bmatrix}}_{\n          \\widehat{\\boldsymbol{\\mu}}}, \\underbrace{\\text{diag}\\left(\n         \\begin{bmatrix} \\boldsymbol{\\sigma}_1^2 \\\\  \\vdots\\\\\n         \\boldsymbol{\\sigma}_K^2  \\end{bmatrix}\n         \\right)}_{ \\left(\\widehat{\\boldsymbol{\\sigma}}^2\\right)^{\\text{T}} \\textbf{I}}\\right)\n       $$\n\n       [Burgess et al. (2019)](https://arxiv.org/abs/1901.11390) chose\n       a unit Gaussian distribution as the latent prior $p(\\textbf{z})\n       \\sim \\mathcal{N} \\left(\\textbf{0}, \\textbf{I} \\right)$ with\n       $\\text{dim}(\\textbf{0}) = \\text{dim}(\\hat{\\boldsymbol{\\mu}})$.\n       The KL divergence between those two Gaussian distributions\n       can be calculated in closed form (see Appendix B of [Kingma and Welling (2013)](https://arxiv.org/abs/1312.6114))\n\n       $$\n       \\begin{align}\n          D_{KL} \\left( \\prod_{k=1}^K q_{\\boldsymbol{\\phi}}\n       \\left(\\textbf{z}_k \\right) || p(\\textbf{z}) \\right) &= -\\frac\n       {1}{2} \\sum_{j=1}^{K \\cdot L} \\left(1 + \\log \\left(\n       \\widehat{\\sigma}_j^2 \\right) - \\widehat{\\mu}_j^2 - \\widehat{\\sigma}_j^2 \\right),\n       \\end{align}\n       $$\n\n       where $L$ denotes the dimensionality of the latent space.\n\n    3. *Reconstruction Error between $\\widetilde{\\textbf{m}}_k$ and\n       $\\textbf{m}_k$*: Remind that the attention network\n       $\\boldsymbol{\\alpha}_{\\boldsymbol{\\psi}}$ produces $K$\n       segmentation masks in logaritmic units, i.e., $\\log\n       \\textbf{m}_k$. By construction $\\sum_{k=1}^K \\textbf{m}_k =\n       \\textbf{1}$, i.e., concatentation of the attention masks\n       $\\textbf{m} = \\begin{bmatrix} \\textbf{m}_1 & \\dots &\n       \\textbf{m}_K \\end{bmatrix}^{\\text{T}}$ can be interpreted as a\n       pixel-wise categorical distribution[^7]. Similarly,\n       concatenating the logits probabilties of the component VAE and\n       applying a pixel-wise softmax, i.e.,\n\n       $$\n       \\widetilde{\\textbf{m}} = \\begin{bmatrix} \\widetilde{\\textbf{m}}_1 \\\\ \\vdots \\\\\n       \\widetilde{\\textbf{m}}_K \\end{bmatrix} = \\text{Softmax}\\left(\\begin{bmatrix} \\text{logits }\\textbf{p}_1 \\\\ \\vdots \\\\\n       \\text{logits }\\textbf{p}_K \\end{bmatrix}\\right),\n       $$\n\n       transforms the logits outputs of the component VAE into a pixel-wise\n       categorical distribution. Thus, the KL-divergence can be\n       calculated as follows\n\n       $$\n       \\begin{align}\n         D_{KL} \\left( q_{\\boldsymbol{\\psi}} \\left( \\textbf{c} |\n    \\textbf{x} \\right) || p_{\\boldsymbol{\\theta}} \\left( \\textbf{c} | \\{\n    \\textbf{z}_k \\} \\right) \\right) &=\n         \\sum_{i=1}^{H\\cdot W} D_{KL} \\left( {\\textbf{m}}_i || \\widetilde{\\textbf{m}}_i \\right) \\\\\n         &= \\sum_{i=1}^{H\\cdot W} \\textbf{m}_i \\odot \\left(\\log \\textbf{m}_i - \\log \\widetilde{\\textbf{m}}_i \\right),\n       \\end{align}\n       $$\n\n       where $i$ denotes the pixel space, i.e., $\\textbf{m}_i \\in [0,\n       1]^{K}$. To make the computation more efficient, we directly\n       compute the reconstructed segmentations in logaritmic units\n       using pixel-wise logsoftmax, i.e.,\n\n       $$\n       \\log \\widetilde{\\textbf{m}} = \\text{LogSoftmax}\\left(\\begin{bmatrix} \\text{logits }\\textbf{p}_1 \\\\ \\vdots \\\\\n       \\text{logits }\\textbf{p}_K \\end{bmatrix}\\right).\n       $$\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\n\n\nclass MONet(pl.LightningModule):\n    \"\"\"Multi-Object Network class as described by Burgess et al. (2019)\n    \n    Atributes:\n        n_samples (int): number of samples in training dataset\n        attention_network (AttentionNetwork)\n        component_VAE (ComponentVAE)\n        ############## loss specific ##############\n        bg_var (float): background variance\n        fg_var (float): foreground variance\n        beta (float): hyperparamater for loss\n        gamma (float): hyperparameter for loss\n        ###########################################\n        ############ training specific ############\n        num_slots_train (int): number of slots used during training time\n        lr (float): learning rate\n        batch_size (int): batch size used during training\n        log_every_k_epochs (int): how often current result img should be logged\n        ###########################################\n    \"\"\"\n\n    def __init__(self, n_samples):\n        super(MONet, self).__init__()\n        self.n_samples = n_samples\n        self.attention_network = AttentionNetwork()\n        self.component_VAE = ComponentVAE()\n        # initialize all biases to zero\n        self.attention_network.apply(MONet.weight_init)\n        self.component_VAE.apply(MONet.weight_init)\n        ############## loss specific ##############\n        self.num_slots_train = 3\n        self.bg_var, self.fg_var = 0.09**2, 0.11**2\n        self.beta = 0.5\n        self.gamma = 0.5\n        ###########################################\n        ############ training specific ############\n        self.lr, self.batch_size = 0.0001, 64\n        self.log_every_k_epochs = 1\n        # Initialise pixel output standard deviations (NLL calculation)\n        var = self.fg_var * torch.ones(1, self.num_slots_train, 1, 1, 1)\n        var[0, 0, 0, 0, 0] = self.bg_var  # first step\n        self.register_buffer(\"var\", var)\n        self.save_hyperparameters()\n        return\n\n    def forward(self, x, num_slots):\n        \"\"\"\n        defines the inference procedure of MONet, i.e., computes the latent\n        space and keeps track of useful metrics\n\n        Args:\n            x (torch.tensor): image [batch_size, img_channels, img_dim, img_dim]\n            num_slots (int): number of slots\n\n        Returns:\n            out (dict): output dictionary containing\n                log_m_k (torch.tensor) [batch, slots, 1, img_dim, img_dim]\n                    (logarithmized attention masks of attention_network)\n                mu_k (torch.tensor) [batch, slots, latent_dim]\n                    (means of component VAE latent space)\n                log_var_k (torch.tensor) [batch, slots, latent_dim]\n                    (logarithmized variances of component VAE latent space)\n                x_r_k (torch.tensor) [batch, slots, img_channels, img_dim, img_dim]\n                    (slot-wise VAE image reconstructions)\n                logits_m_r_k (torch.tensor) [batch, slots, 1, img_dim, img_dim]\n                    (slot-wise VAE mask reconstructions in logits)\n                x_tilde (torch.tensor) [batch, img_channels, img_dim, img_dim]\n                    (reconstructed image using x_r_k and log_m_k)\n        \"\"\"\n        # compute all logarithmized masks (iteratively)\n        log_m_k = self.attention_network(x, num_slots)\n        # compute all VAE reconstructions (parallel)\n        [mu_z_k, log_var_z_k, z_k, x_r_k, logits_m_r_k] = self.component_VAE(x, \n                                                                             log_m_k.exp())\n        # store output in dict\n        output = dict()\n        output[\"log_m_k\"] = log_m_k\n        output[\"mu_z_k\"] = mu_z_k\n        output[\"log_var_z_k\"] = log_var_z_k\n        output[\"z_k\"] = z_k\n        output[\"x_r_k\"] = x_r_k\n        output[\"logits_m_r_k\"] = logits_m_r_k\n        output[\"x_tilde\"] = (log_m_k.exp() * x_r_k).sum(axis=1)\n        return output\n    \n    \n    ########################################\n    #########  TRAINING FUNCTIONS  #########\n    ########################################\n\n    def training_step(self, batch, batch_idx):\n        x, labels = batch  # labels are not used here (unsupervised)\n        output = self.forward(x, self.num_slots_train)        \n        ############ NLL \\sum_k m_k log p(x_k) #############################\n        NLL = (\n            output[\"log_m_k\"].exp() * \n            (((x.unsqueeze(1) - output[\"x_r_k\"]) ** 2 / (2 * self.var)))\n        ).sum(axis=(1, 2, 3, 4))\n        # compute KL divergence of latent space (component VAE) per batch\n        KL_div_VAE = -0.5 * (\n            1 + output[\"log_var_z_k\"] - output[\"mu_z_k\"] ** 2 \n            - output[\"log_var_z_k\"].exp()\n        ).sum(axis=(1, 2))\n        # compute KL divergence between masks\n        log_m_r_k = output[\"logits_m_r_k\"].log_softmax(dim=1)\n        KL_div_masks = (output[\"log_m_k\"].exp() * (output[\"log_m_k\"] - log_m_r_k)).sum(\n            axis=(1, 2, 3, 4)\n        )\n        # compute loss\n        loss = (NLL.mean() + self.beta * KL_div_VAE.mean() \n                + self.gamma * KL_div_masks.mean())\n        # log results in TensorBoard\n        step = self.global_step\n        self.logger.experiment.add_scalar(\"loss/NLL\", NLL.mean(), step)\n        self.logger.experiment.add_scalar(\"loss/KL VAE\", KL_div_VAE.mean(), step)\n        self.logger.experiment.add_scalar(\"loss/KL masks\", KL_div_masks.mean(), step)\n        self.logger.experiment.add_scalar(\"loss/loss\", loss, step)\n        return {\"loss\":loss, \"x\": x}\n\n    def training_epoch_end(self, outputs):\n        \"\"\"this function is called after each epoch\"\"\"\n        step = int(self.current_epoch)\n        if (step + 1) % self.log_every_k_epochs == 0:\n            # log some images, their segmentations and reconstructions\n            n_samples = 7\n            \n            last_x = outputs[-1][\"x\"]\n            i_samples = np.random.choice(range(len(last_x)), n_samples, False)\n            images = last_x[i_samples]\n            \n            fig_rec = self.plot_reconstructions_and_decompositions(images, \n                                                                   self.num_slots_train)\n            self.logger.experiment.add_figure(\"image and reconstructions\", \n                                              fig_rec, global_step=step)\n        return\n    \n    ########################################\n    ######### TRAINING SETUP HOOKS #########\n    ########################################\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.RMSprop(self.parameters(), lr=self.lr)\n        return optimizer\n    \n    @staticmethod\n    def weight_init(m):\n        \"\"\"initialize all bias to zero\"\"\"\n        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n            if m.bias is not None:\n                torch.nn.init.zeros_(m.bias)\n        return\n    \n    ########################################\n    ####### PLOT AND HELPER FUNCTIONS ######\n    ########################################\n\n    @staticmethod\n    def convert_masks_indices_to_mask_rgb(masks_ind, slots):\n        colors = plt.cm.get_cmap(\"hsv\", slots + 1)\n        cmap_rgb = colors(np.linspace(0, 1, slots + 1))[:, 0:3]\n        masks_RGB = cmap_rgb[masks_ind].squeeze(1)\n        masks_RGB_tensor = torch.from_numpy(masks_RGB)\n        return masks_RGB_tensor\n\n    def plot_reconstructions_and_decompositions(self, images, num_slots):\n        monet_output = self.forward(images, num_slots)\n        batch_size, img_channels = images.shape[0:2]\n        \n        colors = plt.cm.get_cmap(\"hsv\", num_slots + 1)\n        cmap = colors(np.linspace(0, 1, num_slots + 1))\n        \n        # get mask indices using argmax [batch_size, 1, 64, 64]\n        masks_ind = monet_output[\"log_m_k\"].exp().argmax(1).detach().cpu()\n        # convert into RGB values  [batch_size, 64, 64, 3]\n        masks_RGB = MONet.convert_masks_indices_to_mask_rgb(masks_ind, num_slots)              \n        fig = plt.figure(figsize=(14, 10))\n        for counter in range(batch_size):\n            orig_img = images[counter]\n            # data\n            plt.subplot(3 + num_slots, batch_size + 1, counter + 2)\n            plt.imshow(transforms.ToPILImage()(orig_img))\n            plt.axis('off')\n            # reconstruction mixture\n            x_tilde = monet_output[\"x_tilde\"][counter].clamp(0, 1)\n            plt.subplot(3 + num_slots, batch_size + 1, counter + 2 + (batch_size + 1))\n            plt.imshow(transforms.ToPILImage()(x_tilde))\n            plt.axis('off')\n            # segmentation (binary) from attention network\n            plt.subplot(3 + num_slots, batch_size + 1, counter + 2 + (batch_size + 1)*2)\n            plt.imshow(masks_RGB[counter])\n            plt.axis('off')\n            # unmasked component reconstructions\n            x_r_k = monet_output[\"x_r_k\"][counter].clamp(0, 1)\n            for slot in range(num_slots):\n                x_rec = x_r_k[slot]\n                plot_idx =  counter + 2 + (batch_size + 1)*(slot+3)\n                plt.subplot(3 + num_slots, batch_size + 1, plot_idx)\n                plt.imshow(transforms.ToPILImage()(x_rec))\n                plt.axis('off')\n        # annotation plots\n        ax = plt.subplot(3 + num_slots, batch_size + 1, 1)\n        ax.annotate('Data', xy=(1, 0.5), xycoords='axes fraction',\n                    fontsize=14, va='center', ha='right')\n        ax.set_aspect('equal')\n        ax.axis('off')\n        ax = plt.subplot(3 + num_slots, batch_size + 1, batch_size + 2)\n        ax.annotate('Reconstruction\\nmixture', xy=(1, 0.5), xycoords='axes fraction',\n                    fontsize=14, va='center', ha='right')\n        ax.set_aspect('equal')\n        ax.axis('off')\n        ax = plt.subplot(3 + num_slots, batch_size + 1, 2*batch_size + 3)\n        ax.annotate('Segmentation', xy=(1, 0.5), xycoords='axes fraction',\n                    fontsize=14, va='center', ha='right')\n        ax.set_aspect('equal')\n        ax.axis('off')\n        for slot in range(num_slots):\n            ax = plt.subplot(3 + num_slots, batch_size + 1, \n                             1 + (batch_size + 1)*(slot+3))\n            ax.annotate(f'S{slot+1}', xy=(1, 0.5), xycoords='axes fraction',\n                        fontsize=14, va='center', ha='right', weight='bold',\n                        color=cmap[slot])\n            ax.set_aspect('equal')\n            ax.axis('off')\n        return fig\n    \n    def plot_ComponentVAE_results(self, images, num_slots):\n        monet_output = self.forward(images, num_slots)\n        x_r_k = monet_output[\"x_r_k\"]\n        masks = monet_output[\"log_m_k\"].exp()\n        # get mask indices using argmax [batch_size, 1, 64, 64]\n        masks_ind = masks.argmax(1).detach().cpu()\n        # convert into RGB values  [batch_size, 64, 64, 3]\n        masks_RGB = MONet.convert_masks_indices_to_mask_rgb(masks_ind, num_slots) \n        \n        colors = plt.cm.get_cmap('hsv', num_slots + 1)\n        cmap = colors(np.linspace(0, 1, num_slots + 1))\n        n_samples, img_channels = images.shape[0:2]\n        fig = plt.figure(constrained_layout=False, figsize=(14, 14))\n        grid_spec = fig.add_gridspec(2, n_samples, hspace=0.1)\n        \n        for counter in range(n_samples):\n            orig_img = images[counter]\n            x_tilde = monet_output[\"x_tilde\"][counter].clamp(0, 1)\n            segmentation_mask = masks_RGB[counter]\n            # upper plot: Data, Reconstruction Mixture, Segmentation\n            upper_grid = grid_spec[0, counter].subgridspec(3, 1)\n            for upper_plot_index in range(3):\n                ax = fig.add_subplot(upper_grid[upper_plot_index])\n                if upper_plot_index == 0:\n                    plt.imshow(transforms.ToPILImage()(orig_img))\n                elif upper_plot_index == 1:\n                    plt.imshow(transforms.ToPILImage()(x_tilde))   \n                else:\n                    plt.imshow(segmentation_mask)\n                plt.axis('off')\n                if counter == 0:  # annotations\n                    if upper_plot_index == 0:  # Data\n                        ax.annotate('Data', xy=(-0.1, 0.5), \n                                    xycoords='axes fraction', ha='right',\n                                    fontsize=14, va='center',)\n                    elif upper_plot_index == 1:  # Reconstruction mixture\n                        ax.annotate('Reconstruction\\nmixture', xy=(-0.1, 0.5), \n                                     va='center',\n                                     xycoords='axes fraction', fontsize=14, ha='right')\n                    else:  # Segmentation\n                        ax.annotate('Segmentation', xy=(-0.1, 0.5), va='center',\n                                     xycoords='axes fraction', fontsize=14, ha='right')\n            # lower plot: Component VAE reconstructions\n            lower_grid = grid_spec[1, counter].subgridspec(num_slots, 2, \n                                                           wspace=0.1, hspace=0.1)\n            for row_index in range(num_slots):\n                x_slot_r = x_r_k[counter][row_index]\n                m_slot_r = masks[counter][row_index]\n                for col_index in range(2):\n                    ax = fig.add_subplot(lower_grid[row_index, col_index])\n                    if col_index == 0:  # unmasked\n                        plt.imshow(transforms.ToPILImage()(x_slot_r.clamp(0, 1)))\n                        if row_index == 0:\n                            plt.title('Unmasked', fontsize=14)\n                        plt.axis('off')\n                    else:  # masked\n                        masked = ((1 - m_slot_r)*torch.ones_like(x_slot_r) \n                                  + m_slot_r*x_slot_r)\n                        #masked = m_slot_r*x_slot_r\n                        plt.imshow(transforms.ToPILImage()(masked.clamp(0, 1)))\n                        if row_index == 0:\n                            plt.title('Masked', fontsize=14)\n                        plt.axis('off')\n                    ax.set_aspect('equal')\n                    if counter == 0 and col_index == 0:  # annotations\n                        ax.annotate(f'S{row_index+1}', xy=(-0.1, 0.5), \n                                    xycoords='axes fraction', ha='right',\n                                    fontsize=14, va='center', weight='bold',\n                                    color=cmap[row_index])\n        return\n                                   \n    ########################################\n    ########## DATA RELATED HOOKS ##########\n    ########################################\n\n    def prepare_data(self) -> None:\n        n_samples = self.n_samples\n        self.dataset = generate_dataset(n_samples=n_samples)\n        return\n\n    def train_dataloader(self):\n        return DataLoader(self.dataset, batch_size=self.batch_size, \n                          num_workers=12, shuffle=True)\n```\n:::\n\n\n[^7]: Note that concatenation of masks leads to a three dimensional\n    tensor.\n\n\n* **Training Procedure**: [Burgess et al.\n  (2019)](https://arxiv.org/abs/1901.11390) chose `RMSProp` for the\n  optimization with a learning rate of `0.0001` and a batch size of\n  `64`, see Appendix B.3. Thanks to the [PyTorch-Lightning](https://pytorch-lightning.readthedocs.io/en/latest/)\n  framework, these paramters are already defined in the model and we can easily integrate \n  tensorboard into our training procedure:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom pytorch_lightning import seed_everything\nfrom pytorch_lightning.loggers import TensorBoardLogger\n\n\ndef train(n_samples, num_epochs, SEED=1):\n    seed_everything(SEED)\n\n    monet = MONet(n_samples)\n    logger = TensorBoardLogger('./results', name=\"SimplifiedMultiSprites\")\n    # initialize pytorch lightning trainer\n    num_gpus = 1 if torch.cuda.is_available() else 0\n    trainer = pl.Trainer(\n        deterministic=True,\n        gpus=num_gpus,\n        track_grad_norm=2,\n        gradient_clip_val=2,  # don't clip\n        max_epochs=num_epochs,\n        progress_bar_refresh_rate=20,\n        logger=logger,\n    )\n     # train model\n    trainer.fit(monet)\n    trained_monet = monet\n    return trained_monet\n\ntrained_monet = train(n_samples=50000, num_epochs=10)\n```\n:::\n\n\n![Training](./img/MONET_train.png \"Training\")\n\n\n### Results\n\nThe following visualization are inspired by Figure 3 and 7 of \n[Burgess et al. (2019)](https://arxiv.org/abs/1901.11390) and mainly serve to evaluate\nthe representation quality of the trained model.\n\n\n* **MONet Reconstructions and Decompositions**: The most intuitive\n  visualization is to show some (arbitrarly chosen) fully\n  reconstructed images (i.e, `Reconstruction mixture` $\\widetilde{\\textbf{x}} = \\sum_{k=1}^K\n  \\textbf{m}_k \\odot \\widetilde{\\textbf{x}}_k$) compared to the\n  original input $\\textbf{x}$ (`Data`) together with the learned segmentation\n  masks (i.e., `Segmentation` $\\{ \\textbf{m}_k \\}$) of the attention network. Note\n  that in order to visualize the segmentations\n  in one plot, we cast the attenion masks into binary\n  attention masks by applying `arg max` pixel-wise over all $K$\n  attention masks. In addition, all\n  umasked component VAE reconstructions (i.e., `S(k)`\n  $\\widetilde{\\textbf{x}}_k$) are shown, see figure below.\n\n  | ![MONet Reconstruction and Decompositions](./img/reconstruction_and_decompositions.png \"MONet Reconstructions and Decompositions\") |\n  | :--         |\n  | **Figure 7 of** [Burgess et al. (2019)](https://arxiv.org/abs/1901.11390): Each example shows the image fed as input data to the model, with corresponding outputs from the model. Reconstruction mixtures show sum of components from all slots, weighted by the learned masks from the attention network. Colour-coded segmentation maps summarize the attention masks $\\{\\textbf{m}_k \\}$. Rows labeld S1-5 show the reconstruction components of each slot. |\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ndataloader = trained_monet.train_dataloader()\nrandom_batch = next(iter(dataloader))[0]\nfig = trained_monet.plot_reconstructions_and_decompositions(batch[0: 4], 3)\n```\n:::\n\n\n![MONet Reconstructions and Decompositions after Train](./img/MONET_rec.png \"Reconstructions and Decompositions\")\n\n* **Component VAE Results**: In order to evaluate the perfomance of\n  the component VAE, we are interested in the unmasked\n  slot-wise reconstructions (i.e., `unmasked` refers to\n  $\\widetilde{\\textbf{x}}_k$ for each slot $k$) and the slot-wise\n  reconstructions masked by the VAE's reconstructed masks (i.e.,\n  `masked` refers to $\\widetilde{\\textbf{m}}_k \\odot\n  \\widetilde{\\textbf{x}}_k$). Ideally, masked versions capture either\n  a single object, the background or nothing at all (representing no\n  object), see figure below. In addition, we are going to plot the\n  ground truth masked reconstructions (i.e., `gt masked` refers to\n  $\\textbf{m}_k \\odot \\widetilde{\\textbf{x}}_k$) such that the\n  difference between `gt masked` and `masked` indicates the\n  reconstruction error of the attention masks.\n\n  | ![Component VAE Results](./img/component_VAE_results.png \"Component VAE Results\") |\n  | :--         |\n  | **Figure 3 of** [Burgess et al. (2019)](https://arxiv.org/abs/1901.11390): Each example shows the image fet as input data to the model, with corresponding outputs from the model. Reconstruction mixtures show sum of components from all slots, weighted by the learned masks from the attention network. Color-coded segmentation maps summarise the attention masks $\\{\\textbf{m}_k\\}$. Rows labeled S1-7 show the reconstruction components of each slot. Unmasked version are shown side-by-side with corresponding versions that are masked with the VAE's reconstructed masks $\\widetilde{\\textbf{m}}_k$. |\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ndataloader = trained_monet.train_dataloader()\nrandom_batch = next(iter(dataloader))[0]\nfig = trained_monet.plot_ComponentVAE_results(batch[0: 4], 3)\n```\n:::\n\n\n![MONet Component VAE](./img/MONET_CompVAE.png \"MONet Component VAE\")\n\n\n## Drawbacks of Paper\n\n* deterministic attention mechanism implying that objective function is not a\n  valid lower bound on the marginal likelihood (as mentioned by [Engelcke et al. (2020)](https://arxiv.org/abs/1907.13052))\n* image generation suffers from discrepancy between inferred and reconstructed masks\n<!-- * only works on simple images in which multiple objects of the same class occur -->\n<!-- * even simple images require high training times -->\n* lots of hyperparameters (network architectures, $\\beta$, $\\gamma$, optimization)\n\n## Acknowledgment\n\nThere are a lot of implementations out there that helped me very much in\nunderstanding the paper:\n\n* [Darwin Bautista's implementation](https://github.com/baudm/MONet-pytorch)\n  includes derivation of the NLL (which in the end, I did not use for simplicity).\n* [Karl Stelzner's implementation](https://github.com/stelzner/monet/) is kept\n  more simplisitic and is therefore easier to understand.\n* [Martin Engelcke, Claas Voelcker and Max\n  Morrison](https://github.com/applied-ai-lab/genesis) included an\n  implementation of MONet in the Genesis repository.\n\n\n\n[^6]: In practice, the reconstruction error for a fixed component\n    (fixed $k$) becomes incalculable for binary attention masks, since $\\log 0$ is\n    undefined. However, the reconstruction error is effectively\n    unconstrained outside of masked regions, since for $\\lim \\log\n    m_{k,i} \\rightarrow -\\infty$ the reconstruction error for the\n    corresponding pixel and slot approaches 0.\n\n\n--------------------------------------------------------------------------------------------\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}