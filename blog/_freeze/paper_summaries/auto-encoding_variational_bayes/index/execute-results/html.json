{
  "hash": "6d1472f0d7a78379f2cdc5809b004ba4",
  "result": {
    "markdown": "---\ntitle: \"Auto-Encoding Variational Bayes\"\ncategories: [\"reimplementation\", \"VAE\", \"generative\"]\ndate: \"2020-07-25\"\nexecute:\n  eval: false # true\nengine: jupyter\nformat:\n  html: \n    code-fold: show \n    highlight-style: github \n    code-block-bg: true\n    code-tools: \n      toggle: true\n      source: \"https://github.com/borea17/Notebooks/blob/master/01_Auto_Encoding_Variational_Bayes.ipynb\"\n---\n\n<!-- nextjournal_link: \"https://nextjournal.com/borea17/auto-encoding-variational-bayes\" -->\n\n[Kingma and Welling (2013)](https://arxiv.org/abs/1312.6114) introduced the\nVariational Auto-Encoder (VAE) to showcase how their Auto-Encoding\nVariational Bayes (AEVB) algorithm can be used in practice. Assuming\ni.i.d. datasets and continuous latent variables, the AEVB algorithm\nlearns an approximate probabilistic encoder\n$q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x})$ jointly with the\nprobabilisitc decoder $p_{\\boldsymbol{\\theta}}\n(\\textbf{x}|\\textbf{z})$ (where $\\boldsymbol{\\phi},\n\\boldsymbol{\\theta}$ parametrize the corresponding distributions) by\nlearning the optimal model parameters $\\boldsymbol{\\phi},\n\\boldsymbol{\\theta}$ through optimizing an objective function with \nstandard gradient ascent methods. In summary, a **VAE is probabilistic\nautoencoder which uses variational inference to regularize the coding\nspace**. Furthermore, a VAE is a deep generative model as sampling from\nthe coding space is possible, i.e., new observations can be generated. \n\n## Model Description\n\nThe AEVB algorithm basically assumes a generative process, introduces a\nvariational approximation (see figure below) and optimizes the model\nparameters by maximizing an objective function. The objective function consists\nof the (reparametrized) variational lower bound of each datapoint.\nReparametrization is necessary to allow the explicit formulation of gradients\nwith respect to the model parameters. \n\n| ![Generative Process and Variational Approximation](./img/DAG.png \"Generative Process and Variational Approximation\") |\n| :--         |\n| The directed graphical models represent the assumed generative process (a) and the variational approximation of the intractable posterior (b) in the AEVB algorithm. |\n\n\n**Objective Function Derivation**: Let $\\textbf{X}=\\{\\textbf{x}^{(i)}\\}_{i=1}^{N}$\ndenote the dataset consisting of $N$ i.i.d. samples and let\n$\\textbf{z}$ denote the unobserved continuous random variable (i.e.,\nhidden or code variable). [Kingma and\nWelling (2013)](https://arxiv.org/abs/1312.6114) assume that each observed\nsample $\\textbf{x}^{(i)}$ comes from a generative process in which:\nFirstly, a hidden variable $\\textbf{z}^{(i)}$ is generated from a\nprior distribution $p_{\\boldsymbol{\\theta}} (\\textbf{z})$. Secondly,\n$\\textbf{x}^{(i)}$ is generated from the conditional distribution\n$p_{\\boldsymbol{\\theta}}(\\textbf{x}|\\textbf{z}^{(i)})$. Note that we\ndo not know $\\boldsymbol{\\theta}$ nor do we have information about\n$\\textbf{z}^{(i)}$. In order to recover this generative process, they\nintroduce $q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x})$ as an\napproximation to the intractable true posterior[^1]\n$p_{\\boldsymbol{\\theta}} (\\textbf{z}|\\textbf{x})$. The marginal log likelihood of\neach individual datapoint $\\textbf{x}^{(i)}$ can then be stated as\nfollows (see [Eric Jang's amazing blog\npost](https://blog.evjang.com/2016/08/variational-bayes.html) for\ndetailed derivation)\n\n[^1]: The true posterior could be calculated via Bayes theorem\n    $\\displaystyle p_{\\boldsymbol{\\theta}} (\\textbf{z}|\\textbf{x}) =\n    \\frac {p_{\\boldsymbol{\\theta}} (\\textbf{x}|\\textbf{z})\n    p_{\\boldsymbol{\\theta}} (\\textbf{z})} {\\int\n    p_{\\boldsymbol{\\theta}} (\\textbf{x}|\\textbf{z})\n    p_{\\boldsymbol{\\theta}} (\\textbf{z}) d\\textbf{z}}$. However, the\n    integral in the denominator is intractable in practice. \n    \n$$\n  \\log p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}^{(i)}\\right) =\n  \\underbrace{D_{KL} \\left(q_{\\boldsymbol{\\phi}}\\left(\\textbf{z} | \\textbf{x}^{(i)}\\right)\n  || p_{\\boldsymbol{\\theta}} \\left(\\textbf{z}|\\textbf{x}^{(i)}\\right)\\right)}_{\\ge 0} +\n  \\mathcal{L} \\left( \\boldsymbol{\\theta}, \\boldsymbol{\\phi}, \\textbf{x}^{(i)}\\right) \\ge\n  \\mathcal{L} \\left( \\boldsymbol{\\theta}, \\boldsymbol{\\phi}, \\textbf{x}^{(i)}\\right),\n$$\n\nwhere $D_{KL}(\\cdot)$ denotes the KL divergence of the approximate\nfrom the true posterior (this quantity remains unknown since the true\nposterior $p_{\\boldsymbol{\\theta}} (\\textbf{z}|\\textbf{x}^{(i)})$ is\nintractable). $\\mathcal{L} \\left(\\boldsymbol{\\theta},\n\\boldsymbol{\\phi}, \\textbf{x}^{(i)}\\right)$ is called the variational\nlower bound or evidence lower bound (ELBO). The goal is to optimize\n$\\boldsymbol{\\phi}, \\boldsymbol{\\theta}$ such that variational lower bound is\nmaximized, thereby we indirectly maximize the marginal log likelihood.\nThe variational lower bound can rewritten such that the objective\nfunction is obtained (also derived in [Eric Jang's blog post](https://blog.evjang.com/2016/08/variational-bayes.html))\n\n$$\n\\begin{align}\n  \\mathcal{L} \\left(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}, \\textbf{x}^{(i)}\\right) &=\n  \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x}^{(i)})}\n  \\left[ -\\log \n  q_{\\boldsymbol{\\phi}} (\\textbf{z} | \\textbf{x}^{(i)} ) +\n  \\log p_{\\boldsymbol{\\theta}} (\\textbf{z}) + \\log\n  p_{\\boldsymbol{\\theta}} \n  (\\textbf{x}^{(i)}|\\textbf{z}) \\right] \\\\\n  &= \\underbrace{-D_{KL} \\left(  q_{\\boldsymbol{\\phi}} \\left( \n  \\textbf{z} | \\textbf{x}^{(i)} \\right), p_{\\boldsymbol{\\theta}} \n  (\\textbf{z}) \\right)}_{\\text{Regularization Term}} +\n    \\underbrace{\n    \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x}^{(i)})} \n    \\left[ \\log p_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)}| \\textbf{z} \\right) \\right]}_{\\text{Reconstruction Accuracy}}\n    .\n\\end{align}\n$$\n\n\nThe two terms have an associated interpretation in autoencoder\nlanguage:\n\n* *Reconstruction Accuracy* (opposite of *Reconstruction Error*): The expectation can be interpreted using Monte Carlo\n    integration,i.e.,\n    \n    $$\n      \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x}^{(i)})} \n      \\left[ \\log p_{\\boldsymbol{\\theta}} \\left(\n      \\textbf{x}^{(i)} | \\textbf{z}\n      \\right) \\right] \\approx \\frac {1}{N} \\sum_{k=1}^{N} \\log p_{\\boldsymbol{\\theta}}\n      \\left( \\textbf{x}^{(i)} | \\textbf{z}^{(k)} \\right) \\qquad \\textbf{z}^{(k)} \\sim\n      q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x}^{(i)}),\n    $$\n\n    which results in an unbiased estimate. Sampling $\\textbf{z}^{(k)}\\sim\n  q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x}^{(i)})$ can be understood as encoding the\n  observed input $\\textbf{x}^{(i)}$ into a code $\\textbf{z}^{(k)}$ using the\n  probabilistic encoder $q_{\\boldsymbol{\\phi}}$. Clearly, the expectation is maximized\n  when the decoder $p_{\\boldsymbol{\\theta}}$ maps the encoded input \n  $\\textbf{z}^{(k)}$ back\n  the original input $\\textbf{x}^{(i)}$, i.e., assigns high probability to\n  $p_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)} | \\textbf{z}^{(i)} \\right)$.\n* *Regularization Term*: The KL divergence is non-negative and only\n  zero if both distributions are identical. Thus, maximizing this term forces\n  the encoder distribution $q_{\\boldsymbol{\\phi}}$ to be close to the prior\n  $p_{\\boldsymbol{\\theta}}(\\textbf{z})$. In VAEs, the prior is typically set to be an\n  isotropic normal distribution resulting in a regularized code space, i.e.,\n  encouraging a code space that is close to a normal distribution.\n  \n**Reparametrization Trick**: While the KL-divergence $D_{KL} \\left(\n q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}^{(i)} \\right),\n p_{\\boldsymbol{\\theta}} (\\textbf{z})\\right)$ (i.e., the\n regularization term) can often be integrated analytically, the second\n term $\\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x}^{(i)})} \\left[ \\log\n p_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)}| \\textbf{z} \\right) \\right]$ (i.e.,\n the reconstruction accuracy) requires sampling from $q_{\\boldsymbol{\\phi}}$.\n There are two downsides associated wih sampling from $q_{\\boldsymbol{\\phi}}$\n approaches:  \n\n 1. Backpropagation does not work with a sampling operation, i.e., the\n    implementation of VAEs would be more difficult.\n 2. The usual Monte Carlo gradient estimator (which relies on sampling from\n    $q_{\\boldsymbol{\\phi}}$) w.r.t. $\\boldsymbol{\\phi}$ exhibits very high variance.\n\nTo overcome these problems, [Kingma and\nWelling (2013)](https://arxiv.org/abs/1312.6114) use the **reparametrization\n  trick**:\n\n<center>Substitute sampling $\\textbf{z} \\sim q_{\\boldsymbol{\\phi}}$ by using a\ndeterministic mapping $\\textbf{z} = g_{\\boldsymbol{\\phi}}\n(\\boldsymbol{\\epsilon},\n\\textbf{x})$ with the differential transformation\n$g_{\\boldsymbol{\\phi}}$ of an auxiliary noise variable\n$\\boldsymbol{\\epsilon}$ with $\\boldsymbol{\\epsilon}\\sim p(\\boldsymbol{\\epsilon})$. \n</center>\n<br>\nAs a result, the reparametrized objective function can be written as follows \n\n$$\n  \\mathcal{L} \\left(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\textbf{x}^{(i)}\\right) =\n  -D_{KL} \\left(  q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}^{(i)} \\right),\n  p_{\\boldsymbol{\\theta}} (\\textbf{z}) \\right) +\n  \\mathbb{E}_{p(\\boldsymbol{\\epsilon})} \\left[ \\log\n  p_{\\boldsymbol{\\theta}} \n  \\left( \\textbf{x}^{(i)}| g_{\\boldsymbol{\\phi}} \\left(\n  \\boldsymbol{\\epsilon}, \n  \\textbf{x}^{(i)} \\right) \\right) \\right]\n$$\n\nin which the second term can be approximated with Monte Carlo integration\nyielding\n\n$$\n  \\widetilde{\\mathcal{L}} \\left(\\boldsymbol{\\theta},\n  \\boldsymbol{\\phi}; \n  \\textbf{x}^{(i)}\\right) =\n  -D_{KL} \\left(  q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}^{(i)} \\right),\n  p_{\\boldsymbol{\\theta}} (\\textbf{z}) \\right) +\n  \\frac {1}{L} \\sum_{l=1}^{L} \\log p_{\\boldsymbol{\\theta}}\\left(\n  \\textbf{x}^{(i)}| g_{\\boldsymbol{\\phi}}\n \\left( \\boldsymbol{\\epsilon}^{(i, l)}, \\textbf{x}^{(i)} \\right)\\right)\n$$\n\nwith $\\boldsymbol{\\epsilon} \\sim p(\\boldsymbol{\\epsilon})$. \nNote that [Kingma and Welling](https://arxiv.org/abs/1312.6114)\ndenote this estimator as the second version of the Stochastic Gradient\nVariational Bayes (SGVB) estimator. Assuming that the KL-divergence can be\nintegrated analytically, the derivatives\n$\\nabla_{\\boldsymbol{\\theta},\\boldsymbol{\\phi}} \\widetilde{L}$ can be taken (see\nfigure below), i.e., this estimator can\nbe optimized using standard stochastic gradient methods.\n\n\n| ![Computation Graph under Reparametrization](./img/reparameterization_trick.png \"Computation Graph under Reparametrization\") |\n| :--         |\n| The computation graphs summarize the difference between the computation of the reconstruction accuracy in the original objective (a) and the reparametrized objective (b). Circles indicate a sampling operation through which backpropagation is not allowed. |\n\nTo increase stability and performance, [Kingma and\nWelling](https://arxiv.org/abs/1312.6114) introduce a minibatch\nestimator of the lower bound: \n\n$$\n  \\widetilde{\\mathcal{L}}^{M} (\\boldsymbol{\\theta}, \n  \\boldsymbol{\\phi}; \\textbf{X}^{M})  =  \\frac {N}{M} \n  \\sum_{i=1}^{M}\\widetilde{\\mathcal{L}} \\left(\n  \\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\textbf{x}^{(i)}\\right),\n$$\n\nwhere $\\textbf{X}^{M} = \\left\\{ \\textbf{x}^{(i)} \\right\\}_{i=1}^{M}$ denotes a\nminibatch of $M$ datapoints from the full dataset $\\textbf{X}$ of $N$ datapoints.\n\n## Learning the Model\n\nLearning the probabilistic encoder $q_{\\boldsymbol{\\phi}}$ and decoder \n$p_{\\boldsymbol{\\theta}}$ comes down to learning the optimal model\nparameters $\\boldsymbol{\\phi}, \\boldsymbol{\\theta}$ using the AEVB algorithm which can\nbe summarized in 5 steps: \n\n1. Initialize model parameters $\\boldsymbol{\\phi}, \\boldsymbol{\\theta}$ randomly.\n2. Sample random minibatch $\\textbf{X}^{M} = \\left\\{ \\textbf{x}^{(i)} \\right\\}_{i=1}^{M}$.\n3. Compute gradients $\\nabla_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}}\n    \\widetilde{\\mathcal{L}}^{M} \\left(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\textbf{X}^{M} \\right)$.\n4. Update model parameters $\\boldsymbol{\\phi}, \\boldsymbol{\\theta}$ by taking a gradient\n    ascent step.\n5. Repeat steps 2-4 until model parameters converged\n\n## VAE Implementation\n\nA VAE simply uses deep neural networks (DNNs)\nas function approximators to parametrize the probabilistic encoder\n$q_{\\boldsymbol{\\phi}}$ and decoder $p_{\\boldsymbol{\\theta}}$. The\noptimal parameters $\\boldsymbol{\\phi}, \\boldsymbol{\\theta}$ are learned jointly\nby training the VAE using the AEVB algorithm. \n\n| ![Schematic of a standard VAE](./img/schematic_VAE.png \"Schematic of a standard VAE\") |\n| :--:        |\n| Schematic of a standard VAE |\n\n**Regularization Term**: Typically, the prior over the latent variables is set to be the\ncentered isotropic Gaussian, i.e., $p_{\\boldsymbol{\\theta}} (\\textbf{z}) \\sim\n\\mathcal{N} (\\textbf{0}, \\textbf{I})$. Note that this prior is needed\nto compute the regularization term in the objective function.\nFurthermore, it is commonly assumed that the true posterior\n$p_{\\boldsymbol{\\theta}}\\left(\\textbf{z} | \\textbf{x}^{(i)}\\right)$\nmay be approximated by $q_{\\boldsymbol{\\phi}} \\left(\\textbf{z}  |\n\\textbf{x}^{(i)} \\right) \\sim \\mathcal{N}\\left(\\boldsymbol{\\mu}_E^{(i)},\n\\boldsymbol{\\sigma}_E^{2 (i)} \\textbf{I}  \\right)$ (subscripts denote\nthat these parameters come from the *encoder network*). As a result, the\nregularization term can be integrated analytically leading to a term\nthat only depends on $\\boldsymbol{\\mu}_E^{(i)},\n\\boldsymbol{\\sigma}_E^{2 (i)}$ (see Appendix B of [Kingma and\nWelling](https://arxiv.org/abs/1312.6114))\n\n$$\n-D_{KL} \\left(  q_{\\boldsymbol{\\phi}} \\left( \n  \\textbf{z} | \\textbf{x}^{(i)} \\right), p_{\\boldsymbol{\\theta}} \n  (\\textbf{z}) \\right) = \\frac {1}{2} \\sum_{j=1}^{J} \\left( \n  1 + \\log \\left( \\left( \\sigma_{E_j}^{(i)} \\right)^2 \\right)\n  - \\left( \\mu_{E_j}^{(i)}  \\right)^2 -  \\left( \\sigma_{E_j}^{(i)} \\right)^2\n  \\right),\n$$\n\nwhere $J$ denotes the latent space dimension. \n\n**Encoder/Decoder Network**: [Kingma and Welling\n(2013)](https://arxiv.org/abs/1312.6114) use simple neural networks\nwith only one hidden layer to approximate the parameters of the\nprobabilistic encoder and decoder. As stated above, the encoder\nnetwork is fixed to compute the parameters $\\boldsymbol{\\mu}^{(i)}_E,\n\\boldsymbol{\\sigma}_E^{(i)} \\in \\mathbb{R}^{L}$ of the Gaussian\ndistribution $\\mathcal{N}\\left(\\boldsymbol{\\mu}_E^{(i)},\n\\boldsymbol{\\sigma}_E^{2 (i)} \\textbf{I}  \\right)$. In fact, the\nencoder network takes a sample $\\textbf{x}^{(i)}$ and outputs the mean\n$\\boldsymbol{\\mu}_E^{(i)}$ and logarithmized variance, i.e.,\n\n$$\n\\begin{bmatrix} \\boldsymbol{\\mu}_E^{(i)} & \\log\n\\boldsymbol{\\sigma}^{2(i)} \\end{bmatrix} = f_{\\boldsymbol{\\phi}} \\left( \\textbf{x}^{(i)} \\right).\n$$\n\nNote that using the logarithmized version of the variance increases\nstability and simplifies the training[^2].  \n\n[^2]: Note that the variance is by definition greater than zero.\n    Furthermore, the variance is typically relatively small. Thus,\n    using the logarithmized variance as network output increases\n    stability and performance (see [this\n    answer](https://stats.stackexchange.com/a/353222) for details).\n\nIn principle, the encoder and decoder network are very similar only\nthat the dimension of the input and output are reversed. While the\nencoder network is fixed to approximate a multivariate Gaussian with\ndiagonal covariance structure, the decoder network can approximate a\nmultivariate Gaussian (real-valued data) or Bernoulli (binary data)\ndistribution. \n\nBelow is a simple Python class that can be used to\ninstantiate the encoder or decoder network as described in appendix C\nof [Kingma and Welling (2013)](https://arxiv.org/abs/1312.6114).\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport torch.nn as nn\nfrom collections import OrderedDict\n\n\nclass CoderNetwork(nn.Module):\n    r\"\"\"Encoder/Decoder for use in VAE based on Kingma and Welling\n    \n    Args:\n        input_dim: input dimension (int)\n        output_dim: output dimension (int)\n        hidden_dim: hidden layer dimension (int)\n        coder_type: encoder/decoder type can be \n                   'Gaussian'   - Gaussian with diagonal covariance structure\n                   'I-Gaussian' - Gaussian with identity as covariance matrix \n                   'Bernoulli'  - Bernoulli distribution       \n    \"\"\"\n    \n    def __init__(self, input_dim, hidden_dim, output_dim, coder_type='Gaussian'):\n        super().__init__()\n        \n        assert coder_type in  ['Gaussian', 'I-Gaussian' ,'Bernoulli'], \\\n            'unknown coder_type'\n        \n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.coder_type = coder_type\n        \n        self.coder = nn.Sequential(OrderedDict([\n            ('h', nn.Linear(input_dim, hidden_dim)),\n            ('ReLU', nn.ReLU()) # ReLU instead of Tanh proposed by K. and W.       \n        ]))\n        self.fc_mu = nn.Linear(hidden_dim, output_dim)\n        \n        if coder_type == 'Gaussian':\n            self.fc_log_var = nn.Linear(hidden_dim, output_dim)\n        elif coder_type == 'Bernoulli':\n            self.sigmoid_mu = nn.Sigmoid()\n        return\n    \n    def forward(self, inp):\n        out = self.coder(inp)\n        mu = self.fc_mu(out)\n        \n        if self.coder_type == 'Gaussian':\n            log_var = self.fc_log_var(out)\n            return [mu, log_var]\n        elif self.coder_type == 'I-Gaussian':\n            return mu\n        elif self.coder_type == 'Bernoulli':\n            return self.sigmoid_mu(mu)\n        return\n```\n:::\n\n\n**Reconstruction Accuracy**: Sampling from the encoder distribution is\navoided by using the reparameterization trick, i.e., the latent\nvariable $\\textbf{z}^{(i)}$ is expressed as a deterministic variable \n\n$$\n  \\textbf{z}^{(i, l)}=g_{\\boldsymbol{\\phi}} (\\textbf{x}^{(i)}, \n  \\boldsymbol{\\epsilon}) = \\boldsymbol{\\mu}_E^{(i)} +\n  \\boldsymbol{\\sigma}_E^{(i)} \\odot \\boldsymbol{\\epsilon}^{(l)} \\quad\n  \\text{where} \n  \\quad \\boldsymbol{\\epsilon} \\sim\n  \\mathcal{N} (\\textbf{0}, \\textbf{I}),\n$$\n\nand $\\odot$ denotes element-wise multiplication.\n\nNote that we do not need to sample from the decoder distribution,\nsince during training the reconstruction accuracy in the objective\nfunction only sums the log-likelihood of each sample $\\textbf{z}^{(i,\nl)}$ and during test time we are mostly interested in the\nreconstructed $\\textbf{x}^{\\prime}$ with highest probability, i.e.,\nthe mean. \n\nThe reconstruction accuracy in the reparametrized form is given by\n\n$$ \n\\text{Reconstruction Accuracy} = \n   \\frac {1}{L} \n\\sum_{l=1}^{L} \\log p_{\\boldsymbol{\\theta}}\\left(\n  \\textbf{x}^{(i)}| \\textbf{z}^{(i,l)}\\right),\n$$\n\nwhere $L$ denotes the number of samples used during the\nreparameterization trick. Depending on the chosen decoder\ndistribution, the log-likelihood can be stated in terms of the\nestimated distribution parameters:\n\n- *Gaussian distribution with diagonal covariance structure* $p_{\\boldsymbol{\\theta}} \\sim \\mathcal{N} \\left( \\textbf{x}^\\prime | \\boldsymbol{\\mu}_D^{(i)} , \\text{diag} \\left( \\boldsymbol{\\sigma}_D^{2(i)} \\right) \\right)$\n  \n  $$\n   \\log_e p_{\\boldsymbol{\\theta}}\\left(\n  \\textbf{x}^{(i)}| \\textbf{z}^{(i,l)}\\right) = \\underbrace{- \\frac {D}{2} \\ln\n   2\\pi}_{\\text{const}} - \\frac {1}{2} \\ln \\left( \\prod_{k=1}^{D} \\sigma_{D_k}^{2(i)} \\right) \n  - \\frac {1}{2}\\sum_{k=1}^{D} \\frac {1}{\\sigma_{D_k}^{2(i)}}\\left( x_k^{(i)}  - \\mu_{D_k}^{(i)}\\right)^2\n  $$\n  \n  with the original observation $\\textbf{x}^{(i)} \\in \\mathbb{R}^{D}$.\n  In this form, the objective function is ill-posed since there are no\n  limitations on the form of the normal distribution. As a result the \n  objective function is unbounded, i.e., the VAE\n  could learn the true mean $\\boldsymbol{\\mu}_D^{(i)} =\n  \\textbf{x}^{(i)}$ with arbitrary variance\n  $\\boldsymbol{\\sigma}_D^{2(i)}$ or huge variances with arbitrary\n  means to maximize the log-likelihood (see \n  [this\n  post](https://stats.stackexchange.com/questions/373858/is-the-optimization-of-the-gaussian-vae-well-posed)).\n  Note that in the encoder network, the prior \n  $p_{\\boldsymbol{\\theta}}(\\textbf{z})$ is used to constrain the\n  encoder distribution (i.e., the mean and variance).\n\n- *Gaussian distribution with identity as covariance variance* $p_{\\boldsymbol{\\theta}} \\sim \\mathcal{N} \\left( \\textbf{x}^\\prime | \\boldsymbol{\\mu}_D^{(i)} , \\textbf{I} \\right)$\n  \n  $$\n   \\log_e p_{\\boldsymbol{\\theta}}\\left(\n  \\textbf{x}^{(i)}| \\textbf{z}^{(i,l)}\\right) = \n  - \\frac {1}{2}\\sum_{k=1}^{D} \\left( x_k^{(i)}  -\n  \\mu_{D_k}^{(i)}\\right)^2 + \\text{const}\n  $$\n  \n  \n  with the original observation $\\textbf{x}^{(i)} \\in \\mathbb{R}^{D}$.\n  In this case the reconstruction accuracy is proportional to the negative mean\n  squarred error which is typically used as the loss function in\n  standard autoencoders.\n  \n  <!-- https://stats.stackexchange.com/questions/373858/is-the-optimization-of-the-gaussian-vae-well-posed  -->\n  \n\n- *Bernoulli distribution* $p_{\\boldsymbol{\\theta}} \\sim\\text{Bern} \\left(\\textbf{x}^\\prime | \\boldsymbol{\\mu}_D^{(i)} \\right) =  \\prod_{k=1}^{D} \\left( \\mu_{D_k}^{(i)}\\right)^{x_k^\\prime} \\left( 1 - \\mu_{D_k}^{(i)}\\right)^{1 - x_k^\\prime}$  \n  \n  $$\n     \\log_e p_{\\boldsymbol{\\theta}}\\left(\n  \\textbf{x}^{(i)}| \\textbf{z}^{(i,l)}\\right) = \\sum_{k=1}^{D} \\left(\n     x_k^{(i)} \\ln \\left( \\mu_{D_k}^{(i)} \\right) + \\left(1 - x_k^{(i)}\n     \\right) \\ln \\left( 1 - \\mu_{D_k}^{(i)} \\right) \\right)\n  $$\n\n  with the original observation $\\textbf{x}^{(i)} \\in \\{0, 1\\}^{D}$.\n  In this case the reconstruction accuracy equals the\n  negative binary cross entropy loss. Note that there are plenty of\n  VAE implementations that use the binary cross entropy loss\n  on non-binary observations, see discussions in [this\n  thread](https://stats.stackexchange.com/questions/394582/why-is-binary-cross-entropy-or-log-loss-used-in-autoencoders-for-non-binary-da). \n  \n \nTo put this into practice, below is a simple VAE Python class which\nwill be used to compare the different decoder distributions.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport torch\nfrom torch.distributions.multivariate_normal import MultivariateNormal\n\n\nclass VAE(nn.Module):\n    r\"\"\"A simple VAE class based on Kingma and Welling\n        \n    Args:\n        encoder_network:  instance of CoderNetwork class\n        decoder_network:  instance of CoderNetwork class\n        L:                number of samples used during reparameterization trick\n    \"\"\"\n    \n    def __init__(self, encoder_network, decoder_network, L=1):\n        super().__init__()\n        self.encoder = encoder_network\n        self.decoder = decoder_network\n        self.L = L\n        \n        latent_dim = encoder_network.output_dim\n                \n        self.normal_dist = MultivariateNormal(torch.zeros(latent_dim), \n                                              torch.eye(latent_dim))\n        return\n    \n    def forward(self, x):\n        L = self.L\n        \n        z, mu_E, log_var_E = self.encode(x, L)\n        # regularization term per batch, i.e., size: (batch_size)\n        regularization_term = (1/2) * (1 + log_var_E - mu_E**2\n                                       - torch.exp(log_var_E)).sum(axis=1)\n        \n        # upsample x and reshape\n        batch_size = x.shape[0]\n        x_ups = x.repeat(L, 1).view(batch_size, L, -1)    \n        if self.decoder.coder_type == 'Gaussian':\n            # mu_D, log_var_D have shape (batch_size, L, output_dim)\n            mu_D, log_var_D = self.decode(z)\n            # reconstruction accuracy per batch, i.e., size: (batch_size)\n            recons_acc = (1/L) * (-(0.5)*(log_var_D.sum(axis=2)).sum(axis=1)\n               -(0.5) * ((1/torch.exp(log_var_D))*((x_ups - mu_D)**2)\n                         ).sum(axis=2).sum(axis=1))\n        elif self.decoder.coder_type == 'I-Gaussian':\n            # mu_D has shape (batch_size, L, output_dim)\n            mu_D = self.decode(z)\n            # reconstruction accuracy per batch, i.e., size: (batch_size)\n            recons_acc = (1/L) * (-(0.5) * ((x_ups - mu_D)**2\n                                            ).sum(axis=2).sum(axis=1))\n        elif self.decoder.coder_type == 'Bernoulli':\n            # mu_D has shape (batch_size, L, output_dim)\n            mu_D = self.decode(z)     \n            # reconstruction accuracy per batch, i.e., size: (batch_size)\n            # corresponds to the negative binary cross entropy loss (BCELoss)\n            recons_acc = (1/L) * (x_ups * torch.log(mu_D) + \n                                  (1 - x_ups) * torch.log(1 - mu_D)\n                                  ).sum(axis=2).sum(axis=1)\n        loss = - regularization_term.sum() - recons_acc.sum()\n        return loss\n    \n    def encode(self, x, L=1):\n        # get encoder distribution parameters\n        mu_E, log_var_E = self.encoder(x)\n        # sample noise variable L times for each batch\n        batch_size = x.shape[0]\n        epsilon = self.normal_dist.sample(sample_shape=(batch_size, L, ))\n        # upsample mu_E, log_var_E and reshape\n        mu_E_ups = mu_E.repeat(L, 1).view(batch_size, L, -1) \n        log_var_E_ups = log_var_E.repeat(L, 1).view(batch_size, L, -1)\n        # get latent variable by reparametrization trick\n        z = mu_E_ups + torch.sqrt(torch.exp(log_var_E_ups)) * epsilon\n        return z, mu_E, log_var_E\n    \n    def decode(self, z):\n        # get decoder distribution parameters\n        if self.decoder.coder_type == 'Gaussian':\n            mu_D, log_var_D = self.decoder(z)\n            return mu_D, log_var_D\n        elif self.decoder.coder_type == 'I-Gaussian':\n            mu_D = self.decoder(z)\n            return mu_D\n        elif self.decoder.coder_type == 'Bernoulli':\n            mu_D = self.decoder(z)\n            return mu_D\n        return\n```\n:::\n\n\nLet's train the three different VAEs on the MNIST digits dataset\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n\ndef train(decoder_type, dataset, x_dim, hid_dim, z_dim, batch_size, L, epochs):\n    encoder_network = CoderNetwork(input_dim=x_dim, \n                                   hidden_dim=hid_dim, \n                                   output_dim=z_dim,\n                                   coder_type='Gaussian')\n    decoder_network = CoderNetwork(input_dim=z_dim, \n                                   hidden_dim=hid_dim, \n                                   output_dim=x_dim,\n                                   coder_type=decoder_type)\n    \n    model = VAE(encoder_network, decoder_network, L=L)\n    data_loader = DataLoader(dataset, batch_size, shuffle=True)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n    \n    print('Start training with {} decoder distribution\\n'.format(decoder_type))\n    for epoch in range(1, epochs + 1):\n        print('Epoch {}/{}'.format(epoch, epochs))\n        avg_loss = 0\n        for counter, (mini_batch_data, label) in enumerate(data_loader):\n            \n            model.zero_grad()\n            \n            loss = model(mini_batch_data.view(-1, x_dim))\n            loss.backward()\n            optimizer.step()\n            \n            avg_loss += loss.item() / len(dataset)\n            \n            if counter % 20 == 0 or (counter + 1)==len(data_loader):\n                batch_loss = loss.item() / len(mini_batch_data)\n                print('\\r[{}/{}] batch loss: {:.2f}'.format(counter + 1,\n                                                            len(data_loader),\n                                                            batch_loss),\n                      end='', flush=True)\n        print('\\nAverage loss: {:.3f}'.format(avg_loss)) \n    print('Done!\\n')\n    trained_VAE = model\n    return trained_VAE\n\ndataset = datasets.MNIST('data/', transform=transforms.ToTensor(), download=True)\nx_dim, hid_dim, z_dim = 28*28, 400, 20\nbatch_size, L, epochs = 128, 5, 3\n\nBernoulli_VAE = train('Bernoulli', dataset, x_dim, hid_dim, z_dim, \n                      batch_size, L, epochs)\nGaussian_VAE = train('Gaussian', dataset, x_dim, hid_dim, z_dim, \n                     batch_size, L, epochs)\nI_Gaussian_VAE = train('I-Gaussian', dataset, x_dim, hid_dim, z_dim, \n                       batch_size, L, epochs)\n```\n:::\n\n\n<!-- Note how the Gaussian distribution with diagonal covariance structure -->\n<!-- (i.e., `Gaussian`) as decoder distribution approaches huge negative -->\n<!-- losses. These would further increase towards negative infinity when -->\n<!-- training for more epochs. In contrast, the `Bernoulli` and `I-Gaussian` -->\n<!-- distribution losses clearly saturate. -->\n\nLet's look at the differences in the reconstructions:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\ndef plot_results(trained_model, dataset, n_samples):\n    decoder_type = trained_model.decoder.coder_type\n    \n    fig = plt.figure(figsize=(14, 3))\n    fig.suptitle(decoder_type + ' Distribution: Observations (top row) and ' +\n                 'their reconstructions (bottom row)')\n    for i_sample in range(n_samples):\n        x_sample = dataset[i_sample][0].view(-1, 28*28)\n        \n        z, mu_E, log_var_E = trained_model.encode(x_sample, L=1)\n        if decoder_type in ['Bernoulli', 'I-Gaussian']:\n            x_prime = trained_model.decode(z)\n        else:\n            x_prime = trained_model.decode(z)[0]\n    \n        plt.subplot(2, n_samples, i_sample + 1)\n        plt.imshow(x_sample.view(28, 28).data.numpy())\n        plt.axis('off')\n        plt.subplot(2, n_samples, i_sample + 1 + n_samples)\n        plt.imshow(x_prime.view(28, 28).data.numpy())\n        plt.axis('off')\n    return\n\n\nn_samples = 10\n\nplot_results(Bernoulli_VAE, dataset, n_samples)\nplot_results(Gaussian_VAE, dataset, n_samples)\nplot_results(I_Gaussian_VAE, dataset, n_samples)\n```\n:::\n\n\n## Acknowledgement\n\n[Daniel Daza's](https://dfdazac.github.io/) blog was really helpful\nand the presented code is highly inspired by his [summary on\nVAEs](https://dfdazac.github.io/01-vae.html).\n\n\n---------------------------------------------------------------------------\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}