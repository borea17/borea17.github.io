{
  "hash": "4fecd50a84c72b97c652a410613e74b3",
  "result": {
    "markdown": "---\ntitle: \"Attend, Infer, Repeat: Fast Scene Understanding with Generative Models\"\ncategories: [\"reimplementation\", \"unsupervised\", \"generative\"]\ndate: \"2020-09-20\"\nexecute:\n  eval: false # true\nengine: jupyter\nformat:\n  html: \n    code-fold: show \n    highlight-style: github \n    code-block-bg: true\n    code-tools: \n      toggle: true\n      source: \"https://github.com/borea17/Notebooks/blob/master/05_Attend-Infer-Repeat.ipynb\"\n---\n\n<!-- nextjournal_link: \"https://nextjournal.com/borea17/attend-infer-repeat/\" -->\n\n[Eslami et al. (2016)](https://arxiv.org/abs/1603.08575) introduce the\n**Attend-Infer-Repeat (AIR)** framework as an end-to-end trainable\ngenerative model capable of decomposing multi-object scenes into its\nconstituent objects in an unsupervised learning setting. AIR builds\nupon the inductive bias that real-world scenes can be understood as a\ncomposition of (locally) self-contained objects. Therefore, AIR uses a\nstructured probabilistic model whose parameters are obtained by\ninference/optimization. As the name suggests, the image decomposition\nprocess can be abstracted into three steps:\n\n* **Attend**: Firstly, the model uses a [Spatial\n  Transformer (ST)](https://borea17.github.io/paper_summaries/spatial_transformer)\n  to focus on a specific region of the image, i.e., crop the image.\n* **Infer**: Secondly, the cropped image is encoded by a [Variational\n  Auto-Encoder (VAE)](https://borea17.github.io/paper_summaries/auto-encoding_variational_bayes).\n  Note the same VAE is used for every cropped image.\n* **Repeat**: Lastly, these steps are repreated until the full image\n  is described or the maximum number of repetitions is reached.\n\nNotably, the model can handle a variable number of objects\n(upper-bounded) by treating inference as an iterative process. As a\nproof of concept, they show that AIR could successfully learn to\ndecompose multi-object scenes in multiple datasets (multiple MNIST,\nSprites, Omniglot, 3D scenes).\n\n\n\n| ![Paper Results](./img/paper_results.gif \"Paper Results\") |\n| :--  |\n|  **Paper Results**. Taken from [this presentation](https://www.youtube.com/watch?v=4tc84kKdpY4). Note that the aim of unsupervised representation learning is to obtain good representations rather than perfect reconstructions.  |\n\n\n<!-- - aim: good representations for downstream tasks -->\n<!-- - scheme for efficient variational inference in latent spaces of -->\n<!--   variable dimensionality -->\n<!-- Motivated by human perception -->\n<!-- - produce representations that are more useful for downstream tasks  -->\n<!-- - structured models for image understanding -->\n<!-- - standard VAEs lack interpretations of latent space (unstructured** -->\n<!-- - AIR imposes structure on its representation through the generative -->\n<!--   model/process rather than supervision from labels -->\n<!-- - aiming to obtain good representations rather than good reconstructions -->\n\n\n## Model Description\n\nAIR is a rather sophisticated framework with some non-trivial\nsubtleties. For the sake of clarity, the following description is\norganized as follows:\nFirstly, a high-level overview of the main ideas is given. Secondly,\nthe transition from these ideas into a mathematical formulation\n(ignoring difficulties) is described. Lastly, the main difficulties\nare highlighted and how [Eslami et al.\n(2016)](https://arxiv.org/abs/1603.08575) proposed to tackle them.\n\n### High-Level Overview\n\nIn essence, the model can be understood as a special VAE architecture\nin which an image $\\textbf{x}$ is encoded to some kind of latent\ndistribution from which we sample the latent representation\n$\\textbf{z}$ which then can be decoded into an reconstructed\nimage $\\widetilde{\\textbf{x}}$, see image below. The main idea by [Eslami et al.\n(2016)](https://arxiv.org/abs/1603.08575) consists of imposing\nadditional structure in the model using the inductive bias that\nreal-world scenes can often be approximated as multi-object scenes,\ni.e., compositions of several (variable number) objects. Additionally,\nthey assume that all of these objects live in the same domain, i.e.,\neach object is an instantiation from the same class.\n\n| ![Standard VAE Architecture](./img/standard_VAE.png \"Standard VAE Architecture\") |\n| :---       |\n| **Standard VAE Architecture**. AIR can be understood as a modified VAE architecture. |\n\nTo this end, [Eslami et al. (2016)](https://arxiv.org/abs/1603.08575)\nreplace the encoder with an recurrent, variable-length inference\nnetwork to obtain a group-structured latent representation.\nEach group $\\textbf{z}^{(i)}$ should ideally correspond to one object\nwhere the entries can be understood as the compressed attributes of\nthat object (e.g., type, appearance, pose). The main purpose of the\ninference network is to explain the whole scene by iteratively\nupdating what remains to be explained, i.e., each step is conditioned\non the image and on its knowledge of previously explained objects, see\nimage below. Since they assume that each object lives in the same\ndomain, the decoder is applied group-wise, i.e., each vector\n$\\textbf{z}^{(i)}$ is fed through the same decoder network, see image\nbelow.\n\n| ![VAE with Recurrent Inference Network](./img/VAE_inference.png \"VAE with Recurrent Inference Network\") |\n| :---       |\n| **VAE with Recurrent Inference Network**. A group-structured latent representation is obtained by replacing the encoder with a recurrent, variable-length inference network. This network should ideally attend to one object at a time and is conditioned on the image $\\textbf{x}$ and its knowledge of previously epxlained objects $\\textbf{h}$, $\\textbf{z}$. |\n\n[Eslami et al. (2016)](https://arxiv.org/abs/1603.08575) put\nadditional structure to the model by dividing the latent space of\neach object into `what`, `where` and `pres`. As the names suggest,\n$\\textbf{z}^{(i)}_{\\text{what}}$ corresponds to the objects\nappearance, while $\\textbf{z}^{(i)}_{\\text{where}}$ gives information\nabout the position and scale. $\\text{z}_{\\text{pres}}^{(i)}$ is\na binary variable describing whether an object is present, it is\nrather a helper variable to allow for a variable number of objects to\nbe detected (going to be explained in the [Difficulties section](https://borea17.github.io/paper_summaries/AIR#difficulties)).\n\nTo disentangle `what`from `where`, the inference network extracts\nattentions crops $\\textbf{x}^{(i)}_{\\text{att}}$ of the image\n$\\textbf{x}$ based on a three-dimensional vector\n$\\textbf{z}^{(i)}_{\\text{where}} \\left( \\textbf{h}^{(i)} \\right)$\nwhich specifies the affine parameters $(s^{(i)}, t_x^{(i)}, t_y^{(i)})$ of the attention\ntransformation[^1]. These attention crops are then put through a\nstandard VAE to encode the latent `what`-vector\n$\\textbf{z}^{(i)}_{\\text{what}}$. Note that each attention crop is\nput through the same VAE, thereby consistency between compressed\nobject attributes is achieved (i.e., each object is an instantiation\nof the same class).\n\nOn the decoder side, the reconstructed attention crop\n$\\widetilde{\\textbf{x}}^{(i)}_{\\text{att}}$ is transformed to\n$\\widetilde{\\textbf{x}}^{(i)}$ using the information from\n$\\textbf{z}^{(i)}_{\\text{where}}$. $\\widetilde{\\textbf{x}}^{(i)}$ can\nbe understood as a reconstructed image of the $i$-th object in the\noriginal image $\\textbf{x}$. Note that\n$\\text{z}^{(i)}_{\\text{pres}}$ is used to decide whether the\ncontribution of $\\widetilde{\\textbf{x}}^{(i)}_{\\text{att}}$ is added\nto the otherwise empty canvas $\\widetilde{\\textbf{x}}^{(i)}$.\n\nThe schematic below summarizes the whole AIR architecture.\n\n[^1]: Visualization of a standard attention transformation, for more\n    details refer to my [Spatial Transformer\n    description](https://borea17.github.io/paper_summaries/spatial_transformer#model-description).\n\n    | ![Attention Transfrom](./img/attention_transform.gif \"Attention Transform\") |\n    | :--  |\n    |  This transformation is more constrained with only 3-DoF. Therefore it only allows cropping, translation and isotropic scaling to be applied to the input feature map.|\n\n| ![Schematic of AIR](./img/AIR_model2.png \"Schematic of AIR\") |\n| :---:      |\n| **Schematic of AIR**                                          |\n\n**Creation of Attention Crops and Inverse Transformation**: As stated\nbefore, a [Spatial\n  Transformer (ST)](https://borea17.github.io/paper_summaries/spatial_transformer) module is used to produce the\nattention crops using a standard attention transformation. Remind that\nthis means that the regular grid $\\textbf{G} = \\{\\begin{bmatrix}\nx_k^t & y_k^t \\end{bmatrix}^{\\text{T}} \\}$ defined on the output is\ntransformed into a new sampling grid $\\widetilde{\\textbf{G}} = \\{\\begin{bmatrix}\nx_k^s & y_k^s \\end{bmatrix}^{\\text{T}} \\}$ defined\non the input. The latent vector $\\textbf{z}^{(i)}_{\\text{where}}$ can\nbe used to build the attention transformation matrix, i.e.,\n\n$$\n  \\textbf{A}^{(i)} = \\begin{bmatrix} s^{(i)} & 0 & t_x^{(i)} \\\\\n   0 & s^{(i)} & t_y^{(i)} \\\\ 0 & 0 & 1\\end{bmatrix}, \\quad \\quad \\quad\n  \\begin{bmatrix} x_k^s \\\\ y_k^s \\\\ 1 \\end{bmatrix} = \\textbf{A}^{(i)}\n  \\begin{bmatrix} x_k^t \\\\ y_k^t \\\\ 1\\end{bmatrix}\n$$\n\nThis is nothing new, but how do we map the reconstructed attention\ncrop $\\tilde{\\textbf{x}}^{(i)}_{\\text{att}}$ back to the original\nimage space, i.e., how can we produce $\\widetilde{\\textbf{x}}^{(i)}$\nfrom $\\widetilde{\\textbf{x}}^{(i)}_{\\text{att}}$ and\n$\\textbf{z}^{(i)}_{\\text{where}}$? The answer is pretty simple, we\nuse the (pseudo)inverse[^2] of the formerly defined attention transformation\nmatrix, i.e.,\n\n$$\n\\begin{bmatrix} x_k^s \\\\ y_k^s \\\\ 1 \\end{bmatrix} = \\left(\\textbf{A}^{(i)}\\right)^{+}\n  \\begin{bmatrix} x_k^t \\\\ y_k^t \\\\ 1\\end{bmatrix} \\stackrel{s\\neq\n  0}{=} \\begin{bmatrix} \\frac {1}{s^{(i)}} & 0 & - \\frac{t_x^{(i)}}{s} \\\\\n   0 & \\frac {1}{s^{(i)}} & -\\frac {t_y^{(i)}}{s} \\\\ 0 & 0 &\n   1\\end{bmatrix}\\begin{bmatrix} x_k^t \\\\ y_k^t \\\\ 1\\end{bmatrix},\n$$\n\nwhere  $\\left(\\textbf{A}^{(i)}\\right)^{+}$ denotes the Moore-Penrose\ninverse of $\\textbf{A}^{(i)}$, and the regular grid $\\textbf{G} = \\{\\begin{bmatrix}\nx_k^t & y_k^t \\end{bmatrix}^{\\text{T}} \\}$ is now defined on the\noriginal image space[^3]. Below is a self-written interactive\nvisualization where $\\widetilde{\\textbf{x}}^{(i)}_{\\text{att}} =\n\\textbf{x}^{(i)}_{\\text{att}}$. It shows nicely that the whole\nprocess can abstractly be understood as cutting of a crop from the\noriginal image and placing the reconstructed version with the inverse\nscaling and shifting on an otherwise empty (black) canvas. The code and visualization can be\nfound [here](https://github.com/borea17/InteractiveTransformations).\n\n\n| ![Interactive Transformation Visualization](./img/transformation.gif \"Interactive Transformation Visualization\") |\n| :--: |\n|  **Interactive Transformation Visualization** |\n\n[^2]: Using the pseudoinverse (or Moore-Penrose inverse) is beneficial\n    to allow inverse mappings even if $\\textbf{A}^{(i)}$ becomes\n    non-invertible, i.e., if $s^{(i)} = 0$.\n\n[^3]: Note that we assume normalized coordinates with same\n    resolutions such that the notation is not completely messed up.\n\n### Mathematical Model\n\nWhile the former model description gave an overview about the\ninner workings and ideas of AIR, the following section introduces the\nprobabilistic model over which AIR operates. Similar to the [VAE\npaper](https://borea17.github.io/paper_summaries/auto-encoding_variational_bayes)\nby [Kingma and Welling (2013)](https://arxiv.org/abs/1312.6114),\n[Eslami et al. (2016)](https://arxiv.org/abs/1603.08575) introduce a\nmodeling assumption for the generative process and use a variational\napproximation for the true posterior of that process to allow for\njoint optimization of the inference (encoder) and generator (decoder)\nparameters.\n\nIn contrast to standard VAEs, the modeling assumption for the\ngenerative process is more structured in AIR, see image below. It\nassumes that:\n\n1. The number of objects $n$ is sampled from some discrete prior\ndistribution $p_N$ (e.g., geometric distribution) with maximum value\n$N$.\n2. The latent scene descriptor $\\textbf{z} =\n\\left(\\textbf{z}^{(1)}, \\textbf{z}^{(2)}, \\dots, \\textbf{z}^{(n)}\n\\right)$ (length depends on sampled $n$) is sampled from a scene model\n$\\textbf{z} \\sim p_{\\boldsymbol{\\theta}}^{z} \\left( \\cdot | n\n\\right)$, where each vector $\\textbf{z}^{(i)}$ describes the\nattributes of one object in the scene. Furthermore, [Eslami et al.\n(2016)](https://arxiv.org/abs/1603.08575) assume that\n$\\textbf{z}^{(i)}$ are independent for each possible $n$, i.e.,\n$p_{\\boldsymbol{\\theta}}^{z} \\left( \\textbf{z} | n \\right) =\n\\prod_{i=1}^n p_{\\boldsymbol{\\theta}}^z \\left( \\textbf{z}^{(i)}\\right)$.\n3. $\\textbf{x}$ is generated by sampling from the conditional\ndistribution $p_{\\boldsymbol{\\theta}}^{x} \\left( \\textbf{x} |\n\\textbf{z} \\right)$.\n\nAs a result, the marginal likelihood of an image\ngiven the generative model parameters can be stated as follows\n\n$$\n p_{\\boldsymbol{\\theta}} (\\textbf{x}) = \\sum_{n=1}^N p_N (n) \\int\n p_{\\boldsymbol{\\theta}}^z \\left( \\textbf{z} | n \\right)\n p_{\\boldsymbol{\\theta}}^x \\left( \\textbf{x} | \\textbf{z}\\right) d \\textbf{z}\n$$\n\n| ![Generative Model VAE vs AIR](./img/VAE_vs_AIR.png \"Generative Model VAE vs AIR\") |\n| :--  |\n|  **Generative Model VAE vs AIR**. Note that for a given dataset $\\textbf{X} = \\{ \\textbf{x}^{(i)}\\}_{i=1}^{L}$ the marginal likelihood of the whole dataset can be computed via $p_{\\boldsymbol{\\theta}} ( \\textbf{X} ) = \\prod_{i=1}^{L} p_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)} \\right) $. |\n\n\n**Learning by optimizing the ELBO**: Since the integral is intractable\nfor most models, [Eslami et al. (2016)](https://arxiv.org/abs/1603.08575) introduce an amortized[^4]\nvariational approximation\n$q_{\\boldsymbol{\\phi}} \\left(\\textbf{z}, n | \\textbf{x}\\right)$\nfor the true posterior $p_{\\boldsymbol{\\theta}}\\left(\\textbf{z}, n\n|\\textbf{x}\\right)$.\nFrom here on, the steps are very similar to the [VAE paper](https://borea17.github.io/paper_summaries/auto-encoding_variational_bayes)\nby [Kingma and Welling (2013)](https://arxiv.org/abs/1312.6114): The\nobjective of minimizing the KL divergence between the parameterized\nvariational approximation (using a neural network) and the true (but\nunknown) posterior $p_{\\boldsymbol{\\theta}}\\left(\\textbf{z}, n\n|\\textbf{x}\\right)$\nis approximated by maximizing the evidence lower bound\n([ELBO](https://borea17.github.io/ML_101/probability_theory/evidence_lower_bound)):\n\n$$\n\\mathcal{L} \\left( \\boldsymbol{\\theta}, \\boldsymbol{\\phi};\n\\textbf{x}^{(i)} \\right) = \\underbrace{- D_{KL} \\left( q_{\\boldsymbol{\\phi}}\n\\left( \\textbf{z}, n | \\textbf{x}^{(i)}\\right) || p_{\\boldsymbol{\\theta}}\n(\\textbf{z}, n)\\right)}_{\\text{Regularization Term}} + \\underbrace{\\mathbb{E}_{q_{\\boldsymbol{\\phi}} \\left(\n\\textbf{z}, n | \\textbf{x}^{(i)} \\right)} \\left[ \\log\np_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)} | \\textbf{z}, n\n\\right) \\right]}_{\\text{Reconstruction Accuracy}},\n$$\n\nwhere $p_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)} | \\textbf{z},\n n \\right)$ is a parameterized probabilistic decoder[^5] (using a neural\n network) and $p_{\\boldsymbol{\\theta}} (\\textbf{z}, n) =\n p_{\\boldsymbol{\\theta}} \\left(\\textbf{z} | n \\right)\n p \\left( n \\right)$ is prior on the joint\n probability of $\\textbf{z}$ and $n$ that we need to define a priori.\n As a result, the optimal parameters $\\boldsymbol{\\theta}$,\n $\\boldsymbol{\\phi}$ can be learnt jointly by optimizing (maximizing)\n the ELBO.\n\n[^4]: `Amortized` variational approximation means basically\n    `parameterized` variational approximation, i.e., we introduce a\n    parameterized function $q_{\\boldsymbol{\\phi}} \\left( \\textbf{z},\n    n | \\textbf{x}\\right)$ (e.g., neural network parameterized by\n    $\\boldsymbol{\\phi}$) that maps from an image $\\textbf{x}$ to the\n    distribution parameters for number of objects $n$ and their latent\n    representation $\\textbf{z}$, see [this excellent answer on\n    variational\n    inference](https://www.quora.com/What-is-amortized-variational-inference).\n\n[^5]: The probabilistic decoder $p_{\\boldsymbol{\\theta}} \\left(\n    \\textbf{x}^{(i)} | \\textbf{z}, n \\right)$ is also just an\n    approximation to the true generative process. However, note that\n    for each $\\textbf{x}^{(i)}$ we know how the reconstruction should\n    look like. I.e., if $p_{\\boldsymbol{\\theta}} \\left(\n    \\textbf{x}^{(i)} | \\textbf{z}, n \\right)$ approximates the true\n    generative process, we can optimize it by maximizing its\n    expectation for given $\\textbf{z}$ and $n$ sampled from the\n    approximate true posterior $q_{\\boldsymbol{\\phi}} \\left(\n    \\textbf{z}, n | \\textbf{x}^{(i)}\\right)$.\n\n\n### Difficulties\n\nIn the former explanation, it was assummed that we could easily define\nsome parameterized probabilistic encoder $q_{\\boldsymbol{\\phi}} \\left(\n\\textbf{z}, n | \\textbf{x}^{(i)} \\right)$ and\ndecoder $p_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)} |\n\\textbf{z}, n \\right)$ using neural networks. However, there are some\nobstacles in our way:\n\n- How can we infer a variable number of objects $n$? Actually, we\n  would need to evaluate $p_N \\left(n | \\textbf{x}\\right) = \\int\n  q_{\\boldsymbol{\\phi}} \\left(\\textbf{z}, n | \\textbf{x} \\right)\n  d \\textbf{z}$ for all $n=1,\\dots, N$ and then sample from the\n  resulting distribution.\n  <!-- Depending on the maximum number of objects -->\n  <!-- $N$, this would quickly become computationally inefficient. -->\n\n- The number of objects $n$ is clearly a discrete variable. How can we\n  backprograte if we sample from a discrete distribution?\n\n- What priors should we choose? Especially, the prior for the number of\n  objects in a scene $n \\sim p_N$ is unclear.\n\n- What the `first` or `second` object in a scene constitutes is\n  somewhat arbitrary. As a result, object assigments\n  $\\begin{bmatrix} \\textbf{z}^{(1)} & \\dots & \\textbf{z}^{(n)}\n  \\end{bmatrix} =\\textbf{z} \\sim q_{\\boldsymbol{\\phi}} \\left(\\textbf{z} |\n  \\textbf{x}^{(i)}, n \\right)$\n  should be exchangeable and the decoder $p_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)} |\n\\textbf{z}, n \\right)$ should be permutation invariant in terms of\n  $\\textbf{z}^{(i)}$. Thus, the latent representation needs to\n  preserve some strong symmetries.\n\n[Eslami et al. (2016)](https://arxiv.org/abs/1603.08575) tackle these\nchallenges by defining inference as an iterative process using a\nrecurrent neural network (RNN) that is run for $N$ steps (maximum number of\nobjects). As a result, the number of objects $n$ can be encoded in the latent\ndistribution by defining the approximated posterior as follows\n\n$$\n  q_{\\boldsymbol{\\phi}} \\left( \\textbf{z}, \\textbf{z}_{\\text{pres}} |\n  \\textbf{x} \\right) = q_{\\boldsymbol{\\phi}} \\left(\n  z_{\\text{pres}}^{(n+1)} = 0 | \\textbf{z}^{(1:n)} , \\textbf{x}\\right)\n  \\prod_{i=1}^n q_{\\boldsymbol{\\phi}} \\left( \\textbf{z}^{(i)} ,\n  z_{\\text{pres}}^{(i)}=1 | \\textbf{x}, \\textbf{z}^{(1:i-1)}\\right),\n$$\n\nwhere $z_{\\text{pres}}^{(i)}$ is an introduced binary variable sampled from a\nBernoulli distribution $z_{\\text{pres}}^{(i)} \\sim \\text{Bern} \\left(\np_{\\text{pres}}^{(i)} \\right)$ whose probability $p_{\\text{pres}}^{(i)}$ is\npredicted at each iteration step. Whenever $z_{\\text{pres}}^{(i)}=0$ the\ninference process stops and no more objects can be described, i.e., we enforce $z_{\\text{pres}}^{(i+1)}=0$ for all\nsubsequent steps such that the vector $\\textbf{z}_{\\text{pres}}$ looks as follows\n\n$$\n\\textbf{z}_{\\text{pres}} = \\begin{bmatrix} \\smash[t]{\\overbrace{\\begin{matrix}1 & 1 & \\dots &\n1\\end{matrix}}^{n \\text{ times}}}   & 0 &\\dots & 0 \\end{bmatrix}\n$$\n\n\nThus, $z_{\\text{pres}}^{(i)}$ may be understood as an *interruption\nvariable*. Recurrence is required to avoid explaining the same object\ntwice.\n\n**Backpropagation for Discrete Variables**: While we can easily draw\nsamples from a Bernoulli distribution $z_{\\text{pres}}^{(i)} \\sim \\text{Bern} \\left(\np_{\\text{pres}}^{(i)} \\right)$,\nbackpropagation turns out to be problematic. Remind that for continuous\nvariables such as Gaussian distributions parameterized by mean and\nvariance (e.g., $\\textbf{z}^{(i)}_{\\text{what}}$,\n$\\textbf{z}^{(i)}_{\\text{where}}$) there is the reparameterization\ntrick to circumvent this problem. However, any reparameterization of\ndiscrete variables includes discontinuous operations through which we\ncannot backprograte. Thus, [Eslami et al.\n(2016)](https://arxiv.org/abs/1603.08575)  use a variant of the [score-function\nestimator](https://borea17.github.io/ML_101/probability_theory/score_function_estimator)\nas a gradient estimator. More precisely, the reconstruction accuracy\ngradient w.r.t. $\\textbf{z}_{\\text{pres}}$ is approximated by the\nscore-function estimator, i.e.,\n\n$$\n\\begin{align}\n\\nabla_{\\boldsymbol{\\phi}}\\mathbb{E}_{q_{\\boldsymbol{\\phi}} \\left(\n\\textbf{z}, n | \\textbf{x}^{(i)} \\right)} \\left[ \\log\np_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)} | \\textbf{z}, n\n\\right) \\right] &= \\mathbb{E}_{q_{\\boldsymbol{\\phi}} \\left(\n\\textbf{z}, n | \\textbf{x}^{(i)} \\right)} \\left[ \\log\np_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)} | \\textbf{z}, n\n\\right) \\nabla_{\\boldsymbol{\\phi}} q_{\\boldsymbol{\\phi}} \\left(\n\\textbf{z}, n | \\textbf{x}^{(i)} \\right) \\right] \\\\\n&\\approx \\frac {1}{N} \\sum_{k=1}^N \\log p_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)} | \\left(\\textbf{z}, n\\right)^{(k)}\n\\right) \\nabla_{\\boldsymbol{\\phi}} q_{\\boldsymbol{\\phi}} \\left(\n \\left(\\textbf{z}, n\\right)^{(k)} | \\textbf{x}^{(i)} \\right)\\\\\n &\\quad \\text{with} \\quad \\left(\\textbf{z}, n\\right)^{(k)} \\sim q_{\\boldsymbol{\\phi}} \\left(\n\\textbf{z}, n | \\textbf{x}^{(i)} \\right)\n\\end{align}\n$$\n\n[Eslami et al. (2016)](https://arxiv.org/abs/1603.08575) note that in\nthis raw form the gradient estimate `is likely to have high variance`.\nTo reduce variance, they use `appropriately structured neural\nbaselines` citing a paper from [Minh and Gregor,\n2014](https://arxiv.org/abs/1402.0030). Without going into too much detail,\nappropriately structured neural baselines build upon the idea of [variance\nreduction in score function estimators](https://borea17.github.io/ML_101/probability_theory/better_score_function_estimator)\nby introducing a scalar baseline $\\lambda$ as follows\n\n$$\n\\begin{align}\n&\\nabla_{\\boldsymbol{\\phi}} \\mathbb{E}_{q_{\\boldsymbol{\\phi}} \\left(\n\\textbf{z}_{\\text{pres}} | \\textbf{x}^{(i)} \\right)} \\left[ \\log\np_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)} | \\textbf{z}, n\n\\right) \\right] = \\mathbb{E}_{q_{\\boldsymbol{\\phi}} \\left(\n\\textbf{z}, n | \\textbf{x}^{(i)} \\right)} \\left[ \\Big(\nf_{\\boldsymbol{\\theta}} \\left( \\textbf{x}, \\textbf{z} \\right) - \\lambda  \\Big)\n\\nabla_{\\boldsymbol{\\phi}} q_{\\boldsymbol{\\phi}} \\left(\\textbf{z}, n |\n\\textbf{x}^{(i)} \\right) \\right]\\\\\n&\\text{with} \\quad f_{\\boldsymbol{\\theta}} \\left( \\textbf{x}, \\textbf{z} \\right)\n= \\log p_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)} | \\textbf{z}, n \\right), \\quad\n\\text{since} \\quad\\mathbb{E}_{q_{\\boldsymbol{\\phi}} \\left(\n\\textbf{z}_{\\text{pres}} | \\textbf{x}^{(i)} \\right)} \\left[ \\nabla_{\\boldsymbol{\\phi}} q_{\\boldsymbol{\\phi}} \\left(\\textbf{z}, n |\n\\textbf{x}^{(i)} \\right) \\right] = \\textbf{0}.\n\\end{align}\n$$\n\n[Minh and Gregor, 2014](https://arxiv.org/abs/1402.0030) propose to use a\ndata-dependent neural baseline $\\lambda_{\\boldsymbol{\\psi}} (\\textbf{x})$ that\nis trained to match its target $f_{\\boldsymbol{\\theta}}$. For further reading,\n[pyro's SVI part III](https://pyro.ai/examples/svi_part_iii.html#Reducing-Variance-with-Data-Dependent-Baselines)\nis a good starting point.\n\n\n**Prior Distributions**: Before we take a closer look on the prior distribution,\nit will be helpful to rewrite the regularization term\n\n$$\n\\begin{align}\nD_{KL} & \\left(q_{\\boldsymbol{\\phi}} \\left(\\textbf{z}, n | \\textbf{x}^{(i)}\n\\right) || p_{\\boldsymbol{\\theta}} \\left( \\textbf{z}, n\\right) \\right) = D_{KL}\n\\left( \\prod_{i=1}^n q_{\\boldsymbol{\\phi}} \\left(\\textbf{z}^{(i)}| \\textbf{x},\n\\textbf{z}^{(1:i-1)} \\right) || \\prod_{i=1}^n p_{\\boldsymbol{\\theta}} \\left(\n\\textbf{z}^{(i)} \\right) \\right)\\\\\n&\\stackrel{\\text{independent dists.}}{=} \\sum_{i=1}^n D_{KL} \\left[\n\\prod_{k=1}^{3} q_{\\boldsymbol{\\phi}} \\left(\\textbf{z}^{(i)}_k |  \\textbf{x},\n\\textbf{z}^{(1:i-1)} \\right) || \\prod_{k=1}^3 p_{\\boldsymbol{\\theta}} \\left(\n\\textbf{z}^{(i)}_k \\right)  \\right]\\\\\n&\\stackrel{\\text{independent dists.}}{=} \\sum_{i=1}^n \\sum_{k\\in \\{\\text{pres},\n\\text{where}, \\text{what}\\}} D_{KL} \\left[ q_{\\boldsymbol{\\phi}} \\left(\\textbf{z}^{(i)}_k| \\textbf{x},\n\\textbf{z}^{(1:i-1)} \\right) || p_{\\boldsymbol{\\theta}} \\left(\n\\textbf{z}^{(i)}_k \\right)  \\right]\n\\end{align}\n$$\n\nNote that we assume that each $\\textbf{z}_k^{(i)}$ is sampled independently from\ntheir respective distribution such that products could equally be rewritten as\nconcatenated vectors. Clearly, there are three different prior distributions\nthat we need to define in advance:\n\n* $p_{\\boldsymbol{\\theta}} \\left(\\textbf{z}_{\\text{what}}^{(i)} \\right) \\sim\n  \\mathcal{N} \\left(\\textbf{0}, \\textbf{I} \\right)$: A centerd isotropic Gaussian prior\n  is a typical choice in standard VAEs and has proven to be effective[^6].\n  Remind that the `what`-VAE should ideally receive patches of standard MNIST digits.\n\n[^6]: On a standard MNIST dataset, [Kingma and Welling\n    (2013)](https://arxiv.org/abs/1312.6114) successfully used the centered\n    isotropic multivariate Gaussian as a prior distribution.\n\n* $p_{\\boldsymbol{\\theta}} \\left(\\textbf{z}_{\\text{where}}^{(i)}\\right) \\sim\n  \\mathcal{N} \\left( \\boldsymbol{\\mu}_{\\text{w}} ,\n  \\boldsymbol{\\sigma}_{\\text{w}}^2 \\textbf{I}  \\right)$: In this distribution,\n  we can encode prior knowledge about the objects locality, i.e.,\n  average size and location of objects and their standard deviations.\n\n* $p_{\\boldsymbol{\\theta}} \\left(\\textbf{z}_{\\text{pres}}^{(i)}\\right) \\sim\n  \\text{Bern} (p_{\\text{pres}})$: [Eslami et\nal. (2016)](https://arxiv.org/abs/1603.08575) used an annealing geometric\n  distribution as a prior on the number of objects[^7], i.e., the success\n  probability decreases from a value close to 1 to some small value close to 0\n  during the course of the training. The intuitive idea behind this process is\n  to encourage the model to explore the use of objects (in the initial phase),\n  and then to constrain the model to use as few objects as possible (trade-off\n  between number of objects and reconstruction accuracy).\n\n  For simplicity, we use a fixed Bernoulli distribution for each step as\n  suggested in [the pyro tutorial](https://pyro.ai/examples/air.html#In-practice) with\n  $p_{\\text{pres}} = 0.01$, i.e., we will constrain the number of objects from\n  the beginning. To encourage the model to use objects we initialize the\n  `what`-decoder to produce empty scenes such that things do not get much worse\n  in terms of reconstruction accuracy when objects are used (also inspired by\n  [pyro](https://pyro.ai/examples/air.html#In-practice)).\n\n\n[^7]: That [Eslami et al. (2016)](https://arxiv.org/abs/1603.08575) used an\n    anealing geometric distribution is not mentioned in their paper, however\n    Adam Kosiorek emailed the authors and received that information, see [his\n    blog post]([this blog\n    post](http://akosiorek.github.io/ml/2017/09/03/implementing-air.html)).\n\n\n\n## Implementation\n\nThe following reimplementation aims to reproduce the results of the\nmulti-MNIST experiment, see image below. We will make some adaptations inspired\nby [this pyro tutorial](https://pyro.ai/examples/air.html) and [this pytorch\nreimplementation](https://github.com/addtt/attend-infer-repeat-pytorch) from\nAndrea Dittadi. As a result, the following reimplementation receives a huge\nspeed up in terms of convergence time and can be trained in less than 10 minutes on a\nNvidia Tesla K80 (compared to 2 days on a Nvidia Quadro K4000 GPU by [Eslami et\nal. (2016)](https://arxiv.org/abs/1603.08575)).\n\nAs noted by [Eslami et al. (2016)](https://arxiv.org/abs/1603.08575), their\nmodel successfully learned to count the number of digits and their location in\neach image (i.e., appropriate attention windows) without any supervision.\nFurthermore, the scanning policy of the inference network (i.e., object\nassignment policy) converges to spatially divided regions where the direction of\nthe spatial border seems to be random (dependent on random initialization).\nLastly, the model also learned that it never needs to assign a third object (all\nimages in the training dataset contained a maximum of two digits).\n<!-- This ensures that different regions are -->\n<!-- assigned as different objects. -->\n\n| ![Multi-MNIST Paper Results](./img/paper_results.png \"Multi-MNIST Paper Results\") |\n| :--  |\n|  **Paper Results of Multi-MNIST Experiment**. Taken from [Eslami et al. (2016)](https://arxiv.org/abs/1603.08575). |\n\n[Eslami et al. (2016)](https://arxiv.org/abs/1603.08575) argue that the\nthe structure of AIR puts an important inductive bias onto explaining\nmulti-object scenes by using two adversaries:\n* AIR wants to explain the scene, i.e., the reconstruction error should be\n  minimized.\n* AIR is penalized for each instantiated object due to the KL divergence.\n  Furthermore, the `what`-VAE puts an additional prior of instantiating similar\n  objects.\n\n### Multi-MNIST Dataset\n\nThe multi-MNIST datasets consists of $50 \\times 50$ gray-scale images containing\nzero, one or two non-overlapping random MNIST digits with equal probability, see\nimage below. This dataset can easily be generated by taking a blank $50 \\times\n50$ canvas and positioning a random number of digits (drawn uniformly from MNIST\ndataset) onto it. To ensure that MNIST digits ($28\\times28$) will not overlap,\nwe scale them to $24\\times 24$ and then position them such that the centers of\ntwo MNIST digits do not overlap. Note that some small overlap may occur which we\nsimply accept. At the same time, we record the number of digits in each\ngenerated image to measure the count accuracy during training.\n\n| ![Multi-MNIST Dataset Examples](./img/multi-MNIST_dataset.png \"Multi-MNIST Dataset Examples\") |\n| :---: |\n|  **Multi-MNIST Dataset Examples**. |\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport torch\nimport numpy as np\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import TensorDataset\n\nCANVAS_SIZE = 50                # canvas in which 0/1/2 MNIST digits are put\nMNIST_SIZE = 24                 # size of original MNIST digits (resized)\n\ndef generate_dataset(num_images, SEED=1):\n    \"\"\"generates multiple MNIST dataset with 0, 1 or 2 non-overlaping digits\n\n    Args:\n        num_images (int): number of images inside dataset\n\n    Returns:\n        multiple_MNIST (torch dataset)\n    \"\"\"\n    data = torch.zeros([num_images, 1, CANVAS_SIZE, CANVAS_SIZE])\n\n    original_MNIST = datasets.MNIST('./data', train=True, download=True,\n        transform=transforms.Compose([\n          transforms.Resize(size=(MNIST_SIZE, MNIST_SIZE)),\n          transforms.ToTensor()]))\n    # sample random digits and positions\n    np.random.seed(SEED)\n    pos_positions = np.arange(int(MNIST_SIZE/2),CANVAS_SIZE - int(MNIST_SIZE/2))\n\n    mnist_indices = np.random.randint(len(original_MNIST), size=(num_images, 2))\n    num_digits = np.random.randint(3, size=(num_images))\n    positions_0 = np.random.choice(pos_positions, size=(num_images, 2),\n                                   replace=True)\n\n    for i_data in range(num_images):\n        if num_digits[i_data] > 0:\n            # add random digit at random position\n            random_digit = original_MNIST[mnist_indices[i_data][0]][0]\n            x_0, y_0 = positions_0[i_data][0], positions_0[i_data][1]\n            x = [x_0-int(MNIST_SIZE/2), x_0+int(MNIST_SIZE/2)]\n            y = [y_0-int(MNIST_SIZE/2), y_0+int(MNIST_SIZE/2)]\n            data[i_data,:,y[0]:y[1],x[0]:x[1]] += random_digit\n            if num_digits[i_data] == 2:\n                # add second non overlaping random digit\n                random_digit = original_MNIST[mnist_indices[i_data][1]][0]\n                impos_x_pos = np.arange(x_0-int(MNIST_SIZE/2),\n                                        x_0+int(MNIST_SIZE/2))\n                impos_y_pos = np.arange(y_0-int(MNIST_SIZE/2),\n                                        y_0+int(MNIST_SIZE/2))\n                x_1 = np.random.choice(np.setdiff1d(pos_positions, impos_x_pos),\n                                       size=1)[0]\n                y_1 = np.random.choice(np.setdiff1d(pos_positions, impos_y_pos),\n                                       size=1)[0]\n                x = [x_1-int(MNIST_SIZE/2), x_1+int(MNIST_SIZE/2)]\n                y = [y_1-int(MNIST_SIZE/2), y_1+int(MNIST_SIZE/2)]\n                data[i_data,:,y[0]:y[1],x[0]:x[1]] += random_digit\n    labels = torch.from_numpy(num_digits)\n    return TensorDataset(data.type(torch.float32), labels)\n```\n:::\n\n\n### Model Implementation\n\nFor the sake of clarity, the model implementation is divided into its constitutive\nparts:\n\n* `what`-**VAE implementation**: The `what`-VAE can be implemented as an\n  independent class that receives an image patch (crop) and outputs its reconstruction\n  as well as its latent distribution parameters. Note that we could also compute\n  the KL divergence and reconstruction error within that class, however we will\n  put the whole loss computation in another function to have everything in one\n  place. As shown in a [previous\n  summary](https://borea17.github.io/paper_summaries/auto-encoding_variational_bayes#vae-implementation),\n  two fully connected layers with ReLU non-linearity in between suffice for\n  decent reconstructions of MNIST digits.\n\n  We have additional prior knowledge about the output distribution: It should\n  only be between 0 and 1. It is always useful to put as much prior knowledge as\n  possible into the architecture, but how to achieve this?\n\n  * *Clamping*: The most intuitive idea would be to simply clamp the network\n  outputs, however this is a bad idea as gradients wont propagate if the outputs\n  are outside of the clamped region.\n\n  * *Network Initialization*: Another approach would be to simply initialize the\n    weights and biases of the output layer to zero such that further updates\n    push the outputs into the positive direction. However, as the reconstruction\n    of the whole image in AIR is a sum over multiple reconstructions, this turns\n    out to be a bad idea as well. I tried it and the `what`-VAE produces\n    negative outputs which it compensates with another object that has outputs\n    greater than 1.\n\n  * *Sigmoid Layer*: This is a typical choice in classification problems and is\n    commonly used in VAEs when the decoder approximates a Bernoulli\n    distribution. However, it should be noted that using MSE loss (Gaussian\n    decoder distribution) with a sigmoid is generally not advised due to the\n    vanishing/saturating gradients (explained\n    [here](https://borea17,gitbub.io/ML_101/probability_theory/sigmoid_loss)).\n\n    On the other hand, using a Bernoulli distribution for the reconstruction of\n    the whole image (sum over multiple reconstruction) comes with additional\n    problems, e.g., numerical instabilities due to empty canvas (binary cross\n    entropy can not be computed when probabilties are exactly 0) and due to\n    clamping (as the sum over multiple bernoulli means could easily overshoot\n    1). While there might be some workarounds, I decided to go an easier path.\n\n  * **Output Distribution** $\\mathbb{R}_{+}$: This is motivated by the\n    observations I made during the *network initialization* approach. By\n    impeding the network to produce negative outputs, we indirectly force\n    outputs between $0$ and $1$. Thereby, we do not need to get our hands dirty\n    with a Bernoulli distribution or vanishing gradient problems. Furthermore,\n    we use Pytorch's default initialization[^8] to produce mostly empty objects.\n    This encourages the model to try out objects in the beginning of the\n    training.\n\n[^8]: Note that weights and biases of linear layers are [defaulty\n    initialized](https://discuss.pytorch.org/t/how-are-layer-weights-and-biases-initialized-by-default/13073/2)\n    by drawing uniformly in the interval $[-\\frac {1}{\\sqrt{n_o}}, \\frac\n    {1}{\\sqrt{n_o}}]$, where $n_o$ denotes the number of outputs of the linear layer.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom torch import nn\n\nWINDOW_SIZE = MNIST_SIZE        # patch size (in one dimension) of what-VAE\nZ_WHAT_HIDDEN_DIM = 400         # hidden dimension of what-VAE\nZ_WHAT_DIM = 20                 # latent dimension of what-VAE\nFIXED_VAR = 0.5**2              # fixed variance of Gaussian decoder\n\n\nclass VAE(nn.Module):\n    \"\"\"simple VAE class with a Gaussian encoder (mean and diagonal variance\n    structure) and a Gaussian decoder with fixed variance\n\n    Attributes:\n        encoder (nn.Sequential): encoder network for mean and log_var\n        decoder (nn.Sequential): decoder network for mean (fixed var)\n    \"\"\"\n\n    def __init__(self):\n        super(VAE, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(WINDOW_SIZE**2, Z_WHAT_HIDDEN_DIM),\n            nn.ReLU(),\n            nn.Linear(Z_WHAT_HIDDEN_DIM, Z_WHAT_DIM*2),\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(Z_WHAT_DIM, Z_WHAT_HIDDEN_DIM),\n            nn.ReLU(),\n            nn.Linear(Z_WHAT_HIDDEN_DIM, WINDOW_SIZE**2),\n        )\n        return\n\n    def forward(self, x_att_i):\n        z_what_i, mu_E_i, log_var_E_i = self.encode(x_att_i)\n        x_tilde_att_i = self.decode(z_what_i)\n        return x_tilde_att_i, z_what_i, mu_E_i, log_var_E_i\n\n    def encode(self, x_att_i):\n        batch_size = x_att_i.shape[0]\n        # get encoder distribution parameters\n        out_encoder = self.encoder(x_att_i.view(batch_size, -1))\n        mu_E_i, log_var_E_i = torch.chunk(out_encoder, 2, dim=1)\n        # sample noise variable for each batch\n        epsilon = torch.randn_like(log_var_E_i)\n        # get latent variable by reparametrization trick\n        z_what_i = mu_E_i + torch.exp(0.5*log_var_E_i) * epsilon\n        return z_what_i, mu_E_i, log_var_E_i\n\n    def decode(self, z_what_i):\n        # get decoder distribution parameters\n        x_tilde_att_i = self.decoder(z_what_i)\n        # force output to be positive\n        x_tilde_att_i = x_tilde_att_i.abs()\n        # reshape to [1, WINDOW_SIZE, WINDOW_SIZE] (input shape)\n        x_tilde_att_i = x_tilde_att_i.view(-1, 1, WINDOW_SIZE, WINDOW_SIZE)\n        return x_tilde_att_i\n```\n:::\n\n\n* **Recurrent Inference Network**: [Eslami et al.\n  (2016)](https://arxiv.org/abs/1603.08575) used a standard recurrent neural\n  network (RNN) which in each step $i$ computes\n\n  $$\n   \\left(\\underbrace{p^{(i)}_{\\text{pres}}, \\boldsymbol{\\mu}_{\\text{where}},\n   \\boldsymbol{\\sigma}^2_{\\text{where}}}_{\\boldsymbol{\\omega}^{(i)}},\n   \\textbf{h}^{(i)} \\right) = RNN \\left(\\textbf{x},\n   \\underbrace{\\text{z}_{\\text{pres}}^{(i-1)}, \\textbf{z}_{\\text{what}}^{(i-1)},\n   \\textbf{z}_{\\text{where}}^{(i-1)}}_{\\textbf{z}^{(i-1)}}, \\textbf{h}^{(i-1)}\n   \\right),\n  $$\n\n  i.e., the distribution parameters of $\\text{z}_{\\text{pres}}^{(i)}\\sim\n  \\text{Bern}\\left( p^{(i)}_{\\text{pres}} \\right)$ and\n  $\\textbf{z}_{\\text{where}}^{(i)} \\sim \\mathcal{N} \\left( \\boldsymbol{\\mu}_{\\text{where}},\n   \\boldsymbol{\\sigma}^2_{\\text{where}}\\textbf{I}\\right)$, and the next hidden\n  state $\\textbf{h}^{(i)}$. They did not provide any specifics about the network\n  architecture, however in my experiments it turned out that a simple 3 layer\n  (fully-connected) network suffices for this task.\n\n  To speed up convergence, we initialize useful distribution parameters:\n\n  * $p_{\\text{pres}}^{(i)}\\approx 0.8$: This encourages AIR to use objects in the\n    beginning of training.\n  * $\\boldsymbol{\\mu}_{\\text{where}} = \\begin{bmatrix} -3 & 0 &\n    0\\end{bmatrix}^{\\text{T}}$: This leads to a center crop with (approximate)\n    size of the inserted digits.\n  * $\\boldsymbol{\\sigma}_{\\text{where}}^2 \\approx \\begin{bmatrix} 0.05 & 0.05 &\n    0.05\\end{bmatrix}^{\\text{T}}$: Start with low variance.\n\n  Note: We use a very similar recurrent network architecture for the neural baseline\n  model (to predict the negative log-likelihood), see code below.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nZ_PRES_DIM = 1                      # latent dimension of z_pres\nZ_WHERE_DIM = 3                     # latent dimension of z_where\nRNN_HIDDEN_STATE_DIM = 256          # hidden state dimension of RNN\nP_PRES_INIT = [2.]                  # initialization p_pres (sigmoid -> 0.8)\nMU_WHERE_INIT = [3.0, 0., 0.]       # initialization z_where mean\nLOG_VAR_WHERE_INIT = [-3.,-3.,-3.]  # initialization z_where log var\nZ_DIM = Z_PRES_DIM + Z_WHERE_DIM + Z_WHAT_DIM\n\n\nclass RNN(nn.Module):\n\n    def __init__(self, baseline_net=False):\n        super(RNN, self).__init__()\n        self.baseline_net = baseline_net\n        INPUT_SIZE = (CANVAS_SIZE**2) + RNN_HIDDEN_STATE_DIM + Z_DIM\n        if baseline_net:\n            OUTPUT_SIZE = (RNN_HIDDEN_STATE_DIM + 1)\n        else:\n            OUTPUT_SIZE = (RNN_HIDDEN_STATE_DIM + Z_PRES_DIM + 2*Z_WHERE_DIM)\n        output_layer = nn.Linear(RNN_HIDDEN_STATE_DIM, OUTPUT_SIZE)\n\n        self.fc_rnn = nn.Sequential(\n            nn.Linear(INPUT_SIZE, RNN_HIDDEN_STATE_DIM),\n            nn.ReLU(),\n            nn.Linear(RNN_HIDDEN_STATE_DIM, RNN_HIDDEN_STATE_DIM),\n            nn.ReLU(),\n            output_layer\n        )\n        if not baseline_net:\n            # initialize distribution parameters\n            output_layer.weight.data[0:7] = nn.Parameter(\n                torch.zeros(Z_PRES_DIM+2*Z_WHERE_DIM, RNN_HIDDEN_STATE_DIM)\n            )\n            output_layer.bias.data[0:7] = nn.Parameter(\n                torch.tensor(P_PRES_INIT + MU_WHERE_INIT + LOG_VAR_WHERE_INIT)\n            )\n        return\n\n    def forward(self, x, z_im1, h_im1):\n        batch_size = x.shape[0]\n        rnn_input = torch.cat((x.sum(axis=1).view(batch_size, -1), z_im1, h_im1), dim=1)\n        rnn_output = self.fc_rnn(rnn_input)\n        if self.baseline_net:\n            baseline_value_i = rnn_output[:, 0:1]\n            h_i = rnn_output[:, 1::]\n            return baseline_value_i, h_i\n        else:\n            omega_i = rnn_output[:, 0:(Z_PRES_DIM+2*Z_WHERE_DIM)]\n            h_i = rnn_output[:, (Z_PRES_DIM+2*Z_WHERE_DIM)::]\n            # omega_i[:, 0] corresponds to z_pres probability\n            omega_i[:, 0] = torch.sigmoid(omega_i[:, 0])\n            return omega_i, h_i\n```\n:::\n\n\n* **AIR Implementation**: The whole AIR model is obtained by putting everything\n  together. To better understand what's happening, let's take a closer look on\n  the two main functions:\n\n  * `forward(x)`: This function essentially does what is described in\n    [High-Level\n    Overview](https://borea17.github.io/paper_summaries/AIR#high-level-overview).\n    Its purpose is to obtain a structured latent representation $\\textbf{z}=\\bigg\\{ \\left[\n    \\textbf{z}_{\\text{pres}}^{(i)}, \\textbf{z}_{\\text{where}}^{(i)},\n    \\textbf{z}_{\\text{what}}^{(i)}\\right]_{i=1}^N \\bigg\\}$ for a given input\n    (batch of images) $\\textbf{x}$ and to collect everything needed to compute\n    the loss.\n\n  * `compute_loss(x)`: This function is only necessary for training. It computes\n    four loss quantities:\n\n    1. *KL Divergence*: As noted\n       [above](https://borea17.github.io/paper_summaries/AIR#difficulties) the\n       KL divergence term can be computed by summing the KL divergences of each\n       type (`pres`, `what`, `where`) for each step.\n\n    2. *NLL*: We assume a Gaussian decoder such that the negative log-likelihood\n       can be computed as follows\n\n       $$\n        \\text{NLL} = \\frac {1}{2 \\cdot \\sigma^2}\\sum_{i=1}^{W \\cdot H} \\left(x_i - \\widetilde{x}_i \\right)^2,\n       $$\n\n       where $i$ enumerates the pixel space, $\\textbf{x}$ denotes the original\n       image, $\\widetilde{\\textbf{x}}$ the reconstructed image and $\\sigma^2$ is\n       a the fixed variance of the Gaussian distribution (hyperparameter).\n\n    3. *REINFORCE Term*: Since the image reconstruction is build by sampling\n       from a discrete distribution, backpropagation stops at the sampling\n       operation. In order to optimize the distribution parameters\n       $p_{\\text{pres}}^{(i)}$, we use a score-function estimator with a\n       data-dependent neural baseline.\n\n    4. *Baseline Loss*: This loss is needed to approximately fit the neural\n       baseline to the true NLL in order to reduce the variance of the REINFORCE\n       estimator.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport torch.nn.functional as F\nfrom torch.distributions import Bernoulli\n\n\nN = 3                                 # number of inference steps\nEPS = 1e-32                           # numerical stability\nPRIOR_MEAN_WHERE = [3., 0., 0.]       # prior for mean of z_i_where\nPRIOR_VAR_WHERE = [0.1**2, 1., 1.]    # prior for variance of z_i_where\nPRIOR_P_PRES = [0.01]                 # prior for p_i_pres of z_i_pres\nBETA = 0.5                            # hyperparameter to scale KL div\nOMEGA_DIM = Z_PRES_DIM + 2*Z_WHERE_DIM + 2*Z_WHAT_DIM\n\n\nclass AIR(nn.Module):\n\n    PRIOR_MEAN_Z_WHERE = nn.Parameter(torch.tensor(PRIOR_MEAN_WHERE),\n                                      requires_grad=False)\n    PRIOR_VAR_Z_WHERE = nn.Parameter(torch.tensor(PRIOR_VAR_WHERE),\n                                     requires_grad=False)\n    PRIOR_P_Z_PRES = nn.Parameter(torch.tensor(PRIOR_P_PRES),\n                                  requires_grad=False)\n\n    expansion_indices = torch.LongTensor([1, 0, 2, 0, 1, 3])\n    target_rectangle = torch.tensor(\n      [[-1., -1., 1., 1., -1.],\n       [-1., 1., 1., -1, -1.],\n       [1., 1., 1., 1., 1.]]\n    ).view(1, 3, 5)\n\n    def __init__(self):\n        super(AIR, self).__init__()\n        self.vae = VAE()\n        self.rnn = RNN()\n        self.baseline = RNN(True)\n        return\n\n    def compute_loss(self, x):\n        \"\"\"compute the loss of AIR (essentially a VAE loss)\n        assuming the following prior distributions for the latent variables\n\n            z_where ~ N(PRIOR_MEAN_WHERE, PRIOR_VAR_WHERE)\n            z_what ~ N([0, 1])\n            z_pres ~ Bern(p_pres)\n\n        and a\n\n            Gaussian decoder with fixed diagonal var (FIXED_VAR)\n        \"\"\"\n        batch_size = x.shape[0]\n        results = self.forward(x, True)\n        # kl_div for z_pres (between two Bernoulli distributions)\n        q_z_pres = results['all_prob_pres']\n        P_Z_PRES = AIR.PRIOR_P_Z_PRES.expand(q_z_pres.shape).to(x.device)\n        kl_div_pres = AIR.bernoulli_kl(q_z_pres, P_Z_PRES).sum(axis=2)\n        # kl_div for z_what (standard VAE regularization term)\n        q_z_what = [results['all_mu_what'], results['all_log_var_what']]\n        P_MU_WHAT = torch.zeros_like(results['all_mu_what'])\n        P_VAR_WHAT = torch.ones_like(results['all_log_var_what'])\n        P_Z_WHAT = [P_MU_WHAT, P_VAR_WHAT]\n        kl_div_what = AIR.gaussian_kl(q_z_what, P_Z_WHAT).sum(axis=2)\n        # kl_div for z_where (between two Gaussian distributions)\n        q_z_where = [results['all_mu_where'], results['all_log_var_where']]\n        P_MU_WHERE=AIR.PRIOR_MEAN_Z_WHERE.expand(results['all_mu_where'].shape)\n        P_VAR_WHERE=AIR.PRIOR_VAR_Z_WHERE.expand(results['all_mu_where'].shape)\n        P_Z_WHERE = [P_MU_WHERE.to(x.device), P_VAR_WHERE.to(x.device)]\n        kl_div_where = AIR.gaussian_kl(q_z_where, P_Z_WHERE).sum(axis=2)\n        # sum all kl_divs and use delayed mask to zero out irrelevants\n        delayed_mask = results['mask_delay']\n        kl_div = (kl_div_pres + kl_div_where + kl_div_what) * delayed_mask\n        # negative log-likelihood for Gaussian decoder (no gradient for z_pres)\n        factor = 0.5 * (1/FIXED_VAR)\n        nll = factor * ((x - results['x_tilde'])**2).sum(axis=(1,2,3))\n        # REINFORCE estimator for nll (gradient for z_pres)\n        baseline_target = nll.unsqueeze(1)\n        reinforce_term = ((baseline_target - results['baseline_values']\n                           ).detach()\n                          *results['z_pres_likelihood']*delayed_mask).sum(1)\n\n        # baseline model loss\n        baseline_loss = ((results['baseline_values'] -\n                          baseline_target.detach())**2 * delayed_mask).sum(1)\n        loss = dict()\n        loss['kl_div'] = BETA*kl_div.sum(1).mean()\n        loss['nll'] = nll.mean()\n        loss['reinforce'] = reinforce_term.mean()\n        loss['baseline'] = baseline_loss.mean()\n        return loss, results\n\n    def forward(self, x, save_attention_rectangle=False):\n        batch_size = x.shape[0]\n        # initializations\n        all_z = torch.empty((batch_size, N, Z_DIM), device=x.device)\n        z_pres_likelihood = torch.empty((batch_size, N), device=x.device)\n        mask_delay = torch.empty((batch_size, N), device=x.device)\n        all_omega = torch.empty((batch_size, N, OMEGA_DIM), device=x.device)\n        all_x_tilde = torch.empty((batch_size, N, CANVAS_SIZE, CANVAS_SIZE),\n                                 device=x.device)\n        baseline_values = torch.empty((batch_size, N), device=x.device)\n\n        z_im1 = torch.ones((batch_size, Z_DIM)).to(x.device)\n        h_im1 = torch.zeros((batch_size, RNN_HIDDEN_STATE_DIM)).to(x.device)\n        h_im1_b = torch.zeros((batch_size, RNN_HIDDEN_STATE_DIM)).to(x.device)\n        if save_attention_rectangle:\n            attention_rects = torch.empty((batch_size, N, 2, 5)).to(x.device)\n        for i in range(N):\n            z_im1_pres = z_im1[:, 0:1]\n            # mask_delay is used to zero out all steps AFTER FIRST z_pres = 0\n            mask_delay[:, i] = z_im1_pres.squeeze(1)\n            # obtain parameters of sampling distribution and hidden state\n            omega_i, h_i = self.rnn(x, z_im1, h_im1)\n            # baseline version\n            baseline_i, h_i_b = self.baseline(x.detach(), z_im1.detach(),\n                                              h_im1_b)\n            # set baseline 0 if z_im1_pres = 0\n            baseline_value = (baseline_i * z_im1_pres).squeeze()\n            # extract sample distributions parameters from omega_i\n            prob_pres_i = omega_i[:, 0:1]\n            mu_where_i = omega_i[:, 1:4]\n            log_var_where_i = omega_i[:, 4:7]\n            # sample from distributions to obtain z_i_pres and z_i_where\n            z_i_pres_post = Bernoulli(probs=prob_pres_i)\n            z_i_pres = z_i_pres_post.sample() * z_im1_pres\n            # likelihood of sampled z_i_pres (only if z_im_pres = 1)\n            z_pres_likelihood[:, i] = (z_i_pres_post.log_prob(z_i_pres) *\n                                       z_im1_pres).squeeze(1)\n            # get z_i_where by reparametrization trick\n            epsilon_w = torch.randn_like(log_var_where_i)\n            z_i_where = mu_where_i + torch.exp(0.5*log_var_where_i)*epsilon_w\n            # use z_where and x to obtain x_att_i\n            x_att_i = AIR.image_to_window(x, z_i_where)\n            # put x_att_i through VAE\n            x_tilde_att_i, z_i_what, mu_what_i, log_var_what_i = \\\n                self.vae(x_att_i)\n            # create image reconstruction\n            x_tilde_i = AIR.window_to_image(x_tilde_att_i, z_i_where)\n            # update im1 with current versions\n            z_im1 = torch.cat((z_i_pres, z_i_where, z_i_what), 1)\n            h_im1 = h_i\n            h_im1_b = h_i_b\n            # put all distribution parameters into omega_i\n            omega_i = torch.cat((prob_pres_i, mu_where_i, log_var_where_i,\n                                 mu_what_i, log_var_what_i), 1)\n            # store intermediate results\n            all_z[:, i:i+1] = z_im1.unsqueeze(1)\n            all_omega[:, i:i+1] = omega_i.unsqueeze(1)\n            all_x_tilde[:, i:i+1] = x_tilde_i\n            baseline_values[:, i] = baseline_value\n            # for nice visualization\n            if save_attention_rectangle:\n                attention_rects[:, i] = (AIR.get_attention_rectangle(z_i_where)\n                                         *z_i_pres.unsqueeze(1))\n        # save results in dict (easy accessibility)\n        results = dict()\n        # fixes Z_PRES_DIM = 1 and Z_WHERE_DIM = 3\n        results['z_pres_likelihood'] = z_pres_likelihood\n        results['all_z_pres'] = all_z[:, :, 0:1]\n        results['mask_delay'] = mask_delay\n        results['all_prob_pres'] = all_omega[:, :, 0:1]\n        results['all_z_where'] = all_z[:, :, 1:4]\n        results['all_mu_where'] =  all_omega[:, :, 1:4]\n        results['all_log_var_where'] = all_omega[:, :, 4:7]\n        results['all_z_what'] = all_z[:, :, 4::]\n        results['all_mu_what'] =  all_omega[:, :, 7:7+Z_WHAT_DIM]\n        results['all_log_var_what'] = all_omega[:, :, 7+Z_WHAT_DIM::]\n        results['baseline_values'] = baseline_values\n        if save_attention_rectangle:\n            results['attention_rects'] = attention_rects\n        # compute reconstructed image (take only x_tilde_i with z_i_pres=1)\n        results['x_tilde_i'] = all_x_tilde\n        x_tilde = (all_z[:, :, 0:1].unsqueeze(2) * all_x_tilde).sum(axis=1,\n                                                              keepdim=True)\n        results['x_tilde'] = x_tilde\n        # compute counts as identified objects (sum z_i_pres)\n        results['counts'] = results['all_z_pres'].sum(1).to(dtype=torch.long)\n        return results\n\n    @staticmethod\n    def image_to_window(x, z_i_where):\n        grid_shape = (z_i_where.shape[0], 1, WINDOW_SIZE, WINDOW_SIZE)\n        z_i_where_inv = AIR.invert_z_where(z_i_where)\n        x_att_i = AIR.spatial_transform(x, z_i_where_inv, grid_shape)\n        return x_att_i\n\n    @staticmethod\n    def window_to_image(x_tilde_att_i, z_i_where):\n        grid_shape = (z_i_where.shape[0], 1, CANVAS_SIZE, CANVAS_SIZE)\n        x_tilde_i = AIR.spatial_transform(x_tilde_att_i, z_i_where, grid_shape)\n        return x_tilde_i\n\n    @staticmethod\n    def spatial_transform(x, z_where, grid_shape):\n        theta_matrix = AIR.z_where_to_transformation_matrix(z_where)\n        grid = F.affine_grid(theta_matrix, grid_shape, align_corners=False)\n        out = F.grid_sample(x, grid, align_corners=False)\n        return out\n\n    @staticmethod\n    def z_where_to_transformation_matrix(z_i_where):\n        \"\"\"taken from\n        https://github.com/pyro-ppl/pyro/blob/dev/examples/air/air.py\n        \"\"\"\n        batch_size = z_i_where.shape[0]\n        out = torch.cat((z_i_where.new_zeros(batch_size, 1), z_i_where), 1)\n        ix = AIR.expansion_indices\n        if z_i_where.is_cuda:\n            ix = ix.cuda()\n        out = torch.index_select(out, 1, ix)\n        theta_matrix = out.view(batch_size, 2, 3)\n        return theta_matrix\n\n    @staticmethod\n    def invert_z_where(z_where):\n        z_where_inv = torch.zeros_like(z_where)\n        scale = z_where[:, 0:1] + 1e-9\n        z_where_inv[:, 1:3] = -z_where[:, 1:3] / scale\n        z_where_inv[:, 0:1] = 1 / scale\n        return z_where_inv\n\n    @staticmethod\n    def get_attention_rectangle(z_i_where):\n        batch_size = z_i_where.shape[0]\n        z_i_where_inv = AIR.invert_z_where(z_i_where)\n        theta_matrix = AIR.z_where_to_transformation_matrix(z_i_where_inv)\n        target_rectangle = AIR.target_rectangle.expand(batch_size, 3,\n                                                       5).to(z_i_where.device)\n        source_rectangle_normalized = torch.matmul(theta_matrix,\n                                                   target_rectangle)\n        # remap into absolute values\n        source_rectangle = 0 + (CANVAS_SIZE/2)*(source_rectangle_normalized + 1)\n        return source_rectangle\n\n    @staticmethod\n    def bernoulli_kl(q_probs, p_probs):\n        # https://github.com/pytorch/pytorch/issues/15288\n        p1 = p_probs\n        p0 = 1 - p1\n        q1 = q_probs\n        q0 = 1 - q1\n\n        logq1 = (q1 + EPS).log()\n        logq0 = (q0 + EPS).log()\n        logp1 = (p1).log()\n        logp0 = (p0).log()\n\n        kl_div_1 = q1*(logq1 - logp1)\n        kl_div_0 = q0*(logq0 - logp0)\n        return kl_div_1 + kl_div_0\n\n    @staticmethod\n    def gaussian_kl(q, p):\n        # https://pytorch.org/docs/stable/_modules/torch/distributions/kl.html\n        mean_q, log_var_q = q[0], q[1]\n        mean_p, var_p = p[0], p[1]\n\n        var_ratio = log_var_q.exp()/var_p\n        t1 = (mean_q - mean_p).pow(2)/var_p\n        return -0.5 * (1 + var_ratio.log() - var_ratio - t1)\n```\n:::\n\n\n* **Training Procedure**: Lastly, a standard training procedure is implemented.\n  We will use two optimizers, one for the model parameters and one\n  for the neural baseline parameters. Note that the training process is\n  completely unsupervised, i.e., the model only receives a batch of images to\n  compute the losses.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom livelossplot import PlotLosses, outputs\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\n\n\nEPOCHS = 50\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-4\nBASE_LEARNING_RATE = 1e-2\nEPOCHS_TO_SAVE_MODEL = [1, 10, EPOCHS]\n\n\ndef train(air, dataset):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    print('Device: {}'.format(device))\n\n    data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True,\n                             num_workers=4)\n    optimizer = torch.optim.Adam([{'params': list(air.rnn.parameters()) +\n                                   list(air.vae.parameters()),\n                                   'lr': LEARNING_RATE,},\n                                  {'params': air.baseline.parameters(),\n                                   'lr': BASE_LEARNING_RATE}])\n    air.to(device)\n\n    # prettify livelossplot\n    def custom(ax: plt.Axes, group: str, x_label: str):\n        ax.legend()\n        if group == 'accuracy':\n            ax.set_ylim(0, 1)\n        elif group == 'loss base':\n            ax.set_ylim(0, 300)\n\n    matplot = [outputs.MatplotlibPlot(after_subplot=custom,max_cols=3)]\n    losses_plot = PlotLosses(groups={'loss model':['KL div','NLL','REINFORCE'],\n                                     'loss base': ['baseline'],\n                                     'accuracy': ['count accuracy']},\n                             outputs=matplot)\n    for epoch in range(1, EPOCHS+1):\n        avg_kl_div, avg_nll, avg_reinforce, avg_base, avg_acc = 0, 0, 0, 0, 0\n        for x, label in data_loader:\n            air.zero_grad()\n\n            losses, results = air.compute_loss(x.to(device, non_blocking=True))\n            loss  = (losses['kl_div'] + losses['nll'] + losses['reinforce']\n                     +losses['baseline'])\n            loss.backward()\n            optimizer.step()\n\n            # compute accuracy\n            label = label.unsqueeze(1).to(device)\n            acc = (results['counts']==label).sum().item()/len(results['counts'])\n            # update epoch means\n            avg_kl_div += losses['kl_div'].item() / len(data_loader)\n            avg_nll += losses['nll'].item() / len(data_loader)\n            avg_reinforce += losses['reinforce'].item() / len(data_loader)\n            avg_base += losses['baseline'].item() / len(data_loader)\n            avg_acc += acc / len(data_loader)\n\n        if epoch in EPOCHS_TO_SAVE_MODEL:  # save model\n            torch.save(air, f'./results/checkpoint_{epoch}.pth')\n        losses_plot.update({'KL div': avg_kl_div,\n                            'NLL': avg_nll,\n                            'REINFORCE': avg_reinforce,\n                            'baseline': avg_base,\n                            'count accuracy': avg_acc}, current_step=epoch)\n        losses_plot.send()\n    print(f'Accuracy after Training {avg_acc:.2f} (on training dataset)')\n    torch.save(air, f'./results/checkpoint_{epoch}.pth')\n    trained_air = air\n    return trained_air\n```\n:::\n\n\n### Results\n\nLet's train our model:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nair_model = AIR()\ntrain_dataset = generate_dataset(num_images=10000, SEED=42)\ntrained_air = train(air_model, train_dataset)\n```\n:::\n\n\n![Training Results](./img/training_results.png \"Training Results\")\n\nThis looks pretty awesome! It seems that our model nearly perfectly learns to\ncount the number of digits without even knowing what a digit is.\n\nLet us take a closer look and plot the results of the model\nat different stages of the training against a test dataset.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ndef plot_results(dataset):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    n_samples = 7\n\n    i_samples = np.random.choice(range(len(dataset)), n_samples, replace=False)\n    colors_rect = ['red', 'green', 'yellow']\n    num_rows = len(EPOCHS_TO_SAVE_MODEL) + 1\n\n    fig = plt.figure(figsize=(14, 8))\n    for counter, i_sample in enumerate(i_samples):\n        orig_img = dataset[i_sample][0]\n        # data\n        ax = plt.subplot(num_rows, n_samples, 1 + counter)\n        plt.imshow(orig_img[0].numpy(), cmap='gray', vmin=0, vmax=1)\n        plt.axis('off')\n        if counter == 0:\n            ax.annotate('Data', xy=(-0.05, 0.5), xycoords='axes fraction',\n                        fontsize=14, va='center', ha='right', rotation=90)\n        # outputs after epochs of training\n        MODELS = [, , ]\n        for j, (epoch, model) in enumerate(zip(EPOCHS_TO_SAVE_MODEL, MODELS)):\n            trained_air = torch.load(model)\n            trained_air.to(device)\n\n            results = trained_air(orig_img.unsqueeze(0).to(device), True)\n\n            attention_recs = results['attention_rects'].squeeze(0)\n            x_tilde = torch.clamp(results['x_tilde'][0], 0 , 1)\n\n            ax = plt.subplot(num_rows, n_samples, 1 + counter + n_samples*(j+1))\n            plt.imshow(x_tilde[0].cpu().detach().numpy(), cmap='gray',\n                       vmin=0, vmax=1)\n            plt.axis('off')\n            # show attention windows\n            for step_counter, step in enumerate(range(N)):\n                rect = attention_recs[step].detach().cpu().numpy()\n                if rect.sum() > 0:  # valid rectangle\n                    plt.plot(rect[0], rect[1]-0.5,\n                             color=colors_rect[step_counter])\n            if counter == 0:\n                # compute accuracy\n                data_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n                avg_acc = 0\n                for batch, label in data_loader:\n                    label = label.unsqueeze(1).to(device)\n                    r = trained_air(batch.to(device))\n                    acc = (r['counts']==label).sum().item()/len(r['counts'])\n                    avg_acc += acc / len(data_loader)\n                # annotate plot\n                ax.annotate(f'Epoch {epoch}\\n Acc {avg_acc:.2f}',\n                            xy=(-0.05, 0.5), va='center',\n                            xycoords='axes fraction', fontsize=14,  ha='right',\n                            rotation=90)\n    return\n\n\ntest_dataset = generate_dataset(num_images=300, SEED=2)\nplot_results(test_dataset)\n```\n:::\n\n\n![Test Results](./img/test_results.png \"Test Results\")\n\nVery neat results, indeed! Note that this looks very similar to Figure 3 in the\n[AIR paper](https://arxiv.org/abs/1603.08575). The accuracy on the left denotes\nthe count accuracy of the test dataset.\n\n\n### Closing Notes\n\nAlright, time to step down from our high horse. Actually, it took me quite some\ntime to tweak the hyperparameters to obtain such good results. I put a lot of\nprior knowledge into the model so that `completely unsupervised` is probably\nexaggerated. Using a slightly different setup might lead to entirely different\nresults. Furthermore, even in this setup, there may be cases in which the training\nconverges to some local maximum (depending on the random network initializations\nand random training dataset).\n\n## Acknowledgements\n\nThe [blog post](http://akosiorek.github.io/ml/2017/09/03/implementing-air.html)\nby Adam Kosiorek, the [pyro tutorial on\nAIR](https://pyro.ai/examples/air.html) and [the pytorch\nimplementation](https://github.com/addtt/attend-infer-repeat-pytorch) by Andrea\nDittadi are great resources and helped very much to understand the details of\nthe paper.\n\n--------------------------------------------------------------------------------------\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}