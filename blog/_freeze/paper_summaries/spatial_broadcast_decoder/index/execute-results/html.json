{
  "hash": "0008056b6fe0a5df52c6d38e2c2e64bd",
  "result": {
    "markdown": "---\ntitle: \"Spatial Broadcast Decoder: A Simple Architecture for Learning Disentangled Representations in VAEs\"\ncategories: [reimplementation, VAE, disentanglement]\ndate: \"2020-08-07\"\nexecute:\n  eval: false # true\nengine: jupyter\nformat:\n  html: \n    code-fold: show \n    highlight-style: github \n    code-block-bg: true\n    code-tools: \n      toggle: true\n      source: \"https://github.com/borea17/Notebooks/blob/master/02_Spatial_Broadcast_Decoder.ipynb\"\n---\n\n<style>\n\nth {\n  border: 10px solid #bfbfbf;\n  text-align: center;\n}\n\ntable th {\n  background-color: blue;\n}\n</style>\n\n\n[Watters et al. (2019)](https://arxiv.org/abs/1901.07017) introduce\nthe *Spatial Broadcast Decoder (SBD)* as an architecture for the\ndecoder in Variational Auto-Encoders\n[(VAEs)](https://borea17.github.io/paper_summaries/auto-encoding_variational_bayes)\nto improve\ndisentanglement in the latent\nspace[^1], reconstruction accuracy and\ngeneralization in limited datasets  (i.e., held-out regions in data\nspace). Motivated by the limitations of deconvolutional layers in traditional decoders,\nthese upsampling layers are replaced by a tiling operation in the Spatial\nBroadcast decoder. Furthermore, explicit spatial information (inductive bias) is\nappended in the form of coordinate channels leading to a simplified optimization\nproblem and improved positional generalization. As a proof of concept, they\ntested the model on the colored sprites dataset (known factors of\nvariation such as position, size, shape), Chairs and 3D Object-in-Room datasets\n(no positional variation), a dataset with small objects and a\ndataset with dependent factors. They could show that the Spatial Broadcast\ndecoder can be used complementary or as an improvement to state-of-the-art\ndisentangling techniques.\n\n[^1]: As outlined by [Watters et al. (2019)](https://arxiv.org/abs/1901.07017), there is \"yet no consensus on the definition of a disentangled representation\". However, in their paper they focus on *feature compositionality* (i.e., composing a scene in terms of independent features such as color and object) and refer to it as *disentangled representation*.\n\n## Model Description\n\nAs stated in the title, the model architecture of the Spatial Broadcast decoder\nis very simple: Take a standard VAE decoder and replace all upsampling\ndeconvolutional layers by tiling the latent code $\\textbf{z}$ across the original\nimage space, appending fixed coordinate channels and applying an convolutional\nnetwork with $1 \\times 1$ stride, see the figure below.\n\n| ![Schematic of the Spatial Broadcast VAE](./img/sbd.png \"Schematic of the Spatial Broadcast VAE\") |\n| :--         |\n| Schematic of the Spatial Broadcast VAE. In the decoder, the latent code $\\textbf{z}\\in\\mathbb{R}^{k}$ is broadcasted (*tiled*) to the image width $w$ and height $h$. Additionally, two \"coordinate\" channels are appended. The result is fed to an unstrided convolutional decoder. (right) Pseudo-code of the spatial operation. Taken from [Watters et al. (2019)](https://arxiv.org/abs/1901.07017).|\n\n<!-- | (left) Schematic of the Spatial Broadcast VAE. In the decoder, we broadcast (tile) the latent code $\\textbf{z}$ of size $k$ to the image width $w$ and height $h$, and concatenate two \"coordinate\" channels. This is then fed to an unstrided convolutional decoder. (right) Pseudo-code of the spatial operation. Taken from [Watters et al. (2019)](https://arxiv.org/abs/1901.07017).| -->\n\n**Motivation**: The presented architecture is mainly motivated by two reasons:\n\n* **Deconvolution layers cause optimization difficulties**: [Watters\n   et al. (2019)](https://arxiv.org/abs/1901.07017) argue that\n   upsampling deconvolutional layers should be avoided, since these\n   are prone to produce checkerboard\n   artifacts, i.e., a\n   checkerboard pattern can be identified on the resulting images\n   (when looking closer), see figure below. These artifacts constrain\n   the reconstruction accuracy and [Watters et al.\n   (2019)](https://arxiv.org/abs/1901.07017) hypothesize that the\n  resulting effects may raise problems for learning a disentangled\n  representation in the latent space.\n\n    | ![Checkerboard Artifacts](./img/cherckerboard_artifacts.png \"Checkerboard Artifacts\") |\n    | :--         |\n    | A checkerboard pattern can often be identified in artifically generated images that use deconvolutional layers. <br>Taken from [Odena et al. (2016)](https://distill.pub/2016/deconv-checkerboard/) (very worth reading).|\n\n* **Appended coordinate channels improve positional generalization and\n  optimization**: Previous work by [Liu et al.\n  (2018)](https://arxiv.org/abs/1807.03247) showed that standard\n  convolution/deconvolution networks (CNNs) perform badly when trying to learn trivial\n  coordinate transformations (e.g., learning a mapping from Cartesian space\n  into one-hot pixel space or vice versa). This behavior may seem\n  counterintuitive (easy task, small dataset), however the feature of translational\n  equivariance (i.e., shifting an object in the input equally shifts its\n  representation in the output) in CNNs[^2]\n  hinders learning this task: The filters have by design no\n  information about their position. Thus, coordinate transformations\n  result in complicated functions which makes optimization difficult.\n  E.g., changing the input coordinate slighlty might push the\n  resulting function in a completelty different direction.\n\n    **CoordConv Solution**: To overcome this problem, [Liu et al.\n  (2018)](https://arxiv.org/abs/1807.03247) propose to\n  append coordinate channels before convolution and term the resulting layer\n  *CoordConv*, see figure below. In principle, this layer can\n  learn to use or discard translational equivariance and\n  keeps the other advantages of convolutional layers (fast computations, few\n  parameters). Under this modification learning coordinate transformation\n  problems works out of the box with perfect generalization in less time (150\n  times faster) and less memory (10-100 times fewer parameters).\n  As coordinate transformations are implicitely needed in a variaty of tasks (such as\n  producing bounding boxes in object detection) using CoordConv instead of\n  standard convolutions might increase the performance of several other models.\n\n    | ![CoordConv Layer](./img/CoordConv.png \"CoordConv Layer\") |\n    | :--         |\n    | Comparison of 2D convolutional and CoordConv layers. <br>Taken from [Liu et al. (2018)](https://arxiv.org/abs/1807.03247). |\n\n    **Positional Generalization**: Appending fixed coordinate channels is\n  mainly beneficial in datasets in which same objects may appear at distinct\n  positions (i.e., there is positional variation). The main idea is that\n  rendering an object at a specific position without spatial information (i.e.,\n  standard convolution/deconvolution) results in a very complicated function. In\n  contrast,the Spatial Broadcast decoder architecture can\n  leverage the spatial information to reveal objects easily: E.g., by convolving\n  the positions in the latent space with the fixed coordinate channels and\n  applying a threshold operation. Thus, [Watters\n   et al. (2019)](https://arxiv.org/abs/1901.07017) argue that the\n  Spatial Broadcast decoder architecture puts a prior on dissociating\n  positional from non-positional features in the latent distribution.\n  Datasets without positional variation in turn seem unlikely to benefit from this\n  architecture. However, [Watters et al.\n  (2019)](https://arxiv.org/abs/1901.07017) showed that the Spatial\n  Broadcast decoder could still help in these datasets and attribute this to the\n  replacement of deconvolutional layers.\n\n[^2]: In typical image classification problems, translational equivariance is highly valued since it ensures that if a filter detects an object (e.g., edges), it will detect it irrespective of its position.\n\n<!-- ## Learning the Model -->\n\n<!-- Basically, the Spatial Broadcast decoder is a function approximator for -->\n<!-- probabilistic decoder in a VAE. Thus, learning the model works exactly -->\n<!-- as in VAEs (see my -->\n<!-- [post](https://borea17.github.io/blog/auto-encoding_variational_bayes)): -->\n<!-- The optimal parameters are learned jointly  -->\n<!-- by training the VAE using the AEVB algorithm ([Kingma and Welling, -->\n<!-- 2013](https://arxiv.org/abs/1312.6114)). The remaining  -->\n<!-- part of this post aims to reproduce some of results -->\n<!-- by [Watters et al. (2019)](https://arxiv.org/abs/1901.07017), i.e., -->\n<!-- comparing the Spatial Broadcast decoder with a standard -->\n<!-- deconvolutional decoder.  -->\n\n## Implementation\n\n[Watters et al. (2019)](https://arxiv.org/abs/1901.07017) conducted\nexperiments with several datasets and could show that\nincorporating the Spatial Broadcast decoder into state-of-the-art VAE\narchitectures consistently increased their perfomance. While this is\nimpressive, it is always frustrating to not being able to reproduce\nresults due to missing implementation details, less computing\nresources or simply not having enough time to work on a\nreimplementation.\n\nThe following reimplementation intends to eliminate that frustration\nby reproducing some of their experiments on much smaller datasets with\nsimilar characteristics such that training will take less time\n(less than 30 minutes with a NVIDIA Tesla K80 GPU).\n\n### Data Generation\n\nA dataset that is similar in spirit to the *colored sprites dataset*\nwill be generated, i.e., procedurally generated objects from known\nfactors of variation. [Watters et al.\n(2019)](https://arxiv.org/abs/1901.07017) use a binary [dsprites\ndataset](https://github.com/deepmind/dsprites-dataset) consisting of\n737,280 images and transform these during training into colored images\nby uniformly sampling from a predefined HSV space (see Appendix A.3).\nAs a result, the dataset has 8 factors of variation ($x$-position,\n$y$-position, size, shape, angle, 3D-color) with infinite samples (due\nto sampling of color). They used $1.5 \\cdot 10^6$ training steps.\n\nTo reduce training time, we are going to generate a much simpler\ndataset consisting of $3675$ images with a circle\n(fixed size) inside generated from a predefined set of possible colors\nand positions such that there are only 3\nfactors of variation ($x$-position, $y$-position, discretized color).\nIn this case $3.4 \\cdot 10^2$ training steps suffice for approximate convergence.\n\n| ![Examples of Dataset](./img/dataset.png \"Examples of Dataset\") |\n| :---:       |\n| **Visualization of self-written Dataset** |\n\n\nThe code below creates the dataset. Note that it is kept more generic\nthan necessary to allow the creation of several variations of this\ndataset, i.e., more dedicated experiments can be conducted.\n\n<!-- Two datasets will be generated that are similar in spirit to  -->\n<!-- * the *colored sprites dataset*, i.e., procedurally generated objects from -->\n<!--   known factors of variation. -->\n<!-- * a *dataset with small objects*. Note that [Watters et al. -->\n<!--   (2019)](https://arxiv.org/abs/1901.07017) stated that in this case -->\n<!--   the use of the Spatial Broadcast decoder `provides a particularly -->\n<!--   dramatic benefit`. -->\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom PIL import Image, ImageDraw\nimport torchvision.transforms as transforms\nimport numpy as np\nimport torch\nfrom torch.utils.data import TensorDataset\n\n\ndef generate_img(x_position, y_position, shape, color, img_size, size=20):\n    \"\"\"Generate an RGB image from the provided latent factors\n\n    Args:\n        x_position (float): normalized x position\n        y_position (float): normalized y position\n        shape (string): can only be 'circle' or 'square'\n        color (string): color name or rgb string\n        img_size (int): describing the image size (img_size, img_size)\n        size (int): size of shape\n\n    Returns:\n        torch tensor [3, img_size, img_size] (dtype=torch.float32)\n    \"\"\"\n    # creation of image\n    img = Image.new('RGB', (img_size, img_size), color='black')\n    # map (x, y) position to pixel coordinates\n    x_position = (img_size - 2 - size) * x_position\n    y_position = (img_size - 2 - size) * y_position\n    # define coordinates\n    x_0, y_0 = x_position, y_position\n    x_1, y_1 = x_position + size, y_position + size\n    # draw shapes\n    img1 = ImageDraw.Draw(img)\n    if shape == 'square':\n        img1.rectangle([(x_0, y_0), (x_1, y_1)], fill=color)\n    elif shape == 'circle':\n        img1.ellipse([(x_0, y_0), (x_1, y_1)], fill=color)\n    return transforms.ToTensor()(img).type(torch.float32)\n\n\ndef generate_dataset(img_size, shape_sizes, num_pos, shapes, colors):\n    \"\"\"procedurally generated from 4 ground truth independent latent factors,\n       these factors are/can be\n           Position X: num_pos values in [0, 1]\n           Poistion Y: num_pos values in [0, 1]\n           Shape: square, circle\n           Color: standard HTML color name or 'rgb(x, y, z)'\n\n    Args:\n           img_size (int): describing the image size (img_size, img_size)\n           shape_sizes (list): sizes of shapes\n           num_pos (int): discretized positions\n           shapes (list): shapes (can only be 'circle', 'square')\n           colors (list): colors\n\n    Returns:\n           data: torch tensor [n_samples, 3, img_size, img_size]\n           latents: each entry describes the latents of corresp. data entry\n    \"\"\"\n    num_shapes, num_colors, sizes = len(shapes), len(colors), len(shape_sizes)\n\n    n_samples = num_pos*num_pos*num_shapes*num_colors*sizes\n    data = torch.empty([n_samples, 3, img_size, img_size])\n    latents = np.empty([n_samples], dtype=object)\n\n    index = 0\n    for x_pos in np.linspace(0, 1, num_pos):\n        for y_pos in np.linspace(0, 1, num_pos):\n            for shape in shapes:\n                for size in shape_sizes:\n                    for color in colors:\n                        img = generate_img(x_pos, y_pos, shape, color,\n                                           img_size, size)\n                        data[index] = img\n                        latents[index] = [x_pos, y_pos, shape, color]\n\n                        index += 1\n    return data, latents\n\n\ncircles_data, latents = generate_dataset(img_size=64, shape_sizes=[16],\n                                         num_pos=35,\n                                         shapes=['circle'],\n                                         colors=['red', 'green', 'blue'])\nsprites_dataset = TensorDataset(circles_data)\n```\n:::\n\n\n### Model Implementation\n\nAlthough in principle implementing a VAE is fairly simple (see [my\npost](https://borea17.github.io/blog/auto-encoding_variational_bayes)\nfor details), in practice one must choose lots of\nhyperparmeters. These can be divided into three broader categories:\n\n* **Encoder/Decoder and Prior Distribution**: As suggested by [Watters et al.\n    (2019)](https://arxiv.org/abs/1901.07017) in Appendix A, we use a\n    Gaussian decoder distribution with fixed diagonal covariance structure\n    $p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}^\\prime | \\textbf{z}^{(i)}\\right) = \\mathcal{N}\\left( \\textbf{x}^\\prime |\n    \\boldsymbol{\\mu}_D^{(i)}, \\sigma^2 \\textbf{I} \\right)$, hence the\n    reconstruction accuracy can be calculated as follows[^3]\n\n    $$\n    \\text{Reconstruction Acc.} = \\log p_{\\boldsymbol{\\theta}} \\left(\n    \\textbf{x}^{(i)} | \\textbf{z}^{(i)} \\right) = - \\frac {1}{2 \\sigma^2}\n    \\sum_{k=1}^{D} \\left(x_k^{(i)} - \\mu_{D_k}^{(i)} \\right)^2 + \\text{const}.\n    $$\n\n    For the encoder distribution a Gaussian with diagonal covariance\n    $q_{\\boldsymbol{\\phi}} \\sim\n    \\mathcal{N} \\left( \\textbf{z} | \\boldsymbol{\\mu}_E,\n    \\boldsymbol{\\sigma}_D^2 \\textbf{I} \\right)$ and as prior a\n    centered multivariate Gaussian $p_{\\boldsymbol{\\theta}}\n    (\\textbf{z}) = \\mathcal{N}\\left( \\textbf{z} | \\textbf{0}, \\textbf{I} \\right)$\n     are chosen (both typical choices).\n\n* **Network Architecture for Encoder/Decoder**: The network\n  architectures for the standard encoder and decoder consist of\n  convolutional and deconvolutional layers (since these perform\n  typically much better on image data). The Spatial Broadcast decoder\n  defines a different kind of architecture, see [Model\n  Description](https://borea17.github.io/blog/spatial_broadcast_decoder#data-generation).\n  The exact architectures are taken from Appendix A.1 of [Watters et\n  al.](https://arxiv.org/abs/1901.07017), see code below[^4]:\n\n\n  ::: {.cell execution_count=3}\n  ``` {.python .cell-code}\n  from torch import nn\n  \n  \n  class Encoder(nn.Module):\n      \"\"\"\"Encoder class for use in convolutional VAE\n  \n      Args:\n          latent_dim: dimensionality of latent distribution\n  \n      Attributes:\n          encoder_conv: convolution layers of encoder\n          fc_mu: fully connected layer for mean in latent space\n          fc_log_var: fully connceted layers for log variance in latent space\n      \"\"\"\n  \n      def __init__(self, latent_dim=6):\n          super().__init__()\n          self.latent_dim = latent_dim\n  \n          self.encoder_conv = nn.Sequential(\n              # shape: [batch_size, 3, 64, 64]\n              nn.Conv2d(3,  64, kernel_size=4, stride=2, padding=1),\n              nn.ReLU(),\n              # shape: [batch_size, 64, 32, 32]\n              nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1),\n              nn.ReLU(),\n              nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1),\n              nn.ReLU(),\n              nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1),\n              nn.ReLU(),\n              # shape: [batch_size, 64, 4, 4],\n              nn.Flatten(),\n              # shape: [batch_size, 1024]\n              nn.Linear(1024, 256),\n              nn.ReLU(),\n              # shape: [batch_size, 256]\n          )\n          self.fc_mu = nn.Sequential(\n              nn.Linear(in_features=256, out_features=self.latent_dim),\n          )\n          self.fc_log_var = nn.Sequential(\n              nn.Linear(in_features=256, out_features=self.latent_dim),\n          )\n          return\n  \n      def forward(self, inp):\n          out = self.encoder_conv(inp)\n          mu = self.fc_mu(out)\n          log_var = self.fc_log_var(out)\n          return [mu, log_var]\n  \n  \n  class Decoder(nn.Module):\n      \"\"\"(standard) Decoder class for use in convolutional VAE,\n      a Gaussian distribution with fixed variance (identity times fixed variance\n      as covariance matrix) used as the decoder distribution\n  \n      Args:\n          latent_dim: dimensionality of latent distribution\n          fixed_variance: variance of distribution\n  \n      Attributes:\n          decoder_upsampling: linear upsampling layer(s)\n          decoder_deconv: deconvolution layers of decoder (also upsampling)\n      \"\"\"\n  \n      def __init__(self, latent_dim, fixed_variance):\n          super().__init__()\n          self.latent_dim = latent_dim\n          self.coder_type = 'Gaussian with fixed variance'\n          self.fixed_variance = fixed_variance\n  \n          self.decoder_upsampling = nn.Sequential(\n              nn.Linear(self.latent_dim, 256),\n              nn.ReLU(),\n              # reshaped into [batch_size, 64, 2, 2]\n          )\n          self.decoder_deconv = nn.Sequential(\n              # shape: [batch_size, 64, 2, 2]\n              nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1),\n              nn.ReLU(),\n              # shape: [batch_size, 64, 4, 4]\n              nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1),\n              nn.ReLU(),\n              nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1),\n              nn.ReLU(),\n              nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1),\n              nn.ReLU(),\n              nn.ConvTranspose2d(64,  3, kernel_size=4, stride=2, padding=1),\n              # shape: [batch_size, 3, 64, 64]\n          )\n          return\n  \n      def forward(self, inp):\n          ups_inp = self.decoder_upsampling(inp)\n          ups_inp = ups_inp.view(-1, 64, 2, 2)\n          mu = self.decoder_deconv(ups_inp)\n          return mu\n  \n  \n  class SpatialBroadcastDecoder(nn.Module):\n      \"\"\"SBD class for use in convolutional VAE,\n        a Gaussian distribution with fixed variance (identity times fixed\n        variance as covariance matrix) used as the decoder distribution\n  \n      Args:\n          latent_dim: dimensionality of latent distribution\n          fixed_variance: variance of distribution\n  \n      Attributes:\n          img_size: image size (necessary for tiling)\n          decoder_convs: convolution layers of decoder (also upsampling)\n      \"\"\"\n  \n      def __init__(self, latent_dim, fixed_variance):\n          super().__init__()\n          self.img_size = 64\n          self.coder_type = 'Gaussian with fixed variance'\n          self.latent_dim = latent_dim\n          self.fixed_variance = fixed_variance\n  \n          x = torch.linspace(-1, 1, self.img_size)\n          y = torch.linspace(-1, 1, self.img_size)\n          x_grid, y_grid = torch.meshgrid(x, y, indexing=\"ij\")\n          # reshape into [1, 1, img_size, img_size] and save in state_dict\n          self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n          self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n  \n          self.decoder_convs = nn.Sequential(\n              # shape [batch_size, latent_dim + 2, 64, 64]\n              nn.Conv2d(in_channels=self.latent_dim+2, out_channels=64,\n                        stride=(1, 1), kernel_size=(3,3), padding=1),\n              nn.ReLU(),\n              # shape [batch_size, 64, 64, 64]\n              nn.Conv2d(in_channels=64, out_channels=64, stride=(1,1),\n                        kernel_size=(3, 3), padding=1),\n              nn.ReLU(),\n              # shape [batch_size, 64, 64, 64]\n              nn.Conv2d(in_channels=64, out_channels=3, stride=(1,1),\n                        kernel_size=(3, 3), padding=1),\n              # shape [batch_size, 3, 64, 64]\n          )\n          return\n  \n      def forward(self, z):\n          batch_size = z.shape[0]\n          # reshape z into [batch_size, latent_dim, 1, 1]\n          z = z.view(z.shape + (1, 1))\n          # tile across image [batch_size, latent_im, img_size, img_size]\n          z_b = z.expand(-1, -1, self.img_size, self.img_size)\n          # upsample x_grid and y_grid to [batch_size, 1, img_size, img_size]\n          x_b = self.x_grid.expand(batch_size, -1, -1, -1)\n          y_b = self.y_grid.expand(batch_size, -1, -1, -1)\n          # concatenate vectors [batch_size, latent_dim+2, img_size, img_size]\n          z_sb = torch.cat((z_b, x_b, y_b), dim=1)\n          # apply convolutional layers\n          mu_D = self.decoder_convs(z_sb)\n          return mu_D\n  ```\n  :::\n  \n  \n  The VAE implementation below combines the encoder and decoder\n  architectures (slightly modified version of my last [VAE\n  implementation](https://borea17.github.io/blog/auto-encoding_variational_bayes#vae-implementation)).\n\n\n  ::: {.cell execution_count=4}\n  ``` {.python .cell-code}\n  from torch.distributions.multivariate_normal import MultivariateNormal\n  \n  \n  class VAE(nn.Module):\n      \"\"\"A simple VAE class\n  \n      Args:\n          vae_tpe: type of VAE either 'Standard' or 'SBD'\n          latent_dim: dimensionality of latent distribution\n          fixed_var: fixed variance of decoder distribution\n      \"\"\"\n  \n      def __init__(self, vae_type, latent_dim, fixed_var):\n          super().__init__()\n          self.vae_type = vae_type\n  \n          if self.vae_type == 'Standard':\n              self.decoder = Decoder(latent_dim=latent_dim,\n                                    fixed_variance=fixed_var)\n          else:\n              self.decoder = SpatialBroadcastDecoder(latent_dim=latent_dim,\n                                                     fixed_variance=fixed_var)\n  \n          self.encoder = Encoder(latent_dim=latent_dim)\n          self.normal_dist = MultivariateNormal(torch.zeros(latent_dim),\n                                                torch.eye(latent_dim))\n          return\n  \n      def forward(self, x):\n          z, mu_E, log_var_E = self.encode(x)\n          # regularization term per batch, i.e., size: (batch_size)\n          regularization_term = 0.5 * (1 + log_var_E - mu_E**2\n                                        - torch.exp(log_var_E)).sum(axis=1)\n  \n          batch_size = x.shape[0]\n          if self.decoder.coder_type == 'Gaussian with fixed variance':\n              # x_rec has shape (batch_size, 3, 64, 64)\n              x_rec = self.decode(z)\n              # reconstruction accuracy per batch, i.e., size: (batch_size)\n              factor = 0.5 * (1/self.decoder.fixed_variance)\n              recons_acc = - factor * ((x.view(batch_size, -1) -\n                                      x_rec.view(batch_size, -1))**2\n                                    ).sum(axis=1)\n          return -regularization_term.mean(), -recons_acc.mean()\n  \n      def reconstruct(self, x):\n          mu_E, log_var_E = self.encoder(x)\n          x_rec = self.decoder(mu_E)\n          return x_rec\n  \n      def encode(self, x):\n          # get encoder distribution parameters\n          mu_E, log_var_E = self.encoder(x)\n          # sample noise variable for each batch\n          batch_size = x.shape[0]\n          epsilon = self.normal_dist.sample(sample_shape=(batch_size, )\n                                            ).to(x.device)\n          # get latent variable by reparametrization trick\n          z = mu_E + torch.exp(0.5*log_var_E) * epsilon\n          return z, mu_E, log_var_E\n  \n      def decode(self, z):\n          # get decoder distribution parameters\n          mu_D = self.decoder(z)\n          return mu_D\n  ```\n  :::\n  \n  \n* **Training Parameters**: Lastly, training neural networks itself\n  consists of several hyperparmeters. Again, we are using the same\n  setup as defined in Appendix A.1 of [Watters et\n  al. (2019)](https://arxiv.org/abs/1901.07017), see code below.\n\n\n  ::: {.cell execution_count=5}\n  ``` {.python .cell-code}\n  from livelossplot import PlotLosses\n  from torch.utils.data import DataLoader\n  \n  \n  def train(dataset, epochs, VAE):\n      device = 'cuda' if torch.cuda.is_available() else 'cpu'\n  \n      print('Device: {}'.format(device))\n  \n      data_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n  \n      VAE.to(device)\n      optimizer = torch.optim.Adam(VAE.parameters(), lr=3e-4)\n  \n      losses_plot = PlotLosses(groups={'avg log loss':\n                                      ['kl loss', 'reconstruction loss']})\n      print('Start training with {} decoder\\n'.format(VAE.vae_type))\n      for epoch in range(1, epochs +1):\n          avg_kl = 0\n          avg_recons_err = 0\n          for counter, mini_batch_data in enumerate(data_loader):\n              VAE.zero_grad()\n  \n              kl_div, recons_err = VAE(mini_batch_data[0].to(device))\n              loss = kl_div + recons_err\n              loss.backward()\n              optimizer.step()\n  \n              avg_kl += kl_div.item() / len(dataset)\n              avg_recons_err += recons_err.item() / len(dataset)\n  \n          losses_plot.update({'kl loss': np.log(avg_kl),\n                              'reconstruction loss': np.log(avg_recons_err)})\n          losses_plot.send()\n      trained_VAE = VAE\n      return trained_VAE\n  ```\n  :::\n  \n  \n### Visualization Functions\n\nEvaluating the representation quality of trained models is a difficult\ntask, since we are not only interested in the reconstruction accuracy\nbut also in the latent space and its properties. Ideally the latent\nspace offers a disentangled representation such that each latent\nvariable represents a factor of variation with perfect reconstruction\naccuracy (i.e., for evaluation it is very helpful to know in advance\nhow many and what factors of variation exist). Although there are some\nmetrics to quantify disentanglement, `many of them have serious\nshortcomings and there is yet no consensus in the literature which to\nuse` ([Watters et al., 2019](https://arxiv.org/abs/1901.07017)).\nInstead of focusing on some metric, we are going to visualize the\nresults by using two approaches:\n\n* **Reconstructions and Latent Traversals**: A very popular and\n  helpful plot is to show some (arbitrarly chosen) reconstructions\n  compared to the original input together with a series of latent\n  space traversals. I.e., taking some encoded input and looking at the\n  reconstructions when sweeping each coordinate in the latent space in\n  a predefined interval (here from -2 to +2) while keeping all other\n  coordinates constant. Ideally, each sweep can be associated with\n  a factor of variation. The code below will be used to generate these\n  plots. Note that the reconstructions are clamped into $[0, 1]$ as\n  this is the allowed image range.\n\n\n  ::: {.cell execution_count=6}\n  ``` {.python .cell-code}\n  import matplotlib.pyplot as plt\n  %matplotlib inline\n  \n  \n  def reconstructions_and_latent_traversals(STD_VAE, SBD_VAE, dataset, SEED=1):\n      np.random.seed(SEED)\n      device = 'cuda' if torch.cuda.is_available() else 'cpu'\n      latent_dims = STD_VAE.encoder.latent_dim\n  \n      n_samples = 7\n      i_samples = np.random.choice(range(len(dataset)), n_samples, replace=False)\n  \n      # preperation for latent traversal\n      i_latent = i_samples[n_samples//2]\n      lat_image = dataset[i_latent][0]\n      sweep = np.linspace(-2, 2, n_samples)\n  \n      fig = plt.figure(constrained_layout=False, figsize=(2*n_samples, 2+latent_dims))\n      grid = plt.GridSpec(latent_dims + 5, n_samples*2 + 3,\n                          hspace=0.2, wspace=0.02, figure=fig)\n      # standard VAE\n      for counter, i_sample in enumerate(i_samples):\n          orig_image = dataset[i_sample][0]\n          # original\n          main_ax = fig.add_subplot(grid[1, counter + 1])\n          main_ax.imshow(transforms.ToPILImage()(orig_image))\n          main_ax.axis('off')\n          main_ax.set_aspect('equal')\n  \n          # reconstruction\n          x_rec = STD_VAE.reconstruct(orig_image.unsqueeze(0).to(device))\n          # clamp output into [0, 1] and prepare for plotting\n          recons_image =  torch.clamp(x_rec, 0, 1).squeeze(0).cpu()\n  \n          main_ax = fig.add_subplot(grid[2, counter + 1])\n          main_ax.imshow(transforms.ToPILImage()(recons_image))\n          main_ax.axis('off')\n          main_ax.set_aspect('equal')\n      # latent dimension traversal\n      z, mu_E, log_var_E = STD_VAE.encode(lat_image.unsqueeze(0).to(device))\n      for latent_dim in range(latent_dims):\n          for counter, z_replaced in enumerate(sweep):\n              z_new = z.detach().clone()\n              z_new[0][latent_dim] = z_replaced\n  \n              # clamp output into [0, 1] and prepare for plotting\n              img_rec = torch.clamp(STD_VAE.decode(z_new), 0, 1).squeeze(0).cpu()\n  \n              main_ax = fig.add_subplot(grid[4 + latent_dim, counter + 1])\n              main_ax.imshow(transforms.ToPILImage()(img_rec))\n              main_ax.axis('off')\n      # SBD VAE\n      for counter, i_sample in enumerate(i_samples):\n          orig_image = dataset[i_sample][0]\n          # original\n          main_ax = fig.add_subplot(grid[1, counter + n_samples + 2])\n          main_ax.imshow(transforms.ToPILImage()(orig_image))\n          main_ax.axis('off')\n          main_ax.set_aspect('equal')\n          # reconstruction\n          x_rec = SBD_VAE.reconstruct(orig_image.unsqueeze(0).to(device))\n          # clamp output into [0, 1] and prepare for plotting\n          recons_image = torch.clamp(x_rec, 0, 1).squeeze(0).cpu()\n  \n          main_ax = fig.add_subplot(grid[2, counter + n_samples + 2])\n          main_ax.imshow(transforms.ToPILImage()(recons_image))\n          main_ax.axis('off')\n          main_ax.set_aspect('equal')\n      # latent dimension traversal\n      z, mu_E, log_var_E = SBD_VAE.encode(lat_image.unsqueeze(0).to(device))\n      for latent_dim in range(latent_dims):\n          for counter, z_replaced in enumerate(sweep):\n              z_new = z.detach().clone()\n              z_new[0][latent_dim] = z_replaced\n              # clamp output into [0, 1] and prepare for plotting\n              img_rec = torch.clamp(SBD_VAE.decode(z_new), 0, 1).squeeze(0).cpu()\n  \n              main_ax = fig.add_subplot(grid[4+latent_dim, counter+n_samples+2])\n              main_ax.imshow(transforms.ToPILImage()(img_rec))\n              main_ax.axis('off')\n      # prettify by adding annotation texts\n      fig = prettify_with_annotation_texts(fig, grid, n_samples, latent_dims)\n      return fig\n  \n  def prettify_with_annotation_texts(fig, grid, n_samples, latent_dims):\n      # figure titles\n      titles = ['Deconv Reconstructions', 'Spatial Broadcast Reconstructions',\n                'Deconv Traversals', 'Spatial Broadcast Traversals']\n      idx_title_pos = [[0, 1, n_samples+1], [0, n_samples+2, n_samples*2+2],\n                      [3, 1, n_samples+1], [3, n_samples+2, n_samples*2+2]]\n      for title, idx_pos in zip(titles, idx_title_pos):\n          fig_ax = fig.add_subplot(grid[idx_pos[0], idx_pos[1]:idx_pos[2]])\n          fig_ax.annotate(title, xy=(0.5, 0), xycoords='axes fraction',\n                          fontsize=14, va='bottom', ha='center')\n          fig_ax.axis('off')\n      # left annotations\n      fig_ax = fig.add_subplot(grid[1, 0])\n      fig_ax.annotate('input', xy=(1, 0.5), xycoords='axes fraction',\n                      fontsize=12,  va='center', ha='right')\n      fig_ax.axis('off')\n      fig_ax = fig.add_subplot(grid[2, 0])\n      fig_ax.annotate('recons', xy=(1, 0.5), xycoords='axes fraction',\n                      fontsize=12, va='center', ha='right')\n      fig_ax.axis('off')\n      fig_ax = fig.add_subplot(grid[4:latent_dims + 4, 0])\n      fig_ax.annotate('latent coordinate traversed', xy=(0.9, 0.5),\n                      xycoords='axes fraction', fontsize=12,\n                      va='center', ha='center', rotation=90)\n      fig_ax.axis('off')\n      # pertubation magnitude\n      for i_y_grid in [[1, n_samples+1], [n_samples+2, n_samples*2+2]]:\n          fig_ax = fig.add_subplot(grid[latent_dims + 4, i_y_grid[0]:i_y_grid[1]])\n          fig_ax.annotate('pertubation magnitude', xy=(0.5, 0),\n                          xycoords='axes fraction', fontsize=12,\n                          va='bottom', ha='center')\n          fig_ax.set_frame_on(False)\n          fig_ax.axes.set_xlim([-2.5, 2.5])\n          fig_ax.xaxis.set_ticks([-2, 0, 2])\n          fig_ax.xaxis.set_ticks_position('top')\n          fig_ax.xaxis.set_tick_params(direction='inout', pad=-16)\n          fig_ax.get_yaxis().set_ticks([])\n      # latent dim\n      for latent_dim in range(latent_dims):\n          fig_ax = fig.add_subplot(grid[4 + latent_dim, n_samples*2 + 2])\n          fig_ax.annotate('lat dim ' + str(latent_dim + 1), xy=(0, 0.5),\n                          xycoords='axes fraction',\n                          fontsize=12, va='center', ha='left')\n          fig_ax.axis('off')\n      return\n  ```\n  :::\n  \n  \n* **Latent Space Geometry**: While latent traversals may be helpful, [Watters et al.\n(2019)](https://arxiv.org/abs/1901.07017) note that this techniques\nsuffers from two shortcommings:\n   1. Latent space entanglement might be difficult to perceive by eye.\n   2. Traversals are only taken at some point in space. It could be\n      that traversals at some points are more disentangled than at\n      other positions. Thus, judging disentanglement by the\n      aforementioned method might be ultimately dependent to randomness.\n\n   To overcome these limitations, they propose a new method which they\n   term *latent space geometry*. The main idea is to visualize a\n   transformation from a 2-dimensional generative factor space\n   (subspace of all generative factors) into the 2-dimensional latent\n   subspace (choosing the two latent components that correspond to the\n   factors of variation). Latent space geometry that preserves the\n   chosen geometry of the generative factor space (while scaling and\n   rotation might be allowed depending on the chosen generative factor\n   space) indicates disentanglement.\n\n   To put this into practice, the code below creates circle images\n   by varying $x$ and $y$ positions uniformly and keeping the other\n   generative factors (*here* only color) constant. Accordingly, the\n   geometry of the generative factor space is a uniform grid (which\n   will be plotted). These images will be encoded into mean and\n   variance of the latent distribution. In order to find the latent components that\n   correspond to the $x$ and $y$ position, we choose the components\n   with smallest mean variance across all reconstructions, i.e., the\n   most informative components[^5]. Then, we can plot the latent space\n   geometry by using the latent components of the mean (encoder\n   distribution), see code below.\n\n\n  ::: {.cell execution_count=7}\n  ``` {.python .cell-code}\n  def latent_space_geometry(STD_VAE, SBD_VAE):\n      device = 'cuda' if torch.cuda.is_available() else 'cpu'\n      plt.figure(figsize=(18, 6))\n      # x,y position grid in [0.2, 0.8] (generative factors)\n      equi = np.linspace(0.2, 0.8, 31)\n      equi_without_vert = np.setdiff1d(equi, np.linspace(0.2, 0.8, 6))\n  \n      x_pos = np.append(np.repeat(np.linspace(0.2, 0.8, 6), len(equi)),\n                        np.tile(equi_without_vert, 6))\n      y_pos = np.append(np.tile(equi, 6),\n                        np.repeat(np.linspace(0.8, 0.2, 6), len(equi_without_vert)))\n      labels = np.append(np.repeat(np.arange(6), 31),\n                        np.repeat(np.arange(6)+10, 25))\n      # plot generative factor geometry\n      plt.subplot(1, 3, 1)\n      plt.scatter(x_pos, y_pos, c=labels, cmap=plt.cm.get_cmap('rainbow', 10))\n      plt.gca().set_title('Ground Truth Factors', fontsize=16)\n      plt.xlabel('X-Position')\n      plt.ylabel('Y-Position')\n  \n      # generate images\n      img_size = 64\n      shape_size = 16\n      images = torch.empty([len(x_pos), 3, img_size, img_size]).to(device)\n      for counter, (x, y) in enumerate(zip(x_pos, y_pos)):\n          images[counter] = generate_img(x, y, 'circle', 'red',\n                                        img_size, shape_size)\n  \n      # STD VAE\n      [all_mu, all_log_var] = STD_VAE.encoder(images)\n      # most informative latent variable\n      lat_1, lat_2 = all_log_var.mean(axis=0).sort()[1][:2]\n      # latent coordinates\n      x_lat = all_mu[:, lat_1].detach().cpu().numpy()\n      y_lat = all_mu[:, lat_2].detach().cpu().numpy()\n      # plot latent space geometry\n      plt.subplot(1, 3, 2)\n      plt.scatter(x_lat, y_lat, c=labels, cmap=plt.cm.get_cmap('rainbow', 10))\n      plt.gca().set_title('DeConv', fontsize=16)\n      plt.xlabel('latent 1 value')\n      plt.ylabel('latent 2 value')\n  \n      # SBD VAE\n      [all_mu, all_log_var] = SBD_VAE.encoder(images)\n      # most informative latent variable\n      lat_1, lat_2 = all_log_var.mean(axis=0).sort()[1][:2]\n      # latent coordinates\n      x_lat = all_mu[:, lat_1].detach().cpu().numpy()\n      y_lat = all_mu[:, lat_2].detach().cpu().numpy()\n      # plot latent space geometry\n      plt.subplot(1, 3, 3)\n      plt.scatter(x_lat, y_lat, c=labels, cmap=plt.cm.get_cmap('rainbow', 10))\n      plt.gca().set_title('Spatial Broadcast', fontsize=16)\n      plt.xlabel('latent 1 value')\n      plt.ylabel('latent 2 value')\n      return\n  ```\n  :::\n  \n  \n [^5]: An intuitve way to understand why latent compontents with smaller variance within the encoder distribution are more informative than others is to think about the sampled noise and the loss function: If the variance is high, the latent code $\\textbf{z}$ will vary a lot which in turn makes the task for the decoder more difficult. However, the regularization term (KL-divergence) pushes the variances towards 1. Thus, the network will only reduce the variance of its components if it helps to increase the reconstruction accuracy.\n\n### Results\n\nLastly, let's train our models and look at the results:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nepochs = 150\nlatent_dims = 5 # x position, y position, color, extra slots\nfixed_variance = 0.3\n\nstandard_VAE = VAE(vae_type='Standard', latent_dim=latent_dims,\n                   fixed_var=fixed_variance)\nSBD_VAE = VAE(vae_type='SBD', latent_dim=latent_dims,\n              fixed_var=fixed_variance)\n```\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ntrained_standard_VAE  = train(sprites_dataset, epochs, standard_VAE)\n```\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\ntrained_SBD_VAE = train(sprites_dataset, epochs, SBD_VAE)\n```\n:::\n\n\nAt the log-losses plots, we can already see that using the Spatial\nBroadcast  decoder results in an improved reconstruction accuracy and\nregularization term. Now let's compare both models visually by their\n\n* **Reconstructions and Latent Traversals**:\n\n\n  ::: {.cell execution_count=11}\n  ``` {.python .cell-code}\n  reconstructions_and_latent_traversals(trained_standard_VAE,\n                                        trained_SBD_VAE, sprites_dataset)\n  ```\n  :::\n  \n  \n  While the reconstructions within both models look pretty good, the\n  latent space traversal shows an entangled representation in the\n  standard (DeConv) VAE whereas the Spatial Broadcast model seems quite\n  disentangled.\n\n* **Latent Space Geometry**:\n\n\n  ::: {.cell execution_count=12}\n  ``` {.python .cell-code}\n  latent_space_geometry(trained_standard_VAE, trained_SBD_VAE)\n  ```\n  :::\n  \n  \n  The latent space geometry verifies our previous findings: The\n  DeConv decoder has an entangled latent space (transformation\n  is highly non linear) whereas in the Spatial Broadcast decoder the\n  latent space geometry highly resembles the generating factors\n  geometry (affine transformation). The transformation of the Spatial\n  Broadcast decoder indicates very similar behavior in the $X-Y$\n  position subspace (of generative factors) as in the corresponding\n  latent subspace.\n\n[^4]: The Spatial Broadcast decoder architecture is slightly modified: Kernel size of 3 instead of 4 to get the desired output shapes.\n\n<!-- [^4]: Note that in the Spatial Broadcast decoder the height and width -->\n<!--     of the CNN input needs to be both 6 larger than the target -->\n<!--     output (image) size to accommodate for the lack of padding. This -->\n<!--     is not stated in the paper, however described in the appendix B.1 -->\n<!--     of the follow up paper by [Burgess et al. (2019)](https://arxiv.org/abs/1901.11390). -->\n\n## Drawbacks of Paper\n\n\n* although there are fewer parameters in the Spatial Broadcast\n  decoder, it does require more memory (in the implementation about\n  50% more)\n* longer training times compared to standard DeConv VAE\n* appended coordinate channels do not help when there is no positional\nvariation\n<!-- * mostly applicable in the context of static images with positional -->\n<!--   variation  -->\n<!--   => temporal correlations -->\n\n## Acknowledgement\n\n[Daniel Daza's](https://dfdazac.github.io/) blog was really helpful\nand the presented code is highly inspired by his [VAE-SBD implementation](https://github.com/dfdazac/vaesbd).\n\n-----------------------------------------------------------\n\n[^3]: For simplicity, we are setting the number of (noise variable)\n    samples $L$ per datapoint to 1 (see equation for\n    $\\displaystyle \\widetilde{\\mathcal{L}}$ in [*Reparametrization\n    Trick*](https://borea17.github.io/paper_summaries/auto-encoding_variational_bayes#model-description)\n    paragraph).\n    Note that [Kingma and Welling\n    (2013)](https://arxiv.org/abs/1312.6114) stated that in their\n    experiments setting $L=1$ sufficed as long as the minibatch size\n    was large enough.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}