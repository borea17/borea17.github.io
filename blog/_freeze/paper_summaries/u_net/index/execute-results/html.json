{
  "hash": "c257789d74d97a3ef997ceeb26bc6f3c",
  "result": {
    "markdown": "---\ntitle: \"U-Net: Convolutional Networks for Biomedical Image Segmentation\"\ncategories: [\"reimplementation\", \"CNN\", \"image segmentation\"]\ndate: \"2020-08-21\"\nexecute:\n  eval: false # true\nengine: jupyter\nformat:\n  html: \n    code-fold: show \n    highlight-style: github \n    code-block-bg: true\n    code-tools: \n      toggle: true\n      source: \"https://github.com/borea17/Notebooks/blob/master/03_U-Net.ipynb\"\n---\n\n<!-- nextjournal_link: \"https://nextjournal.com/borea17/u-net/\" -->\n\n[Ronneberger et al. (2015)](https://arxiv.org/abs/1505.04597)\nintroduced a novel neural network architecture to generate better\nsemantic segmentations (i.e., class label assigend to each\npixel) in limited datasets which is a typical challenge in the area of\nbiomedical image processing (see figure below for an example). In\nessence, their model consists of a U-shaped\nconvolutional neural network (CNN) with skip connections between\nblocks to capture context information, while allowing for precise\nlocalizations. In addition to the network architecture, they describe\nsome data augmentation methods to use available data more\nefficiently. By the time the paper was published, the proposed\narchitecture won several segmentation challenges in the field of\nbiomedical engineering, outperforming state-of-the-art models by a large\nmargin. Due to its success and efficiency, U-Net has become a\nstandard architecture when it comes to image segmentations tasks even\nin the non-biomedical area (e.g., [image-to-image\ntranslation](https://arxiv.org/abs/1611.07004), [neural style\ntransfer](https://arxiv.org/abs/1706.03319), [Multi-Objetct\nNetwork](https://arxiv.org/abs/1901.11390)).\n\n| ![Semantic Segmentation Example](./img/semantic_segmentation.png \"Semantic Segmentation Example\") |\n| :--  |\n| Example of a biomedical image segmentation task in which dental x-ray images should be segmented: <br> (**Left**) Raw dental image. <br> (**Right**) Ground truth segmentation, each color represents some class (e.g., red=pulp, blue=caries). <br>Taken from [ISBI 2015 Challenge on Computer-Automated Detection of Caries in Bitewing Radiography](http://www-o.ntust.edu.tw/~cweiwang/ISBI2015/challenge2/index.html) |\n\n## Model Description\n\nU-Net builds upon the ideas of `Fully Convolutional Networks (FCNs) for Semantic\nSegmentation` by [Long et al. (2015)](https://arxiv.org/abs/1411.4038) who\nsuccessfully trained FCNs (including convolutional prediction, upsampling layers\nand skip connections) end-to-end (pixels-to-pixels) on semantic\nsegmentation tasks. U-Net is basically a modified version of the FCN\nby making the architecture more symmetric, i.e., adding a more\npowerful expansive path. [Ronneberger et al.\n(2015)](https://arxiv.org/abs/1505.04597) argue that this modification\nyields more precise segmentations due to its capacity to better propagate\ncontext information to higher resolution layers.\n\n**FCN architecture**: The main idea of the FCN architecture is to take\na standard classification network (such as VGG-16), discard the final\nclassifier layer, convert fully connected layers into convolutions\n(i.e., prediction layers) and add skip connections to (some) pooling\nlayers, see figure below. The skip connections consist of a prediction\n($1 \\times 1$ convolutional layer with channel dimension equal to\nnumber of possible classes) and a deconvolutional (upsampling) layer.\n\n| ![Example FCN Architecture](./img/FCN_example2.png \"Example FCN Architecture\") |\n| :--  |\n| Example of FCN Architecture. VGG-16 net is used as feature learning part. Numbers under the cubes indicate the number of output channels. The prediction layer is itself a $1 \\times 1$ convolutional layer (the final output consists only of 6 possible classes). A final softmax layer is added to output a normalized classification per pixel. Taken from [Tai et al. (2017)](https://arxiv.org/abs/1610.01732) |\n\n**U-Net architecture**: The main idea of the U-Net architecture is to\nbuild an encoder-decoder FCN with skip connections between\ncorresponding blocks, see figure below. The left side of U-Net, i.e.,\n*contractive path* or *encoder*, is very similar to the left side of\nthe FC architecture above. The right side of U-Net, i.e., *expansive\npath* or *decoder*, differs due to its number of feature channels and the\nconvolutional + ReLu layers. Note that the input image size is\ngreater than the output segmentation size, i.e., the network only segments\nthe inner part of the image[^1].\n\n[^1]: A greater input image than output segmentation size makes sense since\n    the network has no information about the surrounding of the input image.\n\n\n| ![U-Net Architecture](./img/u_net_architecture.png \"U-Net Architecture\") |\n| :--  |\n| U-Net architecture as proposed by [Ronneberger et al. (2015)](https://arxiv.org/abs/1505.04597).  |\n\n**Motivation**: Semantic segmentation of images can be divided into two tasks\n\n* **Context Information Retrieval**: Global information about the\n  different parts of the image, e.g., in a CNN classification\n  network after training there might be some feature representation\n  for *nose*, *eyes* and *mouth*. Depending on the feature combination at hand, the\n  network may classify the image as *human* or *not human*.\n* **Localization of Context Information**: In addition to `what`,\n  localization ensures `where`. Semantic segmentation is only possible\n  when content information can be localized. Note: In image\n  classification, we are often not interested in `where`[^2].\n\n[Long et al. (2015)](https://arxiv.org/abs/1411.4038) argue that\nCNNs during classification tasks must learn useful feature\nrepresentations, i.e., classification nets are capable to solve the *context\ninformation retrieval* task. Fully connected layers are inappropriate\nfor semantic segmentation as they throw away the principle of\nlocalization. These two arguments motivate the use of FCNs that take\nthe feature representation part of classification nets and convert\nfully connected layers into convolutions. During the *contractive*\npath, information gets compressed into coarse appearance/context\ninformation. However, in this process the dimensionality of the input\nis reduced massively. Skip connections are introduced to combine coarse,\nsemantic information of deeper layers with finer, appearance\ninformation of early layers. Thereby, the *localization* task is addressed.\n\n[Ronneberger et al. (2015)](https://arxiv.org/abs/1505.04597) extend\nthese ideas by essentially increasing the capacity of the decoder\npath. The symmetric architecture allows to combine low level feature\nmaps (left side, fine information) with high level feature maps (right side,\ncoarse information) more effectively such that context\ninformation can be better propagated to higher resolution layers (top right).\nAs a result, more precise segmentations can be retrieved even with\nfew training examples, indicating that the optimization problem is\nbetter posed in U-Nets.\n\n[^2]: Actually, CNNs should put more emphasis on the `where` or rather\n    the local relation between context information, see [Geoffrey\n    Hinton's comment about\n    pooling](https://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/clyj4jv/).\n\n## Implementatation\n\n[Ronneberger et al. (2015)](https://arxiv.org/abs/1505.04597)\ndemonstrated U-Net application results for three different\nsegmentation tasks and open-sourced their original\n[U-Net\nimplementation](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/)\n(or rather the ready trained network). The whole training process and data\naugmentation procedures are not provided (except for overlap-tile\nsegmentation). The following reimplementation aims to give an\nunderstanding of the whole paper (data augmentation and training\nprocess included), while being as simple as possible. Note that there\nare lots of open-source U-Net reimplementations out there, however\nmost of them are already modified versions.\n\n### EM Dataset\n\nOnly the first task of the three different U-Net applications is\nreimplemented: The segmentation of neuronal structures in electron\nmicroscopic (EM) recordings. The training data consists of 30 images\n($512 \\times 512$ pixels with 8-bit grayscale) from the ventral nerve\ncord of some species of fruit flies together with the corresponding 30\nbinary segmentation masks (white pixels for segmented objects, black\nfor the rest), see gif below. The dataset formed part of the 2D EM\nsegmentation challenge at the ISBI 2012 conference. Although the workshop\ncompetition is done, the challenge remains open for new contributions.\nFurther details about the data can be found at the [ISBI Challenge\nwebsite](http://brainiac2.mit.edu/isbi_challenge/), where also the training\nand test data can be downloaded (after registration).\n\n| ![EM training data.](./img/EM_dataset.gif \"EM training data\") |\n| :--:  |\n| EM training data. Taken from [ISBI Challenge](http://brainiac2.mit.edu/isbi_challenge/). |\n\nThe following function can be used to load the training dataset.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom PIL import Image\nimport torch\nimport torchvision.transforms as transforms\n\n\ndef load_dataset():\n    num_img, img_size = 30, 512\n    # initialize\n    imgs = torch.zeros(num_img, 1, img_size, img_size)\n    labels = torch.zeros(num_img, 1, img_size, img_size)\n    # fill tensors with data\n    for index in range(num_img):\n        cur_name = str(index) + '.png'\n\n        img_frame = Image.open('./Dataset/train/image/' + cur_name)\n        label_frame = Image.open('./Dataset/train/label/' + cur_name)\n\n        imgs[index] = transforms.ToTensor()(img_frame).type(torch.float32)\n        labels[index] = transforms.ToTensor()(label_frame).type(torch.float32)\n    return imgs, labels\n\n\nimgs, labels = load_dataset()\n```\n:::\n\n\n### Data Augmentation\n\nTraining neural networks on image data typically requires large\namounts of data to make the model robust (i.e., avoid overfitting) and\naccurate (i.e., avoid underfitting). However, data scarcity is a common\nproblem in biomedical segmentation tasks, since labeling is\nexpensive and time consuming. In such cases, **data augmentation**\noffers a solution by generating additional data (using plausible\ntransformations) to expand the training dataset. In most image\nsegmentation tasks the function to be learned has some\ntransformation-invariance properties (e.g., translating the input\nshould result in a translated output). The data augmentation applied by\n[Ronneberger et al. (2015)](https://arxiv.org/abs/1505.04597) can be\ndivided into four parts:\n\n\n* **Overlap-tile strategy** is used to divide an arbitrary large image\n  into several overlaping parts (each forming an input and label to\n  the training algorithm). Remind that the input to the neural network is\n  greater than the output, in case of the EM dataset the input is even\n  greater than the whole image. Therefore, [Ronneberger et al.\n  (2015)](https://arxiv.org/abs/1505.04597) expand the images\n  by mirroring at the sides. The overlap-tile strategy is shown below.\n  Depending on the `stride` (i.e., how much the next rectangle is\n  shifted to the right), the training dataset is enlarged by a factor\n  greater than 4.\n\n  | ![Overlap-Tile Strategy](./img/overlap_tile_self.png \"Overlap-Tile Strategy\") |\n  | :--  |\n  | Overlap-Tile Strategy for seamless segmentation of arbitrary large images. Blue area depicts input to neural network, yellow area corresponds to the prediction area. Missing input is extrapolated by mirroring (white lines). The number of tiles depends on the `stride` length (here: `stride=124`). Image created with `visualize_overlap_tile_strategy` (code presented at the end of this section). |\n\n* **Affine transformations** are mathematically defined as\n  geometric transformations preserving lines and parallelisms, e.g.,\n  scaling, translation, rotation, reflection or any mix of them.\n  [Ronneberger et al. (2015)](https://arxiv.org/abs/1505.04597) state\n  that in case of microscopical image data mainly translation and\n  rotation invariance (as affine transformation invariances)\n  are desired properties of the resulting function. Note that the\n  overlap-tile strategy itself leads to some translation\n  invariance.\n\n  | ![Affine Transformation Visualization](./img/affine_transformation_with_grid.png \"Affine Transformation Visualization\") |\n  | :-- |\n  | Affine transformation visualization. Left side shows input and label data before transformation is applied. Right side shows the corresponding data after random affine transformation (random rotation and shifting). The grid is artificially added to emphasize that image and label are transformed in the same way. Image created with `visualize_data_augmentation` (code presented at the end of this section). |\n\n* **Elastic deformations** are basically distinct affine\n  transformations for each pixel. The term is probably derived from\n  physics in which an elastic deformation describes a temporary change\n  in shape of an elastic material (due to induced\n  force). The transformation result looks similar to the physics\n  phenomenon, see image below. [Ronneberger et al.\n  (2015)](https://arxiv.org/abs/1505.04597) noted that elastic\n  deformations seem to be a key concept for successfully training with\n  few samples. A possible reason may be that the model's\n  generalization capabilities improve more by elastic deformations\n  since the resulting images have more variability than with coherent\n  affine transformations.\n\n\n  | ![Elastic Deformation Visualization](./img/deformation_with_grid2.png \"Elastic Deformation Visualization\") |\n  | :-- |\n  | Elastic deformation visualization. Left side shows input and label data before deformation is applied. Right side shows the corresponding data after deformation. The grid is artificially added to emphasize that image and label are deformed in the same way. Image created with `visualize_data_augmentation` (code presented at the end of this section). |\n\n  Implementing elastic deformations basically consists of generating\n  random displacement fields, convolving these with a Gaussian filter\n  for smoothening, scaling the result by a predefined factor to\n  control the intensity and then computing the new pixel values for\n  each displacement vector (using interpolation within the old grid),\n  see Best Practices for CNNs by [Simard et al.\n  (2003)](https://www.researchgate.net/publication/220860992_Best_Practices_for_Convolutional_Neural_Networks_Applied_to_Visual_Document_Analysis)\n  for more details. <!-- Note that [Ronneberger et al. -->\n  <!-- (2015)](https://arxiv.org/abs/1505.04597) use a slightly different -->\n  <!-- method to generate elastic deformations by directly sampling from a Gaussian -->\n  <!-- distribution instead of convolving uniform sampled random vectors -->\n  <!-- with a Gaussian. -->\n\n* **Color variations** or in this case rather **gray value\n  variations** in the input image should make the network invariant\n  to small color changes. This can easily be implemented by adding\n  Gaussian noise (other distributions may also be possible) to the\n  input image, see image below.\n\n  | ![Gray Value Variation Visualization](./img/gray_variation.png \"Gray Value Variation Visualization\") |\n  | :-- |\n  | Gray value variation visualization. Left side shows input image before noise is applied. Right side shows the corresponding data after transformation (segmentation mask does not change). Image created with `visualize_data_augmentation` (code presented at the end of this section). |\n\n\nThe whole data augmentation process is put into a self written Pytorch `Dataset`\nclass, see code below. Note that while this class includes all\ndescribed transformations (*affine transformation*, *elastic\ndeformation* and *gray value variation*), in the `__get_item__` method\nonly `elastic_deform` is applied to speed up the training process[^3].\nHowever, if you want to create a more sophisticated data augmentation\nprocess, you can easily add the other transformations in the `__get_item__`\nmethod.\n\n[^3]: The implementation intends to be easily understandable, while keeping the computational resources low. Thus, it is not aimed to generate the best training results or model performance.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom torch.utils.data import Dataset\nimport numpy as np\nfrom scipy.ndimage.interpolation import map_coordinates\nfrom scipy.signal import convolve2d\nimport torchvision.transforms.functional as TF\n\n\nclass EM_Dataset(Dataset):\n    \"\"\"EM Dataset (from ISBI 2012) to train U-Net on including data\n    augmentation as proposed by Ronneberger et al. (2015)\n\n    Args:\n        imgs (tensor): torch tensor containing input images [1, 512, 512]\n        labels (tensor): torch tensor containing segmented images [1, 512, 512]\n        stride (int): stride that is used for overlap-tile strategy,\n            Note: stride must be chosen such that all labels are retrieved\n        transformation (bool): transform should be applied (True) or not (False)\n    ------- transformation related -------\n        probability (float): probability that transformation is applied\n        alpha (float): intensity of elastic deformation\n        sigma (float): std dev. of Gaussian kernel, i.e., smoothing parameter\n        kernel dim (int): kernel size is [kernel_dim, kernel_dim]\n    \"\"\"\n\n    def __init__(self, imgs, labels, stride, transformation=False,\n                 probability=None, alpha=None, sigma=None, kernel_dim=None):\n        super().__init__()\n        assert isinstance(stride, int) and stride <= 124 and \\\n          round((512-388)/stride) == (512-388)/stride\n        self.orig_imgs = imgs\n        self.imgs = EM_Dataset._extrapolate_by_mirroring(imgs)\n        self.labels = labels\n        self.stride = stride\n        self.transformation = transformation\n        if transformation:\n            assert 0 <= probability <= 1\n            self.probability = probability\n            self.alpha = alpha\n            self.kernel = EM_Dataset._create_gaussian_kernel(kernel_dim, sigma)\n        return\n\n    def __getitem__(self, index):\n        \"\"\"images and labels are divided into several overlaping parts using the\n        overlap-tile strategy\n        \"\"\"\n        number_of_tiles_1D = (1 + int((512 - 388)/self.stride))\n        number_of_tiles_2D = number_of_tiles_1D**2\n\n        img_index = int(index/number_of_tiles_2D)\n        # tile indexes of image\n        tile_index = (index % number_of_tiles_2D)\n        tile_index_x = (tile_index % number_of_tiles_1D) * self.stride\n        tile_index_y = int(tile_index / number_of_tiles_1D) * self.stride\n\n        img = self.imgs[img_index, :,\n                        tile_index_y:tile_index_y + 572,\n                        tile_index_x:tile_index_x + 572]\n        label = self.labels[img_index, :,\n                            tile_index_y: tile_index_y + 388,\n                            tile_index_x: tile_index_x + 388]\n        if self.transformation:\n            if np.random.random() > 1 - self.probability:\n                img, label = EM_Dataset.elastic_deform(img, label, self.alpha,\n                                                       self.kernel)\n        return (img, label)\n\n    def __len__(self):\n        number_of_imgs = len(self.imgs)\n        number_of_tiles = (1 + int((512 - 388)/self.stride))**2\n        return number_of_imgs * number_of_tiles\n\n    @staticmethod\n    def gray_value_variations(image, sigma):\n        \"\"\"applies gray value variations by adding Gaussian noise\n\n        Args:\n            image (torch tensor): extrapolated image tensor [1, 572, 572]\n            sigma (float): std. dev. of Gaussian distribution\n\n        Returns:\n            image (torch tensor): image tensor w. gray value var. [1, 572, 572]\n        \"\"\"\n        # see https://stats.stackexchange.com/a/383976\n        noise = torch.randn(image.shape, dtype=torch.float32) * sigma\n        return image + noise\n\n    @staticmethod\n    def affine_transform(image, label, angle, translate):\n        \"\"\"applies random affine translations and rotation on image and label\n\n        Args:\n            image (torch tensor): extrapolated image tensor [1, 572, 572]\n            label (torch tensor): label tensor [1, 388, 388]\n            angle (float): rotation angle\n            translate (list): entries correspond to horizontal and vertical shift\n\n        Returns:\n            image (torch tensor): transformed image tensor [1, 572, 572]\n            label (torch tensor): transformed label tensor [1, 388, 388]\n        \"\"\"\n        # transform to PIL\n        image = transforms.ToPILImage()(image[0])\n        label = transforms.ToPILImage()(label[0])\n        # apply affine transformation\n        image = TF.affine(image, angle=angle, translate=translate,\n                          scale=1, shear=0)\n        label = TF.affine(label, angle=angle, translate=translate,\n                          scale=1, shear=0)\n        # transform back to tensor\n        image = transforms.ToTensor()(np.array(image))\n        label = transforms.ToTensor()(np.array(label))\n        return image, label\n\n    @staticmethod\n    def elastic_deform(image, label, alpha, gaussian_kernel):\n        \"\"\"apply smooth elastic deformation on image and label data as\n        described in\n\n        [Simard2003] \"Best Practices for Convolutional Neural Networks applied\n        to Visual Document Analysis\"\n\n        Args:\n            image (torch tensor): extrapolated image tensor [1, 572, 572]\n            label (torch tensor): label tensor [1, 388, 388]\n            alpha (float): intensity of transformation\n            gaussian_kernel (np array): gaussian kernel used for smoothing\n\n        Returns:\n            deformed_img (torch tensor): deformed image tensor [1, 572, 572]\n            deformed_label (torch tensor): deformed label tensor [1, 388, 388]\n\n        code is adapted from https://github.com/vsvinayak/mnist-helper\n        \"\"\"\n        # generate standard coordinate grids\n        x_i, y_i = np.meshgrid(np.arange(572), np.arange(572))\n        x_l, y_l = np.meshgrid(np.arange(388), np.arange(388))\n        # generate random displacement fields (uniform distribution [-1, 1])\n        dx = 2*np.random.rand(*x_i.shape) - 1\n        dy = 2*np.random.rand(*y_i.shape) - 1\n        # smooth by convolving with gaussian kernel\n        dx = alpha * convolve2d(dx, gaussian_kernel, mode='same')\n        dy = alpha * convolve2d(dy, gaussian_kernel, mode='same')\n        # one dimensional coordinates (neccessary for map_coordinates)\n        x_img = np.reshape(x_i + dx, (-1, 1))\n        y_img = np.reshape(y_i + dy, (-1, 1))\n        x_label = np.reshape(x_l + dx[92:480, 92:480], (-1, 1))\n        y_label = np.reshape(y_l + dy[92:480, 92:480], (-1, 1))\n        # deformation using map_coordinates interpolation (spline not bicubic)\n        deformed_img = map_coordinates(image[0], [y_img, x_img], order=1,\n                                       mode='reflect')\n        deformed_label = map_coordinates(label[0], [y_label, x_label], order=1,\n                                         mode='reflect')\n        # reshape into desired shape and cast to tensor\n        deformed_img = torch.from_numpy(deformed_img.reshape(image.shape))\n        deformed_label = torch.from_numpy(deformed_label.reshape(label.shape))\n        return deformed_img, deformed_label\n\n    @staticmethod\n    def _extrapolate_by_mirroring(data):\n        \"\"\"increase data by mirroring (needed for overlap-tile strategy)\n\n        Args:\n            data (torch tensor): shape [num_samples, 1, 512, 512]\n\n        Returns:\n            extrapol_data (torch tensor): shape [num_samples, 1, 696, 696]\n        \"\"\"\n        num_samples = len(data)\n        extrapol_data = torch.zeros(num_samples, 1, 696, 696)\n\n        # put data into center of extrapol data\n        extrapol_data[:,:, 92:92+512, 92:92+512] = data\n        # mirror left\n        extrapol_data[:,:, 92:92+512, 0:92] = data[:,:,:,0:92].flip(3)\n        # mirror right\n        extrapol_data[:,:, 92:92+512, 92+512::] = data[:,:,:,-92::].flip(3)\n        # mirror top\n        extrapol_data[:,:, 0:92,:] = extrapol_data[:,:,92:92+92,:].flip(2)\n        # mirror buttom\n        extrapol_data[:,:, 92+512::,:] = extrapol_data[:,:, 512:512+92,:].flip(2)\n        return extrapol_data\n\n    @staticmethod\n    def _create_gaussian_kernel(kernel_dim, sigma):\n        \"\"\"returns a 2D Gaussian kernel with the standard deviation\n        denoted by sigma\n\n        Args:\n            kernel_dim (int): kernel size will be [kernel_dim, kernel_dim]\n            sigma (float): std dev of Gaussian (smoothing parameter)\n\n        Returns:\n            gaussian_kernel (numpy array): centered gaussian kernel\n\n        code is adapted from https://github.com/vsvinayak/mnist-helper\n        \"\"\"\n        # check if the dimension is odd\n        if kernel_dim % 2 == 0:\n            raise ValueError(\"Kernel dimension should be odd\")\n        # initialize the kernel\n        kernel = np.zeros((kernel_dim, kernel_dim), dtype=np.float16)\n        # calculate the center point\n        center = kernel_dim/2\n        # calculate the variance\n        variance = sigma ** 2\n        # calculate the normalization coefficeint\n        coeff = 1. / (2 * variance)\n        # create the kernel\n        for x in range(0, kernel_dim):\n            for y in range(0, kernel_dim):\n                x_val = abs(x - center)\n                y_val = abs(y - center)\n                numerator = x_val**2 + y_val**2\n                denom = 2*variance\n\n                kernel[x,y] = coeff * np.exp(-1. * numerator/denom)\n        # normalise it\n        return kernel/sum(sum(kernel))\n\n\n# generate datasets\nstride = 124\nwhole_dataset = EM_Dataset(imgs, labels, stride=stride,\n                           transformation=True, probability=0.5, alpha=50,\n                           sigma=5, kernel_dim=25)\n```\n:::\n\n\nThe visualization functions used to generate the images in this section are provided below:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\n\ndef visualize_overlap_tile_strategy(dataset, img_index, tile_indexes):\n    # compute tiling data\n    number_of_tiles_1D = (1 + int((512 - 388)/dataset.stride))\n    number_of_tiles_2D = number_of_tiles_1D**2\n    # original image [1, 512, 512]\n    orig_img = dataset.orig_imgs[img_index]\n    # extrapolated image [1, 696, 696]\n    extrapol_img = dataset.imgs[img_index]\n\n\n    # start plotting\n    fig = plt.figure(figsize=(14, 7))\n    # original image\n    plt.subplot(1, len(tile_indexes) + 1, 1)\n    plt.imshow(transforms.ToPILImage()(orig_img), cmap='gray')\n    plt.title('Original Image')\n    # extrapolated image with bounding boxes and mirror lines for tile_indexes\n    for index, tile_index in enumerate(tile_indexes):\n        plt.subplot(1, len(tile_indexes) + 1, 2 + index)\n        plt.imshow(transforms.ToPILImage()(extrapol_img), cmap='gray')\n        # calculate tile index x and y\n        tile_ix = (tile_index % number_of_tiles_1D) * dataset.stride\n        tile_iy = int(tile_index / number_of_tiles_1D) * dataset.stride\n        # add focus of current input tile\n        plt.plot([tile_ix, tile_ix + 572, tile_ix + 572, tile_ix, tile_ix],\n                 [tile_iy, tile_iy, tile_iy + 572, tile_iy + 572, tile_iy],\n                 'blue', linewidth=2)\n        # add focus of current segmentation mask\n        tile_ix, tile_iy = tile_ix + 92, tile_iy + 92\n        plt.plot([tile_ix, tile_ix + 388, tile_ix + 388, tile_ix, tile_ix],\n                 [tile_iy, tile_iy, tile_iy + 388, tile_iy + 388, tile_iy],\n                 'yellow', linewidth=2)\n        # add mirror lines\n        plt.vlines([92, 604], 0, 696, 'white', linewidth=1)\n        plt.hlines([92, 604], 0, 696, 'white', linewidth=1)\n        plt.title('Extrapolated Image, Tile '+ str(tile_index + 1) + '/' +\n                  str(number_of_tiles_2D))\n        plt.xlim(0, 696)\n        plt.ylim(696, 0)\n    return\n\n\ndef visualize_data_augmentation(dataset, index, show_grid, kind):\n    # get untransformed img, label\n    dataset.transformation = False\n    img, label = dataset[index]\n    # copy image (since it may be modified)\n    cur_img = img.clone().numpy()\n    cur_label = label.clone().numpy()\n    if show_grid:\n        # modify image to include outer grid (outside of label)\n        cur_img[0, 0:91:25] = 10.0\n        cur_img[0, 480::25] = 10.0\n        cur_img[0, :, 0:91:25] = 10.0\n        cur_img[0, :, 480::25] = 10.0\n        # modify image to include label grid\n        cur_img[0, 92:480:20, 92:480] = -5\n        cur_img[0,  92:480, 92:480:20] = -5\n        # modify label to include label grid\n        cur_label[0, ::20] = -5\n        cur_label[0, :, ::20] = -5\n    if kind == 'elastic deformation':\n        # set transformation\n        kernel = dataset.kernel\n        alpha = dataset.alpha\n        new_img, new_label = EM_Dataset.elastic_deform(cur_img, cur_label,\n                                                       alpha, kernel)\n    elif kind == 'affine transformation':\n        angle = np.random.randint(-3, 3)\n        translate = list(np.random.randint(-3, 3, size=2))\n        new_img, new_label = EM_Dataset.affine_transform(cur_img, cur_label,\n                                                         angle, translate)\n    elif kind == 'gray value variation':\n        sigma = 0.2\n        new_img = EM_Dataset.gray_value_variations(img, sigma)\n        new_label = label\n    else:\n        raise NameError('Unknown `kind`, can only be `elastic deformation`, ' +\n                        '`affine transformation` or `gray value variation`')\n    # start plotting\n    fig = plt.figure(figsize=(10,10))\n    plt.subplot(2, 2, 1)\n    plt.title('Before ' + kind)\n    plt.imshow(cur_img[0], cmap='gray', aspect='equal',\n               interpolation='gaussian', vmax=1, vmin=0)\n    # focus of current segmentation mask\n    plt.plot([92, 480, 480, 92, 92], [92, 92, 480, 480, 92],\n            'yellow', linewidth=2)\n    plt.subplots_adjust(hspace=0.01)\n    plt.subplot(2,2,3)\n    plt.imshow(cur_label[0], cmap='gray', aspect='equal',\n               interpolation='gaussian', vmax=1, vmin=0)\n    plt.subplot(2,2,2)\n    plt.title('After ' + kind)\n    plt.imshow(new_img[0], cmap='gray', aspect='equal',\n               interpolation='gaussian', vmax=1, vmin=0)\n    # focus of current segmentation mask\n    plt.plot([92, 480, 480, 92, 92], [92, 92, 480, 480, 92],\n            'yellow', linewidth=2)\n    plt.subplot(2,2,4)\n    plt.imshow(new_label[0], cmap='gray', aspect='equal',\n               interpolation='gaussian', vmax=1, vmin=0)\n    return\n\n\n# generate images in order of appearance\nvisualize_overlap_tile_strategy(whole_dataset, img_index=0,\n                                tile_indexes=[0, 1])\nvisualize_data_augmentation(whole_dataset, index=0, show_grid=True,\n                            kind='affine transformation')\nvisualize_data_augmentation(whole_dataset, index=0, show_grid=True,\n                            kind='elastic deformation')\nvisualize_data_augmentation(whole_dataset, index=0, show_grid=False,\n                            kind='gray value variation')\n```\n:::\n\n\n### Model Implementation\n\nModel implementation can be divided into three tasks:\n\n* **Network Architecture**: The model architecture is given in the\n  [model\n  description](https://borea17.github.io/paper_summaries/u_net#model-description)\n  in which one can identify several blocks of two $3 \\times 3$ convolutional layers each\n  followed by a ReLU non-linearity (called `_block` in the\n  implementation). Note that the output of the last prediction layer can be\n  understood as the unnormalized prediction for each class, i.e.,\n  $a_{i,j}^{(k)} \\in ] -\\infty, +\\infty[$ where $a^{(k)}$ denotes the\n  activation in feature channel $k \\in \\{1, 2\\}$ (one channel for\n  each class) and the indices ${i,j}$ describe the pixel position. In order to get\n  normalized probabilities for each pixel $\\hat{p}_{i,j}^{(k)}$, a pixel-wise softmax is\n  applied at the end (last operation in `forward`), i.e., after this\n  operation the sum of the two output channels equals one for each\n  pixel $\\hat{p}_{i,j}^{(1)} + \\hat{p}_{i,j}^{(2)} = 1$.\n\n\n  ::: {.cell execution_count=5}\n  ``` {.python .cell-code}\n  from torch import nn\n  \n  \n  class Unet(nn.Module):\n      \"\"\"original U-Net architecture proposed by Ronneberger et al. (2015)\n  \n      Attributes:\n          encoder_blocks (list):  four u_net blocks of encoder path\n          bottleneck_bock: block that mediates between encoder and decoder\n          decoder_blocks (list):  four u_net blocks of decoder path\n          cropped_img_size (list): cropped images size in order of encoder blocks\n          up_convs (list): upsampling (transposed convolutional) layers (decoder)\n          max_pool: max pool operation used in encoder path\n      \"\"\"\n  \n      def __init__(self):\n          super().__init__()\n          self.encoder_blocks = nn.ModuleList([\n              Unet._block(1, 64),\n              Unet._block(64, 128),\n              Unet._block(128, 256),\n              Unet._block(256, 512)\n          ])\n          self.bottleneck_block = Unet._block(512, 1024)\n          self.decoder_blocks = nn.ModuleList([\n              Unet._block(1024, 512),\n              Unet._block(512, 256),\n              Unet._block(256, 128),\n              Unet._block(128, 64)\n          ])\n          self.cropped_img_sizes = [392, 200, 104, 56]\n          self.up_convs = nn.ModuleList([\n              nn.ConvTranspose2d(1024, 512, kernel_size=(2,2), stride=2),\n              nn.ConvTranspose2d(512, 256, kernel_size=(2,2), stride=2),\n              nn.ConvTranspose2d(256, 128, kernel_size=(2,2), stride=2),\n              nn.ConvTranspose2d(128, 64, kernel_size=(2,2), stride=2),\n          ])\n          self.max_pool = nn.MaxPool2d(kernel_size=(2,2))\n          self.prediction = nn.Conv2d(64, 2, kernel_size=(1,1), stride=1)\n          return\n  \n      def forward(self, x):\n          # go through encoder path and store cropped images\n          cropped_imgs = []\n          for index, encoder_block in enumerate(self.encoder_blocks):\n              out = encoder_block(x)\n              # center crop and add to cropped image list\n              cropped_img = Unet._center_crop(out, self.cropped_img_sizes[index])\n              cropped_imgs.append(cropped_img)\n              # max pool output of encoder block\n              x = self.max_pool(out)\n          # bottleneck block (no max pool)\n          x = self.bottleneck_block(x)  # [batch_size, 1024, 28, 28]\n          # go through decoder path with stored cropped images\n          for index, decoder_block in enumerate(self.decoder_blocks):\n              x = self.up_convs[index](x)\n              # concatenate x and cropped img along channel dimension\n              x = torch.cat((cropped_imgs[-1-index], x), 1)\n              # feed through decoder_block\n              x = decoder_block(x)\n          # feed through prediction layer [batch_size, 2, 388, 388]\n          x_pred_unnormalized = self.prediction(x)\n          # normalize prediction for each pixel\n          x_pred = torch.softmax(x_pred_unnormalized, 1)\n          return x_pred\n  \n      @staticmethod\n      def _center_crop(x, new_size):\n          \"\"\"center croping of a square input tensor\n  \n          Args:\n              x: input tensor shape [batch_size, channels, resolution, resolution]\n              new_size: the desired output resolution (taking center of input)\n  \n          Returns:\n              x_cropped: tensor shape [batch_size, channels, new_size, new_size]\n          \"\"\"\n          img_size = x.shape[-1]\n          i_start = int((img_size - new_size)/2)\n          i_end = int((img_size + new_size)/2)\n          x_cropped = x[:, :, i_start:i_end, i_start:i_end]\n          return x_cropped\n  \n      @staticmethod\n      def _block(in_channels, out_channels):\n          \"\"\"block for use in U-Net architecture,\n          consists of two conv 3x3, ReLU layers\n  \n          Args:\n              in_channels: number of input channels for first convolution\n              out_channels: number of output channels for both convolutions\n  \n          Returns:\n              u_net_block: Sequential U net block\n          \"\"\"\n          conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(3,3), stride=1)\n          conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(3,3), stride=1)\n  \n          N_1, N_2 = 9*in_channels, 9*out_channels\n          # initialize by drawing weights from Gaussian distribution\n          conv1.weight.data.normal_(mean=0, std=np.sqrt(2/N_1))\n          conv2.weight.data.normal_(mean=0, std=np.sqrt(2/N_2))\n          # define u_net_block\n          u_net_block = nn.Sequential(\n              conv1,\n              nn.ReLU(),\n              conv2,\n              nn.ReLU()\n          )\n          return u_net_block\n  ```\n  :::\n  \n  \n* **Loss Function**: Since the segmentation labels are clearly\n  imbalanced (much more white pixels than black pixels), [Ronneberger et al.\n  (2015)](https://arxiv.org/abs/1505.04597) use the weighted cross\n  entropy as the loss function (which they term *energy function*)\n\n  $$\n  \\begin{align}\n     J (\\textbf{x}, \\textbf{m}) &= -\\sum_{i=1}^{388}\\sum_{j=1}^{388}\n     \\sum_{k=1}^2 w_{i,j} (\\textbf{m}) \\cdot\n     p_{i,j}^{(k)} \\log \\left( \\widehat{p}_{i,j}^{(k)} \\left( \\textbf{x};\n  \\boldsymbol{\\theta} \\right)  \\right) \\\\\n     &\\text{with} \\quad p_{i,j}^{(1)} = \\begin{cases} 1 & \\text{if }\n  m_{i,j}=1 \\\\ 0 &\\text{else} \\end{cases} \\quad \\text{and} \\quad p_{i,j}^{(2)} =\n  \\begin{cases} 1 & \\text{if } m_{i, j} = 0 \\\\0 & \\text{else}, \\end{cases}\n  \\end{align}\n  $$\n\n  where $\\textbf{x}\\in [0, 1]^{572\\times 572}$ denotes the input image, $\\textbf{m} \\in \\{0,\n  1\\}^{388 \\times 388}$ the corresponding segmentation mask,\n  $\\textbf{p}^{(k)}\\in \\{0,\n  1\\}^{388 \\times 388}$ the groundtruth probability for each class $k$, $\\widehat{\\textbf{p}}^{(k)} \\in\n  [0, 1]^{388\\times 388}$ denotes the $k$-th channel output of the network\n  parameterized by $\\boldsymbol{\\theta}$ and $\\textbf{w} \\left(\n  \\textbf{m} \\right) \\in \\mathbb{R}^{388 \\times 388}$ is a introduced weight map\n  (computed via the segmentation mask $\\textbf{m}$) to give some pixels more\n  importance during training. Accordingly, the loss function can be\n  interpreted as penalizing the deviation from 1 for each true class\n  output pixel weighted by the corresponding entry of the\n  weight map.\n\n  **Weight Map**: To compensate for the imbalance between separation\n  borders and segmented object[^4], [Ronneberger et al.\n  (2015)](https://arxiv.org/abs/1505.04597) introduce the following\n  weight map\n\n  $$\n    w(\\textbf{m}) = {w_c (\\textbf{m})} + {w_0 \\cdot \\exp \\left( - \\frac\n    {\\left(d_1 (\\textbf{m}) - d_2 (\\textbf{m})\\right)^2}{2\\sigma^2}\\right)},\n  $$\n\n  where the first term reweights each pixel of the minority class\n  (i.e., black pixels) to balance the class frequencies. In the second\n  term $d_1$ and $d_2$ denote the distance to the border of the\n  nearest and second nearest cell, respectively. $w_0$ and $\\sigma$\n  are predefined hyperparameters. Thus, the second term can be\n  understood as putting additional weight to smaller borders, see code\n  and image below.\n\n\n  ::: {.cell execution_count=6}\n  ``` {.python .cell-code}\n  from mpl_toolkits.axes_grid1 import make_axes_locatable\n  from skimage import measure\n  from scipy.ndimage.morphology import distance_transform_edt\n  from skimage.segmentation import find_boundaries\n  \n  \n  def compute_weight_map(label_mask, w_0, sigma, plot=False):\n      \"\"\"compute weight map for each ground truth segmentation to compensate\n      for the different class frequencies and to put additional\n      emphasis on small borders as proposed by Ronneberger et al.\n  \n      Args:\n          label mask (torch tensor): true segmentation masks [batch_size, 1, 388, 388]\n          w_0 (float): hyperparameter in second term of weight map\n          sigma (float): hyperparameter in second term of weight map\n  \n      Returns:\n          weight_map (torch tensor): computed weight map [batch_size, 1, 388, 388]\n  \n      researchgate.net/post/creating_a_weight_map_from_a_binary_image_U-net_paper\n      \"\"\"\n      batch_size = label_mask.shape[0]\n      weight_map = torch.zeros_like(label_mask)\n      for i in range(batch_size):\n          # compute w_c to balance class frequencies\n          w_c = label_mask[i][0].clone()\n          class_freq_0 = (label_mask[i]==0).sum().item()\n          class_freq_1 = (label_mask[i]==1).sum().item()\n          w_c[label_mask[i][0]==0] = class_freq_1 / class_freq_0\n          # compute d_1, d_2, i.e., euclid. dist. to border of (1st/2nd) closest cell\n          d_1 = np.zeros(label_mask[i][0].shape)\n          d_2 = np.zeros(label_mask[i][0].shape)\n          # distinguish all cells (connected components of ones)\n          all_cells = measure.label(label_mask[i][0], background=0, connectivity=2)\n          num_cells = np.max(all_cells)\n          # initialize distances for all cells\n          dists = np.zeros([num_cells, d_2.shape[0], d_2.shape[1]])\n          # iterate over all zero components\n          for index, i_cell in enumerate(range(1, num_cells + 1)):\n              # cell segmentation (segmented cell 1, rest 0)\n              cell_segmentation = all_cells==i_cell\n              # find boundary (boundary 1, rest 0)\n              boundary = find_boundaries(cell_segmentation, mode='inner')\n              # compute distance to boundary (set boundary 0, rest -1)\n              bound_dists = distance_transform_edt(1 - boundary)\n              dists[index] = bound_dists\n          # sort dists along first axis (each pixel)\n          dists.sort(axis=0)\n          d_1 = dists[0]\n          d_2 = dists[1]\n          w = w_c + w_0 * np.exp(- (d_1 + d_2)**2/(2*sigma**2))\n          # save w to weight map\n          weight_map[i, 0] = w\n  \n          # visualize weight map\n          if plot and i==0:\n              fig = plt.figure(figsize=(18, 14))\n  \n              ax = plt.subplot(1, 3, 1)\n              plt.title('Segmenation Mask')\n              plt.imshow(label_mask[0, 0], cmap='gray')\n              divider = make_axes_locatable(ax)\n              cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n              plt.colorbar(cax=cax)\n  \n              ax = plt.subplot(1, 3, 2)\n              plt.title('w_c')\n              plt.imshow(w_c, cmap='jet')\n              divider = make_axes_locatable(ax)\n              cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n              plt.colorbar(cax=cax)\n  \n  \n              ax = plt.subplot(1, 3, 3)\n              plt.title('w')\n              plt.imshow(w, cmap='jet')\n              divider = make_axes_locatable(ax)\n              cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n              plt.colorbar(cax=cax)\n      return weight_map\n  \n  \n  img, label_mask = whole_dataset[0]\n  weight_map = compute_weight_map(label_mask.unsqueeze(0), w_0=10, sigma=5, plot=True)\n  ```\n  :::\n  \n  \n  ![compute_weight_map](./img/weight_map.png \"compute_weight_map\")\n\n\n[^4]: Since the separation borders are much smaller than the segmented\n    objects, the network could be trapped into merging touching\n    objects without being penalized enough.\n\n\n* **Training Procedure**: A simple `SGD` (*Stochastic Gradient Descent*)\n  optimizer with a high momentum (0.99) and a `batch_size` of 1 are\n  choosen for training as proposed by [Ronneberger et al.\n  (2015)](https://arxiv.org/abs/1505.04597), see code below. Note that\n  we take the mean instead of the sum in the loss function calculation\n  to avoid overflow (i.e., nans). This will only change the strength of\n  a gradient step (which can be adjusted by the learning rate), but\n  not its direction.\n\n\n  ::: {.cell execution_count=7}\n  ``` {.python .cell-code}\n  from livelossplot import PlotLosses\n  from torch.utils.data import DataLoader\n  \n  \n  def train(u_net, dataset, epochs):\n      device = 'cuda' if torch.cuda.is_available() else 'cpu'\n      # hyperparameters weight map\n      w_0, sigma = 10, 5\n  \n      print('Device: {}'.format(device))\n  \n      data_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n  \n      u_net.to(device)\n      optimizer = torch.optim.SGD(u_net.parameters(), lr=0.001, momentum=0.99)\n  \n      losses_plot = PlotLosses()\n      for epoch in range(epochs):\n          avg_loss = 0\n          for counter, (imgs, label_masks) in enumerate(data_loader):\n              u_net.zero_grad()\n              # retrieve predictions of u_net [batch, 2, 388, 388]\n              pred_masks = u_net(imgs.to(device))\n              # compute weight map\n              weight_map = compute_weight_map(label_masks, w_0, sigma).to(device)\n              # put label_masks to device\n              label_masks = label_masks.to(device)\n              # compute weighted binary cross entropy loss\n              loss = -(weight_map*\n                      (pred_masks[:, 0:1].log() * label_masks +\n                        pred_masks[:, 1:2].log() * (1 - label_masks))\n                      ).mean()\n              loss.backward()\n              optimizer.step()\n  \n              avg_loss += loss.item() / len(dataset)\n  \n              losses_plot.update({'current weighted loss': loss.item()},\n                                current_step=epoch + counter/len(data_loader))\n              losses_plot.draw()\n          losses_plot.update({'avg weighted loss': avg_loss},\n                            current_step=epoch + 1)\n          losses_plot.draw()\n      trained_u_net = u_net\n      return trained_u_net\n  ```\n  :::\n  \n  \n  **Beware**: Training for 30 epochs (i.e., the code below) takes about 2\n  hours with a NVIDIA Tesla K80 as GPU. The loss plot (see below `avg weighted\n  loss`) indicates that training for more epochs might improve the\n  model even more[^3]. For people who are interested in using the\n  model without waiting for 2 hours, I stored a trained version on\n  [nextjournal](https://nextjournal.com/borea17/u-net).\n\n\n  ::: {.cell execution_count=8}\n  ``` {.python .cell-code}\n  u_net = Unet()\n  epochs = 30\n  # all image indexes\n  idx = np.arange(30)\n  # random inplace shuffling of indexes\n  np.random.seed(1)\n  np.random.shuffle(idx)\n  # split data into training and test data\n  train_imgs, train_labels = imgs[idx[0:25]], labels[idx[0:25]]\n  test_imgs, test_labels = imgs[idx[25:]], labels[idx[25:]]\n  # generate datasets\n  stride = 124\n  train_dataset = EM_Dataset(train_imgs, train_labels, stride=stride,\n                            transformation=True, probability=0.7, alpha=50,\n                            sigma=5, kernel_dim=25)\n  test_dataset = EM_Dataset(test_imgs, test_labels, stride=stride,\n                            transformation=False)\n  # start training procedure\n  trained_u_net = train(u_net, train_dataset, epochs)\n  ```\n  :::\n  \n  \n  ![train result](./img/loss_plot.png \"train plot\")\n\n### Results\n\nLet's look at some image segmentations generated by the trained model\non the unseen test set:\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ndef visualize_results(trained_u_net, test_dataset, num_test_images=None):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    # take random tile from each test image\n    num_tiles = (1 + int((512 - 388)/test_dataset.stride))**2\n    num_images = int(len(test_dataset) / num_tiles)\n    if num_test_images:\n        # number of images < number of images in test set\n        num_images = min(num_test_images, num_images)\n    random_tile_idx = np.random.choice(range(num_tiles), num_images,\n                                       replace=True)\n\n    fig = plt.figure(figsize=(num_images*6, 10))\n    # annotation plots\n    ax = plt.subplot(3, num_images + 1, 1)\n    ax.annotate('cell image\\n(input)', xy=(1, 0.5), xycoords='axes fraction',\n                 fontsize=14, va='center', ha='right')\n    ax.set_aspect('equal')\n    ax.axis('off')\n    ax = plt.subplot(3, num_images + 1, num_images + 2)\n    ax.annotate('true segmentation\\n(label)', xy=(1, 0.5),\n                xycoords='axes fraction', fontsize=14, va='center', ha='right')\n    ax.set_aspect('equal')\n    ax.axis('off')\n    ax = plt.subplot(3, num_images + 1, 2*(num_images + 1) + 1)\n    ax.annotate('U-net prediction', xy=(1, 0.5), xycoords='axes fraction',\n                 fontsize=14, va='center', ha='right')\n    ax.set_aspect('equal')\n    ax.axis('off')\n    # image, label, predicted label plots\n    for index in range(num_images):\n        img, label = test_dataset[index*num_tiles + random_tile_idx[index]]\n        label_pred = u_net(img.unsqueeze(0).to(device)).squeeze(0)[0] > 0.5\n\n        # plot original image\n        plt.subplot(3, num_images + 1, index + 2)\n        plt.imshow(transforms.ToPILImage()(img), cmap='gray')\n        plt.plot([92, 480, 480, 92, 92], [92, 92, 480, 480, 92],\n                 'yellow', linewidth=2)\n        plt.xticks([])\n        plt.yticks([])\n        # plot original segmentation mask\n        plt.subplot(3, num_images + 1, index + num_images + 3)\n        plt.imshow(transforms.ToPILImage()(label), cmap='gray')\n        plt.xticks([])\n        plt.yticks([])\n        # plot prediction segmentation mask\n        plt.subplot(3, num_images + 1, index + 2*(num_images + 1) + 2)\n        plt.imshow(label_pred.detach().cpu().numpy(), cmap='gray')\n        plt.xticks([])\n        plt.yticks([])\n    return\n\n\nvisualize_results(trained_u_net, test_dataset, num_test_images=3)\n```\n:::\n\n\n![visualize results](./img/u_net_prediction.png \"visualize results\")\n\nThe predictions are pretty decent, though far from perfect. Bear in\nmind, that our model had only 25 example images to learn from and that\ntraining for more epochs might have led to even better predictions.\n\n-----------------------------------------------------------------------------------------------\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}