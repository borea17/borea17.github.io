{
  "hash": "82b5a177439ab7d918577b1b0d5d1f9f",
  "result": {
    "markdown": "---\ntitle: \"Importance Weighted Autoencoders\"\ncategories: [\"reimplementation\", \"VAE\", \"IWAE\", \"generative\"]\ndate: \"2021-01-10\"\nexecute:\n  eval: false # true\nengine: jupyter\nformat:\n  html: \n    code-fold: show \n    highlight-style: github \n    code-block-bg: true\n    code-tools: \n      toggle: true\n      source: \"https://github.com/borea17/Notebooks/blob/master/06_Importance_Weighted_Autoencoders.ipynb\"\n---\n\n<!-- nextjournal_link: \"https://nextjournal.com/borea17/attend-infer-repeat/\" -->\n\n[Burda et al. (2016)](https://arxiv.org/abs/1509.00519) introduce the **Importance\nWeighted Autoencoder (IWAE)** as a simple modification in the training\nof [variational\nautoencoders](https://borea17.github.io/paper_summaries/auto-encoding_variational_bayes)\n**(VAEs)**. Notably, they proved that this modification leads to a **strictly\ntighter lower bound on the data log-likelihood**. Furthermore, the standard VAE\nformulation is contained within the IWAE framework as a special case. In\nessence, the modification consists of using multiple samples from the\n*recognition network* / *encoder* and adapting the loss function with\n*importance-weighted sample losses*. In their experiments, they could emprically\nvalidate that employing IWAEs leads to improved test log-likelihoods and\nricher latent space representations compared to VAEs.\n\n## Model Description\n\nAn IWAE can be understood as a standard VAE in which multiple samples are drawn\nfrom the encoder distribution $q_{\\boldsymbol{\\phi}} \\Big( \\textbf{z} |\n\\textbf{x} \\Big)$ and then fed through the decoder $p_{\\boldsymbol{\\theta}}\n\\Big( \\textbf{z} | \\textbf{x} \\Big)$. In principle, this modification has been\nalready proposed in the original VAE paper by [Kingma and Welling\n(2013)](https://arxiv.org/abs/1312.6114). However, [Burda et al.\n(2016)](https://arxiv.org/abs/1509.00519) additionally proposed to use a\ndifferent objective function. The empirical objective function can be understood\nas the data log-likelihood $\\log p_{\\boldsymbol{\\theta}} (\\textbf{x})$ where the\nsampling distribution is exchanged to $q_{\\boldsymbol{\\phi}} \\Big( \\textbf{z} |\n\\textbf{x} \\Big)$ via the method of *importance sampling*.\n\n### High-Level Overview\n\nThe IWAE framework builds upon a standard VAE architecture.\nThere are two neural networks as approximations for the encoder\n$q_{\\boldsymbol{\\phi}} \\left(\\textbf{z} | \\textbf{x} \\right)$ and the decoder\ndistribution $p_{\\boldsymbol{\\theta}} \\left( \\textbf{x} | \\textbf{z} \\right)$.\nMore precisely, the networks estimate the parameters that parametrize\nthese distributions. Typically, the latent distribution is assumed to be a\nGaussian $q_{\\boldsymbol{\\phi}} \\left(\\textbf{z} | \\textbf{x} \\right) \\sim\n\\mathcal{N}\\left( \\boldsymbol{\\mu}_{\\text{E}}, \\text{diag} \\left(\n\\boldsymbol{\\sigma}^2_{\\text{E}}\\right) \\right)$ such that the encoder network\nestimates its the mean $\\boldsymbol{\\mu}\\_{\\text{E}}$ and variance[^1]\n$\\boldsymbol{\\Sigma} = \\text{diag}\n\\left(\\boldsymbol{\\sigma}^2_{\\text{E}}\\right)$. To allow for backpropagation,\nwe apply the reparametrization trick to the latent distribution which\nessentially consist of transforming samples from some (fixed) random\ndistribution, e.g. $\\boldsymbol{\\epsilon} \\sim \\mathcal{N} \\left(\\textbf{0},\n\\textbf{I} \\right)$, into the desired distribution using a deterministic\nmapping.\n\nThe main difference between a VAE and an IWAE lies in the objective function which\nis explained in more detail in the next section.\n\n[^1]: Since the variance $\\text{diag}\n    \\left(\\boldsymbol{\\sigma}^2_{\\text{E}}\\right)$ needs to be greater than 0,\n    we typically set the output to the variance in logarithmic units.\n\n\n| ![VAE/IWAE Architecture](./img/schematic_VAE.png \"VAE/IWAE Architecture\") |\n| :--:        |\n| **VAE/IWAE Architecture** |\n\n### Derivation\n\nLet $\\textbf{X} = \\{\\textbf{x}^{(i)}\\}_{i=1}^N$ denote a dataset of $N$ i.i.d.\nsamples where each observed datapoint $\\textbf{x}^{(i)}$ is obtained by first\nsampling a latent vector $\\textbf{z}$ from the prior\n$p_{\\boldsymbol{\\theta}}(\\textbf{z})$ and then sampling $\\textbf{x}^{(i)}$\nitself from the scene model $p_{\\boldsymbol{\\theta}} \\Big( \\textbf{x} |\n\\textbf{z} \\Big)$. Now we introduce an auxiliary distribution\n$q_{\\boldsymbol{\\phi}} \\Big( \\textbf{z} | \\textbf{x} \\Big)$ (with its own\nparamaters) as an approximation to the true, but unknown posterior\n$p_{\\boldsymbol{\\theta}} \\left( \\textbf{z} | \\textbf{x} \\right)$. Accordingly,\nthe data likelihood of a one sample $\\textbf{x}^{(i)}$ can be stated as\nfollows\n\n$$\np_{\\boldsymbol{\\theta}} (\\textbf{x}^{(i)}) = \\mathbb{E}_{\\textbf{z} \\sim\np_{\\boldsymbol{\\theta}} \\Big(\\textbf{z} \\Big)} \\Big[\np_{\\boldsymbol{\\theta}} \\left( \\textbf{z} | \\textbf{x}^{(i)} \\right) \\Big] =\n\\int p_{\\boldsymbol{\\theta}} (\\textbf{z}) p_{\\boldsymbol{\\theta}} \\left(\n\\textbf{z} | \\textbf{x}^{(i)} \\right) d\\textbf{z} =\n\\int p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}^{(i)} , \\textbf{z} \\right) d\\textbf{z}\n$$\n\nNow, we use the simple trick of *importance sampling* to change the sampling\ndistribution into the approximated posterior, i.e.,\n\n$$\np_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)}\\right)\n= \\int \\frac {p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}^{(i)} , \\textbf{z}\n\\right)} {q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}^{(i)} \\right)}\nq_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}^{(i)} \\right) d\\textbf{z}\n= \\mathbb{E}_{\\textbf{z} \\sim q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} |\n\\textbf{x}^{(i)} \\right)}\n\\left[ \\frac {p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}^{(i)} , \\textbf{z}\n\\right)} {q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}^{(i)} \\right)}\n\\right]\n$$\n\n#### VAE Formulation\n\nIn the standard VAE approach, we use the **evidence lower bound**\n[(ELBO)](https://borea17.github.io/ML_101/probability_theory/evidence_lower_bound)\non $\\log p_{\\boldsymbol{\\theta}} \\Big( \\textbf{x}\\Big)$ as the objective\nfunction. This can be derived by applying Jensen's Inequality on the data\nlog-likelihood:\n\n$$\n\\log p_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)} \\right)\n= \\log \\mathbb{E}_{\\textbf{z} \\sim q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} |\n\\textbf{x}^{(i)} \\right)}\n\\left[ \\frac {p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}^{(i)} , \\textbf{z}\n\\right)} {q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}^{(i)} \\right)}\n\\right] \\ge \\mathbb{E}_{\\textbf{z} \\sim q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} |\n\\textbf{x}^{(i)} \\right)}\n\\left[ \\log \\frac {p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}^{(i)} , \\textbf{z}\n\\right)} {q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}^{(i)} \\right)}\n\\right] = \\mathcal{L}^{\\text{ELBO}}\n$$\n\nUsing simple algebra, this can be rearranged into\n\n$$\n\\mathcal{L}^{\\text{ELBO}}\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\textbf{x}^{(i)} \\right) =\n\\underbrace{\n\\mathbb{E}_{\\textbf{z} \\sim q_{\\phi} \\left( \\textbf{z}| \\textbf{x}^{(i)} \\right)}\n\\left[ \\log p_{\\boldsymbol{\\theta}} \\Big(\\textbf{x}^{(i)} | \\textbf{z} \\Big)\n\\right]}_{\n\\text{Reconstruction Accuracy}}\n-\n\\underbrace{\nD_{KL} \\left( q_{\\phi} \\Big( \\textbf{z} |\n\\textbf{x}^{(i)} \\Big) || p_{\\boldsymbol{\\theta}} \\Big( \\textbf{z} \\Big) \\right)\n}_{\\text{Regularization}}\n$$\n\nWhile the regularization term can usually be solved analytically, the\nreconstruction accuracy in its current formulation poses a problem for\nbackpropagation: Gradients cannot backpropagate through a sampling operation. To\ncircumvent this problem, the standard VAE formulation includes the\nreparametrization trick:\n<center>Substitute sampling $\\textbf{z} \\sim q_{\\boldsymbol{\\phi}}$ by using a\ndeterministic mapping $\\textbf{z} = g_{\\boldsymbol{\\phi}}\n(\\boldsymbol{\\epsilon},\n\\textbf{x})$ with the differential transformation\n$g_{\\boldsymbol{\\phi}}$ of an auxiliary noise variable\n$\\boldsymbol{\\epsilon}$ with $\\boldsymbol{\\epsilon}\\sim p(\\boldsymbol{\\epsilon})$.\n</center>\n<br>\nAs a result, we can rewrite the EBLO as follows\n\n$$\n\\mathcal{L}^{\\text{ELBO}}\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\phi};\n\\textbf{x}^{(i)} \\right) =\n\\mathbb{E}_{\\boldsymbol{\\epsilon} \\sim p \\left( \\boldsymbol{\\epsilon} \\right)}\n\\left[ \\log p_{\\boldsymbol{\\theta}} \\Big(\\textbf{x}^{(i)} |\ng_{\\boldsymbol{\\phi}} \\left( \\boldsymbol{\\epsilon}, \\textbf{x}^{(i)} \\right) \\Big)\n\\right] -\nD_{KL} \\left( q_{\\phi} \\Big( \\textbf{z} |\n\\textbf{x}^{(i)} \\Big) || p_{\\boldsymbol{\\theta}} \\Big( \\textbf{z} \\Big) \\right)\n$$\n\nLastly, the expectation is approximated using Monte-Carlo integration, leading\nto the standard VAE objective\n\n$$\n\\begin{align}\n  \\widetilde{\\mathcal{L}}^{\\text{VAE}}_k \\left(\\boldsymbol{\\theta},\n  \\boldsymbol{\\phi};\n  \\textbf{x}^{(i)}\\right) &=\n  \\frac {1}{k} \\sum_{l=1}^{k} \\log p_{\\boldsymbol{\\theta}}\\left(\n  \\textbf{x}^{(i)}| g_{\\boldsymbol{\\phi}}\n \\left( \\boldsymbol{\\epsilon}^{(l)}, \\textbf{x}^{(i)} \\right)\\right)\n  -D_{KL} \\left(  q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}^{(i)} \\right),\n  p_{\\boldsymbol{\\theta}} (\\textbf{z}) \\right)\\\\\n  &\\text{with} \\quad \\boldsymbol{\\epsilon}^{(l)} \\sim p(\\boldsymbol{\\epsilon})\n\\end{align}\n$$\n\nNote that commonly $k=1$ in VAEs as long as the minibatch size is large enough.\nAs stated by [Kingma and Welling (2013)](https://arxiv.org/abs/1312.6114):\n\n::: {.callout-tip}\n## Quote \n\n*We found that the number of samples per datapoint can be set to 1 as long as\nthe minibatch size was large enough.*\n:::\n\n\n#### IWAE Formulation\n\nBefore we introduce the IWAE estimator, remind that the Monte-Carlo estimator of\nthe data likelihood (when the sampling distribution is changed via importance\nsampling, see\n[Derivation](https://borea17.github.io/paper_summaries/iwae#derivation)) is\ngiven by\n\n$$\np_{\\boldsymbol{\\theta}} (\\textbf{x} ) =\n\\mathbb{E}_{\\textbf{z} \\sim q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} |\n\\textbf{x}^{(i)} \\right)}\n\\left[ \\frac {p_{\\boldsymbol{\\theta}} \\left(\\textbf{x} , \\textbf{z}\n\\right)} {q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x} \\right)}\n\\right] \\approx \\frac {1}{k} \\sum_{l=1}^{k}\n\\frac {p_{\\boldsymbol{\\theta}} \\left(\\textbf{x} , \\textbf{z}^{(l)}\n\\right)} {q_{\\boldsymbol{\\phi}} \\left( \\textbf{z}^{(l)} | \\textbf{x}\n\\right)} \\quad \\text{with} \\quad \\textbf{z}^{(l)} \\sim q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} |\n\\textbf{x}^{(i)} \\right)\n$$\n\nAs a result, the data log-likelihood estimator for one sample $\\textbf{x}^{(i)}$\ncan be stated as follows\n\n$$\n\\begin{align}\n\\log p_{\\boldsymbol{\\theta}} (\\textbf{x}^{(i)} ) &\\approx \\log \\left[ \\frac {1}{k} \\sum_{l=1}^{k}\n\\frac {p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}^{(i)} , \\textbf{z}^{(i, l)}\n\\right)} {q_{\\boldsymbol{\\phi}} \\left( \\textbf{z}^{(i, l)} | \\textbf{x}^{(i)}\n\\right)}\\right] = \\widetilde{\\mathcal{L}}^{\\text{IWAE}}_k \\left( \\boldsymbol{\\theta},\n\\boldsymbol{\\phi}; \\textbf{x}^{(i)} \\right) \\\\\n&\\text{with} \\quad \\textbf{z}^{(i, l)} \\sim q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} |\n\\textbf{x}^{(i)} \\right)\n\\end{align}\n$$\n\nwhich leads to an empirical estimate of the IWAE objective. However, [Burda et\nal. (2016)](https://arxiv.org/abs/1509.00519) do not use the data log-likelihood\nin its plain form as the true IWAE objective. Instead they introduce the IWAE\nobjective as follows\n\n$$\n\\mathcal{L}^{\\text{IWAE}}_k \\left(\\boldsymbol{\\theta}, \\boldsymbol{\\phi};\n\\textbf{x}^{(i)}\\right)\n=  \\mathbb{E}_{\\textbf{z}^{(1)}, \\dots,  \\textbf{z}^{(k)} \\sim q_{\\phi} \\left( \\textbf{z}|\n\\textbf{x}^{(i)} \\right)}\n\\left[\n\\log \\frac {1}{k}\n\\sum_{l=1}^k\n\\frac {p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}^{(i)}, \\textbf{z}^{(l)}\\right)}\n{q_{\\phi} \\left( \\textbf{z}^{(l)} | \\textbf{x}^{(i)} \\right)}\n\\right]\n$$\n\nFor notation purposes, they denote\n\n$$\n\\text{(unnormalized) importance weights:} \\quad\n{w}^{(i, l)} = \\frac {p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}^{(i)}, \\textbf{z}^{(l)}\\right)}\n{q_{\\phi} \\left( \\textbf{z}^{(l)} | \\textbf{x}^{(i)} \\right)}\n$$\n\nBy applying Jensen's Inequality, we can see that in fact the (true) IWAE estimator is\nmerely a lower-bound on the plain data log-likelihood\n\n$$\n\\mathcal{L}^{\\text{IWAE}}_k \\left( \\boldsymbol{\\theta}, \\boldsymbol{\\phi};\n\\textbf{x}^{(i)} \\right)\n= \\mathbb{E} \\left[ \\log \\frac {1}{k} \\sum_{l=1}^{k} {w}^{(i,\nl)}\\right] \\le \\log \\mathbb{E} \\left[ \\frac {1}{k} \\sum_{l=1}^{k}\n{w}^{(i,l)} \\right] = \\log p_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)} \\right)\n$$\n\nThey could prove that with increasing $k$ the lower bound gets strictly tighter\nand approaches the true data log-likelihood in the limit of $k \\rightarrow\n\\infty$. Note that since the empirical IWAE estimator\n$\\widetilde{\\mathcal{L}}_k^{\\text{IWAE}}$ can be understood as a Monte-Carlo\nestimator on the true data log-likelihood, in the empirical case this property\ncan simply be deduced from the properties of Monte-Carlo integration.\n\n\n::: {.callout-tip collapse=false}\n## What is motivation of the true IWAE objective? \n\nA very well explanation is given by [Domke and Sheldon\n(2018)](https://arxiv.org/abs/1808.09034). Starting from the property\n\n$$\np(\\textbf{x}) = \\mathbb{E} \\Big[ w \\Big] = \\mathbb{E}_{\\textbf{z} \\sim q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} |\n\\textbf{x}^{(i)} \\right)}\n\\left[ \\frac {p_{\\boldsymbol{\\theta}} \\left(\\textbf{x} , \\textbf{z}\n\\right)} {q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x} \\right)}\n\\right]\n$$\n\nWe derived the ELBO using Jensen's inequality\n\n$$\n\\log p(\\textbf{x}) \\ge \\mathbb{E} \\Big[ \\log w \\Big] = \\text{ELBO} \\Big[ q ||\np \\Big]\n$$\n\nSuppose that we could make $w$ more concentrated about its mean $p(\\textbf{x})$.\nClearly, this would yield a tighter lower bound when applying Jensen's\nInequality. \n\n(rhetorical break) \n\nCan we make $w$ more concentrated about its mean? \nYES, WE CAN. \n\nFor example using the sample average $w_k = \\frac {1}{k}\n\\sum_{i=1}^k w^{(i)}$. This leads directly to the true IWAE objective\n\n$$\n\\log p(\\textbf{x})  \\ge \\mathbb{E} \\Big[ \\log w_k \\Big] = \\mathbb{E} \\left[\n\\log \\frac {1}{k} \\sum_{i=1}^{k} w^{(i)} \\right] = \\mathcal{L}^{\\text{IWAE}}_k\n$$\n::: \n\n\n::: {.callout-tip collapse=false}\n## IWAE objective and plain log-likelihood lead to the same empirical estimate. How?\n\nHere it gets interesting. A closer analysis on the IWAE bound by [Nowozin\n(2018)](https://openreview.net/forum?id=HyZoi-WRb) revealed the following property\n\n$$\n\\begin{align}\n&\\quad \\mathcal{L}_k^{\\text{IWAE}} = \\log p(\\textbf{x}) - \\frac {1}{k} \\frac\n{\\mu_2}{2\\mu^2} + \\frac {1}{k^2} \\left( \\frac {\\mu_3}{3\\mu^3} - \\frac\n{3\\mu_2^2}{4\\mu^4} \\right) + \\mathcal{O}(k^{-3})\\\\\n&\\text{with} \\quad\n\\mu = \\mathbb{E}_{\\textbf{z} \\sim q_{\\boldsymbol{\\phi}}} \\left[ \\frac\n{p_{\\boldsymbol{\\theta}}\\left( \\textbf{x}, \\textbf{z}\n\\right)}{q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x} \\right)} \\right]\n\\quad\n\\mu_i = \\mathbb{E}_{\\textbf{z} \\sim q_{\\boldsymbol{\\phi}}} \\left[\n\\left( \\frac\n{p_{\\boldsymbol{\\theta}}\\left( \\textbf{x}, \\textbf{z}\n\\right)}{q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x} \\right)}\n- \\mathbb{E}_{\\textbf{z} \\sim q_{\\boldsymbol{\\phi}}} \\left[ \\frac\n{p_{\\boldsymbol{\\theta}}\\left( \\textbf{x}, \\textbf{z}\n\\right)}{q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x} \\right)} \\right]\n\\right)^2 \\right]\n\\end{align}\n$$\n\nThus, the true objective is a **biased** - in the order of\n$\\mathcal{O}\\left(k^{-1}\\right)$ - and  **consistent** estimator of the\nmarginal log likelihood $\\log p(\\textbf{x})$. The empirical estimator of the\ntrue IWAE objective is basically a special Monte-Carlo estimator (only one\nsample per $k$) on the true IWAE objective. It is more or less luck that we\ncan formulate the same empirical objective and interpret it differently as the\nMonte-Carlo estimator (with $k$ samples) on the data log-likelihood.\n\n:::\n\n<!-- What makes their formulation superior to estimating the true data\n log-likelihood?\n\n- their formulation contains the ELBO as a special case ($k=1$)\n -->\n\nLet us take a closer look on how to compute gradients (fast) for the empirical\nestimate of the IWAE objective:\n\n$$\n\\begin{align}\n\\nabla_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}}\n\\widetilde{\\mathcal{L}}_k^{\\text{IWAE}} \\left( \\boldsymbol{\\theta}, \\boldsymbol{\\phi};\n\\textbf{x}^{(i)} \\right) &= \\nabla_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}}\n\\log \\frac {1}{k} \\sum_{l=1}^k w^{(i,l)} \\left( \\textbf{x}^{(i)},\n\\textbf{z}^{(i, l)}_{\\boldsymbol{\\phi}}, \\boldsymbol{\\theta} \\right) \\quad\n\\text{with} \\quad\n\\textbf{z}^{(i, l)} \\sim q_{\\boldsymbol{\\phi}} \\left(\\textbf{z} |\n\\textbf{x}^{(i)} \\right)\\\\\n&\\stackrel{\\text{(*)}}{=}\n\\sum_{l=1}^{k} \\frac {w^{(i, l)}}{\\sum_{m=1}^{k} w^{(i,\nm)}} \\nabla_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}} \\log w^{(i,l)} =\n\\sum_{l=1}^{k} \\widetilde{w}^{(i, l)} \\nabla_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}} \\log w^{(i,l)},\n\\end{align}\n$$\n\nwhere we introduced the following notation\n\n$$\n\\text{(normalized) importance weights:} \\quad\n\\widetilde{w}^{(i, l)} = \\frac {w^{(i,l)}}{\\sum_{m=1}^k w^{(i, m)}}\n$$\n\n\n::: {.callout-tip collapse=false}\n## $(*)$: **Gradient Derivation**:\n\n$$\n\\begin{align}\n\\frac {\\partial \\left[ \\log \\frac {1}{k} \\sum_i^{k} w_i \\left( \\boldsymbol{\\theta}\n\\right) \\right]}{\\partial \\boldsymbol{\\theta}} &\\stackrel{\\text{chain rule}}{=}  \\frac {\\partial\n\\log a}{\\partial a} \\sum_{i}^{k} \\frac {\\partial a}{\\partial w_i} \\frac\n{\\partial w_i}{\\partial \\boldsymbol{\\theta}} \\quad \\text{with}\n\\quad a = \\frac {1}{k} \\sum_{i}^k w_i (\\boldsymbol{\\theta})\\\\\n&= \\frac {k}{\\sum_l^k w_l} \\sum_{i}^{k}\\frac {1}{k} \\frac {\\partial\nw_i (\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} = \\frac {1}{\\sum_l^k\nw_l} \\sum_{i}^{k} \\frac {\\partial\nw_i (\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}\n\\end{align}\n$$\n\nLastly, we use the following identity\n\n$$\n\\frac {\\partial w_i (\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} = w_i\n(\\boldsymbol{\\theta}) \\cdot\n\\frac {\\partial \\log w_i (\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}\n\\stackrel{\\text{chain rule}}{=} w_i (\\boldsymbol{\\theta}) \\cdot \\frac {1}{w_i\n(\\boldsymbol{\\theta})} \\cdot\n\\frac {\\partial w_i (\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} =\n\\frac {\\partial w_i (\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}\n$$\n:::\n\nSimilar to VAEs, this formulation poses a problem for backpropagation due to the\nsampling operation. We use the same reparametrization trick to circumvent this\nproblem and obtain a low variance update rule:\n\n$$\n\\begin{align}\n\\nabla_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}}\n\\widetilde{\\mathcal{L}}_k^{\\text{IWAE}} &=\n\\sum_{l=1}^{k} \\widetilde{w}^{(i, l)} \\nabla_{\\boldsymbol{\\phi},\n\\boldsymbol{\\theta}} \\log w^{(i,l)} \\left( \\textbf{x}^{(i)},\n\\textbf{z}_{\\boldsymbol{\\phi}}^{(i,l)}, \\boldsymbol{\\theta} \\right)\n\\quad \\text{with} \\quad\n\\textbf{z}^{(i,l)} \\sim q_{\\boldsymbol{\\phi}} \\left(\\textbf{z} | \\textbf{x}^{(i)} \\right)\\\\\n&= \\sum_{l=1}^k \\widetilde{w}^{(i,l)} \\nabla_{\\boldsymbol{\\phi},\n\\boldsymbol{\\theta}} \\log w^{(i,l)} \\left(\\textbf{x}^{(i)},\ng_{\\boldsymbol{\\phi}} \\left( \\textbf{x}^{(i)},\n\\boldsymbol{\\epsilon}^{(l)}\\right), \\textbf{x}^{(i)} \\right), \\quad \\quad\n\\boldsymbol{\\epsilon} \\sim p(\\boldsymbol{\\epsilon})\n\\end{align}\n$$\n\nTo make things clearer for the implementation, let us unpack the log\n\n$$\n\\log w^{(i,l)} = \\log \\frac {p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}^{(i)}, \\textbf{z}^{(l)}\\right)}\n{q_{\\boldsymbol{\\phi}} \\left( \\textbf{z}^{(l)} | \\textbf{x}^{(i)} \\right)} = \\underbrace{\\log\np_{\\boldsymbol{\\theta}} \\left (\\textbf{x}^{(i)} | \\textbf{z}^{(l)}\n\\right)}_{\\text{NLL}} + \\log p_{\\boldsymbol{\\theta}} \\left( \\textbf{z}^{(l)}\n\\right) - \\log q_{\\boldsymbol{\\phi}} \\left( \\textbf{z}^{(l)} | \\textbf{x}^{(i)} \\right)\n$$\n\n\nBefore, we are going to implement this formulation, let us look whether we can\nseparate out the KL divergence for the true IWAE objective of [Burda et al.\n(2016)](https://arxiv.org/abs/1509.00519). Therefore, we state the update\nfor the true objective:\n\n$$\n\\begin{align}\n\\nabla_{\\boldsymbol{\\phi},\n\\boldsymbol{\\theta}}\n\\mathcal{L}_k^{\\text{IWAE}} &=\n\\nabla_{\\boldsymbol{\\phi},\n\\boldsymbol{\\theta}}\n\\mathbb{E}_{\\textbf{z}^{(1)}, \\dots, \\textbf{z}^{(l)}} \\left[ \\log \\frac {1}{k}\n\\sum_{l=1}^{k} w^{(l)} \\left( \\textbf{x},\n\\textbf{z}^{(l)}_{\\boldsymbol{\\phi}}, \\boldsymbol{\\theta} \\right) \\right]\\\\\n&=\n\\mathbb{E}_{\\textbf{z}^{(1)}, \\dots, \\textbf{z}^{(l)}} \\left[\n\\sum_{l=1}^{k} \\widetilde{w}_i\n\\nabla_{\\boldsymbol{\\phi},\n\\boldsymbol{\\theta}}\n\\log w^{(l)} \\left( \\textbf{x}, \\textbf{z}_{\\boldsymbol{\\phi}}^{(l)}, \\boldsymbol{\\theta} \\right) \\right]\\\\\n&=\\sum_{l=1}^{k} \\widetilde{w}_i \\mathbb{E}_{\\textbf{z}^{(l)}} \\left[\n\\nabla_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}} \\log w^{(l)} \\left( \\textbf{x},\n\\textbf{z}_{\\boldsymbol{\\phi}}^{(l)}, \\boldsymbol{\\theta} \\right)\n\\right]\\\\\n&\\neq \\sum_{l=1}^{k} \\widetilde{w}_i\n\\nabla_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}}\n\\mathbb{E}_{\\textbf{z}^{(l)}} \\left[\n\\log w^{(l)} \\left( \\textbf{x},\n\\textbf{z}_{\\boldsymbol{\\phi}}^{(l)}, \\boldsymbol{\\theta} \\right)\n\\right]\n\\end{align}\n$$\n\nUnfortunately, we cannot simply move the gradient outside the expectation. If we\ncould, we could simply rearrange the terms inside the expectation as in the\nstandard VAE case.\n\n------------------------\n\nLet us look, what would happen, if we were to describe the true IWAE estimator\nas the data log-likelihood $\\log p \\left( \\textbf{x} \\right)$ in\nwhich the sampling distribution is exchanged via importance sampling:\n\n$$\n\\begin{align}\n\\nabla_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}} \\log p \\left( \\textbf{x}^{(i)} \\right) &=\n\\nabla_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}} \\log \\mathbb{E}_{\\textbf{z} \\sim\nq_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}^{(i)}\\right)} \\left[ w\n(\\textbf{x}^{(i)}, \\textbf{z}, \\boldsymbol{\\theta})\\right]\\\\\n&\\neq\n\\nabla_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}}  \\mathbb{E}_{\\textbf{z} \\sim\nq_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}^{(i)}\\right)} \\left[ \\log w\n(\\textbf{x}^{(i)}, \\textbf{z}, \\boldsymbol{\\theta})\\right]\n\\end{align}\n$$\n\nHere, we also cannot separate the KL divergence out, since we cannot simply move\nthe log inside the expectation.\n\n\n## Implementation\n\nLet's put this into practice and compare the standard VAE with an IWAE. We are\ngoing to perform a very similar experiment to the density estimation experiment\nby [Burda et al. (2016)](https://arxiv.org/abs/1509.00519), i.e., we are going\nto train both a VAE and IWAE with different number of samples $k\\in \\{1,\n10\\}$ on the binarized MNIST dataset.\n\n### Dataset\n\nLet's first build a binarized version of the MNIST dataset. As noted by [Burda\net al. (2016)](https://arxiv.org/abs/1509.00519), `the generative modeling\nliterature is inconsistent about the method of binarization`. We employ the same\nprocedure as [Burda et al. (2016)](https://arxiv.org/abs/1509.00519):\n`binary-valued observations are sampled with expectations equal to the real\nvalues in the training set`:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport torch.distributions as dists\nimport torch\nfrom torchvision import datasets, transforms\n\n\nclass Binarized_MNIST(datasets.MNIST):\n    def __init__(self, root, train, transform=None, target_transform=None, download=False):\n        super(Binarized_MNIST, self).__init__(root, train, transform, target_transform, download)\n\n    def __getitem__(self, idx):\n        img, target = super().__getitem__(idx)\n        return dists.Bernoulli(img).sample().type(torch.float32)\n```\n:::\n\n\n| ![Binarized MNIST Dataset](./img/binarized_MNIST.png \"Binarized MNIST Dataset\") |\n| :--:        |\n| **Binarized MNIST Dataset** |\n\n\n### Model Implementation\n\n* **VAE Implementation**\n\n  The VAE implementation is straightforward. For later evaluation, I added\n  `create_latent_traversal` and `compute_marginal_log_likelihood`. The ladder\n  computes the marginal log-likelihood $\\log p(\\textbf{x})$ in which the\n  sampling distribution is exchanged to the approximated posterior\n  $q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}\\right )$ using the\n  standard Monte-Carlo estimator, i.e.,\n\n  $$\n    \\log p(\\textbf{x}) = \\mathbb{E}_{z\\sim q_{\\boldsymbol{\\phi}}} \\left[ \\frac\n    {p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}, \\textbf{z}\\right)}\n    {q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x} \\right)} \\right]\n    \\approx \\log \\left[ \\frac {1}{k} \\sum_{l=1}^k w^{(l)} \\right] =\n    \\mathcal{L}^{\\text{IWAE}}_k (\\textbf{x})\n  $$\n\n  Remind that this formulation equals the empirical IWAE estimator. However, we\n  can only compute the (unnormalized) logarithmic importance weights\n\n  $$\n  \\log w^{(i,l)} = \\log \\frac {p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}^{(i)}, \\textbf{z}^{(l)}\\right)}\n  {q_{\\boldsymbol{\\phi}} \\left( \\textbf{z}^{(l)} | \\textbf{x}^{(i)} \\right)} = \\log\n  p_{\\boldsymbol{\\theta}} \\left (\\textbf{x}^{(i)} | \\textbf{z}^{(l)}\n  \\right) + \\log p_{\\boldsymbol{\\theta}} \\left( \\textbf{z}^{(l)}\n  \\right) - \\log q_{\\boldsymbol{\\phi}} \\left( \\textbf{z}^{(l)} | \\textbf{x}^{(i)} \\right)\n  $$\n\n  Accordingly, we compute the marginal log-likelihood as follows\n\n  $$\n  \\begin{align}\n\\widetilde{\\mathcal{L}}^{\\text{IWAE}}_k \\left( \\boldsymbol{\\theta},\n\\boldsymbol{\\phi}; \\textbf{x}^{(i)} \\right) &= \\underbrace{\\log 1}_{=0} - \\log\n  k + \\log \\left( \\sum_{i=1}^k w^{(i,l)} \\right) \\\\\n  &= -\\log k + \\underbrace{\\log \\left( \\sum_{i=1}^k \\exp \\big[ \\log w^{(i, l)} \\big] \\right)}_{=\\text{torch.logsumexp}}\n  \\end{align}\n  $$\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport torch.nn as nn\nimport numpy as np\n\n\nMNIST_SIZE = 28\nHIDDEN_DIM = 400\nLATENT_DIM = 50\n\n\nclass VAE(nn.Module):\n\n    def __init__(self, k):\n        super(VAE, self).__init__()\n        self.k = k\n        self.encoder = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(MNIST_SIZE**2, HIDDEN_DIM),\n            nn.ReLU(),\n            nn.Linear(HIDDEN_DIM, HIDDEN_DIM),\n            nn.ReLU(),\n            nn.Linear(HIDDEN_DIM, 2*LATENT_DIM)\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(LATENT_DIM, HIDDEN_DIM),\n            nn.ReLU(),\n            nn.Linear(HIDDEN_DIM, HIDDEN_DIM),\n            nn.ReLU(),\n            nn.Linear(HIDDEN_DIM, MNIST_SIZE**2),\n            nn.Sigmoid()\n        )\n        return\n\n    def compute_loss(self, x, k=None):\n        if not k:\n            k = self.k\n        [x_tilde, z, mu_z, log_var_z] = self.forward(x, k)\n        # upsample x\n        x_s = x.unsqueeze(1).repeat(1, k, 1, 1, 1)\n        # compute negative log-likelihood\n        NLL = -dists.Bernoulli(x_tilde).log_prob(x_s).sum(axis=(2, 3, 4)).mean()\n        # copmute kl divergence\n        KL_Div = -0.5*(1 + log_var_z - mu_z.pow(2) - log_var_z.exp()).sum(1).mean()\n        # compute loss\n        loss = NLL + KL_Div\n        return loss\n\n    def forward(self, x, k=None):\n        \"\"\"feed image (x) through VAE\n\n        Args:\n            x (torch tensor): input [batch, img_channels, img_dim, img_dim]\n\n        Returns:\n            x_tilde (torch tensor): [batch, k, img_channels, img_dim, img_dim]\n            z (torch tensor): latent space samples [batch, k, LATENT_DIM]\n            mu_z (torch tensor): mean latent space [batch, LATENT_DIM]\n            log_var_z (torch tensor): log var latent space [batch, LATENT_DIM]\n        \"\"\"\n        if not k:\n            k = self.k\n        z, mu_z, log_var_z = self.encode(x, k)\n        x_tilde = self.decode(z, k)\n        return [x_tilde, z, mu_z, log_var_z]\n\n    def encode(self, x, k):\n        \"\"\"computes the approximated posterior distribution parameters and\n        samples from this distribution\n\n        Args:\n            x (torch tensor): input [batch, img_channels, img_dim, img_dim]\n\n        Returns:\n            z (torch tensor): latent space samples [batch, k, LATENT_DIM]\n            mu_E (torch tensor): mean latent space [batch, LATENT_DIM]\n            log_var_E (torch tensor): log var latent space [batch, LATENT_DIM]\n        \"\"\"\n        # get encoder distribution parameters\n        out_encoder = self.encoder(x)\n        mu_E, log_var_E = torch.chunk(out_encoder, 2, dim=1)\n        # increase shape for sampling [batch, samples, latent_dim]\n        mu_E_ups = mu_E.unsqueeze(1).repeat(1, k, 1)\n        log_var_E_ups = log_var_E.unsqueeze(1).repeat(1, k, 1)\n        # sample noise variable for each batch and sample\n        epsilon = torch.randn_like(log_var_E_ups)\n        # get latent variable by reparametrization trick\n        z = mu_E_ups + torch.exp(0.5*log_var_E_ups) * epsilon\n        return z, mu_E, log_var_E\n\n    def decode(self, z, k):\n        \"\"\"computes the Bernoulli mean of p(x|z)\n        note that linear automatically parallelizes computation\n\n        Args:\n            z (torch tensor): latent space samples [batch, k, LATENT_DIM]\n\n        Returns:\n            x_tilde (torch tensor): [batch, k, img_channels, img_dim, img_dim]\n        \"\"\"\n        # get decoder distribution parameters\n        x_tilde = self.decoder(z)  # [batch*samples, MNIST_SIZE**2]\n        # reshape into [batch, samples, 1, MNIST_SIZE, MNIST_SIZE] (input shape)\n        x_tilde = x_tilde.view(-1, k, 1, MNIST_SIZE, MNIST_SIZE)\n        return x_tilde\n\n    def create_latent_traversal(self, image_batch, n_pert, pert_min_max=2, n_latents=5):\n        device = image_batch.device\n        # initialize images of latent traversal\n        images = torch.zeros(n_latents, n_pert, *image_batch.shape[1::])\n        # select the latent_dims with lowest variance (most informative)\n        [x_tilde, z, mu_z, log_var_z] = self.forward(image_batch)\n        i_lats = log_var_z.mean(axis=0).sort()[1][:n_latents]\n        # sweep for latent traversal\n        sweep = np.linspace(-pert_min_max, pert_min_max, n_pert)\n        # take first image and encode\n        [z, mu_E, log_var_E] = self.encode(image_batch[0:1], k=1)\n        for latent_dim, i_lat in enumerate(i_lats):\n            for pertubation_dim, z_replaced in enumerate(sweep):\n                z_new = z.detach().clone()\n                z_new[0][0][i_lat] = z_replaced\n\n                img_rec = self.decode(z_new.to(device), k=1).squeeze(0)\n                img_rec = img_rec[0].clamp(0, 1).cpu()\n\n                images[latent_dim][pertubation_dim] = img_rec\n        return images\n\n    def compute_marginal_log_likelihood(self, x, k=None):\n        \"\"\"computes the marginal log-likelihood in which the sampling\n        distribution is exchanged to q_{\\phi} (z|x),\n        this function can also be used for the IWAE loss computation\n\n        Args:\n            x (torch tensor): images [batch, img_channels, img_dim, img_dim]\n\n        Returns:\n            log_marginal_likelihood (torch tensor): scalar\n            log_w (torch tensor): unnormalized log importance weights [batch, k]\n        \"\"\"\n        if not k:\n            k = self.k\n        [x_tilde, z, mu_z, log_var_z] = self.forward(x, k)\n        # upsample mu_z, std_z, x_s\n        mu_z_s = mu_z.unsqueeze(1).repeat(1, k, 1)\n        std_z_s = (0.5 * log_var_z).exp().unsqueeze(1).repeat(1, k, 1)\n        x_s = x.unsqueeze(1).repeat(1, k, 1, 1, 1)\n        # compute logarithmic unnormalized importance weights [batch, k]\n        log_p_x_g_z = dists.Bernoulli(x_tilde).log_prob(x_s).sum(axis=(2, 3, 4))\n        log_prior_z = dists.Normal(0, 1).log_prob(z).sum(2)\n        log_q_z_g_x = dists.Normal(mu_z_s, std_z_s).log_prob(z).sum(2)\n        log_w = log_p_x_g_z + log_prior_z - log_q_z_g_x\n        # compute marginal log-likelihood\n        log_marginal_likelihood = (torch.logsumexp(log_w, 1) -  np.log(k)).mean()\n        return log_marginal_likelihood, log_w\n```\n:::\n\n\n* **IWAE Implementation**\n\n  For the `IWAE` class implementation, we only need to adapt the loss computation.\n  Everything else can be inherited from the `VAE` class. In fact, we can simply\n  use `compute_marginal_log_likelihood` as the loss function computation.\n\n  For the interested reader, it might be interesting to understand the original\n  implementation. Therefore, I added to other modes of loss function calculation\n  which are based on the idea of **importance-weighted sample losses**.\n\n  As shown in the derivation, we can derive the gradient to be a linear\n  combination of importance-weighted sample losses, i.e.,\n\n  $$\n  \\begin{align}\n  \\nabla_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}}\n  \\widetilde{\\mathcal{L}}_k^{\\text{IWAE}} &=\n  \\sum_{l=1}^{k} \\widetilde{w}^{(i, l)} \\nabla_{\\boldsymbol{\\phi},\n  \\boldsymbol{\\theta}} \\log w^{(i,l)} \\left( \\textbf{x}^{(i)},\n  \\textbf{z}_{\\boldsymbol{\\phi}}^{(i,l)}, \\boldsymbol{\\theta} \\right)\n  \\end{align}\n  $$\n\n  However, computing the normalized importance weights $\\widetilde{w}^{(i,l)}$\n  from the unnormalized logarithmic importance weights $\\log w^{(i,l)}$ turns\n  out to be problematic. To understand why, let's look how the normalized\n  importance weights are defined\n\n  $$\n  \\widetilde{w}^{(i,l)} = \\frac {w^{(i, l)} } {\\sum_{l=1}^k w^{(i, l)}}\n  $$\n\n  Note that $\\log w^{(i, l)} \\in [-\\infty, 0]$ may be some big negative number.\n  Simply taken the logs into the exp function and summing them up, is a\n  bad idea for two reasons. Firstly, we might expect some rounding errors.\n  Secondly, dividing by some really small number will likely produce `nans`. To\n  circumvent this problem, there are two possible strategies:\n\n  1. *Original Implementation*: While looking through the original\n      implementation, I found that they simply shift the unnormalized logarithmic\n      importance weights, i.e.,\n\n      $$\n      \\log s^{(i, l)} = \\log w^{(i,l)} - \\underbrace{\\max_{l \\in [1, k]} \\log w^{(i,l)}}_{=a}\n      $$\n\n      Then, the normalized importance weights can simply be calculated as follows\n\n      $$\n      \\widetilde{w}^{(i,l)} = \\frac {\\exp \\left( \\log s^{(i, l)} \\right)} {\n      \\sum_{l=1}^k \\exp \\left( \\log s^{(i,l)} \\right)} = \\frac { \\frac {\\exp \\left( \\log\n      w^{(i, l)} \\right)}{\\exp a} } {\\sum_{l=1}^k \\frac {\\exp \\left( \\log\n      w^{(i, l)} \\right)}{\\exp a} }\n      $$\n\n      The idea behind this approach is to increase numerical stability by\n      shifting the logarithmic unnormalized importance weights into a range\n      where less numerical issues occur (effectively simply increasing them).\n\n  2. *Use LogSumExp*: Another common trick is to firstly calculate the\n    normalized importance weights in log units. Then, we get\n\n      $$\n        \\log \\widetilde{w}^{(i, l)} = \\log \\frac {w^{(i,l)}}{\\sum_{l=1}^k\n        w^{(i,l)}} = \\log w^{(i, l)} - \\underbrace{\\log \\sum_{l=1}^k \\exp \\left( w^{(i,l)} \\right)}_{=\\text{torch.logsumexp}}\n      $$\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nclass IWAE(VAE):\n\n    def __init__(self, k):\n        super(IWAE, self).__init__(k)\n        return\n\n    def compute_loss(self, x, k=None, mode='fast'):\n        if not k:\n            k = self.k\n        # compute unnormalized importance weights in log_units\n        log_likelihood, log_w = self.compute_marginal_log_likelihood(x, k)\n        # loss computation (several ways possible)\n        if mode == 'original':\n            ####################### ORIGINAL IMPLEMENTAION #######################\n            # numerical stability (found in original implementation)\n            log_w_minus_max = log_w - log_w.max(1, keepdim=True)[0]\n            # compute normalized importance weights (no gradient)\n            w = log_w_minus_max.exp()\n            w_tilde = (w / w.sum(axis=1, keepdim=True)).detach()\n            # compute loss (negative IWAE objective)\n            loss = -(w_tilde * log_w).sum(1).mean()\n        elif mode == 'normalized weights':\n            ######################## LOG-NORMALIZED TRICK ########################\n            # copmute normalized importance weights (no gradient)\n            log_w_tilde = log_w - torch.logsumexp(log_w, dim=1, keepdim=True)\n            w_tilde = log_w_tilde.exp().detach()\n            # compute loss (negative IWAE objective)\n            loss = -(w_tilde * log_w).sum(1).mean()\n        elif mode == 'fast':\n            ########################## SIMPLE AND FAST ###########################\n            loss = -log_likelihood\n        return loss\n```\n:::\n\n\n* **Training Procedure**\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom torch.utils.data import DataLoader\nfrom livelossplot import PlotLosses\n\n\nBATCH_SIZE = 1000\nLEARNING_RATE = 1e-4\nWEIGHT_DECAY = 1e-6\n\n\ndef train(dataset, vae_model, iwae_model, num_epochs):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    print('Device: {}'.format(device))\n\n    data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True,\n                             num_workers=12)\n    vae_model.to(device)\n    iwae_model.to(device)\n\n    optimizer_vae = torch.optim.Adam(vae_model.parameters(), lr=LEARNING_RATE,\n                                     weight_decay=WEIGHT_DECAY)\n    optimizer_iwae = torch.optim.Adam(iwae_model.parameters(), lr=LEARNING_RATE,\n                                     weight_decay=WEIGHT_DECAY)\n    losses_plot = PlotLosses(groups={'Loss': ['VAE (ELBO)', 'IWAE (NLL)']})\n    for epoch in range(1, num_epochs + 1):\n        avg_NLL_VAE, avg_NLL_IWAE = 0, 0\n        for x in data_loader:\n            x = x.to(device)\n            # IWAE update\n            optimizer_iwae.zero_grad()\n            loss = iwae_model.compute_loss(x)\n            loss.backward()\n            optimizer_iwae.step()\n            avg_NLL_IWAE += loss.item() / len(data_loader)\n\n            # VAE update\n            optimizer_vae.zero_grad()\n            loss= vae_model.compute_loss(x)\n            loss.backward()\n            optimizer_vae.step()\n\n            avg_NLL_VAE += loss.item() / len(data_loader)\n        # plot current losses\n        losses_plot.update({'VAE (ELBO)': avg_NLL_VAE, 'IWAE (NLL)': avg_NLL_IWAE},\n                           current_step=epoch)\n        losses_plot.send()\n    trained_vae, trained_iwae = vae_model, iwae_model\n    return trained_vae, trained_iwae\n```\n:::\n\n\n### Results\n\nLet's train both models for $k\\in \\{ 1, 10 \\}$:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ntrain_ds = datasets.MNIST('./data', train=True,\n                          download=True, transform=transforms.ToTensor())\nnum_epochs = 50\nlist_of_ks = [1, 10]\nfor k in list_of_ks:\n    vae_model = VAE(k)\n    iwae_model = IWAE(k)\n    trained_vae, trained_iwae = train(train_ds, vae_model, iwae_model, num_epochs)\n    torch.save(trained_vae, f'./results/trained_vae_{k}.pth')\n    torch.save(trained_iwae, f'./results/trained_iwae_{k}.pth')\n```\n:::\n\n\n$\\textbf{k=1}$\n![Training k=1](./img/k_1.png \"Training k=1\")\n$\\textbf{k=10}$\n![Training k=10](./img/k_10.png \"Training k=10\")\n\nNote that during training, we compared the **loss of the VAE (ELBO)** with the **loss\nof the IWAE (empirical estimate of marginal log-likelihood)**. Clearly, for $k=1$\nthese losses are nearly equal (as expected). For $k=10$, the difference is much\ngreater (also expected). Now let's compare the marginal log-likelihood on\nthe test samples. Since the marginal log-likelihood estimator gets more accurate\nwith increasing $k$, we set $k=200$ for the evaluation on the test set:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom prettytable import PrettyTable\n\n\ndef compute_test_log_likelihood(test_dataset, trained_vae, trained_iwae, k=200):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n    data_loader = DataLoader(test_dataset, batch_size=20,\n                             shuffle=True, num_workers=12)\n    trained_vae.to(device)\n    trained_iwae.to(device)\n\n    avg_marginal_ll_VAE = 0\n    avg_marginal_ll_IWAE = 0\n    for x in data_loader:\n        marginal_ll, _ = trained_vae.compute_marginal_log_likelihood(x.to(device), k)\n        avg_marginal_ll_VAE += marginal_ll.item() / len(data_loader)\n\n        marginal_ll, _ = trained_iwae.compute_marginal_log_likelihood(x.to(device), k)\n        avg_marginal_ll_IWAE += marginal_ll.item() / len(data_loader)\n    return avg_marginal_ll_VAE, avg_marginal_ll_IWAE\n\n\nout_table = PrettyTable([\"k\", \"VAE\", \"IWAE\"])\ntest_ds = Binarized_MNIST('./data', train=False, download=True,\n                                  transform=transforms.ToTensor())\nfor k in list_of_ks:\n    # load models\n    trained_vae = torch.load(f'./results/trained_vae_{k}.pth')\n    trained_iwae = torch.load(f'./results/trained_iwae_{k}.pth')\n    # compute average marginal log-likelihood on test dataset\n    ll_VAE, ll_IWAE = compute_test_log_likelihood(test_ds, trained_vae, trained_iwae)\n    out_table.add_row([k, np.round(ll_VAE, 2), np.round(ll_IWAE, 2)])\nprint(out_table)\n```\n:::\n\n\n![Results NLL](./img/table.png \"Results NLL\")\n\nSimilar to the paper, the IWAE benefits from an increased $k$ whereas the VAE\nperforms nearly equal.\n\n\n### Visualizations\n\nLastly, let's make some nice plots. Note that the differences are very subtle\nand it's not very helpful to make an argument based on the following\nvisualization. They mainly serve as a verification that both models do something\nuseful.\n\n* **Reconstructions**\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\n\n\ndef plot_reconstructions(vae_model, iwae_model, dataset, SEED=1):\n    np.random.seed(SEED)\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n    vae_model.to(device)\n    iwae_model.to(device)\n\n    n_samples = 7\n    i_samples = np.random.choice(range(len(dataset)), n_samples, replace=False)\n\n    fig = plt.figure(figsize=(10, 4))\n    plt.suptitle(\"Reconstructions\", fontsize=16, y=1, fontweight='bold')\n    for counter, i_sample in enumerate(i_samples):\n        orig_img = dataset[i_sample]\n        # plot original img\n        ax = plt.subplot(3, n_samples, 1 + counter)\n        plt.imshow(orig_img[0], vmin=0, vmax=1, cmap='gray')\n        plt.axis('off')\n        if counter == 0:\n            ax.annotate(\"input\", xy=(-0.1, 0.5), xycoords=\"axes fraction\",\n                        va=\"center\", ha=\"right\", fontsize=12)\n        # plot img reconstruction VAE\n        [x_tilde, z, mu_z, log_var_z] = vae_model(orig_img.unsqueeze(0).to(device))\n        ax = plt.subplot(3, n_samples, 1 + counter + n_samples)\n        x_tilde = x_tilde.squeeze(0)[0].detach().cpu().numpy()\n        plt.imshow(x_tilde[0], vmin=0, vmax=1, cmap='gray')\n        plt.axis('off')\n        if counter == 0:\n            ax.annotate(\"VAE recons\", xy=(-0.1, 0.5), xycoords=\"axes fraction\",\n                        va=\"center\", ha=\"right\", fontsize=12)\n        # plot img reconstruction IWAE\n        [x_tilde, z, mu_z, log_var_z] = iwae_model(orig_img.unsqueeze(0).to(device))\n        ax = plt.subplot(3, n_samples, 1 + counter + 2*n_samples)\n        x_tilde = x_tilde.squeeze(0)[0].detach().cpu().numpy()\n        plt.imshow(x_tilde[0], vmin=0, vmax=1, cmap='gray')\n        plt.axis('off')\n        if counter == 0:\n            ax.annotate(\"IWAE recons\", xy=(-0.1, 0.5), xycoords=\"axes fraction\",\n                        va=\"center\", ha=\"right\", fontsize=12)\n    return\n\n\nk = 10\ntrained_vae = torch.load(f'./results/trained_vae_{k}.pth')\ntrained_iwae = torch.load(f'./results/trained_iwae_{k}.pth')\nplot_reconstructions(trained_vae, trained_iwae , test_ds)\n```\n:::\n\n\n![Reconstructions k=10](./img/reconstructions.png \"Reconstructions k=10\")\n\n* **Latent Traversals**\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ndef plot_latent_traversal(vae_model, iwae_model, dataset, SEED=1):\n    np.random.seed(SEED)\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n    vae_model.to(device)\n    iwae_model.to(device)\n\n    n_samples = 128\n    i_samples = np.random.choice(range(len(dataset)), n_samples, replace=False)\n    img_batch = torch.cat([dataset[i].unsqueeze(0) for i in i_samples], 0)\n    img_batch = img_batch.to(device)\n    # generate latent traversals\n    n_pert, pert_min_max, n_lats = 5, 2, 5\n    img_trav_vae = vae_model.create_latent_traversal(img_batch, n_pert, pert_min_max, n_lats)\n    img_trav_iwae = iwae_model.create_latent_traversal(img_batch, n_pert, pert_min_max, n_lats)\n\n    fig = plt.figure(figsize=(12, 7))\n    n_rows, n_cols = n_lats + 1, 2*n_pert + 1\n    gs = GridSpec(n_rows, n_cols + 1)\n    plt.suptitle(\"Latent Traversals\", fontsize=16, y=1, fontweight='bold')\n    for row_index in range(n_lats):\n        for col_index in range(n_pert):\n            img_rec_VAE = img_trav_vae[row_index][col_index]\n            img_rec_IWAE = img_trav_iwae[row_index][col_index]\n\n            ax = plt.subplot(gs[row_index, col_index])\n            plt.imshow(img_rec_VAE[0].detach(), cmap='gray', vmin=0, vmax=1)\n            plt.axis('off')\n\n            if row_index == 0 and col_index == int(n_pert//2):\n                plt.title('VAE', fontsize=14, y=1.1)\n\n            ax = plt.subplot(gs[row_index, col_index + n_pert + 1])\n            plt.imshow(img_rec_IWAE[0].detach(), cmap='gray', vmin=0, vmax=1)\n            plt.axis('off')\n            if row_index == 0 and col_index == int(n_pert//2):\n                plt.title('IWAE', fontsize=14, y=1.1)\n    # add pertubation magnitude\n    for ax in [plt.subplot(gs[n_lats, 0:5]), plt.subplot(gs[n_lats, 6:11])]:\n        ax.annotate(\"pertubation magnitude\", xy=(0.5, 0.6), xycoords=\"axes fraction\",\n                    va=\"center\", ha=\"center\", fontsize=10)\n        ax.set_frame_on(False)\n        ax.axes.set_xlim([-1.15 * pert_min_max, 1.15 * pert_min_max])\n        ax.xaxis.set_ticks([-pert_min_max, 0, pert_min_max])\n        ax.xaxis.set_ticks_position(\"top\")\n        ax.xaxis.set_tick_params(direction=\"inout\", pad=-16)\n        ax.get_yaxis().set_ticks([])\n    # add latent coordinate traversed annotation\n    ax = plt.subplot(gs[0:n_rows-1, n_cols])\n    ax.annotate(\"latent coordinate traversed\", xy=(0.4, 0.5), xycoords=\"axes fraction\",\n                    va=\"center\", ha=\"center\", fontsize=10, rotation=90)\n    plt.axis('off')\n    return\n\n\nk = 10\ntrained_vae = torch.load(f'./results/trained_vae_{k}.pth')\ntrained_iwae = torch.load(f'./results/trained_iwae_{k}.pth')\nplot_latent_traversal(trained_vae, trained_iwae , test_ds)\n```\n:::\n\n\n![Latent Traversal k=10](./img/latent_traversal.png \"Latent Traversal k=10\")\n\n--------------------------------------------------------------------------\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}