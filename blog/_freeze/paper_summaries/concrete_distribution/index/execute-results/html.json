{
  "hash": "0685df36e8ab70be32f4ed90251502ed",
  "result": {
    "markdown": "---\ntitle: \"The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables\"\ncategories: [\"reimplementation\", \"VAE\", \"D-VAE\"]\ndate: \"2021-02-21\"\nexecute:\n  eval: false # true\nengine: jupyter\nformat:\n  html: \n    code-fold: show \n    highlight-style: github-dark \n    code-block-bg: \"#000000\"\n\n    code-tools: \n      toggle: true\n      source: \"https://github.com/borea17/Notebooks/blob/master/07_Concrete_Distribution.ipynb\"\n---\n\n[Maddison et al. (2016)](https://arxiv.org/abs/1611.00712) introduce\n**CON**tinuous relaxations of dis**CRETE** (**concrete**) random variables as an\napproximation to discrete variables. The **Concrete distribution** is motivated\nby the fact that backpropagation through discrete random variables is not\ndirectly possible. While for continuous random variables, the\n**reparametrization trick** is applicable to allow gradients to flow through a\nsampling operation, this does not work for discrete variables due to the\ndiscontinuous operations associated to their sampling. The **concrete\ndistribution** allows for a simple reparametrization through which gradients can\npropagate such that a low-variance biased gradient estimator of the discrete\npath can be obtained.\n\n## Model Description\n\nThe **Concrete distribution** builds upon the (very old) **Gumbel-Max trick**\nthat allows for a reparametrization of a categorical distribution into a\ndeterministic function over the distribution parameters and an auxiliary noise\ndistribution. The problem within this reparameterization is that it relies on an\n$\\text{argmax}$-operation such that backpropagation remains out of reach. Therefore,\n[Maddison et al. (2016)](https://arxiv.org/abs/1611.00712) propose to use the\n$\\text{softmax}$-operation as a continuous relaxation of the $\\text{argmax}$.\nThis idea has been concurrently developed at the same time by [Jang et al.\n(2016)](https://arxiv.org/abs/1611.01144) who called it the **Gumbel-Softmax\ntrick**.\n\n### Gumbel-Max Trick\n\nThe Gumbel-Max trick basically refactors sampling of a deterministic random\nvariable into a component-wise addition of the discrete distribution parameters\nand an auxiliary noise followed by $\\text{argmax}$, i.e.,\n\n$$\n\\begin{align}\n\\text{Sampling }&z \\sim \\text{Cat} \\left(\\alpha_1, \\dots, \\alpha_N \\right)\n\\text{ can equally expressed as } z = \\arg\\max_{k} \\Big(\\log\\alpha_k + G_k\\Big)\\\\\n&\\text{with } G_k \\sim \\text{Gumbel Distribution}\\left(\\mu=0, \\beta=1 \\right)\n\\end{align}\n$$\n\n| ![Gumble Max](./img/Gumble_Max.png \"Gumble Max\") |\n| :--:        |\n| **Computational Graph Gumbel-Max Trick**. Taken from [Maddison et al. (2016)](https://arxiv.org/abs/1611.00712). |\n\n**Derivation**: Let's take a closer look on how and why that works. Firstly, we\nshow that samples from $\\text{Cat} \\left(\\alpha_1, \\dots, \\alpha_N \\right)$ are\nequally distributed to\n\n$$\nz = \\arg \\min_{k} \\frac {\\epsilon_k}{\\alpha_k} \\quad \\text{with} \\quad\n\\epsilon_k \\sim \\text{Exp}\\left( 1 \\right)\n$$\n\nTherefore, we observe that each term inside the $\\text{argmin}$ is independent\nexponentially distributed with ([easy proof](https://math.stackexchange.com/a/85578))\n\n$$\n\\frac {\\epsilon_k}{\\alpha_k} \\sim  \\text{Exp} \\Big( \\alpha_k \\Big)\n$$\n\nThe next step is to show that the index of the variable which achieves the\nminimum is distributed according to the categorical distribution ([easy\nproof](https://en.wikipedia.org/wiki/Exponential_distribution#Distribution_of_the_minimum_of_exponential_random_variables))\n\n$$\n\\arg \\min_{k} \\frac {\\epsilon_k}{\\alpha_k} = P \\left( k | z_k = \\min \\{ z_1, \\dots, z_N \\} \\right) = \\frac\n{\\alpha_k}{\\sum_{i=1}^N \\alpha_i}\n$$\n\nA nice feature of this formulation is that the categorical distribution\nparameters $\\{\\alpha_i\\}_{i=1}^N$ do not need to be normalized before\nreparameterization as normalization is ensured by the factorization itself.\nLastly, we simply reformulate this mapping by applying the log and multiplying\nby minus 1\n\n$$\nz = \\arg \\min_{k} \\frac {\\epsilon_k}{\\alpha_k} =\\arg \\min_k \\Big(\\log \\epsilon_k -\n\\log \\alpha_k \\Big) = \\arg \\max_k \\Big(\\log \\alpha_k  - \\log \\epsilon_k\\Big)\n$$\n\nThis looks already very close to the **Gumbel-Max trick** defined above. Remind\nthat to generate exponential distributed random variables, we can simply\ntransform uniformly distributed samples of the unit interval as follows\n\n$$\n\\epsilon_k = -\\log u_k \\quad \\text{with} \\quad u_k \\sim\n\\text{Uniform Distribution} \\Big(0, 1\\Big)\n$$\n\nThus, we get that\n\n$$\n- \\log \\epsilon_k = - \\log \\Big( - \\log u_k \\Big) = G_k \\sim\n\\text{Gumbel Distribution} \\Big( \\mu=0, \\beta=1 \\Big)\n$$\n\n\n### Gumbel-Softmax Trick\n\nThe problem in the **Gumbel-Max trick** is the $\\text{argmax}$-operation as the\nderivative of $\\text{argmax}$ is 0 everywhere except at the boundary of state\nchanges, where it is undefined. Thus, [Maddison et al.\n(2016)](https://arxiv.org/abs/1611.00712) use the temperature-valued\n$\\text{Softmax}$ as a continuous relaxation of the $\\text{argmax}$ computation\nsuch that\n\n$$\n\\begin{align}\n\\text{Sampling }&z \\sim \\text{Cat} \\left(\\alpha_1, \\dots, \\alpha_N \\right)\n\\text{ is relaxed cont. to } z_k = \\frac {\\exp \\left( \\frac {\\log \\alpha_k + G_k}{\\lambda} \\right)}\n{\\sum_{i=1}^N \\exp \\left( \\frac {\\log \\alpha_i + G_i}{\\lambda} \\right)}\\\\\n&\\text{with } G_k \\sim \\text{Gumbel Distribution}\\left(\\mu=0, \\beta=1 \\right)\n\\end{align}\n$$\n\nwhere $\\lambda \\in [0, \\infty[$ is the temperature and $\\alpha_k \\in [0,\n\\infty[$ are the categorical distribution parameters. The temperature can be\nunderstood as a hyperparameter that controls the *sharpness* of the\n$\\text{softmax}$, i.e., how much the *winner-takes-all* dynamics of the\nsoftmax is taken:\n\n* $\\lambda \\rightarrow 0$: $\\text{softmax}$ smoothly approaches discrete $\\text{argmax}$\n  computation\n* $\\lambda \\rightarrow \\infty$: $\\text{softmax}$ leads to uniform distribution.\n\nNote that the samples $z_k$ obtained by this reparameterization follow a new\nfamily of distributions, the **Concrete distribution**. Thus, $z_k$ are called\n**Concrete random variables**.\n\n| ![Gumble Softmax](./img/Gumble_Softmax.png \"Gumble Softmax\") |\n| :--:        |\n| **Computational Graph Gumbel-Softmax Trick**. Taken from [Maddison et al. (2016)](https://arxiv.org/abs/1611.00712). |\n\n**Intuition**: To better understand the relationship between the *Concrete*\ndistribution and the *discrete categorical* distribution, let's look on an\nexemplary result. Remind that the $\\text{argmax}$ operation for a\n$n$-dimensional categorical distribution returns states on the vertices of the simplex\n\n$$\n\\boldsymbol{\\Delta}^{n-1} = \\left\\{ \\textbf{x}\\in \\{0, 1\\}^n \\mid \\sum_{k=1}^n x_k = 1 \\right\\}\n$$\n\nConcrete random variables are relaxed to return states in the interior of the\nsimplex\n\n$$\n\\widetilde{\\boldsymbol{\\Delta}}^{n-1} = \\left\\{ \\textbf{x}\\in [0, 1]^n \\mid \\sum_{k=1}^n x_k = 1 \\right\\}\n$$\n\nThe image below shows how the distribution of concrete random variables changes\nfor an exemplary discrete categorical distribution $(\\alpha_1, \\alpha_2,\n\\alpha_3) = (2, 0.5, 1)$ and different temperatures $\\lambda$.\n\n| ![Relationship between Concrete and Discrete Variables](./img/simplex.png \"Relationship between Concrete and Discrete Variables\") |\n| :---        |\n| **Relationship between Concrete and Discrete Variables**: A discrete distribution with unnormalized probabilities $(\\alpha_1, \\alpha_2, \\alpha_3) = (2, 0.5, 1)$ and three corresponding **Concrete densities** at increasing temperatures $\\lambda$.<br> Taken from [Maddison et al. (2016)](https://arxiv.org/abs/1611.00712). |\n\n### Concrete Distribution\n\nWhile the Gumbel-Softmax trick defines how to obtain samples from a **Concrete\ndistribution**, [Maddison et al. (2016)](https://arxiv.org/abs/1611.00712)\nprovide a definition of its density and prove some nice properties:\n\n**Definition**: The **Concrete distribution** $\\text{X} \\sim\n\\text{Concrete}(\\boldsymbol{\\alpha}, \\lambda)$ with temperature $\\lambda \\in\n[0, \\infty[$ and location $\\boldsymbol{\\alpha} =\n\\begin{bmatrix} \\alpha_1 & \\dots & \\alpha_n \\end{bmatrix} \\in [0, \\infty]^{n}$\nhas a density\n\n$$\n  p_{\\boldsymbol{\\alpha}, \\lambda} (\\textbf{x}) = (n-1)! \\lambda^{n-1} \\prod_{k=1}^n\n  \\left( \\frac {\\alpha_k x_k^{-\\lambda - 1}} {\\sum_{i=1}^n \\alpha_i x_i^{-\\lambda}} \\right)\n$$\n\n**Nice Properties and their Implications**:\n\n1. *Reparametrization*: Instead of sampling directly from the Concrete\n   distribution, one can obtain samples by the following deterministic ($d$) reparametrization\n\n   $$\n   X_k \\stackrel{d}{=} \\frac {\\exp \\left( \\frac {\\log \\alpha_k + G_k}{\\lambda} \\right)}\n{\\sum_{i=1}^N \\exp \\left( \\frac {\\log \\alpha_i + G_i}{\\lambda} \\right)} \\quad\n   \\text{with} \\quad G_k \\sim \\text{Gumbel}(0, 1)\n   $$\n\n   This property ensures that we can easily compute unbiased low-variance\n   gradients w.r.t. the location parameters $\\boldsymbol{\\alpha}$ of the\n   Concrete distribution.\n\n2. *Rounding*: Rounding a Concrete random variable results in the discrete\n   random variable whose distribution is described by the logits $\\log \\alpha_k$\n\n   $$\n   P (\\text{X}_k > \\text{X}_i \\text{ for } i\\neq k) = \\frac {\\alpha_k}{\\sum_{i=1}^n \\alpha_i}\n   $$\n\n   This property again indicates the close relationship between concrete and discrete distributions.\n\n3. *Convex eventually*:\n\n   $$\n   \\text{If } \\lambda \\le \\frac {1}{n-1}, \\text{ then } p_{\\boldsymbol{\\alpha},\n   \\lambda} \\text{ is log-convex in } x\n   $$\n\n   This property basically tells us if $\\lambda$ is small enough, there are no modes in the\n   interior of the probability simplex.\n\n\n\n### Discrete-Latent VAE\n\nOne use-case of the **Concrete distribution** and its reparameterization is the\ntraining of an variational autoencoder (VAE) with a discrete latent space. The\nmain idea is to use the Concrete distribution during training and use discrete\nsampled latent variables at test-time. An obvious limitation of this approach is\nthat during training non-discrete samples are returned such that our model\nneeds to be able to handle continuous variables[^1]. Let's dive into the\n**discrete-latent VAE** described by [Maddison et al.\n(2016)](https://arxiv.org/abs/1611.00712).\n\nWe assume that we have a dataset $\\textbf{X} =\n\\{\\textbf{x}^{(i)}\\}_{i=1}^N$ of $N$ i.i.d. samples $\\textbf{x}^{(i)}$ which were generated\nby the following process:\n\n1. We sample a  *one-hot* latent vector $\\textbf{d}\\in\\{0, 1\\}^{K}$\n   from a categorical prior distribution $P_{\\boldsymbol{a}} (\\textbf{d})$.\n2. We use our sample $\\textbf{d}^{(i)}$ and put it into the **scene model**\n   $p_{\\boldsymbol{\\theta}}(\\textbf{x}|\\textbf{d})$ from which we sample to\n   generate the observed image $\\textbf{x}^{(i)}$.\n\nAs a result, the marginal likelihood of an image can be stated as follows\n\n$$\np_{\\boldsymbol{\\theta}, \\boldsymbol{a}} (\\textbf{x}) = \\mathbb{E}_{\\textbf{d}\n\\sim P_{\\boldsymbol{a}}(\\textbf{d})} \\Big[ p_{\\boldsymbol{\\theta}} (\\textbf{x} |\n\\textbf{d}) \\Big] = \\sum P_{\\boldsymbol{a}} \\left(\\textbf{d}^{(i)} \\right) p_{\\boldsymbol{\\theta}} \\left( \\textbf{x} |\n\\textbf{d}^{(i)} \\right),\n$$\n\nwhere the sum is over all possible $K$ dimensional one-hot vectors. In order to\nrecover this generative process, we introduce a variational approximation\n$Q_{\\boldsymbol{\\phi}} (\\textbf{d}|\\textbf{x})$ of the true, but unknown posterior.\nNow we exchange the sampling distribution towards this approximation\n\n$$\np_{\\boldsymbol{\\theta}, \\boldsymbol{a}} (\\textbf{x}) = \\sum\n\\frac {p_{\\boldsymbol{\\theta}, \\boldsymbol{a}} \\left(\\textbf{x}, \\textbf{d}^{(i)}\n\\right)}{Q_{\\boldsymbol{\\phi}} \\left(\\textbf{d}^{(i)} | \\textbf{x}\\right)}\nQ_{\\boldsymbol{\\phi}} \\left(\\textbf{d}^{(i)} | \\textbf{x}\\right) =\n\\mathbb{E}_{\\textbf{d} \\sim  Q_{\\boldsymbol{\\phi}} \\left(\\textbf{d} |\n\\textbf{x}\\right)} \\left[ \\frac {p_{\\boldsymbol{\\theta}, \\boldsymbol{a}} \\left(\\textbf{x}, \\textbf{d}\n\\right)}{Q_{\\boldsymbol{\\phi}} \\left(\\textbf{d} | \\textbf{x}\\right)} \\right]\n$$\n\nLastly, applying Jensen's inequality on the log-likelihood leads to the evidence\nlower bound (ELBO) objective of VAEs\n\n$$\n \\log p_{\\boldsymbol{\\theta}, \\boldsymbol{a}} (\\textbf{x}) =\n \\log \\left(\n\\mathbb{E}_{\\textbf{d} \\sim  Q_{\\boldsymbol{\\phi}} \\left(\\textbf{d} |\n\\textbf{x}\\right)} \\left[ \\frac {p_{\\boldsymbol{\\theta}, \\boldsymbol{a}} \\left(\\textbf{x}, \\textbf{d}\n\\right)}{Q_{\\boldsymbol{\\phi}} \\left(\\textbf{d} | \\textbf{x}\\right)} \\right]\\right)\n\\ge\n\\mathbb{E}_{\\textbf{d} \\sim  Q_{\\boldsymbol{\\phi}} \\left(\\textbf{d} |\n\\textbf{x}\\right)} \\left[ \\log  \\frac {p_{\\boldsymbol{\\theta}, \\boldsymbol{a}} \\left(\\textbf{x}, \\textbf{d}\n\\right)}{Q_{\\boldsymbol{\\phi}} \\left(\\textbf{d} | \\textbf{x}\\right)} \\right]\n= \\mathcal{L}^{\\text{ELBO}}\n$$\n\nWhile we are able to compute this objective, we cannot simply optimize it using\nstandard automatic differentiation (AD) due to the discrete sampling operations.\nThe **concrete distribution comes to rescue**: [Maddison et al.\n(2016)](https://arxiv.org/abs/1611.00712) propose to relax the terms\n$P_{\\boldsymbol{a}}(\\textbf{d})$ and\n$Q_{\\boldsymbol{\\phi}}(\\textbf{d}|\\textbf{x})$ using concrete distributions\ninstead, leading to the relaxed objective\n\n$$\n\\mathcal{L}^{\\text{ELBO}}=\n\\mathbb{E}_{\\textbf{d} \\sim  Q_{\\boldsymbol{\\phi}} \\left(\\textbf{d} |\n\\textbf{x}\\right)} \\left[ \\log  \\frac {p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}| \\textbf{d}\n\\right) P_{\\boldsymbol{a}} (\\textbf{d}) }{Q_{\\boldsymbol{\\phi}} \\left(\\textbf{d} | \\textbf{x}\\right)} \\right]\n\\stackrel{\\text{relax}}{\\rightarrow}\n\\mathbb{E}_{\\textbf{z} \\sim  q_{\\boldsymbol{\\phi}, \\lambda_1} \\left(\\textbf{z} |\n\\textbf{x}\\right)} \\left[ \\log  \\frac {p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}| \\textbf{z}\n\\right) p_{\\boldsymbol{a}, \\lambda_2} (\\textbf{z}) }{q_{\\boldsymbol{\\phi}, \\lambda_1} \\left(\\textbf{z} | \\textbf{x}\\right)} \\right]\n$$\n\nThen, during training we optimize the relaxed objective while during test time\nwe evaluate the original objective including discrete sampling operations. The\nreally neat thing here is that switching between the two modes works out of the box:\nwe only need to switch between the $\\text{softmax}$ and $\\text{argmax}$ operations.\n\n| ![Discrete-Latent VAE Architecture](./img/discrete_VAE.png \"Discrete-Latent VAE Architecture\") |\n| :--:        |\n| **Discrete-Latent VAE Architecture** |\n\n**Things to beware of**: [Maddison et al.\n(2016)](https://arxiv.org/abs/1611.00712) noted that `naively implementing [the\nrelaxed objective] will result in numerical issues`. Therefore, they give some\nimplementation hints in Appendix C:\n\n* **Log-Probabilties of Concrete Variables can suffer from underflow**: Let's\n  investigate why this might happen. The log-likelihood of a concrete variable $\\textbf{z}$\n  is given by\n\n  $$\n  \\begin{align}\n    \\log p_{\\boldsymbol{\\alpha}, \\lambda} (\\textbf{z}) =& \\log \\Big((K-1)! \\Big) + (K-1) \\log \\lambda\n    + \\left(\\sum_{i=1}^K  \\log \\alpha_i + (-\\lambda - 1) \\log  z_i \\right) \\\\\n     &- K \\log \\left(\\sum_{i=1}^K \\exp\\left( \\log \\alpha_i - \\lambda \\log z_i\\right)\\right)\n  \\end{align}\n  $$\n\n  Now let's remind that concrete variables are pushing towards one-hot vectors\n  (when $\\lambda$ is set accordingly), i.e., due to rounding/underflow we might get\n  some $z_i=0$. This is problematic, since the $\\log$ is not defined in this\n  case.\n\n  To circumvent this, [Maddison et al. (2016)](https://arxiv.org/abs/1611.00712)\n  propose to work with Concrete random variables in log-space, i.e., to use the\n  following reparameterization\n\n  $$\n  y_i = \\frac {\\log \\alpha_i + G_i}{\\lambda} - \\log \\left( \\sum_{i=1}^K \\exp\n  \\left( \\frac {\\log \\alpha_i + G_i}{\\lambda} \\right) \\right)\n  \\quad G_i \\sim \\text{Gumbel}(0, 1)\n  $$\n\n  The resulting random variable $\\textbf{y}\\in\\mathbb{R}^K$ has the property that\n  $\\exp(Y) \\sim \\text{Concrete}\\left(\\boldsymbol{\\alpha}, \\lambda \\right)$,\n  therefore they denote $Y$ as an $\\text{ExpConcrete}\\left(\\boldsymbol{\\alpha},\n  \\lambda\\right)$. Accordingly, the log-likelihood $\\log\n  \\kappa_{\\boldsymbol{\\alpha}, \\lambda}$ of a variable ExpConcrete variable\n  $\\textbf{y}$ is given by\n\n  $$\n  \\begin{align}\n    \\log p_{\\boldsymbol{\\alpha}, \\lambda} (\\textbf{y}) =& \\log \\Big((K-1)! \\Big) + (K-1) \\log \\lambda\n    + \\left(\\sum_{i=1}^K  \\log \\alpha_i + (\\lambda - 1) y_i \\right) \\\\\n     &- n \\log \\left(\\sum_{i=1}^n \\exp\\left( \\log \\alpha_i - \\lambda y_i\\right)\\right)\n  \\end{align}\n  $$\n\n  This reparameterization does not change our approach due to the fact that the\n  KL terms of a variational loss are invariant under invertible transformations,\n  i.e., since $\\exp$ is invertible, the KL divergence between two $\\text{ExpConcrete}$ is\n  the same the KL divergence between two $\\text{Concrete}$ distributions.\n\n* **Working with $\\text{ExpConcrete}$ random variables**: Remind\n  the relaxed objective\n\n  $$\n  \\mathcal{L}^{\\text{ELBO}}_{rel} =\n  \\mathbb{E}_{\\textbf{z} \\sim  q_{\\boldsymbol{\\phi}, \\lambda_1} \\left(\\textbf{z} |\n  \\textbf{x}\\right)} \\left[\n  \\log p_{\\boldsymbol{\\theta}} \\left( \\textbf{x} | \\textbf{z}\\right) + \\log\n  \\frac{\n  p_{\\boldsymbol{a}, \\lambda_2} (\\textbf{z})\n  } {q_{\\boldsymbol{\\phi},\n  \\lambda_1} \\left(\\textbf{z} | \\textbf{x}\\right)}\n  \\right]\n  $$\n\n  Now let's exchange the $\\text{Concrete}$ by $\\text{ExpConcrete}$ distributions\n\n  $$\n  \\mathcal{L}^{\\text{ELBO}}_{rel} =\n  \\mathbb{E}_{\\textbf{y} \\sim  \\kappa_{\\boldsymbol{\\phi}, \\lambda_1} \\left(\\textbf{z} |\n  \\textbf{x}\\right)} \\left[\n  \\log p_{\\boldsymbol{\\theta}} \\left( \\textbf{x} | \\exp(\\textbf{y})\\right) + \\log\n  \\frac{\n  \\rho_{\\boldsymbol{a}, \\lambda_2} (\\textbf{y})\n  } {\\kappa_{\\boldsymbol{\\phi},\n  \\lambda_1} \\left(\\textbf{z} | \\textbf{y}\\right)}\n  \\right],\n  $$\n\n  where $\\rho_{\\boldsymbol{a}, \\lambda_2} (\\textbf{y})$ is the density of an\n  $\\text{ExpConcrete}$ corresponding to the $\\text{Concrete}$ distribution\n  $p_{\\boldsymbol{a}, \\lambda_2} (\\textbf{z})$. Thus, during the implementation\n  we will simply use $\\text{ExpConcrete}$ random variables $\\textbf{y}$ as\n  random variables and then perform an $\\exp$ computation before putting them\n  through the decoder.\n\n* **Choosing the temperature** $\\lambda$: [Maddison et al.\n  (2016)](https://arxiv.org/abs/1611.00712) note that the success of the\n  training heavily depends on the choice of temperature. It is rather intuitive\n  that the relaxed nodes should not be able to represent precise real valued\n  mode in the interior of the probability simplex, since otherwise the model is\n  designed to fail. In other words, the only modes of the concrete distributions\n  should be at the vertices of the probability simplex. Fortunately, [Maddison et al.\n  (2016)](https://arxiv.org/abs/1611.00712) proved that\n\n   $$\n   \\text{If } \\lambda \\le \\frac {1}{n-1}, \\text{ then } p_{\\boldsymbol{\\alpha},\n   \\lambda} \\text{ is log-convex in } x\n   $$\n\n  In other words, if we keep $\\lambda \\le \\frac {1}{n-1}$, there are no modes in\n  the interior. However, [Maddison et al.\n  (2016)](https://arxiv.org/abs/1611.00712)  note that in practice, this\n  upper-bound on $\\lambda$ might be too tight, e.g., they found for $n=4$ that\n  $\\lambda=1$ was the best temperature and in $n=8$, $\\lambda=\\frac {2}{3}$. As\n  a result, they recommend to rather explore $\\lambda$ as tuneable\n  hyperparameters.\n\n  Last note about the temperature $\\lambda$: They found that choosing different\n  temperatures $\\lambda_1$ and $\\lambda_2$ for the posterior\n  $\\kappa_{\\boldsymbol{\\alpha}, \\lambda_1}$ and prior $\\rho_{\\boldsymbol{a},\n  \\lambda_2}$ could dramatically improve the results.\n\n\n[^1]: While continuous variables do not pose a problem for standard VAEs with\n    neural networks as approximations, it should be noted that there are\n    numerous cases in which we cannot operate with continuous variables, e.g.,\n    when the (discrete) variable is used as a decision variable.\n\n## Implementation\n\nLet's showcase how the **discrete-latent VAE** performs in comparison to the\n**standard VAE** (with Gaussian latents). For the sake of simplicity, I am going\nto create a very (VERY) simple dataset that should mimick the **generative\nprocess** we assume in the discrete-latent VAE, i.e., there are $K$ one-hot\nvectors $\\textbf{d}$ and a Gaussian distribution $p_{\\boldsymbol{\\theta}}\n(\\textbf{x} | \\textbf{d})$.\n\n### Data Generation\n\nThe dataset is made of three ($K=3$) distinct shapes each is assigned a distinct\ncolor such that in fact there are only three images in the dataset\n$\\textbf{X}=\\{\\textbf{x}_i\\}_{i=1}^3$. Therefore, the Gaussian distribution\n$p_{\\boldsymbol{\\theta}} (\\textbf{x} | \\textbf{d})$ has an infinitely small\nvariance. To allow for minibatches during training and to make the epochs larger\nthan one iteration, we upsample the three images by repeating each image $1000$\ntimes in the dataset:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport torch\nimport torchvision.transforms as transforms\nfrom PIL import Image, ImageDraw\nfrom torch.utils.data import TensorDataset\nfrom sklearn.preprocessing import OneHotEncoder\n\n\ndef generate_img(shape, color, img_size):\n    \"\"\"Generate an RGB image from the provided latent factors\n\n    Args:\n        shape (string): can only be 'circle', 'square', 'triangle'\n        color (string): color name or rgb string\n        img_size (int): describing the image size (img_size, img_size)\n        size (int): size of shape\n\n    Returns:\n        torch tensor [3, img_size, img_size]\n    \"\"\"\n    # blank image\n    img = Image.new('RGB', (img_size, img_size), color='black')\n    # center coordinates\n    center = img_size//2\n    # define coordinates\n    x_0, y_0 = center - size//2, center - size//2\n    x_1, y_1 = center + size//2, center + size//2\n    # draw shapes\n    img1 = ImageDraw.Draw(img)\n    if shape == 'square':\n        img1.rectangle([(x_0, y_0), (x_1, y_1)], fill=color)\n    elif shape == 'circle':\n        img1.ellipse([(x_0, y_0), (x_1, y_1)], fill=color)\n    elif shape == 'triangle':\n        y_0, y_1 = center + size//3,  center - size//3\n        img1.polygon([(x_0, y_0), (x_1, y_0), (center, y_1)], fill=color)\n    return transforms.ToTensor()(img)\n\n\ndef generate_dataset(n_samples_per_class, colors, shapes, sizes, img_size):\n    data, labels = [], []\n    for (n_samples, color, shape, size) in zip(n_samples_per_class,colors,shapes,sizes):\n        img = generate_img(shape, color, img_size, size)\n\n        data.append(img.unsqueeze(0).repeat(n_samples, 1, 1, 1))\n        labels.extend(n_samples*[shape])\n    # cast data to tensor [sum(n_samples_per_class), 3, img_size, img_size]\n    data = torch.vstack(data).type(torch.float32)\n    # create one-hot encoded labels\n    labels = OneHotEncoder().fit_transform(np.array(labels).reshape(-1, 1)).toarray()\n    # make tensor dataset\n    dataset = TensorDataset(data, torch.from_numpy(labels))\n    return dataset\n\nIMG_SIZE = 32\nN_SAMPLES_PER_CLASS = [1000, 1000, 1000]\nSHAPES = ['square', 'circle', 'triangle']\nCOLORS = ['red', 'green', 'blue']\nSIZES = [12, 14, 20]\ndataset = generate_dataset(N_SAMPLES_PER_CLASS,COLORS, SHAPES, SIZES, IMG_SIZE)\n```\n:::\n\n\n| ![Dataset](./img/dataset.png \"Dataset\") |\n| :--:        |\n| **Dataset** |\n\n### Model Implementation\n\n* **Standard VAE**\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport torch.nn as nn\nimport torch.distributions as dists\n\nHIDDEN_DIM = 200\nLATENT_DIM = 3\nFIXED_VAR = 0.1**2\n\n\nclass VAE(nn.Module):\n\n    def __init__(self):\n        super(VAE, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear((IMG_SIZE**2)*3, HIDDEN_DIM),\n            nn.ReLU(),\n            nn.Linear(HIDDEN_DIM, 2*LATENT_DIM)\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(LATENT_DIM, HIDDEN_DIM),\n            nn.ReLU(),\n            nn.Linear(HIDDEN_DIM, (IMG_SIZE**2)*3),\n        )\n        return\n\n    def compute_loss(self, x):\n        [x_tilde, z, mu_z, log_var_z] = self.forward(x)\n        # compute negative log-likelihood\n        NLL = -dists.Normal(x_tilde, FIXED_VAR).log_prob(x).sum(axis=(1, 2, 3)).mean()\n        # copmute kl divergence\n        KL_Div = -0.5*(1 + log_var_z - mu_z.pow(2) - log_var_z.exp()).sum(1).mean()\n        # compute loss\n        loss = NLL + KL_Div\n        return loss, NLL, KL_Div\n\n    def forward(self, x):\n        \"\"\"feed image (x) through VAE\n\n        Args:\n            x (torch tensor): input [batch, img_channels, img_dim, img_dim]\n\n        Returns:\n            x_tilde (torch tensor): [batch, img_channels, img_dim, img_dim]\n            z (torch tensor): latent space samples [batch, LATENT_DIM]\n            mu_z (torch tensor): mean latent space [batch, LATENT_DIM]\n            log_var_z (torch tensor): log var latent space [batch, LATENT_DIM]\n        \"\"\"\n        z, mu_z, log_var_z = self.encode(x)\n        x_tilde = self.decode(z)\n        return [x_tilde, z, mu_z, log_var_z]\n\n    def encode(self, x):\n        \"\"\"computes the approximated posterior distribution parameters and\n        samples from this distribution\n\n        Args:\n            x (torch tensor): input [batch, img_channels, img_dim, img_dim]\n\n        Returns:\n            z (torch tensor): latent space samples [batch, LATENT_DIM]\n            mu_E (torch tensor): mean latent space [batch, LATENT_DIM]\n            log_var_E (torch tensor): log var latent space [batch, LATENT_DIM]\n        \"\"\"\n        # get encoder distribution parameters\n        out_encoder = self.encoder(x)\n        mu_E, log_var_E = torch.chunk(out_encoder, 2, dim=1)\n        # sample noise variable for each batch and sample\n        epsilon = torch.randn_like(log_var_E)\n        # get latent variable by reparametrization trick\n        z = mu_E + torch.exp(0.5*log_var_E) * epsilon\n        return z, mu_E, log_var_E\n\n    def decode(self, z):\n        \"\"\"computes the Gaussian mean of p(x|z)\n\n        Args:\n            z (torch tensor): latent space samples [batch, LATENT_DIM]\n\n        Returns:\n            x_tilde (torch tensor): [batch, img_channels, img_dim, img_dim]\n        \"\"\"\n        # get decoder distribution parameters\n        x_tilde = self.decoder(z).view(-1, 3, IMG_SIZE, IMG_SIZE)\n        return x_tilde\n\n    def create_latent_traversal(self, image_batch, n_pert, pert_min_max=2, n_latents=3):\n        device = image_batch.device\n        # initialize images of latent traversal\n        images = torch.zeros(n_latents, n_pert, *image_batch.shape[1::])\n        # select the latent_dims with lowest variance (most informative)\n        [x_tilde, z, mu_z, log_var_z] = self.forward(image_batch)\n        i_lats = log_var_z.mean(axis=0).sort()[1][:n_latents]\n        # sweep for latent traversal\n        sweep = np.linspace(-pert_min_max, pert_min_max, n_pert)\n        # take first image and encode\n        [z, mu_E, log_var_E] = self.encode(image_batch[0:1])\n        for latent_dim, i_lat in enumerate(i_lats):\n            for pertubation_dim, z_replaced in enumerate(sweep):\n                # copy z and pertubate latent__dim i_lat\n                z_new = z.detach().clone()\n                z_new[0][i_lat] = z_replaced\n\n                img_rec = self.decode(z_new.to(device)).squeeze(0).cpu()\n\n                images[latent_dim][pertubation_dim] = img_rec\n        return images\n```\n:::\n\n\n* **Discrete-Latent VAE**: \n\n  Luckily, [Pytorch distributions](https://pytorch.org/docs/stable/distributions.html) have\n  already implemented the **concrete distribution** which even takes care of\n  using the $\\text{ExpConcrete}$ for the computation of the log probability, see [source\n  code](https://pytorch.org/docs/stable/_modules/torch/distributions/relaxed_categorical.html#RelaxedOneHotCategorical).\n\n  As suggested by [Maddison et al. (2016)](https://arxiv.org/abs/1611.00712), we\n  set $\\lambda_1 = \\frac{2}{3}$. Setting $\\lambda_2 = 2$ seemed to improve\n  stability, however I did not take time to really tune these hyperparameters\n  (which is just not necessary due to the simplicity of the task).\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nLAMBDA_1 = torch.tensor([2/3])\nLAMBDA_2 = torch.tensor([2.])\nPRIOR_PROBS = 1/LATENT_DIM*torch.ones(LATENT_DIM)\n\n\nclass DiscreteVAE(nn.Module):\n\n    def __init__(self):\n        super(DiscreteVAE, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear((IMG_SIZE**2)*3, HIDDEN_DIM),\n            nn.ReLU(),\n            nn.Linear(HIDDEN_DIM, LATENT_DIM)\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(LATENT_DIM, HIDDEN_DIM),\n            nn.ReLU(),\n            nn.Linear(HIDDEN_DIM, (IMG_SIZE**2)*3),\n        )\n        self.register_buffer(\"LAMBDA_1\", LAMBDA_1)\n        self.register_buffer(\"LAMBDA_2\", LAMBDA_2)\n        self.register_buffer(\"PRIOR_PROBS\", PRIOR_PROBS)\n        return\n\n    def compute_loss(self, x):\n        [x_tilde, z, latent_dist] = self.forward(x, \"Train\")\n        # compute negative log-likelihood\n        NLL = -dists.Normal(x_tilde, FIXED_VAR).log_prob(x).sum(axis=(1, 2, 3)).mean()\n        # copmute kl divergence\n        PRIOR_DIST = dists.RelaxedOneHotCategorical(self.LAMBDA_2, self.PRIOR_PROBS)\n        KL_Div =  (latent_dist.log_prob(z) - PRIOR_DIST.log_prob(z)).mean()\n        # compute loss\n        loss = NLL + KL_Div\n        return loss, NLL, KL_Div\n\n    def forward(self, x, mode=\"Train\"):\n        latent_dist, z = self.encode(x, mode)\n        x_tilde = self.decode(z)\n        return [x_tilde, z, latent_dist]\n\n    def encode(self, x, mode=\"Train\"):\n        \"\"\"computes the approximated posterior distribution parameters and\n        returns the distribution (torch distribution) and a sample from that\n        distribution\n\n        Args:\n            x (torch tensor): input [batch, img_channels, img_dim, img_dim]\n\n        Returns:\n            dist (torch distribution): latent distribution\n        \"\"\"\n        # get encoder distribution parameters\n        log_alpha = self.encoder(x)\n        probs = log_alpha.exp()\n        if mode == \"Train\":\n            # concrete distribution\n            latent_dist = dists.RelaxedOneHotCategorical(self.LAMBDA_1, probs)\n            z = latent_dist.rsample()\n            return [latent_dist, z]\n        elif mode == \"Test\":\n            # discrete distribution\n            latent_dist = dists.OneHotCategorical(probs)\n            d = latent_dist.sample()\n            return [latent_dist, d]\n\n    def create_latent_traversal(self):\n        \"\"\"in the discrete case there are only LATENT_DIM possible latent states\"\"\"\n        # initialize images of latent traversal\n        images = torch.zeros(LATENT_DIM, 3, IMG_SIZE, IMG_SIZE)\n        latent_samples = torch.zeros(LATENT_DIM, LATENT_DIM)\n        for i_lat in range(LATENT_DIM):\n            d = torch.zeros(1, LATENT_DIM).to(self.LAMBDA_1.device)\n            d[0][i_lat] = 1\n            images[i_lat] = self.decode(d).squeeze(0)\n            latent_samples[i_lat] = d\n        return images, latent_samples\n\n    def decode(self, z):\n        \"\"\"computes the Gaussian mean of p(x|z)\n\n        Args:\n            z (torch tensor): latent space samples [batch, LATENT_DIM]\n\n        Returns:\n            x_tilde (torch tensor): [batch, img_channels, img_dim, img_dim]\n        \"\"\"\n        # get decoder distribution parameters\n        x_tilde = self.decoder(z).view(-1, 3, IMG_SIZE, IMG_SIZE)\n        return x_tilde\n```\n:::\n\n\n* **Training Procedure**\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom torch.utils.data import DataLoader\nfrom livelossplot import PlotLosses\n\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-6\n\ndef train(dataset, std_vae, discrete_vae, num_epochs):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    print('Device: {}'.format(device))\n\n    data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True,\n                             num_workers=12)\n    std_vae.to(device)\n    discrete_vae.to(device)\n\n    optimizer_std_vae = torch.optim.Adam(std_vae.parameters(), lr=LEARNING_RATE,\n                                         weight_decay=WEIGHT_DECAY)\n    optimizer_dis_vae = torch.optim.Adam(discrete_vae.parameters(), lr=LEARNING_RATE,\n                                         weight_decay=WEIGHT_DECAY)\n    losses_plot = PlotLosses(groups={'KL Div': ['STD-VAE KL', 'Discrete-VAE KL'],\n                                     'NLL': ['STD-VAE NLL', 'Discrete-VAE NLL']})\n    for epoch in range(1, num_epochs + 1):\n        avg_KL_STD_VAE, avg_NLL_STD_VAE = 0, 0\n        avg_KL_DIS_VAE, avg_NLL_DIS_VAE = 0, 0\n        for (x, label) in data_loader:\n            x = x.to(device)\n            # standard vae update\n            optimizer_std_vae.zero_grad()\n            loss, NLL, KL_Div  = std_vae.compute_loss(x)\n            loss.backward()\n            optimizer_std_vae.step()\n            avg_KL_STD_VAE += KL_Div.item() / len(data_loader)\n            avg_NLL_STD_VAE += NLL.item() / len(data_loader)\n            # discrete vae update\n            optimizer_dis_vae.zero_grad()\n            loss, NLL, KL_Div  = discrete_vae.compute_loss(x)\n            loss.backward()\n            optimizer_dis_vae.step()\n            avg_KL_DIS_VAE += KL_Div.item() / len(data_loader)\n            avg_NLL_DIS_VAE += NLL.item() / len(data_loader)\n\n        # plot current losses\n        losses_plot.update({'STD-VAE KL': avg_KL_STD_VAE, 'STD-VAE NLL': avg_NLL_STD_VAE,\n                            'Discrete-VAE KL': avg_KL_DIS_VAE,\n                            'Discrete-VAE NLL': avg_NLL_DIS_VAE}, current_step=epoch)\n        losses_plot.send()\n    trained_std_vae, trained_discrete_vae = std_vae, discrete_vae\n    return trained_std_vae, trained_discrete_vae\n```\n:::\n\n\n### Results\n\nLet's train both models for some seconds:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nnum_epochs = 15\nstd_vae = VAE()\ndiscrete_vae = DiscreteVAE()\n\ntrained_std_vae, trained_discrete_vae = train(dataset, std_vae, discrete_vae, num_epochs)\n```\n:::\n\n\n![Training](./img/training.png \"Training\")\n\nBoth models seem to be able to create descent reconstructions (really low NLL).\nFrom here on out, we will only run the **discrete-latent VAE** in test-mode,\ni.e., with a categorical latent distribution.\n\n### Visualizations\n\n* **Reconstructions**: Let's verify that both models are able to create good\n  reconstructions.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ndef plot_reconstructions(std_vae, discrete_vae, dataset, SEED=1):\n    np.random.seed(SEED)\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n    std_vae.to(device)\n    discrete_vae.to(device)\n\n    n_samples = 7\n    i_samples = np.random.choice(range(len(dataset)), n_samples, replace=False)\n\n    fig = plt.figure(figsize=(10, 4))\n    plt.suptitle(\"Reconstructions\", fontsize=16, y=1, fontweight='bold')\n    for counter, i_sample in enumerate(i_samples):\n        orig_img = dataset[i_sample][0]\n        # plot original img\n        ax = plt.subplot(3, n_samples, 1 + counter)\n        plt.imshow(transforms.ToPILImage()(orig_img))\n        plt.axis('off')\n        if counter == 0:\n            ax.annotate(\"input\", xy=(-0.1, 0.5), xycoords=\"axes fraction\",\n                        va=\"center\", ha=\"right\", fontsize=12)\n        # plot img reconstruction STD VAE\n        [x_tilde, z, mu_z, log_var_z] = std_vae(orig_img.unsqueeze(0).to(device))\n\n        ax = plt.subplot(3, n_samples, 1 + counter + n_samples)\n        x_tilde = x_tilde[0].detach().cpu()\n        plt.imshow(transforms.ToPILImage()(x_tilde))\n        plt.axis('off')\n        if counter == 0:\n            ax.annotate(\"STD VAE recons\", xy=(-0.1, 0.5), xycoords=\"axes fraction\",\n                        va=\"center\", ha=\"right\", fontsize=12)\n        # plot img reconstruction IWAE\n        [x_tilde, z, dist] = discrete_vae(orig_img.unsqueeze(0).to(device), \"Test\")\n        ax = plt.subplot(3, n_samples, 1 + counter + 2*n_samples)\n        x_tilde = x_tilde[0].detach().cpu()\n        plt.imshow(transforms.ToPILImage()(x_tilde))\n        plt.axis('off')\n        if counter == 0:\n            ax.annotate(\"Discrete VAE recons\", xy=(-0.1, 0.5), xycoords=\"axes fraction\",\n                        va=\"center\", ha=\"right\", fontsize=12)\n    return\n\n\nplot_reconstructions(trained_std_vae, trained_discrete_vae, dataset)\n```\n:::\n\n\n  ![Reconstructions](./img/recons.png \"Reconstructions\")\n\n\n   Interestingly, the **standard VAE** does not always create valid\n   reconstructions. This is due to the sampling from a Gaussian in the latent\n   space, i.e., the decoder might see some $\\textbf{z}$ it has not yet seen and\n   then creates some weird reconstruction.\n\n* **Latent Traversal**: Let's traverse the latent dimension to see what the\n  model has learnt. Note that for the **standard VAE** the latent space is\n  continuous and therefore infinitely many latent sample exist. As usual, we\n  will only show an limited amount by pertubating each latent dimension between\n  -1 and +1 (while holding the other dimensions constant).\n\n  For the **discrete-latent VAE**, there are only $K$ possible latent states.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ndef plot_latent_traversal(std_vae, discrete_vae, dataset, SEED=1):    \n    np.random.seed(SEED)\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    std_vae.to(device)\n    discrete_vae.to(device)\n    \n    n_samples = 1\n    i_samples = np.random.choice(range(len(dataset)), n_samples, replace=False)\n    img_batch = torch.cat([dataset[i][0].unsqueeze(0) for i in i_samples], 0)\n    img_batch = img_batch.to(device)\n    # generate latent traversals\n    n_pert, pert_min_max, n_lats = 5, 1, 3\n    img_trav_vae = std_vae.create_latent_traversal(img_batch, n_pert, pert_min_max, n_lats)\n    img_discrete_vae, latent_samples = discrete_vae.create_latent_traversal()\n    \n    fig = plt.figure(figsize=(12, 5))\n    n_rows, n_cols = n_lats + 1, 2*n_pert + 1\n    gs = GridSpec(n_rows, n_cols)\n    plt.suptitle(\"Latent Traversals\", fontsize=16, y=1, fontweight='bold')\n    for row_index in range(n_lats):\n        for col_index in range(n_pert):\n            img_rec_VAE = img_trav_vae[row_index][col_index]\n            \n            ax = plt.subplot(gs[row_index, col_index])\n            plt.imshow(transforms.ToPILImage()(img_rec_VAE))\n            plt.axis('off')\n            \n            if row_index == 0 and col_index == int(n_pert//2):\n                plt.title('STD VAE', fontsize=14, y=1.1)\n            \n            ax = plt.subplot(gs[row_index, col_index + n_pert + 1])\n            if col_index == 2:\n                plt.imshow(transforms.ToPILImage()(img_discrete_vae[row_index]))\n            if col_index == 3:\n                d = latent_samples[row_index].type(torch.uint8).tolist()\n                ax.annotate(f\"d = {d}\", xy=(0.5, 0.5))\n                \n            \n            plt.axis('off')\n            if row_index == 0 and col_index == int(n_pert//2):\n                plt.title('Discrete VAE', fontsize=14, y=1.1)\n            \n            \n    # add pertubation magnitude\n    for ax in [plt.subplot(gs[n_lats, 0:5])]:\n        ax.annotate(\"pertubation magnitude\", xy=(0.5, 0.6), xycoords=\"axes fraction\",\n                    va=\"center\", ha=\"center\", fontsize=10)\n        ax.set_frame_on(False)\n        ax.axes.set_xlim([-1.15 * pert_min_max, 1.15 * pert_min_max])\n        ax.xaxis.set_ticks([-pert_min_max, 0, pert_min_max])\n        ax.xaxis.set_ticks_position(\"top\")\n        ax.xaxis.set_tick_params(direction=\"inout\", pad=-16)\n        ax.get_yaxis().set_ticks([])\n    return\n\nplot_latent_traversal(trained_std_vae, trained_discrete_vae, dataset)\n```\n:::\n\n\n  ![Latent Traversal](./img/latent_trav.png \"Latent Traversal\")\n\n  Well, this looks nice for the **discrete VAE** and really confusing for the\n  **Standard VAE**.\n\n\n## Acknowledgements\n\nThe lecture on [discrete latent\nvariables](https://www.youtube.com/watch?v=-KzvHc16HlM) by Artem Sobolev as well\nas the [NIPS presentation](https://www.youtube.com/watch?v=JFgXEbgcT7g) by Eric\nJang were really helpful resources.\n\n-----------------------------------------------------------------------------\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}