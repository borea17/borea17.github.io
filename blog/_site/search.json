[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "mysite",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "paper_summaries.html",
    "href": "paper_summaries.html",
    "title": "Paper Summaries",
    "section": "",
    "text": "Spatial Transformer Networks\n\n\n\n\n\n\n\nreimplementation\n\n\n \n\n\n \n\n\n\n\n  \n\n\n\n\nU-Net: Convolutional Networks for Biomedical Image Segmentation\n\n\n\n\n\n\n\nreimplementation\n\n\nCNN\n\n\nimage segmentation\n\n\n \n\n\n \n\n\n\n\n  \n\n\n\n\nSpatial Broadcast Decoder: A Simple Architecture for Learning Disentangled Representations in VAEs\n\n\n\n\n\n\n\nreimplementation\n\n\nVAE\n\n\ndisentanglement\n\n\n \n\n\n \n\n\n\n\n  \n\n\n\n\nAuto-Encoding Variational Bayes\n\n\n\n\n\n\n\nreimplementation\n\n\nVAE\n\n\n \n\n\n \n\n\n\n\n  \n\n\n\n\nSchema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics\n\n\n\n\n\n\n\nreinforcement learning\n\n\n \n\n\n \n\n\n\n\n  \n\n\n\n\nRelational Inductive Biases, Deep Learning, and Graph Networks\n\n\n\n\n\n\n\ninteraction network\n\n\ngraph networks\n\n\ngeneralization\n\n\n \n\n\n \n\n\n\n\n  \n\n\n\n\nInteraction Networks for Learning about Objects, Relations and Physics\n\n\n\n\n\n\n\ngraph networks\n\n\n \n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "paper_summaries/2020-06-26-IN/index.html",
    "href": "paper_summaries/2020-06-26-IN/index.html",
    "title": "Interaction Networks for Learning about Objects, Relations and Physics",
    "section": "",
    "text": "Battaglia et al. (2016) introduce the Interaction Network (IN) as the first general-purpose learnable physics engine capable of zero-shot generalization in terms of varying configurations of objects and relations. The IN leverages object- and relation-based reasoning by defining a message passing scheme on a graph-structured representation of objects as nodes and relations as edges. As a proof of concept, they show that their model successfully learned to predict physical trajectories in gravitational systems, bouncing ball domains and mass string systems, and that it could also learn to estimate abstract properties such as the potential energy. Although its formulation is based on dynamical physical systems, it might also be applicable to other domains that can be abstracted into a graph-structured representation of objects and relations such as model-based reinforcment learning."
  },
  {
    "objectID": "paper_summaries/2020-06-26-IN/index.html#model-description",
    "href": "paper_summaries/2020-06-26-IN/index.html#model-description",
    "title": "Interaction Networks for Learning about Objects, Relations and Physics",
    "section": "Model Description",
    "text": "Model Description\nIn essence, the IN model can be understood as a graph-based simulator (i.e., state is represented as a graph) that predicts a future state (i.e., altered graph) using a message-passing scheme. Battaglia et al. (2016) used a handcrafted scene encoder/decoder to convert the physical scene into the corresponding graph structure and vice versa.\nDefinition: Let \\(G=\\langle O, R \\rangle\\) be an attributed, directed multigraph in which the set of nodes \\(O=\\\\{o_j\\\\}_{j=1 \\dots N_O}\\) represents objects and the set of edges \\(R = \\\\{ \\langle i, j , r_k \\rangle_k \\\\}\\_{1 \\dots N_R}\\) represents the relations between the objects, i.e., the triplet \\(\\langle i,j, r_k \\rangle_k\\) defines the \\(k^{\\text{th}}\\) relation from sender \\(o_i\\) to receiver \\(o_j\\) with relation attribute \\(r_k\\). Each object \\(o_i\\) may have several attributes1, an object state \\(o_i^{(t)}\\) at time \\(t\\) can be understood as a value assignment to all of its attributes. Additionally, let \\(X=\\\\{ x_j \\\\}\\_{1 \\dots N_O}\\) denote external effects (e.g., active control or gravitation) which are applied to each object separately."
  },
  {
    "objectID": "paper_summaries/2020-06-26-IN/index.html#footnotes",
    "href": "paper_summaries/2020-06-26-IN/index.html#footnotes",
    "title": "Interaction Networks for Learning about Objects, Relations and Physics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn their implementation, Battaglia et al. (2016) assume that all objects share the same attributes, i.e., are instances from the same class. 2: Schematic is taken from the original paper of Battaglia et al. (2016).↩︎"
  },
  {
    "objectID": "paper_summaries/2020-06-26-interaction_network/index.html",
    "href": "paper_summaries/2020-06-26-interaction_network/index.html",
    "title": "Interaction Networks for Learning about Objects, Relations and Physics",
    "section": "",
    "text": "Battaglia et al. (2016) introduce the Interaction Network (IN) as the first general-purpose learnable physics engine capable of zero-shot generalization in terms of varying configurations of objects and relations. The IN leverages object- and relation-based reasoning by defining a message passing scheme on a graph-structured representation of objects as nodes and relations as edges. As a proof of concept, they show that their model successfully learned to predict physical trajectories in gravitational systems, bouncing ball domains and mass string systems, and that it could also learn to estimate abstract properties such as the potential energy. Although its formulation is based on dynamical physical systems, it might also be applicable to other domains that can be abstracted into a graph-structured representation of objects and relations such as model-based reinforcment learning."
  },
  {
    "objectID": "paper_summaries/2020-06-26-interaction_network/index.html#model-description",
    "href": "paper_summaries/2020-06-26-interaction_network/index.html#model-description",
    "title": "Interaction Networks for Learning about Objects, Relations and Physics",
    "section": "Model Description",
    "text": "Model Description\nIn essence, the IN model can be understood as a graph-based simulator (i.e., state is represented as a graph) that predicts a future state (i.e., altered graph) using a message-passing scheme. Battaglia et al. (2016) used a handcrafted scene encoder/decoder to convert the physical scene into the corresponding graph structure and vice versa.\nDefinition: Let \\(G=\\langle O, R \\rangle\\) be an attributed, directed multigraph in which the set of nodes \\(O=\\\\{o_j\\\\}_{j=1 \\dots N_O}\\) represents objects and the set of edges \\(R = \\\\{ \\langle i, j , r_k \\rangle_k \\\\}\\_{1 \\dots N_R}\\) represents the relations between the objects, i.e., the triplet \\(\\langle i,j, r_k \\rangle_k\\) defines the \\(k^{\\text{th}}\\) relation from sender \\(o_i\\) to receiver \\(o_j\\) with relation attribute \\(r_k\\). Each object \\(o_i\\) may have several attributes1, an object state \\(o_i^{(t)}\\) at time \\(t\\) can be understood as a value assignment to all of its attributes. Additionally, let \\(X=\\\\{ x_j \\\\}\\_{1 \\dots N_O}\\) denote external effects (e.g., active control or gravitation) which are applied to each object separately."
  },
  {
    "objectID": "paper_summaries/2020-06-26-interaction_network/index.html#footnotes",
    "href": "paper_summaries/2020-06-26-interaction_network/index.html#footnotes",
    "title": "Interaction Networks for Learning about Objects, Relations and Physics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn their implementation, Battaglia et al. (2016) assume that all objects share the same attributes, i.e., are instances from the same class. 2: Schematic is taken from the original paper of Battaglia et al. (2016).↩︎"
  },
  {
    "objectID": "paper_summaries/interaction_network/index.html",
    "href": "paper_summaries/interaction_network/index.html",
    "title": "Interaction Networks for Learning about Objects, Relations and Physics",
    "section": "",
    "text": "Battaglia et al. (2016) introduce the Interaction Network (IN) as the first general-purpose learnable physics engine capable of zero-shot generalization in terms of varying configurations of objects and relations. The IN leverages object- and relation-based reasoning by defining a message passing scheme on a graph-structured representation of objects as nodes and relations as edges. As a proof of concept, they show that their model successfully learned to predict physical trajectories in gravitational systems, bouncing ball domains and mass string systems, and that it could also learn to estimate abstract properties such as the potential energy. Although its formulation is based on dynamical physical systems, it might also be applicable to other domains that can be abstracted into a graph-structured representation of objects and relations such as model-based reinforcment learning."
  },
  {
    "objectID": "paper_summaries/interaction_network/index.html#model-description",
    "href": "paper_summaries/interaction_network/index.html#model-description",
    "title": "Interaction Networks for Learning about Objects, Relations and Physics",
    "section": "Model Description",
    "text": "Model Description\nIn essence, the IN model can be understood as a graph-based simulator (i.e., state is represented as a graph) that predicts a future state (i.e., altered graph) using a message-passing scheme. Battaglia et al. (2016) used a handcrafted scene encoder/decoder to convert the physical scene into the corresponding graph structure and vice versa.\nDefinition: Let \\(G=\\langle O, R \\rangle\\) be an attributed, directed multigraph in which the set of nodes \\(O=\\\\{o_j\\\\}_{j=1 \\dots N_O}\\) represents objects and the set of edges \\(R = \\\\{ \\langle i, j , r_k \\rangle_k \\\\}\\_{1 \\dots N_R}\\) represents the relations between the objects, i.e., the triplet \\(\\langle i,j, r_k \\rangle_k\\) defines the \\(k^{\\text{th}}\\) relation from sender \\(o_i\\) to receiver \\(o_j\\) with relation attribute \\(r_k\\). Each object \\(o_i\\) may have several attributes1, an object state \\(o_i^{(t)}\\) at time \\(t\\) can be understood as a value assignment to all of its attributes. Additionally, let \\(X=\\\\{ x_j \\\\}\\_{1 \\dots N_O}\\) denote external effects (e.g., active control or gravitation) which are applied to each object separately.\nIntuition: The ultimate goal of the IN is to predict all future object states \\(o_i^{(t+1)}\\) based on the graph \\(G\\), the current external effects per object \\(x_i^{(t)}\\) and all current object states \\(o_i^{(t)}\\). A message passing scheme is defined to achieve this goal in which first effects resulting from interactions are computed (relational reasoning), then these effects (messages) together with the external effects are aggregated towards the objects, lastly the aggregated information is used to update the object states (object reasoning).\nFormally, the basic IN is defined as follows\n\\[\n\\begin{align}\n&\\text{IN}(G) = \\phi_O \\Bigg( a\\Big( G, X, \\phi_R \\big( m (G)\\big) \\Big)\\Bigg)\n\\end{align}\n\\]\n\\[\n\\begin{align}\n  \\begin{aligned}\n    & m(G) = B = \\{ b_k\\}_{k=1\\dots N_R} \\\\\n    & f_{R} (b_k) = e_k \\\\\n    & \\phi_{R} (B) = E = \\{e_k\\}_{k=1 \\dots N_R}\n  \\end{aligned}\n    &&\n       \\begin{aligned}\n         & a(G, X, E) = C = \\{c_j\\}_{j=1\\dots N_O} \\\\\n         & f_O (c_j) = p_j \\\\\n         & \\phi_O (C) = P = \\{p_j\\}_{j=1\\dots N_o}\n       \\end{aligned}\n\\end{align}\n\\]\nIn this definition \\(m\\) denotes the marshalling function which rearranges objects and relations into interaction terms \\(b_k= \\langle o_i, o_j, r_k \\rangle \\in B\\) on which the relational function \\(\\phi_R\\) can operate (element-wise by applying \\(f_R\\) on each interaction term) to predict the effects of each interaction \\(e_k\\in E\\). The aggregation function \\(a\\) builds a a set of of object model inputs \\(c_j \\in C\\) (one per object) by collecting and merging all incoming effects per object and combining the result with the object state \\(o_{j}^{(t)}\\) and the external effects for that object \\(x_j^{(t)}\\). Lastly, the object model \\(\\phi_O\\) predicts for all objects their result \\(p_j\\in P\\), i.e., future object states \\(o_{j}^{(t+1)}\\), by applying \\(f_O\\) to each \\(c_j\\). The figure below represents the described procedure of an (exemplary) IN.\n\n\n\n\n\n\n\n\nSchematic of the IN’s update procedure for an exemplary IN2:Firstly, the marshalling function \\(m\\) rearranges objects \\(o_i\\) based on the relations \\(r_j\\) into interaction terms \\(b_k = \\langle o_i, o_j, r_k \\rangle\\). Secondly, the function \\(f_R\\) is applied on each interaction term to compute the corresponding (directed) effects.Thirdly, the aggregation function \\(a\\) uses the graph structure to collect and merge the incoming effects, and to add the corresponding object state and external effects into a new object term \\(c_k = \\langle o_k, x_k, \\hat{e}_k \\rangle\\) (\\(\\hat{e}_k\\) denotes aggregated effect). Lastly, this representation is used to predict the results \\(p_k\\), i.e., future object states, by applying \\(f_O\\) to each \\(c_k\\).\n\n\n\nIntuition: Computing the trajectories of planets in a solar system may be a good example to motivate and understand the IN definition. Objects in the graph shall be the planets and relations the pairwise gravitational forces on each other, i.e., each object has an arrow pointing to all other objects. Object attributes could be the mass, acceleration, velocity and position. As external effects we could define the step size (necessary for approximate integration). Relational attributes are not needed then. The physics approach to compute the approximated trajacetories would be to first compute all gravitational forces per object which corresponds to computing the effects in the IN. Then, the net force would be computed as the sum of all forces per object, i.e., aggregation in the IN. Lastly, the object attributes would be updated (except mass) using the calculated net force, current object state and step size which corresponds to the object-centric update in the IN."
  },
  {
    "objectID": "paper_summaries/interaction_network/index.html#footnotes",
    "href": "paper_summaries/interaction_network/index.html#footnotes",
    "title": "Interaction Networks for Learning about Objects, Relations and Physics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn their implementation, Battaglia et al. (2016) assume that all objects share the same attributes, i.e., are instances from the same class. 2: Schematic is taken from the original paper of Battaglia et al. (2016).↩︎"
  },
  {
    "objectID": "paper_summaries/interaction_network/index.html#learning-the-model",
    "href": "paper_summaries/interaction_network/index.html#learning-the-model",
    "title": "Interaction Networks for Learning about Objects, Relations and Physics",
    "section": "Learning the Model",
    "text": "Learning the Model\nIn the IN defintion, there are no limitations whatsoever for the functions and how they operate over their inputs (e.g., objects itself could also be graphs). Thus, learning the underlying dynamics given the graph structured representation and the scene encoder/decoder (and assuming that there is some IN that can simulate the dynamics) without further assumptions remains an intractable quest. To overcome this problem, Battaglia et al. (2016) present a learnable implementation of the IN which uses deep neural networks as function approximators and a specific object and relation representation using matrices. Then, learning the model comes down to training from data using the standard deep learning framework.\nImplementation: Let each object \\(o_i^{(t)}\\in \\mathbb{R}^{D_s}\\) be represented by a \\(D_s\\)-dimensional vector where each entry corresponds to an attribute, i.e., attributes have a predefined order which is fix over all objects. Then, \\(O\\) is defined as a \\(D_S \\times N_O\\) matrix where each column represents an object. Similarly, \\(X\\) is defined as a \\(D_X \\times N_O\\) matrix where each \\(D_x\\)-dimensional column represents the external effects that correspond to the object defined in the same column of \\(O\\). Let each relation be formalized into a triple of three vectors \\(r_k = \\langle r_r, r_s, r_a \\rangle\\) where \\(r_a \\in \\mathbb{R}^{D_R}\\) represents the (ordered) relational attributes and \\(r_r, r_s \\in \\{0, 1\\}^{N_O}\\) are one-hot encodings of the receiver and sender object, respectively. Then, all relations can be represented by the triplet \\(R = \\langle R_r, R_s, R_a \\rangle\\) where the matrices \\(R_r, R_s \\in \\{0,1\\}^{N_O \\times N_R}\\) and \\(R_a \\in \\mathbb{R}^{D_R \\times N_R}\\) are generated by stacking the relations column-wise.\nIt follows that the interaction terms \\(b_k\\) can be vectorized by concatenation of the receiver and sender object attributes and the relational attributes into a \\((2 D_s + D_R)\\)-length vector. The marshalling function \\(m\\) can be stated as follows\n\\[\n\\begin{align}\n  m(G)\n  = \\begin{bmatrix}  O R_r \\\\ O R_s \\\\ R_a  \\end{bmatrix} = \\begin{bmatrix} b_1  &\n    \\dots & b_{N_R} \\end{bmatrix} = B.\n\\end{align}\n\\]\n\\(B\\) is the input to the relational model \\(\\phi_R\\) which is defined through the application of \\(f_R\\) on each column of \\(B\\) (each interaction term), i.e.,\n\\[\n\\begin{align}\n  \\phi_R (B) = \\begin{bmatrix} f_R \\big(b_1\\big) & \\dots & f_R \\big(b_{N_R}\\big) \\end{bmatrix}\n             = \\begin{bmatrix} e_1 & \\dots e_{N_R}\\end{bmatrix}  = E.\n\\end{align}\n\\]\n\\(f_R\\) shall be approximated by a neural network to estimate a \\(D_E\\)-length vector \\(e_k\\) that encodes the resulting effect. Similar to the marshalling function, the aggregation function \\(a\\) constructs vectorized object terms \\(c_k = \\begin{bmatrix} o_k^{(t)} & x_k^{(t)} & \\hat{e}_k^{(t)} \\end{bmatrix}^{\\text{T}}\\) by concatenation of the object attributes, the external effects and the aggregated effect (summation of all incoming effects per object):\n\\[\n\\begin{align}\n  a(O, R, X, E) = \\begin{bmatrix} O & X & E R_r^{\\text{T}}\n  \\end{bmatrix}^{\\text{T}} =\n  \\begin{bmatrix} c_1 & \\dots & c_{N_O} \\end{bmatrix} = C.\n\\end{align}\n\\]\nLastly, \\(C\\) is used as the input to the object model \\(\\phi_O\\) which is defined through the application of \\(f_O\\) on each column of \\(C\\), i.e.,\n\\[\n  \\phi_O (C) =\n  \\begin{bmatrix} f_O \\big( c_1 \\big) & \\dots & f_O \\big( c_{N_O} \\big) \\end{bmatrix}=\n                                                \\begin{bmatrix} o_1^{(t+1)} & \\dots & o_{N_O}^{(t+1)} \\end{bmatrix}.\n\\]\nThe result can be used to update the graph structured representation. The figure below summarizes the implementation of the IN.\n\n\n\n\n\n\n\n\nOne step roll out of the IN implementation. The physical scene is encoded (decoded) into (from) a graph structured representation using a handcrafted scene encoder (decoder). Battaglia et al. (2016) present a learnable implementation by using neural networks (blue boxes) as function approximators for the relational model (\\(\\phi_R\\)) and the object model (\\(\\phi_O\\))."
  },
  {
    "objectID": "paper_summaries/interaction_network/index.html#drawbacks-of-paper",
    "href": "paper_summaries/interaction_network/index.html#drawbacks-of-paper",
    "title": "Interaction Networks for Learning about Objects, Relations and Physics",
    "section": "Drawbacks of Paper",
    "text": "Drawbacks of Paper\n\nhandcrafted scene encoder/decoder \\(\\Rightarrow\\) not end-to-end\nobject states could blow up since all objects share the same attributes"
  },
  {
    "objectID": "paper_summaries/graph_networks/index.html",
    "href": "paper_summaries/graph_networks/index.html",
    "title": "Relational Inductive Biases, Deep Learning, and Graph Networks",
    "section": "",
    "text": "Few years after the IN paper, Battaglia et al. (2018) showed that the IN can be cast into a special case of a broader framework, termed Graph Networks (GNs). They hypothesize that despite the recent successes in deep learning with minimal representational biases, key ingredients of human-like intelligence such as combinatorial generalization1 remain out of reach. Arguing that solving this challenge should be a top priority of current research, they recommend2 the use of integrative approaches that build strong relational inductive biases3 into deep learning architectures. By presenting and formalizing a general GN framework for entity- and relation-based reasoning, they conclude that this framework could be a stepping stone towards combinatorial generalization."
  },
  {
    "objectID": "paper_summaries/graph_networks/index.html#model-description",
    "href": "paper_summaries/graph_networks/index.html#model-description",
    "title": "Relational Inductive Biases, Deep Learning, and Graph Networks",
    "section": "Model Description",
    "text": "Model Description\nSimiliar to the IN model, the GN block can be understood as a graph-to-graph module using a message passing scheme. In contrast to the IN, the GN block includes global attributes instead of external effects and uses these global attributes to update the edge attributes and node attributes instead of only updating the node attributes. Accordingly, the message passing scheme is slightly more complex.\nDefinition: Let \\(G=\\langle \\textbf{u}, V, E \\rangle\\) be an attributed, directed multigraph in which the global attribute (vector) \\(\\textbf{u}\\) represents system-level properties (e.g., gravitational field), the set of nodes \\(V=\\\\{\\textbf{v}_j\\\\}\\_{j=1 \\dots N_V}\\) represents entities4 and the set of edges \\(E = \\\\{ \\langle \\textbf{e}_k, r_k, s_k \\rangle \\\\}\\_{k=1\\dots N_E}\\) represents the attributed relations, i.e., the triplet \\(\\langle \\textbf{e}_k, r_k, s_k \\rangle\\) defines the \\(k^{\\text{th}}\\) relation from sender \\(o\\_{s_k}\\) to receiver \\(o\\_{r_k}\\) with relation attribute(s) \\(\\textbf{e}_k\\).\nFormally, the (full) GN block is defined as follows\n\\[\n\\begin{align}\n  \\begin{split}\n    \\text{GN}(G) &= \\text{GN} (\\langle \\textbf{u}, V, E \\rangle) = G^\\prime\\\\\n               &= \\left\\langle\n                 \\underbrace{\\phi^{u} \\Big(\\textbf{u}, \\rho^{v\\rightarrow u} \\big(V^\\prime\\big), \\rho^{e\\rightarrow u}\\big(E^{\\prime} \\big) \\Big)}_{\\textbf{u}^\\prime},\n   \\underbrace{\\phi^{v} \\Big(\\textbf{u}, V, \\rho^{e\\rightarrow v}\\big(E^\\prime \\big)\\Big)}_{V^\\prime},\n   \\underbrace{\\phi^{e} \\Big(\\textbf{u}, V, E\\Big)}_{E^\\prime} \\right\\rangle,\n   \\end{split}\n\\end{align}\n\\]\nwhere the updates within the graph triple \\(G=\\langle \\textbf{u}, V, E \\rangle\\) occur from right to left. More specifically, \\(\\phi^e\\) updates the edge attributes of all edges to compute the updated edge set \\(E^\\prime\\) as follows\n\\[\n\\begin{align}\n  E^\\prime  = \\phi^{e} (\\textbf{u}, \\textbf{V}, E)\n  = \\left\\{ f^e \\big(\\textbf{e}_1, \\textbf{v}_{r_1}, \\textbf{v}_{s_1}, \\textbf{u}\\big), \\dots,\n    f^{e} \\big(\\textbf{e}_{N_E}, \\textbf{v}_{r_{N_E}}, \\textbf{v}_{s_{N_E}}, \\textbf{u}\\big)\\right\\}.\n\\end{align}\n\\]\nThe updated edge set \\(E^\\prime\\) is used to compute the aggregated updated edge attributes per node \\(\\overline{\\textbf{e}}_i\\) using the aggregation function \\(\\rho^{e\\rightarrow v}\\), i.e.,\n\\[\n\\begin{align}\n  \\forall i \\in \\{1, \\dots, N_V\\}: \\overline{\\textbf{e}}_i = \\rho^{e\\rightarrow v} (E^\\prime)\n  = \\rho^{e\\rightarrow v} \\Big( \\left\\{  \\big(\\textbf{e}_k^\\prime, r_k, s_k\\big)  \\right\\}_{r_k=i, k=1:N_E}\\Big).\n\\end{align}\n\\]\nThe results are used to compute the updated node set \\(V^\\prime\\) using \\(\\phi^v\\) as follows\n\\[\n\\begin{align}\n  V^\\prime = \\phi^v \\Big(\\textbf{u}, V, \\rho^{e\\rightarrow v} \\big( E^\\prime \\big) \\Big)\n  = \\{ f^v \\big(\\overline{\\textbf{e}}_1, \\textbf{v}_1, \\textbf{u}\\big), \\dots,\n  f^v\\big(\\overline{\\textbf{e}}_{N_V}, \\textbf{v}_{N_V}, \\textbf{u}\\big)\\}.\n\\end{align}\n\\]\nLastly, the global attribute is updated towards \\(\\textbf{u}^\\prime\\) by aggregating the edge and node attributes globally, and then applying \\(\\phi^u\\). The figure below summarizes the internal structure within a (full) GN block and shows how different variants such as the relation network (Raposo et al., 2017) can be identified within the GN framework.\n\n\n\n\n\n\n\n\n(a) The internal GN block structure in its broadest formulation is shown including three update and three aggregation functions. (b) The relation network by Raposo et al. (2017) can be identified as a special case of the broader GN framework which only uses the edge predictions to predict global attributes. Taken from Battaglia et al. (2018)\n\n\n\nThe GN block can be understood as a building block to compose complex multi-block architectures, e.g., by stacking GN blocks similar to stacking layers in MLPs or reusing a GN block in a recurrent fashion. Additionally, the features inside the GN such as node attributes can be input to a standard MLP to infer abstract properties such as the potential energy (which was done in the IN paper (Battaglia et al., 2016))."
  },
  {
    "objectID": "paper_summaries/graph_networks/index.html#footnotes",
    "href": "paper_summaries/graph_networks/index.html#footnotes",
    "title": "Relational Inductive Biases, Deep Learning, and Graph Networks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBattaglia et al. (2018) define the principle of combinatorial generalization as the ability of constructing new inferences, predictions, and behaviors from known building blocks.↩︎\nThe paper was written by a large group of 27 researchers from DeepMind, GoogleBrain, MIT and University of Edinburgh. As directly stated in the abstract, it is part position paper, part review, and part unification.↩︎\nBattaglia et al. (2018) use the term relational inductive bias to refer generally to inductive biases which impose constraints on relationships and interactions among entities in a learning process. They motivate the use of relational inductive biases by human cognition which also uses (yet-to-understand) mechanisms for representing structure (e.g., world is understood as composition of objects) and relations (e.g., distance between objects).↩︎\nBattaglia et al. (2018) define an entity as an element with attributes. Thus, the term entity is more general than object capturing objects, parts of objects or any other attributed structure.↩︎"
  },
  {
    "objectID": "paper_summaries/schema_networks/index.html",
    "href": "paper_summaries/schema_networks/index.html",
    "title": "Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics",
    "section": "",
    "text": "Kansky et al. (2017) showed remarkable results of zero-shot transfer in several variations of Breakout by introducing Schema Networks as a generative model for object-oriented reinforcement learning and planning. This model incorporates objects as entities, represents local cause-effect relationships including one or more entities and is based on Probabilistic Graphical Models (PGMs). Due to its foundation in PGMs, Schema Networks support flexible inference and search strategies for planning."
  },
  {
    "objectID": "paper_summaries/schema_networks/index.html#model-description",
    "href": "paper_summaries/schema_networks/index.html#model-description",
    "title": "Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics",
    "section": "Model Description",
    "text": "Model Description\nBuilding upon the ideas of object-oriented Markov decision processes (OO-MDPs), states are represented as a list of entities where each entity can be understood as a different instantiation from the same class (i.e., all entities share the same attributes). Additionally, the attributes, actions and rewards are binarized using discretization and one-hot encoding, see image below. This representation comes from a handcrafted image parser with the handwavy argument that in practice a vision module could be responsible for this task.\n\n\n\n\n\n\n\n\n\n\n\nExemplary State Representation in a Schema Network. A handcrafted image parser converts an image (left) into the state representation (right), where filled green circles indicate that the binary variable is set to True.\n\n\n\nKansky et al. (2017) define 53 attributes for each entity in the Breakout domain (21 for bricks, 30 for the paddle, 1 for walls, 1 for the ball). However, they do not elaborate on what these attributes describe exactly. Furthermore, each pixel is identified as a part of an object and assigned the corresponding attributes. Accordingly, their representation could rather be understood as a 53-channel image where each entry can either be 0 or 1, e.g., one layer showing the walls. In this form, the entity-based state representation can also be provided to other algorithms such as A3C.\nSimiliar to OO-MDPs, state transitions are determined by a change of entity-attributes. However, due to the specific representation in Schema Networks, entity-attributes can only be active or inactive (with an associated probability). An attribute becomes activated if a grounded schema1 is active. Grounded schemas can include a variable size of entity attributes from a variable number of entities and may include one or more actions. Thus, these schemas can be interpreted as local cause-effect relationships. Formally, a grounded schema \\(\\phi^{k}\\) is a binary variable that becomes activated via a probabilistic AND over the binary variables \\(v_1, \\dots, v_n\\) that are included in it:\n\\[\n\\begin{align}\n  \\phi^{k} = \\text{AND} (v_1, \\dots, v_n) = \\prod_{i=1}^n P(v_i = 1).\n\\end{align}\n\\]\nThe binary variables \\(v_1, \\dots, v_n\\) may be entity-attributes2 or actions, see image below.\nMultiple grounded schemas can predict the same attribute which is formalized through an OR factor, e.g., let \\(\\alpha_{i, j}^{(t+1)}\\) denote the \\(j^{th}\\) attribute of the \\(i^{th}\\) entity at time \\(t+1\\) and assume there are \\(n\\) grounded schemas that predict this entity attribute. Then, this formalizes into\n\\[\n\\begin{align}\n  \\alpha_{i,j}^{(t+1)} = \\text{OR} (\\phi_{i,j}^{1}, \\dots, \\phi_{i, j}^{n}) = 1 - \\prod_{k=1}^n  \\big(1 - P(\\phi_{i,j}^k)\\big).\n\\end{align}\n\\]\nKansky et al. (2017) divide entity attributes into two classes: * Positional Attributes: These attributes correspond to discrete positions. * Non-Positional Attributes: The semantic meaning of those attributes is unknown to the model such that they may encode completely different things, e.g., color and shape.\nA self-transition variable is introduced for positional attributes which represents the probability that a position attribute will remain active in the next time step when no schema predicts a change from that position. Note that through this mechanism, they include the bias that an object cannot be at multiple positions at the same time.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransition dynamics in Schema Networks are governed by changes in entity-attributes due to activated grounded schemas. In this example all relevant gates are shown to illustrate the state transition dynamics via this mechanics. Note that there are two schemas that predict the same behavior, i.e., only one input in OR is necessary to activate \\(y=1\\).\n\n\n\nFormally, a self transition is a NOR factor over the grounded schemas that predict a change from a position attribute (i.e., the grounded schemas that predict towards a different position) combined with an AND factor over the NOR factor and the position attribute at the current time step. E.g., let \\(\\alpha_{i,j}^{t}\\) denote the \\(j^{th}\\) position attribute of the the \\(i^{th}\\) entity at time \\(t\\) and assume that the set \\(\\{\\phi^1, \\dots, \\phi^{n} \\}\\) includes all schemas predicting towards a different position of that entity. Then, the self-transition is formalized as follows\n\\[\n\\begin{align}\n  \\Lambda_{i,j}^{t+1}\n  = \\text{AND} \\big(\\lnot \\phi^{1}, \\dots, \\lnot \\phi^{n}, \\alpha_{i,j}^{t} \\big).\n\\end{align}\n\\]\nFinally, the transition function in this model can be factorized as\n\\[\n\\begin{align}\n  T\\left(s^{(t+1)} | s^{(t)}, a^{(t)}\\right) = \\prod_{i=1}^N \\prod_{j=1}^M T_{i, j} \\left(s_{i, j}^{(t+1)}|s^{(t)}, a^{(t)}\\right),\n\\end{align}\n\\]\nwhere \\(T_{i,j}\\) denotes the transition probability of the \\(j^{th}\\) attribute of the \\(i^{th}\\) entity towards its value defined in \\(s_{i,j}^{(t+1)}\\). The entity attribute \\(s_{i,j}^{(t+1)}\\) is by definition activated if one of its grounded schema is active or if a self-transition occured, thus the entity-attribute transition probability is defined as\n\\[\n\\begin{align}\n  T_{i,j} \\left( s_{i,j}^{(t+1)} | s^{(t)}, a^{(t)}\\right) = \\text{OR}\\left( \\phi^{k_1}, \\dots, \\phi^{k_Q}, \\Lambda_{i,j} \\right),\n\\end{align}\n\\]\nwhere \\(\\Lambda_{i,j}\\) denotes the self-transition variable of \\(s_{i,j}\\) and \\(k_1, \\dots, k_Q\\) are the indices of all grounded schemas that predict \\(s_{i,j}\\). Note that although all variables are defined as binary variables, this model could still be used for non-deterministic environments.\nTo increase the generality of their model such that the attribute change of two entities is described by the same schema, Kansky et al. (2017) introduce the term ungrounded schema or template. An ungrounded schema can be understood as template for specific grounded schemas, i.e., it describes a grounded schema where the included entity-attributes are assigned relative to the position of the entity-attribute that should be predicted."
  },
  {
    "objectID": "paper_summaries/schema_networks/index.html#learning-the-model",
    "href": "paper_summaries/schema_networks/index.html#learning-the-model",
    "title": "Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics",
    "section": "Learning the Model",
    "text": "Learning the Model\nThe Schema Network is essentially a factor graph that is aimed to be a probabilistic simulator of the game environment using the aforementioned representation. Assuming that the environment dynamics can be represented by some Schema Network, learning the model comes down to structure learning in graphical models. Kansky et al. (2017) preprocess a sequence of observations (dataset of state-action-reward-new state tuples over time) into a convenient representation, define a NP-hard optimization problem based on this representation as the optimal solution and retrieve the schemas (and Schema Network) through solving the optimization problem approximately using linear programming (LP) relaxations.\n\nNotation\nLet \\(\\alpha\\_{i,j}^{(t)}\\) denote the \\(j^{th}\\) attribute of the \\(i^{th}\\) entity at time \\(t\\) and let \\(\\textbf{e}\\_i^{(t)} \\in \\\\{0, 1\\\\}^{M}\\) be an\n\\(M\\)-dimensional binary vector representing all entity-attributes values of the \\(i^{th}\\) entity at time \\(t\\), i.e., \\(\\textbf{e}\\_i^{(t)} = \\begin{bmatrix}\\alpha\\_{i,1}^{(t)} & \\dots & \\alpha\\_{i,M}^{(t)} \\end{bmatrix}^{\\text{T}}\\) where \\(M\\) denotes the number of entity-attributes.\nLet \\(\\boldsymbol{\\beta}\\_{i}^{(t)}\\in \\\\{0,1\\\\}^E\\) be a row vector representing the attribute values of the \\(i^{th}\\) entity and the entity-attributes of the \\(R-1\\) (fixed radius) spatial neighbors, i.e., \\(\\boldsymbol{\\beta}_{i}^{(t)} = \\begin{bmatrix} \\textbf{e}\\_{i}^{(t)} & \\textbf{e}\\_{i+1}^{(t)} & \\dots & \\textbf{e}\\_{R-1}^{(t)} \\end{bmatrix}\\) has length \\(E=M(R-1) + M = MR\\).\nSuppose there are \\(N\\) entities observed for \\(\\tau\\) timesteps. Then, let \\(\\textbf{X}\\in\\\\{0,1\\\\}^{D\\times E}\\) be a binary matrix where each row consists of a \\(\\boldsymbol{\\beta}\\_{i}^{(t)}\\) and there are \\(D=N\\cdot \\tau\\) rows (all entities and time steps). Similarly, let \\(\\textbf{y}\\in\\\\{0, 1\\\\}^D\\) be a binary vector where each entry refers to the future attribute value \\(\\alpha\\_{i,j}^{(t+1)}\\) corresonding to the a row of \\(\\textbf{X}\\) with entity \\(i\\) and time \\(t\\), i.e.,\n\\[\n\\begin{align*}\n  \\textbf{X} &=\n  \\begin{bmatrix}\n    \\begin{bmatrix} \\boldsymbol{\\beta}_{1}^{(1)} & \\dots & \\boldsymbol{\\beta}_{N}^{(1)} \\end{bmatrix}^{\\text{T}} \\\\\n    \\vdots\\\\\n    \\begin{bmatrix} \\boldsymbol{\\beta}_{1}^{(\\tau)} & \\dots & \\boldsymbol{\\beta}_{N}^{(\\tau)} \\end{bmatrix}^{\\text{T}}\n  \\end{bmatrix} , \\quad \\textbf{y} =\n   \\begin{bmatrix}\n\\begin{bmatrix} \\alpha_{1,j}^{(2)} & \\dots & \\alpha_{N,j}^{(2)} \\end{bmatrix}^{\\text{T}}\n\\\\ \\vdots \\\\\n\\begin{bmatrix} \\alpha_{1,j}^{(\\tau+1)} & \\dots & \\alpha_{N,j}^{(\\tau+1)} \\end{bmatrix}^{\\text{T}}\n    \\end{bmatrix}\n\\end{align*}\n\\]\n\n\nLearning Problem\nThe goal is to predict \\(\\alpha\\_{i,j}^{(t+1)}\\) based on the entity-attributes of itself and its spatial neighbors. Using the introduced notation, the learning problem can be defined as follows\n\\[\n  \\textbf{y} = f_{\\textbf{W}} (\\textbf{X}) = \\overline{\\overline{\\textbf{X}} \\textbf{W}} \\textbf{1},\n\\]\nwhere \\(f\\_{\\textbf{W}}\\) denotes the desired function of ungrounded schemas which is applied row-wise to the argument \\(\\textbf{X}\\) to produce either active or inactive grounded schemas. \\(f\\_{\\textbf{W}}\\) is parametrized by a binary matrix \\(\\textbf{W} \\in \\\\{0, 1\\\\}^{E \\times L}\\) with each column representing one ungrounded schema for a maximum of \\(L\\) schemas. Each element that is set to 1 in a column of \\(\\textbf{W}\\) indicates that for this schema the corresponding input attribute (from \\(\\textbf{X}\\)) is necessary for an activated grounded schema. On the right-hand side of the equation above all variables and operations follow Boolean logic: addition corresponds to ORing and overlining to negation.\n\nE.g., let \\(\\textbf{w}\\_i = \\begin{bmatrix} w\\_{i_1} & \\dots & w\\_{i_ E}\\end{bmatrix}^{\\text{T}}\\) denote the \\(i^{th}\\) column of \\(\\hspace{0.1cm}\\textbf{W}\\) and \\(\\textbf{x}_r = \\begin{bmatrix} x\\_{r_1} & \\dots & x\\_{r_E} \\end{bmatrix}\\) be the \\(r^{th}\\) row of \\(\\hspace{0.1cm}\\textbf{X}\\). Then, the (dot) product of the two vectors leads to\n\\[\n\\begin{align}\n\\textbf{x}_r \\textbf{w}_i = \\sum_{k=1}^{E} x_{r_k} \\cdot w_{i_k} = \\text{OR} \\left( x_{r_1} w_{i_1}, \\dots, x_{r_E} w_{i_E} \\right),\n\\end{align}\n\\]\nin this form the corresponding grounded schema would be activated as soon as one precondition is satisfied, i.e., as soon as for one \\(w\\_{i_j} = 1\\) the corresponding attribute variable is also \\(x\\_{i_j}=1\\).\n\nA grounded schema \\(\\phi\\) is defined through a logical AND over the necessary attributes (i.e., all preconditions)3 \n\\[\n\\begin{align}\n  \\phi = \\text{AND}\\left( \\{x_{r_j} \\mid \\forall j: w_{i_j} = 1 \\}  \\right) = \\text{Not} \\left(\n  \\text{OR} \\left(\\text{Not } \\{x_{r_j} \\mid \\forall j: w_{i_j} = 1 \\} \\right) \\right)\n  = \\overline{\\overline{\\textbf{x}}_r \\textbf{w}_i}.\n\\end{align}\n\\]\nThis equation states how one individual schema (\\(\\textbf{w}_{i}\\)) is applied to one attribute vector \\(\\textbf{x}_R\\). The first equation of this section summarizes this result into a matrix-matrix multiplication.\nAt the end all outputs of each individual schema are ORed to produce the final prediction for each attribute (corresponding to the provided attribute vector). This is done through multiplication with the identity tensor \\(\\textbf{1} \\in \\\\{1\\\\}^{L \\times D}\\). Remind that this is in alignment with the entity-attribute transition probability definition for non-positional attributes\n\\[\n\\begin{align}\n  T_{i,j} \\left( s_{i,j}^{(t+1)} | s^{(t)}, a^{(t)}\\right) = \\text{OR}\\left( \\phi^{k_1}, \\dots, \\phi^{k_Q}\\right)\n\\end{align}\n\\]\nAs stated above, for positional attributes a self-transition \\(\\Lambda_{i,j}\\) is added to allow these attributes to remain active when no change is predicted. Unfortunately, Kansky et al. (2017) did not elaborate on self-transitions in the learning problem. Thus, we can only guess how they are included. My idea would be to preprocess the data such that only positional attributes that changed (either from 0 to 1 or vice versa) are included in the learning problem. In the prediction phase, we then simply group all positional grounded schemas and apply the self-transition as a post-processing step.\n\n\nObjective Function\nAs there might be multiple Schemas that explain certain behaviors, the objective function is aimed to minimize the prediction error while keeping ungrounded schemas as simple as possible4:\n\\[\n\\begin{align}\n  \\min_{\\textbf{W}} J(\\textbf{W}) = \\min_{\\textbf{W}} \\underbrace{\\frac {1}{D} \\cdot \\Bigg| \\textbf{y} -\n  f_{\\textbf{W}} (\\textbf{X}) \\Bigg|_1 }_{\\text{Prediction Error}}+\n  \\underbrace{C \\cdot \\Bigg| \\textbf{W} \\Bigg|_1}_{\\text{Model Complexity}},\n\\end{align}\n\\]\nwhere \\(C\\) is a hyperparameter that can be used to control the trade-off between the complexity of the model and the accuracy of the predictions. This is a NP-hard optimization problem, since \\(\\textbf{W}\\in \\\\{0,1\\\\}^{E\\times L}\\) is a binary matrix. Furthermore, the search space is combinatorially large, i.e., there are \\(2^{E \\cdot L}\\) possible realizations of \\(\\textbf{W}\\). Hence, finding the optimal solution \\(\\textbf{W}^{*}\\) is infeasible (for larger environments such as Breakout).\n\n\nSchema Learning\nKansky et al. (2017) search for an approximate solution with the desired features (low prediction error and low model complexity) using a greedy algorithm of linear programming (LP) relaxations. This algorithm works as follows\n\nStart with an empty set of schemas, i.e., \\(\\textbf{W} = \\textbf{0}\\).\nGreedily select a schema \\(\\textbf{w}\\) that perfectly predicts a cluster of input samples:\n\nRandomly select an input sample \\(\\textbf{x}\\_n\\) for which \\(y_n = 1\\) and \\(f\\_{\\textbf{W}} (\\textbf{x}\\_n) = 0\\).\nPut sample in the set solved, then solve the following LP\n\\[\n\\begin{align}\n   \\begin{split}\n   &\\max_{\\textbf{w}\\in \\{0,1\\}^D} \\sum_{n: y_n = 1} \\overline{ \\overline{\\textbf{x}}_n \\textbf{w}} =\n   \\min_{\\textbf{w} \\in \\{0, 1\\}^{D}} \\sum (1 - {\\textbf{x}}_n \\textbf{w}) \\\\\n   &\\quad \\quad \\text{s.t. } \\forall_{n:y_n=0} \\quad (1 - \\textbf{x}_n) \\textbf{w} &gt; 1 \\qquad \\text{(no false alarms)}\\\\\n   &\\qquad \\quad \\hspace{0.3cm} \\forall_{n\\in \\text{solved}} \\hspace{0.2cm} (1-\\textbf{x}_n) \\textbf{w} = 0  \\qquad \\text{(active grounded schema)}\n   \\end{split}\n\\end{align}\n\\]\nUpdate the solved set, i.e., put all samples in for which \\((1-\\textbf{x}_n) \\textbf{w} = 0\\) in the solved set.\n\nSimplify resulting schema by making \\(\\textbf{w}\\) as sparse as possible while keeping the predictions correct without introducing false alarms:\n\\[\\begin{align}\n   \\begin{split}\n     &\\min_{\\textbf{w} \\in \\{0, 1\\}^D} \\textbf{w}^{\\text{T}} \\textbf{1} \\\\\n     &\\quad \\quad \\text{s.t. } \\forall_{n:y_n=0} \\quad (1 - \\textbf{x}_n) \\textbf{w} &gt; 1\\\\\n     &\\qquad \\quad \\hspace{0.3cm} \\forall_{n\\in \\text{solved}} \\hspace{0.2cm} (1-\\textbf{x}_n) \\textbf{w} = 0\n   \\end{split}\n\\end{align}\\]\nBinarize \\(\\textbf{w}\\)"
  },
  {
    "objectID": "paper_summaries/schema_networks/index.html#planning",
    "href": "paper_summaries/schema_networks/index.html#planning",
    "title": "Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics",
    "section": "Planning",
    "text": "Planning\nTODO: Summarize planning"
  },
  {
    "objectID": "paper_summaries/schema_networks/index.html#drawbacks",
    "href": "paper_summaries/schema_networks/index.html#drawbacks",
    "title": "Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics",
    "section": "Drawbacks",
    "text": "Drawbacks\n\nbased on specific and exact representation \\(\\Rightarrow\\) not end-to-end\nblow up of entity attributes, since:\n\nbinarized attributes using discretization and one-hot encoding\nall entities share the same set of attributes\n\nimage parser needs to know the whole attribute space beforehand\nNote: also the reward space needs to be defined beforehand.\nlearning algorithm is only capable of learning deterministic environments"
  },
  {
    "objectID": "paper_summaries/schema_networks/index.html#footnotes",
    "href": "paper_summaries/schema_networks/index.html#footnotes",
    "title": "Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA grounded schema in Schema Networks is similar to a rule in OO-MDP terms. Each attribute may have several grounded schemas. When one of those schemas is active (active effect condition in OO-MDP terms), the corresponding attribute is actived (set to True) in the next step.↩︎\nKansky et al. (2017) refer to the entity attributes in the binary variables \\(v_1, \\dots, v_n\\) of a grounded schema as entity-attribute preconditions. This terminology relates attributes to preconditions, since the actual condition (i.e., grounded schema) is only active iff all preconditions are active.↩︎\nIn Boolean logic, De Morgan’s law states that \\(\\text{AND} \\Big(A, B\\Big) =\\text{NOT} \\Big( \\text{OR} \\big(\\text{NOT } A, \\text{NOT } B \\big)\\Big)\\).↩︎\nOccam’s razor (law of parsimony) states that simplest solution is most likely the right one.↩︎"
  },
  {
    "objectID": "paper_summaries/auto-encoding_variational_bayes/index.html",
    "href": "paper_summaries/auto-encoding_variational_bayes/index.html",
    "title": "Auto-Encoding Variational Bayes",
    "section": "",
    "text": "Kingma and Welling (2013) introduced the Variational Auto-Encoder (VAE) to showcase how their Auto-Encoding Variational Bayes (AEVB) algorithm can be used in practice. Assuming i.i.d. datasets and continuous latent variables, the AEVB algorithm learns an approximate probabilistic encoder \\(q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x})\\) jointly with the probabilisitc decoder \\(p_{\\boldsymbol{\\theta}} (\\textbf{x}|\\textbf{z})\\) (where \\(\\boldsymbol{\\phi}, \\boldsymbol{\\theta}\\) parametrize the corresponding distributions) by learning the optimal model parameters \\(\\boldsymbol{\\phi}, \\boldsymbol{\\theta}\\) through optimizing an objective function with standard gradient ascent methods. In summary, a VAE is probabilistic autoencoder which uses variational inference to regularize the coding space. Furthermore, a VAE is a deep generative model as sampling from the coding space is possible, i.e., new observations can be generated."
  },
  {
    "objectID": "paper_summaries/auto-encoding_variational_bayes/index.html#model-description",
    "href": "paper_summaries/auto-encoding_variational_bayes/index.html#model-description",
    "title": "Auto-Encoding Variational Bayes",
    "section": "Model Description",
    "text": "Model Description\nThe AEVB algorithm basically assumes a generative process, introduces a variational approximation (see figure below) and optimizes the model parameters by maximizing an objective function. The objective function consists of the (reparametrized) variational lower bound of each datapoint. Reparametrization is necessary to allow the explicit formulation of gradients with respect to the model parameters.\n\n\n\n\n\n\n\n\n\n\n\nThe directed graphical models represent the assumed generative process (a) and the variational approximation of the intractable posterior (b) in the AEVB algorithm.\n\n\n\nObjective Function Derivation: Let \\(\\textbf{X}=\\{\\textbf{x}^{(i)}\\}_{i=1}^{N}\\) denote the dataset consisting of \\(N\\) i.i.d. samples and let \\(\\textbf{z}\\) denote the unobserved continuous random variable (i.e., hidden or code variable). Kingma and Welling (2013) assume that each observed sample \\(\\textbf{x}^{(i)}\\) comes from a generative process in which: Firstly, a hidden variable \\(\\textbf{z}^{(i)}\\) is generated from a prior distribution \\(p_{\\boldsymbol{\\theta}} (\\textbf{z})\\). Secondly, \\(\\textbf{x}^{(i)}\\) is generated from the conditional distribution \\(p_{\\boldsymbol{\\theta}}(\\textbf{x}|\\textbf{z}^{(i)})\\). Note that we do not know \\(\\boldsymbol{\\theta}\\) nor do we have information about \\(\\textbf{z}^{(i)}\\). In order to recover this generative process, they introduce \\(q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x})\\) as an approximation to the intractable true posterior1 \\(p_{\\boldsymbol{\\theta}} (\\textbf{z}|\\textbf{x})\\). The marginal log likelihood of each individual datapoint \\(\\textbf{x}^{(i)}\\) can then be stated as follows (see Eric Jang’s amazing blog post for detailed derivation)\n\\[\n  \\log p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}^{(i)}\\right) =\n  \\underbrace{D_{KL} \\left(q_{\\boldsymbol{\\phi}}\\left(\\textbf{z} | \\textbf{x}^{(i)}\\right)\n  || p_{\\boldsymbol{\\theta}} \\left(\\textbf{z}|\\textbf{x}^{(i)}\\right)\\right)}_{\\ge 0} +\n  \\mathcal{L} \\left( \\boldsymbol{\\theta}, \\boldsymbol{\\phi}, \\textbf{x}^{(i)}\\right) \\ge\n  \\mathcal{L} \\left( \\boldsymbol{\\theta}, \\boldsymbol{\\phi}, \\textbf{x}^{(i)}\\right),\n\\]\nwhere \\(D_{KL}(\\cdot)\\) denotes the KL divergence of the approximate from the true posterior (this quantity remains unknown since the true posterior \\(p_{\\boldsymbol{\\theta}} (\\textbf{z}|\\textbf{x}^{(i)})\\) is intractable). \\(\\mathcal{L} \\left(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}, \\textbf{x}^{(i)}\\right)\\) is called the variational lower bound or evidence lower bound (ELBO). The goal is to optimize \\(\\boldsymbol{\\phi}, \\boldsymbol{\\theta}\\) such that variational lower bound is maximized, thereby we indirectly maximize the marginal log likelihood. The variational lower bound can rewritten such that the objective function is obtained (also derived in Eric Jang’s blog post)\n\\[\n\\begin{align}\n  \\mathcal{L} \\left(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}, \\textbf{x}^{(i)}\\right) &=\n  \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x}^{(i)})}\n  \\left[ -\\log\n  q_{\\boldsymbol{\\phi}} (\\textbf{z} | \\textbf{x}^{(i)} ) +\n  \\log p_{\\boldsymbol{\\theta}} (\\textbf{z}) + \\log\n  p_{\\boldsymbol{\\theta}}\n  (\\textbf{x}^{(i)}|\\textbf{z}) \\right] \\\\\n  &= \\underbrace{-D_{KL} \\left(  q_{\\boldsymbol{\\phi}} \\left(\n  \\textbf{z} | \\textbf{x}^{(i)} \\right), p_{\\boldsymbol{\\theta}}\n  (\\textbf{z}) \\right)}_{\\text{Regularization Term}} +\n    \\underbrace{\n    \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x}^{(i)})}\n    \\left[ \\log p_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)}| \\textbf{z} \\right) \\right]}_{\\text{Reconstruction Accuracy}}\n    .\n\\end{align}\n\\]\nThe two terms have an associated interpretation in autoencoder language:\n\nReconstruction Accuracy (opposite of Reconstruction Error): The expectation can be interpreted using Monte Carlo integration,i.e.,\n\\[\n    \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x}^{(i)})}\n    \\left[ \\log p_{\\boldsymbol{\\theta}} \\left(\n    \\textbf{x}^{(i)} | \\textbf{z}\n    \\right) \\right] \\approx \\frac {1}{N} \\sum_{k=1}^{N} \\log p_{\\boldsymbol{\\theta}}\n    \\left( \\textbf{x}^{(i)} | \\textbf{z}^{(k)} \\right) \\qquad \\textbf{z}^{(k)} \\sim\n    q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x}^{(i)}),\n  \\]\nwhich results in an unbiased estimate. Sampling \\(\\textbf{z}^{(k)}\\sim q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x}^{(i)})\\) can be understood as encoding the observed input \\(\\textbf{x}^{(i)}\\) into a code \\(\\textbf{z}^{(k)}\\) using the probabilistic encoder \\(q_{\\boldsymbol{\\phi}}\\). Clearly, the expectation is maximized when the decoder \\(p_{\\boldsymbol{\\theta}}\\) maps the encoded input \\(\\textbf{z}^{(k)}\\) back the original input \\(\\textbf{x}^{(i)}\\), i.e., assigns high probability to \\(p_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)} | \\textbf{z}^{(i)} \\right)\\).\nRegularization Term: The KL divergence is non-negative and only zero if both distributions are identical. Thus, maximizing this term forces the encoder distribution \\(q_{\\boldsymbol{\\phi}}\\) to be close to the prior \\(p_{\\boldsymbol{\\theta}}(\\textbf{z})\\). In VAEs, the prior is typically set to be an isotropic normal distribution resulting in a regularized code space, i.e., encouraging a code space that is close to a normal distribution.\n\nReparametrization Trick: While the KL-divergence \\(D_{KL} \\left(  q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}^{(i)} \\right),  p_{\\boldsymbol{\\theta}} (\\textbf{z})\\right)\\) (i.e., the regularization term) can often be integrated analytically, the second term \\(\\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x}^{(i)})} \\left[ \\log  p_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)}| \\textbf{z} \\right) \\right]\\) (i.e., the reconstruction accuracy) requires sampling from \\(q_{\\boldsymbol{\\phi}}\\). There are two downsides associated wih sampling from \\(q_{\\boldsymbol{\\phi}}\\) approaches:\n\nBackpropagation does not work with a sampling operation, i.e., the implementation of VAEs would be more difficult.\nThe usual Monte Carlo gradient estimator (which relies on sampling from \\(q_{\\boldsymbol{\\phi}}\\)) w.r.t. \\(\\boldsymbol{\\phi}\\) exhibits very high variance.\n\nTo overcome these problems, Kingma and Welling (2013) use the reparametrization trick:\n\nSubstitute sampling \\(\\textbf{z} \\sim q_{\\boldsymbol{\\phi}}\\) by using a deterministic mapping \\(\\textbf{z} = g_{\\boldsymbol{\\phi}} (\\boldsymbol{\\epsilon}, \\textbf{x})\\) with the differential transformation \\(g_{\\boldsymbol{\\phi}}\\) of an auxiliary noise variable \\(\\boldsymbol{\\epsilon}\\) with \\(\\boldsymbol{\\epsilon}\\sim p(\\boldsymbol{\\epsilon})\\).\n\n As a result, the reparametrized objective function can be written as follows\n\\[\n  \\mathcal{L} \\left(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\textbf{x}^{(i)}\\right) =\n  -D_{KL} \\left(  q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}^{(i)} \\right),\n  p_{\\boldsymbol{\\theta}} (\\textbf{z}) \\right) +\n  \\mathbb{E}_{p(\\boldsymbol{\\epsilon})} \\left[ \\log\n  p_{\\boldsymbol{\\theta}}\n  \\left( \\textbf{x}^{(i)}| g_{\\boldsymbol{\\phi}} \\left(\n  \\boldsymbol{\\epsilon},\n  \\textbf{x}^{(i)} \\right) \\right) \\right]\n\\]\nin which the second term can be approximated with Monte Carlo integration yielding\n\\[\n  \\widetilde{\\mathcal{L}} \\left(\\boldsymbol{\\theta},\n  \\boldsymbol{\\phi};\n  \\textbf{x}^{(i)}\\right) =\n  -D_{KL} \\left(  q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}^{(i)} \\right),\n  p_{\\boldsymbol{\\theta}} (\\textbf{z}) \\right) +\n  \\frac {1}{L} \\sum_{l=1}^{L} \\log p_{\\boldsymbol{\\theta}}\\left(\n  \\textbf{x}^{(i)}| g_{\\boldsymbol{\\phi}}\n\\left( \\boldsymbol{\\epsilon}^{(i, l)}, \\textbf{x}^{(i)} \\right)\\right)\n\\]\nwith \\(\\boldsymbol{\\epsilon} \\sim p(\\boldsymbol{\\epsilon})\\). Note that Kingma and Welling denote this estimator as the second version of the Stochastic Gradient Variational Bayes (SGVB) estimator. Assuming that the KL-divergence can be integrated analytically, the derivatives \\(\\nabla_{\\boldsymbol{\\theta},\\boldsymbol{\\phi}} \\widetilde{L}\\) can be taken (see figure below), i.e., this estimator can be optimized using standard stochastic gradient methods.\n\n\n\n\n\n\n\n\n\n\n\nThe computation graphs summarize the difference between the computation of the reconstruction accuracy in the original objective (a) and the reparametrized objective (b). Circles indicate a sampling operation through which backpropagation is not allowed.\n\n\n\nTo increase stability and performance, Kingma and Welling introduce a minibatch estimator of the lower bound:\n\\[\n  \\widetilde{\\mathcal{L}}^{M} (\\boldsymbol{\\theta},\n  \\boldsymbol{\\phi}; \\textbf{X}^{M})  =  \\frac {N}{M}\n  \\sum_{i=1}^{M}\\widetilde{\\mathcal{L}} \\left(\n  \\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\textbf{x}^{(i)}\\right),\n\\]\nwhere \\(\\textbf{X}^{M} = \\left\\{ \\textbf{x}^{(i)} \\right\\}_{i=1}^{M}\\) denotes a minibatch of \\(M\\) datapoints from the full dataset \\(\\textbf{X}\\) of \\(N\\) datapoints."
  },
  {
    "objectID": "paper_summaries/auto-encoding_variational_bayes/index.html#learning-the-model",
    "href": "paper_summaries/auto-encoding_variational_bayes/index.html#learning-the-model",
    "title": "Auto-Encoding Variational Bayes",
    "section": "Learning the Model",
    "text": "Learning the Model\nLearning the probabilistic encoder \\(q_{\\boldsymbol{\\phi}}\\) and decoder \\(p_{\\boldsymbol{\\theta}}\\) comes down to learning the optimal model parameters \\(\\boldsymbol{\\phi}, \\boldsymbol{\\theta}\\) using the AEVB algorithm which can be summarized in 5 steps:\n\nInitialize model parameters \\(\\boldsymbol{\\phi}, \\boldsymbol{\\theta}\\) randomly.\nSample random minibatch \\(\\textbf{X}^{M} = \\left\\{ \\textbf{x}^{(i)} \\right\\}_{i=1}^{M}\\).\nCompute gradients \\(\\nabla_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}}  \\widetilde{\\mathcal{L}}^{M} \\left(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\textbf{X}^{M} \\right)\\).\nUpdate model parameters \\(\\boldsymbol{\\phi}, \\boldsymbol{\\theta}\\) by taking a gradient ascent step.\nRepeat steps 2-4 until model parameters converged"
  },
  {
    "objectID": "paper_summaries/auto-encoding_variational_bayes/index.html#vae-implementation",
    "href": "paper_summaries/auto-encoding_variational_bayes/index.html#vae-implementation",
    "title": "Auto-Encoding Variational Bayes",
    "section": "VAE Implementation",
    "text": "VAE Implementation\nA VAE simply uses deep neural networks (DNNs) as function approximators to parametrize the probabilistic encoder \\(q_{\\boldsymbol{\\phi}}\\) and decoder \\(p_{\\boldsymbol{\\theta}}\\). The optimal parameters \\(\\boldsymbol{\\phi}, \\boldsymbol{\\theta}\\) are learned jointly by training the VAE using the AEVB algorithm.\n\n\n\n\n\n\n\n\n\n\n\nSchematic of a standard VAE\n\n\n\nRegularization Term: Typically, the prior over the latent variables is set to be the centered isotropic Gaussian, i.e., \\(p_{\\boldsymbol{\\theta}} (\\textbf{z}) \\sim \\mathcal{N} (\\textbf{0}, \\textbf{I})\\). Note that this prior is needed to compute the regularization term in the objective function. Furthermore, it is commonly assumed that the true posterior \\(p_{\\boldsymbol{\\theta}}\\left(\\textbf{z} | \\textbf{x}^{(i)}\\right)\\) may be approximated by \\(q_{\\boldsymbol{\\phi}} \\left(\\textbf{z} | \\textbf{x}^{(i)} \\right) \\sim \\mathcal{N}\\left(\\boldsymbol{\\mu}_E^{(i)}, \\boldsymbol{\\sigma}_E^{2 (i)} \\textbf{I} \\right)\\) (subscripts denote that these parameters come from the encoder network). As a result, the regularization term can be integrated analytically leading to a term that only depends on \\(\\boldsymbol{\\mu}_E^{(i)}, \\boldsymbol{\\sigma}_E^{2 (i)}\\) (see Appendix B of Kingma and Welling)\n\\[\n-D_{KL} \\left(  q_{\\boldsymbol{\\phi}} \\left(\n  \\textbf{z} | \\textbf{x}^{(i)} \\right), p_{\\boldsymbol{\\theta}}\n  (\\textbf{z}) \\right) = \\frac {1}{2} \\sum_{j=1}^{J} \\left(\n  1 + \\log \\left( \\left( \\sigma_{E_j}^{(i)} \\right)^2 \\right)\n  - \\left( \\mu_{E_j}^{(i)}  \\right)^2 -  \\left( \\sigma_{E_j}^{(i)} \\right)^2\n  \\right),\n\\]\nwhere \\(J\\) denotes the latent space dimension.\nEncoder/Decoder Network: Kingma and Welling (2013) use simple neural networks with only one hidden layer to approximate the parameters of the probabilistic encoder and decoder. As stated above, the encoder network is fixed to compute the parameters \\(\\boldsymbol{\\mu}^{(i)}_E, \\boldsymbol{\\sigma}_E^{(i)} \\in \\mathbb{R}^{L}\\) of the Gaussian distribution \\(\\mathcal{N}\\left(\\boldsymbol{\\mu}_E^{(i)}, \\boldsymbol{\\sigma}_E^{2 (i)} \\textbf{I} \\right)\\). In fact, the encoder network takes a sample \\(\\textbf{x}^{(i)}\\) and outputs the mean \\(\\boldsymbol{\\mu}_E^{(i)}\\) and logarithmized variance, i.e.,\n\\[\n\\begin{bmatrix} \\boldsymbol{\\mu}_E^{(i)} & \\log\n\\boldsymbol{\\sigma}^{2(i)} \\end{bmatrix} = f_{\\boldsymbol{\\phi}} \\left( \\textbf{x}^{(i)} \\right).\n\\]\nNote that using the logarithmized version of the variance increases stability and simplifies the training2.\nIn principle, the encoder and decoder network are very similar only that the dimension of the input and output are reversed. While the encoder network is fixed to approximate a multivariate Gaussian with diagonal covariance structure, the decoder network can approximate a multivariate Gaussian (real-valued data) or Bernoulli (binary data) distribution.\nBelow is a simple Python class that can be used to instantiate the encoder or decoder network as described in appendix C of Kingma and Welling (2013).\n\n\nCode\nimport torch.nn as nn\nfrom collections import OrderedDict\n\n\nclass CoderNetwork(nn.Module):\n    r\"\"\"Encoder/Decoder for use in VAE based on Kingma and Welling\n    \n    Args:\n        input_dim: input dimension (int)\n        output_dim: output dimension (int)\n        hidden_dim: hidden layer dimension (int)\n        coder_type: encoder/decoder type can be \n                   'Gaussian'   - Gaussian with diagonal covariance structure\n                   'I-Gaussian' - Gaussian with identity as covariance matrix \n                   'Bernoulli'  - Bernoulli distribution       \n    \"\"\"\n    \n    def __init__(self, input_dim, hidden_dim, output_dim, coder_type='Gaussian'):\n        super().__init__()\n        \n        assert coder_type in  ['Gaussian', 'I-Gaussian' ,'Bernoulli'], \\\n            'unknown coder_type'\n        \n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.coder_type = coder_type\n        \n        self.coder = nn.Sequential(OrderedDict([\n            ('h', nn.Linear(input_dim, hidden_dim)),\n            ('ReLU', nn.ReLU()) # ReLU instead of Tanh proposed by K. and W.       \n        ]))\n        self.fc_mu = nn.Linear(hidden_dim, output_dim)\n        \n        if coder_type == 'Gaussian':\n            self.fc_log_var = nn.Linear(hidden_dim, output_dim)\n        elif coder_type == 'Bernoulli':\n            self.sigmoid_mu = nn.Sigmoid()\n        return\n    \n    def forward(self, inp):\n        out = self.coder(inp)\n        mu = self.fc_mu(out)\n        \n        if self.coder_type == 'Gaussian':\n            log_var = self.fc_log_var(out)\n            return [mu, log_var]\n        elif self.coder_type == 'I-Gaussian':\n            return mu\n        elif self.coder_type == 'Bernoulli':\n            return self.sigmoid_mu(mu)\n        return\n\n\nReconstruction Accuracy: Sampling from the encoder distribution is avoided by using the reparameterization trick, i.e., the latent variable \\(\\textbf{z}^{(i)}\\) is expressed as a deterministic variable\n\\[\n  \\textbf{z}^{(i, l)}=g_{\\boldsymbol{\\phi}} (\\textbf{x}^{(i)},\n  \\boldsymbol{\\epsilon}) = \\boldsymbol{\\mu}_E^{(i)} +\n  \\boldsymbol{\\sigma}_E^{(i)} \\odot \\boldsymbol{\\epsilon}^{(l)} \\quad\n  \\text{where}\n  \\quad \\boldsymbol{\\epsilon} \\sim\n  \\mathcal{N} (\\textbf{0}, \\textbf{I}),\n\\]\nand \\(\\odot\\) denotes element-wise multiplication.\nNote that we do not need to sample from the decoder distribution, since during training the reconstruction accuracy in the objective function only sums the log-likelihood of each sample \\(\\textbf{z}^{(i, l)}\\) and during test time we are mostly interested in the reconstructed \\(\\textbf{x}^{\\prime}\\) with highest probability, i.e., the mean.\nThe reconstruction accuracy in the reparametrized form is given by\n\\[\n\\text{Reconstruction Accuracy} =\n   \\frac {1}{L}\n\\sum_{l=1}^{L} \\log p_{\\boldsymbol{\\theta}}\\left(\n  \\textbf{x}^{(i)}| \\textbf{z}^{(i,l)}\\right),\n\\]\nwhere \\(L\\) denotes the number of samples used during the reparameterization trick. Depending on the chosen decoder distribution, the log-likelihood can be stated in terms of the estimated distribution parameters:\n\nGaussian distribution with diagonal covariance structure \\(p_{\\boldsymbol{\\theta}} \\sim \\mathcal{N} \\left( \\textbf{x}^\\prime | \\boldsymbol{\\mu}_D^{(i)} , \\text{diag} \\left( \\boldsymbol{\\sigma}_D^{2(i)} \\right) \\right)\\)\n\\[\n\\log_e p_{\\boldsymbol{\\theta}}\\left(\n\\textbf{x}^{(i)}| \\textbf{z}^{(i,l)}\\right) = \\underbrace{- \\frac {D}{2} \\ln\n2\\pi}_{\\text{const}} - \\frac {1}{2} \\ln \\left( \\prod_{k=1}^{D} \\sigma_{D_k}^{2(i)} \\right)\n- \\frac {1}{2}\\sum_{k=1}^{D} \\frac {1}{\\sigma_{D_k}^{2(i)}}\\left( x_k^{(i)}  - \\mu_{D_k}^{(i)}\\right)^2\n\\]\nwith the original observation \\(\\textbf{x}^{(i)} \\in \\mathbb{R}^{D}\\). In this form, the objective function is ill-posed since there are no limitations on the form of the normal distribution. As a result the objective function is unbounded, i.e., the VAE could learn the true mean \\(\\boldsymbol{\\mu}_D^{(i)} = \\textbf{x}^{(i)}\\) with arbitrary variance \\(\\boldsymbol{\\sigma}_D^{2(i)}\\) or huge variances with arbitrary means to maximize the log-likelihood (see this post). Note that in the encoder network, the prior \\(p_{\\boldsymbol{\\theta}}(\\textbf{z})\\) is used to constrain the encoder distribution (i.e., the mean and variance).\nGaussian distribution with identity as covariance variance \\(p_{\\boldsymbol{\\theta}} \\sim \\mathcal{N} \\left( \\textbf{x}^\\prime | \\boldsymbol{\\mu}_D^{(i)} , \\textbf{I} \\right)\\)\n\\[\n\\log_e p_{\\boldsymbol{\\theta}}\\left(\n\\textbf{x}^{(i)}| \\textbf{z}^{(i,l)}\\right) =\n- \\frac {1}{2}\\sum_{k=1}^{D} \\left( x_k^{(i)}  -\n\\mu_{D_k}^{(i)}\\right)^2 + \\text{const}\n\\]\nwith the original observation \\(\\textbf{x}^{(i)} \\in \\mathbb{R}^{D}\\). In this case the reconstruction accuracy is proportional to the negative mean squarred error which is typically used as the loss function in standard autoencoders.\n\nBernoulli distribution \\(p_{\\boldsymbol{\\theta}} \\sim\\text{Bern} \\left(\\textbf{x}^\\prime | \\boldsymbol{\\mu}_D^{(i)} \\right) = \\prod_{k=1}^{D} \\left( \\mu_{D_k}^{(i)}\\right)^{x_k^\\prime} \\left( 1 - \\mu_{D_k}^{(i)}\\right)^{1 - x_k^\\prime}\\)\n\\[\n   \\log_e p_{\\boldsymbol{\\theta}}\\left(\n\\textbf{x}^{(i)}| \\textbf{z}^{(i,l)}\\right) = \\sum_{k=1}^{D} \\left(\n   x_k^{(i)} \\ln \\left( \\mu_{D_k}^{(i)} \\right) + \\left(1 - x_k^{(i)}\n   \\right) \\ln \\left( 1 - \\mu_{D_k}^{(i)} \\right) \\right)\n\\]\nwith the original observation \\(\\textbf{x}^{(i)} \\in \\{0, 1\\}^{D}\\). In this case the reconstruction accuracy equals the negative binary cross entropy loss. Note that there are plenty of VAE implementations that use the binary cross entropy loss on non-binary observations, see discussions in this thread.\n\nTo put this into practice, below is a simple VAE Python class which will be used to compare the different decoder distributions.\n\n\nCode\nimport torch\nfrom torch.distributions.multivariate_normal import MultivariateNormal\n\n\nclass VAE(nn.Module):\n    r\"\"\"A simple VAE class based on Kingma and Welling\n        \n    Args:\n        encoder_network:  instance of CoderNetwork class\n        decoder_network:  instance of CoderNetwork class\n        L:                number of samples used during reparameterization trick\n    \"\"\"\n    \n    def __init__(self, encoder_network, decoder_network, L=1):\n        super().__init__()\n        self.encoder = encoder_network\n        self.decoder = decoder_network\n        self.L = L\n        \n        latent_dim = encoder_network.output_dim\n                \n        self.normal_dist = MultivariateNormal(torch.zeros(latent_dim), \n                                              torch.eye(latent_dim))\n        return\n    \n    def forward(self, x):\n        L = self.L\n        \n        z, mu_E, log_var_E = self.encode(x, L)\n        # regularization term per batch, i.e., size: (batch_size)\n        regularization_term = (1/2) * (1 + log_var_E - mu_E**2\n                                       - torch.exp(log_var_E)).sum(axis=1)\n        \n        # upsample x and reshape\n        batch_size = x.shape[0]\n        x_ups = x.repeat(L, 1).view(batch_size, L, -1)    \n        if self.decoder.coder_type == 'Gaussian':\n            # mu_D, log_var_D have shape (batch_size, L, output_dim)\n            mu_D, log_var_D = self.decode(z)\n            # reconstruction accuracy per batch, i.e., size: (batch_size)\n            recons_acc = (1/L) * (-(0.5)*(log_var_D.sum(axis=2)).sum(axis=1)\n               -(0.5) * ((1/torch.exp(log_var_D))*((x_ups - mu_D)**2)\n                         ).sum(axis=2).sum(axis=1))\n        elif self.decoder.coder_type == 'I-Gaussian':\n            # mu_D has shape (batch_size, L, output_dim)\n            mu_D = self.decode(z)\n            # reconstruction accuracy per batch, i.e., size: (batch_size)\n            recons_acc = (1/L) * (-(0.5) * ((x_ups - mu_D)**2\n                                            ).sum(axis=2).sum(axis=1))\n        elif self.decoder.coder_type == 'Bernoulli':\n            # mu_D has shape (batch_size, L, output_dim)\n            mu_D = self.decode(z)     \n            # reconstruction accuracy per batch, i.e., size: (batch_size)\n            # corresponds to the negative binary cross entropy loss (BCELoss)\n            recons_acc = (1/L) * (x_ups * torch.log(mu_D) + \n                                  (1 - x_ups) * torch.log(1 - mu_D)\n                                  ).sum(axis=2).sum(axis=1)\n        loss = - regularization_term.sum() - recons_acc.sum()\n        return loss\n    \n    def encode(self, x, L=1):\n        # get encoder distribution parameters\n        mu_E, log_var_E = self.encoder(x)\n        # sample noise variable L times for each batch\n        batch_size = x.shape[0]\n        epsilon = self.normal_dist.sample(sample_shape=(batch_size, L, ))\n        # upsample mu_E, log_var_E and reshape\n        mu_E_ups = mu_E.repeat(L, 1).view(batch_size, L, -1) \n        log_var_E_ups = log_var_E.repeat(L, 1).view(batch_size, L, -1)\n        # get latent variable by reparametrization trick\n        z = mu_E_ups + torch.sqrt(torch.exp(log_var_E_ups)) * epsilon\n        return z, mu_E, log_var_E\n    \n    def decode(self, z):\n        # get decoder distribution parameters\n        if self.decoder.coder_type == 'Gaussian':\n            mu_D, log_var_D = self.decoder(z)\n            return mu_D, log_var_D\n        elif self.decoder.coder_type == 'I-Gaussian':\n            mu_D = self.decoder(z)\n            return mu_D\n        elif self.decoder.coder_type == 'Bernoulli':\n            mu_D = self.decoder(z)\n            return mu_D\n        return\n\n\nLet’s train the three different VAEs on the MNIST digits dataset\n\n\nCode\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n\ndef train(decoder_type, dataset, x_dim, hid_dim, z_dim, batch_size, L, epochs):\n    encoder_network = CoderNetwork(input_dim=x_dim, \n                                   hidden_dim=hid_dim, \n                                   output_dim=z_dim,\n                                   coder_type='Gaussian')\n    decoder_network = CoderNetwork(input_dim=z_dim, \n                                   hidden_dim=hid_dim, \n                                   output_dim=x_dim,\n                                   coder_type=decoder_type)\n    \n    model = VAE(encoder_network, decoder_network, L=L)\n    data_loader = DataLoader(dataset, batch_size, shuffle=True)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n    \n    print('Start training with {} decoder distribution\\n'.format(decoder_type))\n    for epoch in range(1, epochs + 1):\n        print('Epoch {}/{}'.format(epoch, epochs))\n        avg_loss = 0\n        for counter, (mini_batch_data, label) in enumerate(data_loader):\n            \n            model.zero_grad()\n            \n            loss = model(mini_batch_data.view(-1, x_dim))\n            loss.backward()\n            optimizer.step()\n            \n            avg_loss += loss.item() / len(dataset)\n            \n            if counter % 20 == 0 or (counter + 1)==len(data_loader):\n                batch_loss = loss.item() / len(mini_batch_data)\n                print('\\r[{}/{}] batch loss: {:.2f}'.format(counter + 1,\n                                                            len(data_loader),\n                                                            batch_loss),\n                      end='', flush=True)\n        print('\\nAverage loss: {:.3f}'.format(avg_loss)) \n    print('Done!\\n')\n    trained_VAE = model\n    return trained_VAE\n\ndataset = datasets.MNIST('data/', transform=transforms.ToTensor(), download=True)\nx_dim, hid_dim, z_dim = 28*28, 400, 20\nbatch_size, L, epochs = 128, 5, 3\n\nBernoulli_VAE = train('Bernoulli', dataset, x_dim, hid_dim, z_dim, \n                      batch_size, L, epochs)\nGaussian_VAE = train('Gaussian', dataset, x_dim, hid_dim, z_dim, \n                     batch_size, L, epochs)\nI_Gaussian_VAE = train('I-Gaussian', dataset, x_dim, hid_dim, z_dim, \n                       batch_size, L, epochs)\n\n\n\n\n\n\n\nLet’s look at the differences in the reconstructions:\n\n\nCode\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\ndef plot_results(trained_model, dataset, n_samples):\n    decoder_type = trained_model.decoder.coder_type\n    \n    fig = plt.figure(figsize=(14, 3))\n    fig.suptitle(decoder_type + ' Distribution: Observations (top row) and ' +\n                 'their reconstructions (bottom row)')\n    for i_sample in range(n_samples):\n        x_sample = dataset[i_sample][0].view(-1, 28*28)\n        \n        z, mu_E, log_var_E = trained_model.encode(x_sample, L=1)\n        if decoder_type in ['Bernoulli', 'I-Gaussian']:\n            x_prime = trained_model.decode(z)\n        else:\n            x_prime = trained_model.decode(z)[0]\n    \n        plt.subplot(2, n_samples, i_sample + 1)\n        plt.imshow(x_sample.view(28, 28).data.numpy())\n        plt.axis('off')\n        plt.subplot(2, n_samples, i_sample + 1 + n_samples)\n        plt.imshow(x_prime.view(28, 28).data.numpy())\n        plt.axis('off')\n    return\n\n\nn_samples = 10\n\nplot_results(Bernoulli_VAE, dataset, n_samples)\nplot_results(Gaussian_VAE, dataset, n_samples)\nplot_results(I_Gaussian_VAE, dataset, n_samples)"
  },
  {
    "objectID": "paper_summaries/auto-encoding_variational_bayes/index.html#acknowledgement",
    "href": "paper_summaries/auto-encoding_variational_bayes/index.html#acknowledgement",
    "title": "Auto-Encoding Variational Bayes",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nDaniel Daza’s blog was really helpful and the presented code is highly inspired by his summary on VAEs."
  },
  {
    "objectID": "paper_summaries/auto-encoding_variational_bayes/index.html#footnotes",
    "href": "paper_summaries/auto-encoding_variational_bayes/index.html#footnotes",
    "title": "Auto-Encoding Variational Bayes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe true posterior could be calculated via Bayes theorem \\(\\displaystyle p_{\\boldsymbol{\\theta}} (\\textbf{z}|\\textbf{x}) = \\frac {p_{\\boldsymbol{\\theta}} (\\textbf{x}|\\textbf{z}) p_{\\boldsymbol{\\theta}} (\\textbf{z})} {\\int p_{\\boldsymbol{\\theta}} (\\textbf{x}|\\textbf{z}) p_{\\boldsymbol{\\theta}} (\\textbf{z}) d\\textbf{z}}\\). However, the integral in the denominator is intractable in practice.↩︎\nNote that the variance is by definition greater than zero. Furthermore, the variance is typically relatively small. Thus, using the logarithmized variance as network output increases stability and performance (see this answer for details).↩︎"
  },
  {
    "objectID": "paper_summaries/spatial_broadcast_decoder/index.html",
    "href": "paper_summaries/spatial_broadcast_decoder/index.html",
    "title": "Spatial Broadcast Decoder: A Simple Architecture for Learning Disentangled Representations in VAEs",
    "section": "",
    "text": "Watters et al. (2019) introduce the Spatial Broadcast Decoder (SBD) as an architecture for the decoder in Variational Auto-Encoders (VAEs) to improve disentanglement in the latent space1, reconstruction accuracy and generalization in limited datasets (i.e., held-out regions in data space). Motivated by the limitations of deconvolutional layers in traditional decoders, these upsampling layers are replaced by a tiling operation in the Spatial Broadcast decoder. Furthermore, explicit spatial information (inductive bias) is appended in the form of coordinate channels leading to a simplified optimization problem and improved positional generalization. As a proof of concept, they tested the model on the colored sprites dataset (known factors of variation such as position, size, shape), Chairs and 3D Object-in-Room datasets (no positional variation), a dataset with small objects and a dataset with dependent factors. They could show that the Spatial Broadcast decoder can be used complementary or as an improvement to state-of-the-art disentangling techniques."
  },
  {
    "objectID": "paper_summaries/spatial_broadcast_decoder/index.html#model-description",
    "href": "paper_summaries/spatial_broadcast_decoder/index.html#model-description",
    "title": "Spatial Broadcast Decoder: A Simple Architecture for Learning Disentangled Representations in VAEs",
    "section": "Model Description",
    "text": "Model Description\nAs stated in the title, the model architecture of the Spatial Broadcast decoder is very simple: Take a standard VAE decoder and replace all upsampling deconvolutional layers by tiling the latent code \\(\\textbf{z}\\) across the original image space, appending fixed coordinate channels and applying an convolutional network with \\(1 \\times 1\\) stride, see the figure below.\n\n\n\n\n\n\n\n\nSchematic of the Spatial Broadcast VAE. In the decoder, the latent code \\(\\textbf{z}\\in\\mathbb{R}^{k}\\) is broadcasted (tiled) to the image width \\(w\\) and height \\(h\\). Additionally, two “coordinate” channels are appended. The result is fed to an unstrided convolutional decoder. (right) Pseudo-code of the spatial operation. Taken from Watters et al. (2019).\n\n\n\n\nMotivation: The presented architecture is mainly motivated by two reasons:\n\nDeconvolution layers cause optimization difficulties: Watters et al. (2019) argue that upsampling deconvolutional layers should be avoided, since these are prone to produce checkerboard artifacts, i.e., a checkerboard pattern can be identified on the resulting images (when looking closer), see figure below. These artifacts constrain the reconstruction accuracy and Watters et al. (2019) hypothesize that the resulting effects may raise problems for learning a disentangled representation in the latent space.\n\n\n\n\n\n\n\n\nA checkerboard pattern can often be identified in artifically generated images that use deconvolutional layers. Taken from Odena et al. (2016) (very worth reading).\n\n\n\nAppended coordinate channels improve positional generalization and optimization: Previous work by Liu et al. (2018) showed that standard convolution/deconvolution networks (CNNs) perform badly when trying to learn trivial coordinate transformations (e.g., learning a mapping from Cartesian space into one-hot pixel space or vice versa). This behavior may seem counterintuitive (easy task, small dataset), however the feature of translational equivariance (i.e., shifting an object in the input equally shifts its representation in the output) in CNNs2 hinders learning this task: The filters have by design no information about their position. Thus, coordinate transformations result in complicated functions which makes optimization difficult. E.g., changing the input coordinate slighlty might push the resulting function in a completelty different direction.\nCoordConv Solution: To overcome this problem, Liu et al. (2018) propose to append coordinate channels before convolution and term the resulting layer CoordConv, see figure below. In principle, this layer can learn to use or discard translational equivariance and keeps the other advantages of convolutional layers (fast computations, few parameters). Under this modification learning coordinate transformation problems works out of the box with perfect generalization in less time (150 times faster) and less memory (10-100 times fewer parameters). As coordinate transformations are implicitely needed in a variaty of tasks (such as producing bounding boxes in object detection) using CoordConv instead of standard convolutions might increase the performance of several other models.\n\n\n\n\n\n\n\n\nComparison of 2D convolutional and CoordConv layers. Taken from Liu et al. (2018).\n\n\n\nPositional Generalization: Appending fixed coordinate channels is mainly beneficial in datasets in which same objects may appear at distinct positions (i.e., there is positional variation). The main idea is that rendering an object at a specific position without spatial information (i.e., standard convolution/deconvolution) results in a very complicated function. In contrast,the Spatial Broadcast decoder architecture can leverage the spatial information to reveal objects easily: E.g., by convolving the positions in the latent space with the fixed coordinate channels and applying a threshold operation. Thus, Watters et al. (2019) argue that the Spatial Broadcast decoder architecture puts a prior on dissociating positional from non-positional features in the latent distribution. Datasets without positional variation in turn seem unlikely to benefit from this architecture. However, Watters et al. (2019) showed that the Spatial Broadcast decoder could still help in these datasets and attribute this to the replacement of deconvolutional layers."
  },
  {
    "objectID": "paper_summaries/spatial_broadcast_decoder/index.html#implementation",
    "href": "paper_summaries/spatial_broadcast_decoder/index.html#implementation",
    "title": "Spatial Broadcast Decoder: A Simple Architecture for Learning Disentangled Representations in VAEs",
    "section": "Implementation",
    "text": "Implementation\nWatters et al. (2019) conducted experiments with several datasets and could show that incorporating the Spatial Broadcast decoder into state-of-the-art VAE architectures consistently increased their perfomance. While this is impressive, it is always frustrating to not being able to reproduce results due to missing implementation details, less computing resources or simply not having enough time to work on a reimplementation.\nThe following reimplementation intends to eliminate that frustration by reproducing some of their experiments on much smaller datasets with similar characteristics such that training will take less time (less than 30 minutes with a NVIDIA Tesla K80 GPU).\n\nData Generation\nA dataset that is similar in spirit to the colored sprites dataset will be generated, i.e., procedurally generated objects from known factors of variation. Watters et al. (2019) use a binary dsprites dataset consisting of 737,280 images and transform these during training into colored images by uniformly sampling from a predefined HSV space (see Appendix A.3). As a result, the dataset has 8 factors of variation (\\(x\\)-position, \\(y\\)-position, size, shape, angle, 3D-color) with infinite samples (due to sampling of color). They used \\(1.5 \\cdot 10^6\\) training steps.\nTo reduce training time, we are going to generate a much simpler dataset consisting of \\(3675\\) images with a circle (fixed size) inside generated from a predefined set of possible colors and positions such that there are only 3 factors of variation (\\(x\\)-position, \\(y\\)-position, discretized color). In this case \\(3.4 \\cdot 10^2\\) training steps suffice for approximate convergence.\n\n\n\n\n\n\n\n\nVisualization of self-written Dataset\n\n\n\nThe code below creates the dataset. Note that it is kept more generic than necessary to allow the creation of several variations of this dataset, i.e., more dedicated experiments can be conducted.\n\n\n\n\n\n\n\n\n\nCode\nfrom PIL import Image, ImageDraw\nimport torchvision.transforms as transforms\nimport numpy as np\nimport torch\nfrom torch.utils.data import TensorDataset\n\n\ndef generate_img(x_position, y_position, shape, color, img_size, size=20):\n    \"\"\"Generate an RGB image from the provided latent factors\n\n    Args:\n        x_position (float): normalized x position\n        y_position (float): normalized y position\n        shape (string): can only be 'circle' or 'square'\n        color (string): color name or rgb string\n        img_size (int): describing the image size (img_size, img_size)\n        size (int): size of shape\n\n    Returns:\n        torch tensor [3, img_size, img_size] (dtype=torch.float32)\n    \"\"\"\n    # creation of image\n    img = Image.new('RGB', (img_size, img_size), color='black')\n    # map (x, y) position to pixel coordinates\n    x_position = (img_size - 2 - size) * x_position\n    y_position = (img_size - 2 - size) * y_position\n    # define coordinates\n    x_0, y_0 = x_position, y_position\n    x_1, y_1 = x_position + size, y_position + size\n    # draw shapes\n    img1 = ImageDraw.Draw(img)\n    if shape == 'square':\n        img1.rectangle([(x_0, y_0), (x_1, y_1)], fill=color)\n    elif shape == 'circle':\n        img1.ellipse([(x_0, y_0), (x_1, y_1)], fill=color)\n    return transforms.ToTensor()(img).type(torch.float32)\n\n\ndef generate_dataset(img_size, shape_sizes, num_pos, shapes, colors):\n    \"\"\"procedurally generated from 4 ground truth independent latent factors,\n       these factors are/can be\n           Position X: num_pos values in [0, 1]\n           Poistion Y: num_pos values in [0, 1]\n           Shape: square, circle\n           Color: standard HTML color name or 'rgb(x, y, z)'\n\n    Args:\n           img_size (int): describing the image size (img_size, img_size)\n           shape_sizes (list): sizes of shapes\n           num_pos (int): discretized positions\n           shapes (list): shapes (can only be 'circle', 'square')\n           colors (list): colors\n\n    Returns:\n           data: torch tensor [n_samples, 3, img_size, img_size]\n           latents: each entry describes the latents of corresp. data entry\n    \"\"\"\n    num_shapes, num_colors, sizes = len(shapes), len(colors), len(shape_sizes)\n\n    n_samples = num_pos*num_pos*num_shapes*num_colors*sizes\n    data = torch.empty([n_samples, 3, img_size, img_size])\n    latents = np.empty([n_samples], dtype=object)\n\n    index = 0\n    for x_pos in np.linspace(0, 1, num_pos):\n        for y_pos in np.linspace(0, 1, num_pos):\n            for shape in shapes:\n                for size in shape_sizes:\n                    for color in colors:\n                        img = generate_img(x_pos, y_pos, shape, color,\n                                           img_size, size)\n                        data[index] = img\n                        latents[index] = [x_pos, y_pos, shape, color]\n\n                        index += 1\n    return data, latents\n\n\ncircles_data, latents = generate_dataset(img_size=64, shape_sizes=[16],\n                                         num_pos=35,\n                                         shapes=['circle'],\n                                         colors=['red', 'green', 'blue'])\nsprites_dataset = TensorDataset(circles_data)\n\n\n\n\nModel Implementation\nAlthough in principle implementing a VAE is fairly simple (see my post for details), in practice one must choose lots of hyperparmeters. These can be divided into three broader categories:\n\nEncoder/Decoder and Prior Distribution: As suggested by Watters et al. (2019) in Appendix A, we use a Gaussian decoder distribution with fixed diagonal covariance structure \\(p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}^\\prime | \\textbf{z}^{(i)}\\right) = \\mathcal{N}\\left( \\textbf{x}^\\prime |  \\boldsymbol{\\mu}_D^{(i)}, \\sigma^2 \\textbf{I} \\right)\\), hence the reconstruction accuracy can be calculated as follows3\n\\[\n  \\text{Reconstruction Acc.} = \\log p_{\\boldsymbol{\\theta}} \\left(\n  \\textbf{x}^{(i)} | \\textbf{z}^{(i)} \\right) = - \\frac {1}{2 \\sigma^2}\n  \\sum_{k=1}^{D} \\left(x_k^{(i)} - \\mu_{D_k}^{(i)} \\right)^2 + \\text{const}.\n  \\]\nFor the encoder distribution a Gaussian with diagonal covariance \\(q_{\\boldsymbol{\\phi}} \\sim  \\mathcal{N} \\left( \\textbf{z} | \\boldsymbol{\\mu}_E,  \\boldsymbol{\\sigma}_D^2 \\textbf{I} \\right)\\) and as prior a centered multivariate Gaussian \\(p_{\\boldsymbol{\\theta}}  (\\textbf{z}) = \\mathcal{N}\\left( \\textbf{z} | \\textbf{0}, \\textbf{I} \\right)\\) are chosen (both typical choices).\nNetwork Architecture for Encoder/Decoder: The network architectures for the standard encoder and decoder consist of convolutional and deconvolutional layers (since these perform typically much better on image data). The Spatial Broadcast decoder defines a different kind of architecture, see Model Description. The exact architectures are taken from Appendix A.1 of Watters et al., see code below4:\n\n\nCode\nfrom torch import nn\n\n\nclass Encoder(nn.Module):\n    \"\"\"\"Encoder class for use in convolutional VAE\n\n    Args:\n        latent_dim: dimensionality of latent distribution\n\n    Attributes:\n        encoder_conv: convolution layers of encoder\n        fc_mu: fully connected layer for mean in latent space\n        fc_log_var: fully connceted layers for log variance in latent space\n    \"\"\"\n\n    def __init__(self, latent_dim=6):\n        super().__init__()\n        self.latent_dim = latent_dim\n\n        self.encoder_conv = nn.Sequential(\n            # shape: [batch_size, 3, 64, 64]\n            nn.Conv2d(3,  64, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(),\n            # shape: [batch_size, 64, 32, 32]\n            nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(),\n            # shape: [batch_size, 64, 4, 4],\n            nn.Flatten(),\n            # shape: [batch_size, 1024]\n            nn.Linear(1024, 256),\n            nn.ReLU(),\n            # shape: [batch_size, 256]\n        )\n        self.fc_mu = nn.Sequential(\n            nn.Linear(in_features=256, out_features=self.latent_dim),\n        )\n        self.fc_log_var = nn.Sequential(\n            nn.Linear(in_features=256, out_features=self.latent_dim),\n        )\n        return\n\n    def forward(self, inp):\n        out = self.encoder_conv(inp)\n        mu = self.fc_mu(out)\n        log_var = self.fc_log_var(out)\n        return [mu, log_var]\n\n\nclass Decoder(nn.Module):\n    \"\"\"(standard) Decoder class for use in convolutional VAE,\n    a Gaussian distribution with fixed variance (identity times fixed variance\n    as covariance matrix) used as the decoder distribution\n\n    Args:\n        latent_dim: dimensionality of latent distribution\n        fixed_variance: variance of distribution\n\n    Attributes:\n        decoder_upsampling: linear upsampling layer(s)\n        decoder_deconv: deconvolution layers of decoder (also upsampling)\n    \"\"\"\n\n    def __init__(self, latent_dim, fixed_variance):\n        super().__init__()\n        self.latent_dim = latent_dim\n        self.coder_type = 'Gaussian with fixed variance'\n        self.fixed_variance = fixed_variance\n\n        self.decoder_upsampling = nn.Sequential(\n            nn.Linear(self.latent_dim, 256),\n            nn.ReLU(),\n            # reshaped into [batch_size, 64, 2, 2]\n        )\n        self.decoder_deconv = nn.Sequential(\n            # shape: [batch_size, 64, 2, 2]\n            nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(),\n            # shape: [batch_size, 64, 4, 4]\n            nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(64,  3, kernel_size=4, stride=2, padding=1),\n            # shape: [batch_size, 3, 64, 64]\n        )\n        return\n\n    def forward(self, inp):\n        ups_inp = self.decoder_upsampling(inp)\n        ups_inp = ups_inp.view(-1, 64, 2, 2)\n        mu = self.decoder_deconv(ups_inp)\n        return mu\n\n\nclass SpatialBroadcastDecoder(nn.Module):\n    \"\"\"SBD class for use in convolutional VAE,\n      a Gaussian distribution with fixed variance (identity times fixed\n      variance as covariance matrix) used as the decoder distribution\n\n    Args:\n        latent_dim: dimensionality of latent distribution\n        fixed_variance: variance of distribution\n\n    Attributes:\n        img_size: image size (necessary for tiling)\n        decoder_convs: convolution layers of decoder (also upsampling)\n    \"\"\"\n\n    def __init__(self, latent_dim, fixed_variance):\n        super().__init__()\n        self.img_size = 64\n        self.coder_type = 'Gaussian with fixed variance'\n        self.latent_dim = latent_dim\n        self.fixed_variance = fixed_variance\n\n        x = torch.linspace(-1, 1, self.img_size)\n        y = torch.linspace(-1, 1, self.img_size)\n        x_grid, y_grid = torch.meshgrid(x, y, indexing=\"ij\")\n        # reshape into [1, 1, img_size, img_size] and save in state_dict\n        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n\n        self.decoder_convs = nn.Sequential(\n            # shape [batch_size, latent_dim + 2, 64, 64]\n            nn.Conv2d(in_channels=self.latent_dim+2, out_channels=64,\n                      stride=(1, 1), kernel_size=(3,3), padding=1),\n            nn.ReLU(),\n            # shape [batch_size, 64, 64, 64]\n            nn.Conv2d(in_channels=64, out_channels=64, stride=(1,1),\n                      kernel_size=(3, 3), padding=1),\n            nn.ReLU(),\n            # shape [batch_size, 64, 64, 64]\n            nn.Conv2d(in_channels=64, out_channels=3, stride=(1,1),\n                      kernel_size=(3, 3), padding=1),\n            # shape [batch_size, 3, 64, 64]\n        )\n        return\n\n    def forward(self, z):\n        batch_size = z.shape[0]\n        # reshape z into [batch_size, latent_dim, 1, 1]\n        z = z.view(z.shape + (1, 1))\n        # tile across image [batch_size, latent_im, img_size, img_size]\n        z_b = z.expand(-1, -1, self.img_size, self.img_size)\n        # upsample x_grid and y_grid to [batch_size, 1, img_size, img_size]\n        x_b = self.x_grid.expand(batch_size, -1, -1, -1)\n        y_b = self.y_grid.expand(batch_size, -1, -1, -1)\n        # concatenate vectors [batch_size, latent_dim+2, img_size, img_size]\n        z_sb = torch.cat((z_b, x_b, y_b), dim=1)\n        # apply convolutional layers\n        mu_D = self.decoder_convs(z_sb)\n        return mu_D\n\n\nThe VAE implementation below combines the encoder and decoder architectures (slightly modified version of my last VAE implementation).\n\n\nCode\nfrom torch.distributions.multivariate_normal import MultivariateNormal\n\n\nclass VAE(nn.Module):\n    \"\"\"A simple VAE class\n\n    Args:\n        vae_tpe: type of VAE either 'Standard' or 'SBD'\n        latent_dim: dimensionality of latent distribution\n        fixed_var: fixed variance of decoder distribution\n    \"\"\"\n\n    def __init__(self, vae_type, latent_dim, fixed_var):\n        super().__init__()\n        self.vae_type = vae_type\n\n        if self.vae_type == 'Standard':\n            self.decoder = Decoder(latent_dim=latent_dim,\n                                  fixed_variance=fixed_var)\n        else:\n            self.decoder = SpatialBroadcastDecoder(latent_dim=latent_dim,\n                                                   fixed_variance=fixed_var)\n\n        self.encoder = Encoder(latent_dim=latent_dim)\n        self.normal_dist = MultivariateNormal(torch.zeros(latent_dim),\n                                              torch.eye(latent_dim))\n        return\n\n    def forward(self, x):\n        z, mu_E, log_var_E = self.encode(x)\n        # regularization term per batch, i.e., size: (batch_size)\n        regularization_term = 0.5 * (1 + log_var_E - mu_E**2\n                                      - torch.exp(log_var_E)).sum(axis=1)\n\n        batch_size = x.shape[0]\n        if self.decoder.coder_type == 'Gaussian with fixed variance':\n            # x_rec has shape (batch_size, 3, 64, 64)\n            x_rec = self.decode(z)\n            # reconstruction accuracy per batch, i.e., size: (batch_size)\n            factor = 0.5 * (1/self.decoder.fixed_variance)\n            recons_acc = - factor * ((x.view(batch_size, -1) -\n                                    x_rec.view(batch_size, -1))**2\n                                  ).sum(axis=1)\n        return -regularization_term.mean(), -recons_acc.mean()\n\n    def reconstruct(self, x):\n        mu_E, log_var_E = self.encoder(x)\n        x_rec = self.decoder(mu_E)\n        return x_rec\n\n    def encode(self, x):\n        # get encoder distribution parameters\n        mu_E, log_var_E = self.encoder(x)\n        # sample noise variable for each batch\n        batch_size = x.shape[0]\n        epsilon = self.normal_dist.sample(sample_shape=(batch_size, )\n                                          ).to(x.device)\n        # get latent variable by reparametrization trick\n        z = mu_E + torch.exp(0.5*log_var_E) * epsilon\n        return z, mu_E, log_var_E\n\n    def decode(self, z):\n        # get decoder distribution parameters\n        mu_D = self.decoder(z)\n        return mu_D\n\n\nTraining Parameters: Lastly, training neural networks itself consists of several hyperparmeters. Again, we are using the same setup as defined in Appendix A.1 of Watters et al. (2019), see code below.\n\n\nCode\nfrom livelossplot import PlotLosses\nfrom torch.utils.data import DataLoader\n\n\ndef train(dataset, epochs, VAE):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n    print('Device: {}'.format(device))\n\n    data_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n\n    VAE.to(device)\n    optimizer = torch.optim.Adam(VAE.parameters(), lr=3e-4)\n\n    losses_plot = PlotLosses(groups={'avg log loss':\n                                    ['kl loss', 'reconstruction loss']})\n    print('Start training with {} decoder\\n'.format(VAE.vae_type))\n    for epoch in range(1, epochs +1):\n        avg_kl = 0\n        avg_recons_err = 0\n        for counter, mini_batch_data in enumerate(data_loader):\n            VAE.zero_grad()\n\n            kl_div, recons_err = VAE(mini_batch_data[0].to(device))\n            loss = kl_div + recons_err\n            loss.backward()\n            optimizer.step()\n\n            avg_kl += kl_div.item() / len(dataset)\n            avg_recons_err += recons_err.item() / len(dataset)\n\n        losses_plot.update({'kl loss': np.log(avg_kl),\n                            'reconstruction loss': np.log(avg_recons_err)})\n        losses_plot.send()\n    trained_VAE = VAE\n    return trained_VAE\n\n\n\n\n\nVisualization Functions\nEvaluating the representation quality of trained models is a difficult task, since we are not only interested in the reconstruction accuracy but also in the latent space and its properties. Ideally the latent space offers a disentangled representation such that each latent variable represents a factor of variation with perfect reconstruction accuracy (i.e., for evaluation it is very helpful to know in advance how many and what factors of variation exist). Although there are some metrics to quantify disentanglement, many of them have serious shortcomings and there is yet no consensus in the literature which to use (Watters et al., 2019). Instead of focusing on some metric, we are going to visualize the results by using two approaches:\n\nReconstructions and Latent Traversals: A very popular and helpful plot is to show some (arbitrarly chosen) reconstructions compared to the original input together with a series of latent space traversals. I.e., taking some encoded input and looking at the reconstructions when sweeping each coordinate in the latent space in a predefined interval (here from -2 to +2) while keeping all other coordinates constant. Ideally, each sweep can be associated with a factor of variation. The code below will be used to generate these plots. Note that the reconstructions are clamped into \\([0, 1]\\) as this is the allowed image range.\n\n\nCode\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\ndef reconstructions_and_latent_traversals(STD_VAE, SBD_VAE, dataset, SEED=1):\n    np.random.seed(SEED)\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    latent_dims = STD_VAE.encoder.latent_dim\n\n    n_samples = 7\n    i_samples = np.random.choice(range(len(dataset)), n_samples, replace=False)\n\n    # preperation for latent traversal\n    i_latent = i_samples[n_samples//2]\n    lat_image = dataset[i_latent][0]\n    sweep = np.linspace(-2, 2, n_samples)\n\n    fig = plt.figure(constrained_layout=False, figsize=(2*n_samples, 2+latent_dims))\n    grid = plt.GridSpec(latent_dims + 5, n_samples*2 + 3,\n                        hspace=0.2, wspace=0.02, figure=fig)\n    # standard VAE\n    for counter, i_sample in enumerate(i_samples):\n        orig_image = dataset[i_sample][0]\n        # original\n        main_ax = fig.add_subplot(grid[1, counter + 1])\n        main_ax.imshow(transforms.ToPILImage()(orig_image))\n        main_ax.axis('off')\n        main_ax.set_aspect('equal')\n\n        # reconstruction\n        x_rec = STD_VAE.reconstruct(orig_image.unsqueeze(0).to(device))\n        # clamp output into [0, 1] and prepare for plotting\n        recons_image =  torch.clamp(x_rec, 0, 1).squeeze(0).cpu()\n\n        main_ax = fig.add_subplot(grid[2, counter + 1])\n        main_ax.imshow(transforms.ToPILImage()(recons_image))\n        main_ax.axis('off')\n        main_ax.set_aspect('equal')\n    # latent dimension traversal\n    z, mu_E, log_var_E = STD_VAE.encode(lat_image.unsqueeze(0).to(device))\n    for latent_dim in range(latent_dims):\n        for counter, z_replaced in enumerate(sweep):\n            z_new = z.detach().clone()\n            z_new[0][latent_dim] = z_replaced\n\n            # clamp output into [0, 1] and prepare for plotting\n            img_rec = torch.clamp(STD_VAE.decode(z_new), 0, 1).squeeze(0).cpu()\n\n            main_ax = fig.add_subplot(grid[4 + latent_dim, counter + 1])\n            main_ax.imshow(transforms.ToPILImage()(img_rec))\n            main_ax.axis('off')\n    # SBD VAE\n    for counter, i_sample in enumerate(i_samples):\n        orig_image = dataset[i_sample][0]\n        # original\n        main_ax = fig.add_subplot(grid[1, counter + n_samples + 2])\n        main_ax.imshow(transforms.ToPILImage()(orig_image))\n        main_ax.axis('off')\n        main_ax.set_aspect('equal')\n        # reconstruction\n        x_rec = SBD_VAE.reconstruct(orig_image.unsqueeze(0).to(device))\n        # clamp output into [0, 1] and prepare for plotting\n        recons_image = torch.clamp(x_rec, 0, 1).squeeze(0).cpu()\n\n        main_ax = fig.add_subplot(grid[2, counter + n_samples + 2])\n        main_ax.imshow(transforms.ToPILImage()(recons_image))\n        main_ax.axis('off')\n        main_ax.set_aspect('equal')\n    # latent dimension traversal\n    z, mu_E, log_var_E = SBD_VAE.encode(lat_image.unsqueeze(0).to(device))\n    for latent_dim in range(latent_dims):\n        for counter, z_replaced in enumerate(sweep):\n            z_new = z.detach().clone()\n            z_new[0][latent_dim] = z_replaced\n            # clamp output into [0, 1] and prepare for plotting\n            img_rec = torch.clamp(SBD_VAE.decode(z_new), 0, 1).squeeze(0).cpu()\n\n            main_ax = fig.add_subplot(grid[4+latent_dim, counter+n_samples+2])\n            main_ax.imshow(transforms.ToPILImage()(img_rec))\n            main_ax.axis('off')\n    # prettify by adding annotation texts\n    fig = prettify_with_annotation_texts(fig, grid, n_samples, latent_dims)\n    return fig\n\ndef prettify_with_annotation_texts(fig, grid, n_samples, latent_dims):\n    # figure titles\n    titles = ['Deconv Reconstructions', 'Spatial Broadcast Reconstructions',\n              'Deconv Traversals', 'Spatial Broadcast Traversals']\n    idx_title_pos = [[0, 1, n_samples+1], [0, n_samples+2, n_samples*2+2],\n                    [3, 1, n_samples+1], [3, n_samples+2, n_samples*2+2]]\n    for title, idx_pos in zip(titles, idx_title_pos):\n        fig_ax = fig.add_subplot(grid[idx_pos[0], idx_pos[1]:idx_pos[2]])\n        fig_ax.annotate(title, xy=(0.5, 0), xycoords='axes fraction',\n                        fontsize=14, va='bottom', ha='center')\n        fig_ax.axis('off')\n    # left annotations\n    fig_ax = fig.add_subplot(grid[1, 0])\n    fig_ax.annotate('input', xy=(1, 0.5), xycoords='axes fraction',\n                    fontsize=12,  va='center', ha='right')\n    fig_ax.axis('off')\n    fig_ax = fig.add_subplot(grid[2, 0])\n    fig_ax.annotate('recons', xy=(1, 0.5), xycoords='axes fraction',\n                    fontsize=12, va='center', ha='right')\n    fig_ax.axis('off')\n    fig_ax = fig.add_subplot(grid[4:latent_dims + 4, 0])\n    fig_ax.annotate('latent coordinate traversed', xy=(0.9, 0.5),\n                    xycoords='axes fraction', fontsize=12,\n                    va='center', ha='center', rotation=90)\n    fig_ax.axis('off')\n    # pertubation magnitude\n    for i_y_grid in [[1, n_samples+1], [n_samples+2, n_samples*2+2]]:\n        fig_ax = fig.add_subplot(grid[latent_dims + 4, i_y_grid[0]:i_y_grid[1]])\n        fig_ax.annotate('pertubation magnitude', xy=(0.5, 0),\n                        xycoords='axes fraction', fontsize=12,\n                        va='bottom', ha='center')\n        fig_ax.set_frame_on(False)\n        fig_ax.axes.set_xlim([-2.5, 2.5])\n        fig_ax.xaxis.set_ticks([-2, 0, 2])\n        fig_ax.xaxis.set_ticks_position('top')\n        fig_ax.xaxis.set_tick_params(direction='inout', pad=-16)\n        fig_ax.get_yaxis().set_ticks([])\n    # latent dim\n    for latent_dim in range(latent_dims):\n        fig_ax = fig.add_subplot(grid[4 + latent_dim, n_samples*2 + 2])\n        fig_ax.annotate('lat dim ' + str(latent_dim + 1), xy=(0, 0.5),\n                        xycoords='axes fraction',\n                        fontsize=12, va='center', ha='left')\n        fig_ax.axis('off')\n    return\n\n\nLatent Space Geometry: While latent traversals may be helpful, Watters et al. (2019) note that this techniques suffers from two shortcommings:\n\nLatent space entanglement might be difficult to perceive by eye.\nTraversals are only taken at some point in space. It could be that traversals at some points are more disentangled than at other positions. Thus, judging disentanglement by the aforementioned method might be ultimately dependent to randomness.\n\nTo overcome these limitations, they propose a new method which they term latent space geometry. The main idea is to visualize a transformation from a 2-dimensional generative factor space (subspace of all generative factors) into the 2-dimensional latent subspace (choosing the two latent components that correspond to the factors of variation). Latent space geometry that preserves the chosen geometry of the generative factor space (while scaling and rotation might be allowed depending on the chosen generative factor space) indicates disentanglement.\nTo put this into practice, the code below creates circle images by varying \\(x\\) and \\(y\\) positions uniformly and keeping the other generative factors (here only color) constant. Accordingly, the geometry of the generative factor space is a uniform grid (which will be plotted). These images will be encoded into mean and variance of the latent distribution. In order to find the latent components that correspond to the \\(x\\) and \\(y\\) position, we choose the components with smallest mean variance across all reconstructions, i.e., the most informative components5. Then, we can plot the latent space geometry by using the latent components of the mean (encoder distribution), see code below.\n\n\nCode\ndef latent_space_geometry(STD_VAE, SBD_VAE):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    plt.figure(figsize=(18, 6))\n    # x,y position grid in [0.2, 0.8] (generative factors)\n    equi = np.linspace(0.2, 0.8, 31)\n    equi_without_vert = np.setdiff1d(equi, np.linspace(0.2, 0.8, 6))\n\n    x_pos = np.append(np.repeat(np.linspace(0.2, 0.8, 6), len(equi)),\n                      np.tile(equi_without_vert, 6))\n    y_pos = np.append(np.tile(equi, 6),\n                      np.repeat(np.linspace(0.8, 0.2, 6), len(equi_without_vert)))\n    labels = np.append(np.repeat(np.arange(6), 31),\n                      np.repeat(np.arange(6)+10, 25))\n    # plot generative factor geometry\n    plt.subplot(1, 3, 1)\n    plt.scatter(x_pos, y_pos, c=labels, cmap=plt.cm.get_cmap('rainbow', 10))\n    plt.gca().set_title('Ground Truth Factors', fontsize=16)\n    plt.xlabel('X-Position')\n    plt.ylabel('Y-Position')\n\n    # generate images\n    img_size = 64\n    shape_size = 16\n    images = torch.empty([len(x_pos), 3, img_size, img_size]).to(device)\n    for counter, (x, y) in enumerate(zip(x_pos, y_pos)):\n        images[counter] = generate_img(x, y, 'circle', 'red',\n                                      img_size, shape_size)\n\n    # STD VAE\n    [all_mu, all_log_var] = STD_VAE.encoder(images)\n    # most informative latent variable\n    lat_1, lat_2 = all_log_var.mean(axis=0).sort()[1][:2]\n    # latent coordinates\n    x_lat = all_mu[:, lat_1].detach().cpu().numpy()\n    y_lat = all_mu[:, lat_2].detach().cpu().numpy()\n    # plot latent space geometry\n    plt.subplot(1, 3, 2)\n    plt.scatter(x_lat, y_lat, c=labels, cmap=plt.cm.get_cmap('rainbow', 10))\n    plt.gca().set_title('DeConv', fontsize=16)\n    plt.xlabel('latent 1 value')\n    plt.ylabel('latent 2 value')\n\n    # SBD VAE\n    [all_mu, all_log_var] = SBD_VAE.encoder(images)\n    # most informative latent variable\n    lat_1, lat_2 = all_log_var.mean(axis=0).sort()[1][:2]\n    # latent coordinates\n    x_lat = all_mu[:, lat_1].detach().cpu().numpy()\n    y_lat = all_mu[:, lat_2].detach().cpu().numpy()\n    # plot latent space geometry\n    plt.subplot(1, 3, 3)\n    plt.scatter(x_lat, y_lat, c=labels, cmap=plt.cm.get_cmap('rainbow', 10))\n    plt.gca().set_title('Spatial Broadcast', fontsize=16)\n    plt.xlabel('latent 1 value')\n    plt.ylabel('latent 2 value')\n    return\n\n\n\n\n\nResults\nLastly, let’s train our models and look at the results:\n\n\nCode\nepochs = 150\nlatent_dims = 5 # x position, y position, color, extra slots\nfixed_variance = 0.3\n\nstandard_VAE = VAE(vae_type='Standard', latent_dim=latent_dims,\n                   fixed_var=fixed_variance)\nSBD_VAE = VAE(vae_type='SBD', latent_dim=latent_dims,\n              fixed_var=fixed_variance)\n\n\n\n\nCode\ntrained_standard_VAE  = train(sprites_dataset, epochs, standard_VAE)\n\n\n\n\nCode\ntrained_SBD_VAE = train(sprites_dataset, epochs, SBD_VAE)\n\n\nAt the log-losses plots, we can already see that using the Spatial Broadcast decoder results in an improved reconstruction accuracy and regularization term. Now let’s compare both models visually by their\n\nReconstructions and Latent Traversals:\n\n\nCode\nreconstructions_and_latent_traversals(trained_standard_VAE,\n                                      trained_SBD_VAE, sprites_dataset)\n\n\nWhile the reconstructions within both models look pretty good, the latent space traversal shows an entangled representation in the standard (DeConv) VAE whereas the Spatial Broadcast model seems quite disentangled.\nLatent Space Geometry:\n\n\nCode\nlatent_space_geometry(trained_standard_VAE, trained_SBD_VAE)\n\n\nThe latent space geometry verifies our previous findings: The DeConv decoder has an entangled latent space (transformation is highly non linear) whereas in the Spatial Broadcast decoder the latent space geometry highly resembles the generating factors geometry (affine transformation). The transformation of the Spatial Broadcast decoder indicates very similar behavior in the \\(X-Y\\) position subspace (of generative factors) as in the corresponding latent subspace."
  },
  {
    "objectID": "paper_summaries/spatial_broadcast_decoder/index.html#drawbacks-of-paper",
    "href": "paper_summaries/spatial_broadcast_decoder/index.html#drawbacks-of-paper",
    "title": "Spatial Broadcast Decoder: A Simple Architecture for Learning Disentangled Representations in VAEs",
    "section": "Drawbacks of Paper",
    "text": "Drawbacks of Paper\n\nalthough there are fewer parameters in the Spatial Broadcast decoder, it does require more memory (in the implementation about 50% more)\nlonger training times compared to standard DeConv VAE\nappended coordinate channels do not help when there is no positional variation"
  },
  {
    "objectID": "paper_summaries/spatial_broadcast_decoder/index.html#acknowledgement",
    "href": "paper_summaries/spatial_broadcast_decoder/index.html#acknowledgement",
    "title": "Spatial Broadcast Decoder: A Simple Architecture for Learning Disentangled Representations in VAEs",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nDaniel Daza’s blog was really helpful and the presented code is highly inspired by his VAE-SBD implementation."
  },
  {
    "objectID": "paper_summaries/spatial_broadcast_decoder/index.html#footnotes",
    "href": "paper_summaries/spatial_broadcast_decoder/index.html#footnotes",
    "title": "Spatial Broadcast Decoder: A Simple Architecture for Learning Disentangled Representations in VAEs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs outlined by Watters et al. (2019), there is “yet no consensus on the definition of a disentangled representation”. However, in their paper they focus on feature compositionality (i.e., composing a scene in terms of independent features such as color and object) and refer to it as disentangled representation.↩︎\nIn typical image classification problems, translational equivariance is highly valued since it ensures that if a filter detects an object (e.g., edges), it will detect it irrespective of its position.↩︎\nFor simplicity, we are setting the number of (noise variable) samples \\(L\\) per datapoint to 1 (see equation for \\(\\displaystyle \\widetilde{\\mathcal{L}}\\) in Reparametrization Trick paragraph). Note that Kingma and Welling (2013) stated that in their experiments setting \\(L=1\\) sufficed as long as the minibatch size was large enough.↩︎\nThe Spatial Broadcast decoder architecture is slightly modified: Kernel size of 3 instead of 4 to get the desired output shapes.↩︎\nAn intuitve way to understand why latent compontents with smaller variance within the encoder distribution are more informative than others is to think about the sampled noise and the loss function: If the variance is high, the latent code \\(\\textbf{z}\\) will vary a lot which in turn makes the task for the decoder more difficult. However, the regularization term (KL-divergence) pushes the variances towards 1. Thus, the network will only reduce the variance of its components if it helps to increase the reconstruction accuracy.↩︎"
  },
  {
    "objectID": "paper_summaries/u_net/index.html",
    "href": "paper_summaries/u_net/index.html",
    "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
    "section": "",
    "text": "Ronneberger et al. (2015) introduced a novel neural network architecture to generate better semantic segmentations (i.e., class label assigend to each pixel) in limited datasets which is a typical challenge in the area of biomedical image processing (see figure below for an example). In essence, their model consists of a U-shaped convolutional neural network (CNN) with skip connections between blocks to capture context information, while allowing for precise localizations. In addition to the network architecture, they describe some data augmentation methods to use available data more efficiently. By the time the paper was published, the proposed architecture won several segmentation challenges in the field of biomedical engineering, outperforming state-of-the-art models by a large margin. Due to its success and efficiency, U-Net has become a standard architecture when it comes to image segmentations tasks even in the non-biomedical area (e.g., image-to-image translation, neural style transfer, Multi-Objetct Network)."
  },
  {
    "objectID": "paper_summaries/u_net/index.html#model-description",
    "href": "paper_summaries/u_net/index.html#model-description",
    "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
    "section": "Model Description",
    "text": "Model Description\nU-Net builds upon the ideas of Fully Convolutional Networks (FCNs) for Semantic Segmentation by Long et al. (2015) who successfully trained FCNs (including convolutional prediction, upsampling layers and skip connections) end-to-end (pixels-to-pixels) on semantic segmentation tasks. U-Net is basically a modified version of the FCN by making the architecture more symmetric, i.e., adding a more powerful expansive path. Ronneberger et al. (2015) argue that this modification yields more precise segmentations due to its capacity to better propagate context information to higher resolution layers.\nFCN architecture: The main idea of the FCN architecture is to take a standard classification network (such as VGG-16), discard the final classifier layer, convert fully connected layers into convolutions (i.e., prediction layers) and add skip connections to (some) pooling layers, see figure below. The skip connections consist of a prediction (\\(1 \\times 1\\) convolutional layer with channel dimension equal to number of possible classes) and a deconvolutional (upsampling) layer.\n\n\n\n\n\n\n\n\nExample of FCN Architecture. VGG-16 net is used as feature learning part. Numbers under the cubes indicate the number of output channels. The prediction layer is itself a \\(1 \\times 1\\) convolutional layer (the final output consists only of 6 possible classes). A final softmax layer is added to output a normalized classification per pixel. Taken from Tai et al. (2017)\n\n\n\nU-Net architecture: The main idea of the U-Net architecture is to build an encoder-decoder FCN with skip connections between corresponding blocks, see figure below. The left side of U-Net, i.e., contractive path or encoder, is very similar to the left side of the FC architecture above. The right side of U-Net, i.e., expansive path or decoder, differs due to its number of feature channels and the convolutional + ReLu layers. Note that the input image size is greater than the output segmentation size, i.e., the network only segments the inner part of the image1.\n\n\n\n\n\n\n\n\nU-Net architecture as proposed by Ronneberger et al. (2015).\n\n\n\nMotivation: Semantic segmentation of images can be divided into two tasks\n\nContext Information Retrieval: Global information about the different parts of the image, e.g., in a CNN classification network after training there might be some feature representation for nose, eyes and mouth. Depending on the feature combination at hand, the network may classify the image as human or not human.\nLocalization of Context Information: In addition to what, localization ensures where. Semantic segmentation is only possible when content information can be localized. Note: In image classification, we are often not interested in where2.\n\nLong et al. (2015) argue that CNNs during classification tasks must learn useful feature representations, i.e., classification nets are capable to solve the context information retrieval task. Fully connected layers are inappropriate for semantic segmentation as they throw away the principle of localization. These two arguments motivate the use of FCNs that take the feature representation part of classification nets and convert fully connected layers into convolutions. During the contractive path, information gets compressed into coarse appearance/context information. However, in this process the dimensionality of the input is reduced massively. Skip connections are introduced to combine coarse, semantic information of deeper layers with finer, appearance information of early layers. Thereby, the localization task is addressed.\nRonneberger et al. (2015) extend these ideas by essentially increasing the capacity of the decoder path. The symmetric architecture allows to combine low level feature maps (left side, fine information) with high level feature maps (right side, coarse information) more effectively such that context information can be better propagated to higher resolution layers (top right). As a result, more precise segmentations can be retrieved even with few training examples, indicating that the optimization problem is better posed in U-Nets."
  },
  {
    "objectID": "paper_summaries/u_net/index.html#implementatation",
    "href": "paper_summaries/u_net/index.html#implementatation",
    "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
    "section": "Implementatation",
    "text": "Implementatation\nRonneberger et al. (2015) demonstrated U-Net application results for three different segmentation tasks and open-sourced their original U-Net implementation (or rather the ready trained network). The whole training process and data augmentation procedures are not provided (except for overlap-tile segmentation). The following reimplementation aims to give an understanding of the whole paper (data augmentation and training process included), while being as simple as possible. Note that there are lots of open-source U-Net reimplementations out there, however most of them are already modified versions.\n\nEM Dataset\nOnly the first task of the three different U-Net applications is reimplemented: The segmentation of neuronal structures in electron microscopic (EM) recordings. The training data consists of 30 images (\\(512 \\times 512\\) pixels with 8-bit grayscale) from the ventral nerve cord of some species of fruit flies together with the corresponding 30 binary segmentation masks (white pixels for segmented objects, black for the rest), see gif below. The dataset formed part of the 2D EM segmentation challenge at the ISBI 2012 conference. Although the workshop competition is done, the challenge remains open for new contributions. Further details about the data can be found at the ISBI Challenge website, where also the training and test data can be downloaded (after registration).\n\n\n\n\n\n\n\n\nEM training data. Taken from ISBI Challenge.\n\n\n\nThe following function can be used to load the training dataset.\n\n\nCode\nfrom PIL import Image\nimport torch\nimport torchvision.transforms as transforms\n\n\ndef load_dataset():\n    num_img, img_size = 30, 512\n    # initialize\n    imgs = torch.zeros(num_img, 1, img_size, img_size)\n    labels = torch.zeros(num_img, 1, img_size, img_size)\n    # fill tensors with data\n    for index in range(num_img):\n        cur_name = str(index) + '.png'\n\n        img_frame = Image.open('./Dataset/train/image/' + cur_name)\n        label_frame = Image.open('./Dataset/train/label/' + cur_name)\n\n        imgs[index] = transforms.ToTensor()(img_frame).type(torch.float32)\n        labels[index] = transforms.ToTensor()(label_frame).type(torch.float32)\n    return imgs, labels\n\n\nimgs, labels = load_dataset()\n\n\n\n\nData Augmentation\nTraining neural networks on image data typically requires large amounts of data to make the model robust (i.e., avoid overfitting) and accurate (i.e., avoid underfitting). However, data scarcity is a common problem in biomedical segmentation tasks, since labeling is expensive and time consuming. In such cases, data augmentation offers a solution by generating additional data (using plausible transformations) to expand the training dataset. In most image segmentation tasks the function to be learned has some transformation-invariance properties (e.g., translating the input should result in a translated output). The data augmentation applied by Ronneberger et al. (2015) can be divided into four parts:\n\nOverlap-tile strategy is used to divide an arbitrary large image into several overlaping parts (each forming an input and label to the training algorithm). Remind that the input to the neural network is greater than the output, in case of the EM dataset the input is even greater than the whole image. Therefore, Ronneberger et al. (2015) expand the images by mirroring at the sides. The overlap-tile strategy is shown below. Depending on the stride (i.e., how much the next rectangle is shifted to the right), the training dataset is enlarged by a factor greater than 4.\n\n\n\n\n\n\n\n\n\n\n\nOverlap-Tile Strategy for seamless segmentation of arbitrary large images. Blue area depicts input to neural network, yellow area corresponds to the prediction area. Missing input is extrapolated by mirroring (white lines). The number of tiles depends on the stride length (here: stride=124). Image created with visualize_overlap_tile_strategy (code presented at the end of this section).\n\n\n\nAffine transformations are mathematically defined as geometric transformations preserving lines and parallelisms, e.g., scaling, translation, rotation, reflection or any mix of them. Ronneberger et al. (2015) state that in case of microscopical image data mainly translation and rotation invariance (as affine transformation invariances) are desired properties of the resulting function. Note that the overlap-tile strategy itself leads to some translation invariance.\n\n\n\n\n\n\n\n\n\n\n\nAffine transformation visualization. Left side shows input and label data before transformation is applied. Right side shows the corresponding data after random affine transformation (random rotation and shifting). The grid is artificially added to emphasize that image and label are transformed in the same way. Image created with visualize_data_augmentation (code presented at the end of this section).\n\n\n\nElastic deformations are basically distinct affine transformations for each pixel. The term is probably derived from physics in which an elastic deformation describes a temporary change in shape of an elastic material (due to induced force). The transformation result looks similar to the physics phenomenon, see image below. Ronneberger et al. (2015) noted that elastic deformations seem to be a key concept for successfully training with few samples. A possible reason may be that the model’s generalization capabilities improve more by elastic deformations since the resulting images have more variability than with coherent affine transformations.\n\n\n\n\n\n\n\n\n\n\n\nElastic deformation visualization. Left side shows input and label data before deformation is applied. Right side shows the corresponding data after deformation. The grid is artificially added to emphasize that image and label are deformed in the same way. Image created with visualize_data_augmentation (code presented at the end of this section).\n\n\n\nImplementing elastic deformations basically consists of generating random displacement fields, convolving these with a Gaussian filter for smoothening, scaling the result by a predefined factor to control the intensity and then computing the new pixel values for each displacement vector (using interpolation within the old grid), see Best Practices for CNNs by Simard et al. (2003) for more details.     \nColor variations or in this case rather gray value variations in the input image should make the network invariant to small color changes. This can easily be implemented by adding Gaussian noise (other distributions may also be possible) to the input image, see image below.\n\n\n\n\n\n\n\n\n\n\n\nGray value variation visualization. Left side shows input image before noise is applied. Right side shows the corresponding data after transformation (segmentation mask does not change). Image created with visualize_data_augmentation (code presented at the end of this section).\n\n\n\n\nThe whole data augmentation process is put into a self written Pytorch Dataset class, see code below. Note that while this class includes all described transformations (affine transformation, elastic deformation and gray value variation), in the __get_item__ method only elastic_deform is applied to speed up the training process3. However, if you want to create a more sophisticated data augmentation process, you can easily add the other transformations in the __get_item__ method.\n\n\nCode\nfrom torch.utils.data import Dataset\nimport numpy as np\nfrom scipy.ndimage.interpolation import map_coordinates\nfrom scipy.signal import convolve2d\nimport torchvision.transforms.functional as TF\n\n\nclass EM_Dataset(Dataset):\n    \"\"\"EM Dataset (from ISBI 2012) to train U-Net on including data\n    augmentation as proposed by Ronneberger et al. (2015)\n\n    Args:\n        imgs (tensor): torch tensor containing input images [1, 512, 512]\n        labels (tensor): torch tensor containing segmented images [1, 512, 512]\n        stride (int): stride that is used for overlap-tile strategy,\n            Note: stride must be chosen such that all labels are retrieved\n        transformation (bool): transform should be applied (True) or not (False)\n    ------- transformation related -------\n        probability (float): probability that transformation is applied\n        alpha (float): intensity of elastic deformation\n        sigma (float): std dev. of Gaussian kernel, i.e., smoothing parameter\n        kernel dim (int): kernel size is [kernel_dim, kernel_dim]\n    \"\"\"\n\n    def __init__(self, imgs, labels, stride, transformation=False,\n                 probability=None, alpha=None, sigma=None, kernel_dim=None):\n        super().__init__()\n        assert isinstance(stride, int) and stride &lt;= 124 and \\\n          round((512-388)/stride) == (512-388)/stride\n        self.orig_imgs = imgs\n        self.imgs = EM_Dataset._extrapolate_by_mirroring(imgs)\n        self.labels = labels\n        self.stride = stride\n        self.transformation = transformation\n        if transformation:\n            assert 0 &lt;= probability &lt;= 1\n            self.probability = probability\n            self.alpha = alpha\n            self.kernel = EM_Dataset._create_gaussian_kernel(kernel_dim, sigma)\n        return\n\n    def __getitem__(self, index):\n        \"\"\"images and labels are divided into several overlaping parts using the\n        overlap-tile strategy\n        \"\"\"\n        number_of_tiles_1D = (1 + int((512 - 388)/self.stride))\n        number_of_tiles_2D = number_of_tiles_1D**2\n\n        img_index = int(index/number_of_tiles_2D)\n        # tile indexes of image\n        tile_index = (index % number_of_tiles_2D)\n        tile_index_x = (tile_index % number_of_tiles_1D) * self.stride\n        tile_index_y = int(tile_index / number_of_tiles_1D) * self.stride\n\n        img = self.imgs[img_index, :,\n                        tile_index_y:tile_index_y + 572,\n                        tile_index_x:tile_index_x + 572]\n        label = self.labels[img_index, :,\n                            tile_index_y: tile_index_y + 388,\n                            tile_index_x: tile_index_x + 388]\n        if self.transformation:\n            if np.random.random() &gt; 1 - self.probability:\n                img, label = EM_Dataset.elastic_deform(img, label, self.alpha,\n                                                       self.kernel)\n        return (img, label)\n\n    def __len__(self):\n        number_of_imgs = len(self.imgs)\n        number_of_tiles = (1 + int((512 - 388)/self.stride))**2\n        return number_of_imgs * number_of_tiles\n\n    @staticmethod\n    def gray_value_variations(image, sigma):\n        \"\"\"applies gray value variations by adding Gaussian noise\n\n        Args:\n            image (torch tensor): extrapolated image tensor [1, 572, 572]\n            sigma (float): std. dev. of Gaussian distribution\n\n        Returns:\n            image (torch tensor): image tensor w. gray value var. [1, 572, 572]\n        \"\"\"\n        # see https://stats.stackexchange.com/a/383976\n        noise = torch.randn(image.shape, dtype=torch.float32) * sigma\n        return image + noise\n\n    @staticmethod\n    def affine_transform(image, label, angle, translate):\n        \"\"\"applies random affine translations and rotation on image and label\n\n        Args:\n            image (torch tensor): extrapolated image tensor [1, 572, 572]\n            label (torch tensor): label tensor [1, 388, 388]\n            angle (float): rotation angle\n            translate (list): entries correspond to horizontal and vertical shift\n\n        Returns:\n            image (torch tensor): transformed image tensor [1, 572, 572]\n            label (torch tensor): transformed label tensor [1, 388, 388]\n        \"\"\"\n        # transform to PIL\n        image = transforms.ToPILImage()(image[0])\n        label = transforms.ToPILImage()(label[0])\n        # apply affine transformation\n        image = TF.affine(image, angle=angle, translate=translate,\n                          scale=1, shear=0)\n        label = TF.affine(label, angle=angle, translate=translate,\n                          scale=1, shear=0)\n        # transform back to tensor\n        image = transforms.ToTensor()(np.array(image))\n        label = transforms.ToTensor()(np.array(label))\n        return image, label\n\n    @staticmethod\n    def elastic_deform(image, label, alpha, gaussian_kernel):\n        \"\"\"apply smooth elastic deformation on image and label data as\n        described in\n\n        [Simard2003] \"Best Practices for Convolutional Neural Networks applied\n        to Visual Document Analysis\"\n\n        Args:\n            image (torch tensor): extrapolated image tensor [1, 572, 572]\n            label (torch tensor): label tensor [1, 388, 388]\n            alpha (float): intensity of transformation\n            gaussian_kernel (np array): gaussian kernel used for smoothing\n\n        Returns:\n            deformed_img (torch tensor): deformed image tensor [1, 572, 572]\n            deformed_label (torch tensor): deformed label tensor [1, 388, 388]\n\n        code is adapted from https://github.com/vsvinayak/mnist-helper\n        \"\"\"\n        # generate standard coordinate grids\n        x_i, y_i = np.meshgrid(np.arange(572), np.arange(572))\n        x_l, y_l = np.meshgrid(np.arange(388), np.arange(388))\n        # generate random displacement fields (uniform distribution [-1, 1])\n        dx = 2*np.random.rand(*x_i.shape) - 1\n        dy = 2*np.random.rand(*y_i.shape) - 1\n        # smooth by convolving with gaussian kernel\n        dx = alpha * convolve2d(dx, gaussian_kernel, mode='same')\n        dy = alpha * convolve2d(dy, gaussian_kernel, mode='same')\n        # one dimensional coordinates (neccessary for map_coordinates)\n        x_img = np.reshape(x_i + dx, (-1, 1))\n        y_img = np.reshape(y_i + dy, (-1, 1))\n        x_label = np.reshape(x_l + dx[92:480, 92:480], (-1, 1))\n        y_label = np.reshape(y_l + dy[92:480, 92:480], (-1, 1))\n        # deformation using map_coordinates interpolation (spline not bicubic)\n        deformed_img = map_coordinates(image[0], [y_img, x_img], order=1,\n                                       mode='reflect')\n        deformed_label = map_coordinates(label[0], [y_label, x_label], order=1,\n                                         mode='reflect')\n        # reshape into desired shape and cast to tensor\n        deformed_img = torch.from_numpy(deformed_img.reshape(image.shape))\n        deformed_label = torch.from_numpy(deformed_label.reshape(label.shape))\n        return deformed_img, deformed_label\n\n    @staticmethod\n    def _extrapolate_by_mirroring(data):\n        \"\"\"increase data by mirroring (needed for overlap-tile strategy)\n\n        Args:\n            data (torch tensor): shape [num_samples, 1, 512, 512]\n\n        Returns:\n            extrapol_data (torch tensor): shape [num_samples, 1, 696, 696]\n        \"\"\"\n        num_samples = len(data)\n        extrapol_data = torch.zeros(num_samples, 1, 696, 696)\n\n        # put data into center of extrapol data\n        extrapol_data[:,:, 92:92+512, 92:92+512] = data\n        # mirror left\n        extrapol_data[:,:, 92:92+512, 0:92] = data[:,:,:,0:92].flip(3)\n        # mirror right\n        extrapol_data[:,:, 92:92+512, 92+512::] = data[:,:,:,-92::].flip(3)\n        # mirror top\n        extrapol_data[:,:, 0:92,:] = extrapol_data[:,:,92:92+92,:].flip(2)\n        # mirror buttom\n        extrapol_data[:,:, 92+512::,:] = extrapol_data[:,:, 512:512+92,:].flip(2)\n        return extrapol_data\n\n    @staticmethod\n    def _create_gaussian_kernel(kernel_dim, sigma):\n        \"\"\"returns a 2D Gaussian kernel with the standard deviation\n        denoted by sigma\n\n        Args:\n            kernel_dim (int): kernel size will be [kernel_dim, kernel_dim]\n            sigma (float): std dev of Gaussian (smoothing parameter)\n\n        Returns:\n            gaussian_kernel (numpy array): centered gaussian kernel\n\n        code is adapted from https://github.com/vsvinayak/mnist-helper\n        \"\"\"\n        # check if the dimension is odd\n        if kernel_dim % 2 == 0:\n            raise ValueError(\"Kernel dimension should be odd\")\n        # initialize the kernel\n        kernel = np.zeros((kernel_dim, kernel_dim), dtype=np.float16)\n        # calculate the center point\n        center = kernel_dim/2\n        # calculate the variance\n        variance = sigma ** 2\n        # calculate the normalization coefficeint\n        coeff = 1. / (2 * variance)\n        # create the kernel\n        for x in range(0, kernel_dim):\n            for y in range(0, kernel_dim):\n                x_val = abs(x - center)\n                y_val = abs(y - center)\n                numerator = x_val**2 + y_val**2\n                denom = 2*variance\n\n                kernel[x,y] = coeff * np.exp(-1. * numerator/denom)\n        # normalise it\n        return kernel/sum(sum(kernel))\n\n\n# generate datasets\nstride = 124\nwhole_dataset = EM_Dataset(imgs, labels, stride=stride,\n                           transformation=True, probability=0.5, alpha=50,\n                           sigma=5, kernel_dim=25)\n\n\nThe visualization functions used to generate the images in this section are provided below:\n\n\nCode\nimport matplotlib.pyplot as plt\n\n\ndef visualize_overlap_tile_strategy(dataset, img_index, tile_indexes):\n    # compute tiling data\n    number_of_tiles_1D = (1 + int((512 - 388)/dataset.stride))\n    number_of_tiles_2D = number_of_tiles_1D**2\n    # original image [1, 512, 512]\n    orig_img = dataset.orig_imgs[img_index]\n    # extrapolated image [1, 696, 696]\n    extrapol_img = dataset.imgs[img_index]\n\n\n    # start plotting\n    fig = plt.figure(figsize=(14, 7))\n    # original image\n    plt.subplot(1, len(tile_indexes) + 1, 1)\n    plt.imshow(transforms.ToPILImage()(orig_img), cmap='gray')\n    plt.title('Original Image')\n    # extrapolated image with bounding boxes and mirror lines for tile_indexes\n    for index, tile_index in enumerate(tile_indexes):\n        plt.subplot(1, len(tile_indexes) + 1, 2 + index)\n        plt.imshow(transforms.ToPILImage()(extrapol_img), cmap='gray')\n        # calculate tile index x and y\n        tile_ix = (tile_index % number_of_tiles_1D) * dataset.stride\n        tile_iy = int(tile_index / number_of_tiles_1D) * dataset.stride\n        # add focus of current input tile\n        plt.plot([tile_ix, tile_ix + 572, tile_ix + 572, tile_ix, tile_ix],\n                 [tile_iy, tile_iy, tile_iy + 572, tile_iy + 572, tile_iy],\n                 'blue', linewidth=2)\n        # add focus of current segmentation mask\n        tile_ix, tile_iy = tile_ix + 92, tile_iy + 92\n        plt.plot([tile_ix, tile_ix + 388, tile_ix + 388, tile_ix, tile_ix],\n                 [tile_iy, tile_iy, tile_iy + 388, tile_iy + 388, tile_iy],\n                 'yellow', linewidth=2)\n        # add mirror lines\n        plt.vlines([92, 604], 0, 696, 'white', linewidth=1)\n        plt.hlines([92, 604], 0, 696, 'white', linewidth=1)\n        plt.title('Extrapolated Image, Tile '+ str(tile_index + 1) + '/' +\n                  str(number_of_tiles_2D))\n        plt.xlim(0, 696)\n        plt.ylim(696, 0)\n    return\n\n\ndef visualize_data_augmentation(dataset, index, show_grid, kind):\n    # get untransformed img, label\n    dataset.transformation = False\n    img, label = dataset[index]\n    # copy image (since it may be modified)\n    cur_img = img.clone().numpy()\n    cur_label = label.clone().numpy()\n    if show_grid:\n        # modify image to include outer grid (outside of label)\n        cur_img[0, 0:91:25] = 10.0\n        cur_img[0, 480::25] = 10.0\n        cur_img[0, :, 0:91:25] = 10.0\n        cur_img[0, :, 480::25] = 10.0\n        # modify image to include label grid\n        cur_img[0, 92:480:20, 92:480] = -5\n        cur_img[0,  92:480, 92:480:20] = -5\n        # modify label to include label grid\n        cur_label[0, ::20] = -5\n        cur_label[0, :, ::20] = -5\n    if kind == 'elastic deformation':\n        # set transformation\n        kernel = dataset.kernel\n        alpha = dataset.alpha\n        new_img, new_label = EM_Dataset.elastic_deform(cur_img, cur_label,\n                                                       alpha, kernel)\n    elif kind == 'affine transformation':\n        angle = np.random.randint(-3, 3)\n        translate = list(np.random.randint(-3, 3, size=2))\n        new_img, new_label = EM_Dataset.affine_transform(cur_img, cur_label,\n                                                         angle, translate)\n    elif kind == 'gray value variation':\n        sigma = 0.2\n        new_img = EM_Dataset.gray_value_variations(img, sigma)\n        new_label = label\n    else:\n        raise NameError('Unknown `kind`, can only be `elastic deformation`, ' +\n                        '`affine transformation` or `gray value variation`')\n    # start plotting\n    fig = plt.figure(figsize=(10,10))\n    plt.subplot(2, 2, 1)\n    plt.title('Before ' + kind)\n    plt.imshow(cur_img[0], cmap='gray', aspect='equal',\n               interpolation='gaussian', vmax=1, vmin=0)\n    # focus of current segmentation mask\n    plt.plot([92, 480, 480, 92, 92], [92, 92, 480, 480, 92],\n            'yellow', linewidth=2)\n    plt.subplots_adjust(hspace=0.01)\n    plt.subplot(2,2,3)\n    plt.imshow(cur_label[0], cmap='gray', aspect='equal',\n               interpolation='gaussian', vmax=1, vmin=0)\n    plt.subplot(2,2,2)\n    plt.title('After ' + kind)\n    plt.imshow(new_img[0], cmap='gray', aspect='equal',\n               interpolation='gaussian', vmax=1, vmin=0)\n    # focus of current segmentation mask\n    plt.plot([92, 480, 480, 92, 92], [92, 92, 480, 480, 92],\n            'yellow', linewidth=2)\n    plt.subplot(2,2,4)\n    plt.imshow(new_label[0], cmap='gray', aspect='equal',\n               interpolation='gaussian', vmax=1, vmin=0)\n    return\n\n\n# generate images in order of appearance\nvisualize_overlap_tile_strategy(whole_dataset, img_index=0,\n                                tile_indexes=[0, 1])\nvisualize_data_augmentation(whole_dataset, index=0, show_grid=True,\n                            kind='affine transformation')\nvisualize_data_augmentation(whole_dataset, index=0, show_grid=True,\n                            kind='elastic deformation')\nvisualize_data_augmentation(whole_dataset, index=0, show_grid=False,\n                            kind='gray value variation')\n\n\n\n\nModel Implementation\nModel implementation can be divided into three tasks:\n\nNetwork Architecture: The model architecture is given in the model description in which one can identify several blocks of two \\(3 \\times 3\\) convolutional layers each followed by a ReLU non-linearity (called _block in the implementation). Note that the output of the last prediction layer can be understood as the unnormalized prediction for each class, i.e., \\(a_{i,j}^{(k)} \\in ] -\\infty, +\\infty[\\) where \\(a^{(k)}\\) denotes the activation in feature channel \\(k \\in \\{1, 2\\}\\) (one channel for each class) and the indices \\({i,j}\\) describe the pixel position. In order to get normalized probabilities for each pixel \\(\\hat{p}_{i,j}^{(k)}\\), a pixel-wise softmax is applied at the end (last operation in forward), i.e., after this operation the sum of the two output channels equals one for each pixel \\(\\hat{p}_{i,j}^{(1)} + \\hat{p}_{i,j}^{(2)} = 1\\).\n\n\nCode\nfrom torch import nn\n\n\nclass Unet(nn.Module):\n    \"\"\"original U-Net architecture proposed by Ronneberger et al. (2015)\n\n    Attributes:\n        encoder_blocks (list):  four u_net blocks of encoder path\n        bottleneck_bock: block that mediates between encoder and decoder\n        decoder_blocks (list):  four u_net blocks of decoder path\n        cropped_img_size (list): cropped images size in order of encoder blocks\n        up_convs (list): upsampling (transposed convolutional) layers (decoder)\n        max_pool: max pool operation used in encoder path\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.encoder_blocks = nn.ModuleList([\n            Unet._block(1, 64),\n            Unet._block(64, 128),\n            Unet._block(128, 256),\n            Unet._block(256, 512)\n        ])\n        self.bottleneck_block = Unet._block(512, 1024)\n        self.decoder_blocks = nn.ModuleList([\n            Unet._block(1024, 512),\n            Unet._block(512, 256),\n            Unet._block(256, 128),\n            Unet._block(128, 64)\n        ])\n        self.cropped_img_sizes = [392, 200, 104, 56]\n        self.up_convs = nn.ModuleList([\n            nn.ConvTranspose2d(1024, 512, kernel_size=(2,2), stride=2),\n            nn.ConvTranspose2d(512, 256, kernel_size=(2,2), stride=2),\n            nn.ConvTranspose2d(256, 128, kernel_size=(2,2), stride=2),\n            nn.ConvTranspose2d(128, 64, kernel_size=(2,2), stride=2),\n        ])\n        self.max_pool = nn.MaxPool2d(kernel_size=(2,2))\n        self.prediction = nn.Conv2d(64, 2, kernel_size=(1,1), stride=1)\n        return\n\n    def forward(self, x):\n        # go through encoder path and store cropped images\n        cropped_imgs = []\n        for index, encoder_block in enumerate(self.encoder_blocks):\n            out = encoder_block(x)\n            # center crop and add to cropped image list\n            cropped_img = Unet._center_crop(out, self.cropped_img_sizes[index])\n            cropped_imgs.append(cropped_img)\n            # max pool output of encoder block\n            x = self.max_pool(out)\n        # bottleneck block (no max pool)\n        x = self.bottleneck_block(x)  # [batch_size, 1024, 28, 28]\n        # go through decoder path with stored cropped images\n        for index, decoder_block in enumerate(self.decoder_blocks):\n            x = self.up_convs[index](x)\n            # concatenate x and cropped img along channel dimension\n            x = torch.cat((cropped_imgs[-1-index], x), 1)\n            # feed through decoder_block\n            x = decoder_block(x)\n        # feed through prediction layer [batch_size, 2, 388, 388]\n        x_pred_unnormalized = self.prediction(x)\n        # normalize prediction for each pixel\n        x_pred = torch.softmax(x_pred_unnormalized, 1)\n        return x_pred\n\n    @staticmethod\n    def _center_crop(x, new_size):\n        \"\"\"center croping of a square input tensor\n\n        Args:\n            x: input tensor shape [batch_size, channels, resolution, resolution]\n            new_size: the desired output resolution (taking center of input)\n\n        Returns:\n            x_cropped: tensor shape [batch_size, channels, new_size, new_size]\n        \"\"\"\n        img_size = x.shape[-1]\n        i_start = int((img_size - new_size)/2)\n        i_end = int((img_size + new_size)/2)\n        x_cropped = x[:, :, i_start:i_end, i_start:i_end]\n        return x_cropped\n\n    @staticmethod\n    def _block(in_channels, out_channels):\n        \"\"\"block for use in U-Net architecture,\n        consists of two conv 3x3, ReLU layers\n\n        Args:\n            in_channels: number of input channels for first convolution\n            out_channels: number of output channels for both convolutions\n\n        Returns:\n            u_net_block: Sequential U net block\n        \"\"\"\n        conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(3,3), stride=1)\n        conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(3,3), stride=1)\n\n        N_1, N_2 = 9*in_channels, 9*out_channels\n        # initialize by drawing weights from Gaussian distribution\n        conv1.weight.data.normal_(mean=0, std=np.sqrt(2/N_1))\n        conv2.weight.data.normal_(mean=0, std=np.sqrt(2/N_2))\n        # define u_net_block\n        u_net_block = nn.Sequential(\n            conv1,\n            nn.ReLU(),\n            conv2,\n            nn.ReLU()\n        )\n        return u_net_block\n\n\nLoss Function: Since the segmentation labels are clearly imbalanced (much more white pixels than black pixels), Ronneberger et al. (2015) use the weighted cross entropy as the loss function (which they term energy function)\n\\[\n\\begin{align}\n   J (\\textbf{x}, \\textbf{m}) &= -\\sum_{i=1}^{388}\\sum_{j=1}^{388}\n   \\sum_{k=1}^2 w_{i,j} (\\textbf{m}) \\cdot\n   p_{i,j}^{(k)} \\log \\left( \\widehat{p}_{i,j}^{(k)} \\left( \\textbf{x};\n\\boldsymbol{\\theta} \\right)  \\right) \\\\\n   &\\text{with} \\quad p_{i,j}^{(1)} = \\begin{cases} 1 & \\text{if }\nm_{i,j}=1 \\\\ 0 &\\text{else} \\end{cases} \\quad \\text{and} \\quad p_{i,j}^{(2)} =\n\\begin{cases} 1 & \\text{if } m_{i, j} = 0 \\\\0 & \\text{else}, \\end{cases}\n\\end{align}\n\\]\nwhere \\(\\textbf{x}\\in [0, 1]^{572\\times 572}\\) denotes the input image, \\(\\textbf{m} \\in \\{0, 1\\}^{388 \\times 388}\\) the corresponding segmentation mask, \\(\\textbf{p}^{(k)}\\in \\{0, 1\\}^{388 \\times 388}\\) the groundtruth probability for each class \\(k\\), \\(\\widehat{\\textbf{p}}^{(k)} \\in [0, 1]^{388\\times 388}\\) denotes the \\(k\\)-th channel output of the network parameterized by \\(\\boldsymbol{\\theta}\\) and \\(\\textbf{w} \\left( \\textbf{m} \\right) \\in \\mathbb{R}^{388 \\times 388}\\) is a introduced weight map (computed via the segmentation mask \\(\\textbf{m}\\)) to give some pixels more importance during training. Accordingly, the loss function can be interpreted as penalizing the deviation from 1 for each true class output pixel weighted by the corresponding entry of the weight map.\nWeight Map: To compensate for the imbalance between separation borders and segmented object4, Ronneberger et al. (2015) introduce the following weight map\n\\[\n  w(\\textbf{m}) = {w_c (\\textbf{m})} + {w_0 \\cdot \\exp \\left( - \\frac\n  {\\left(d_1 (\\textbf{m}) - d_2 (\\textbf{m})\\right)^2}{2\\sigma^2}\\right)},\n\\]\nwhere the first term reweights each pixel of the minority class (i.e., black pixels) to balance the class frequencies. In the second term \\(d_1\\) and \\(d_2\\) denote the distance to the border of the nearest and second nearest cell, respectively. \\(w_0\\) and \\(\\sigma\\) are predefined hyperparameters. Thus, the second term can be understood as putting additional weight to smaller borders, see code and image below.\n\n\nCode\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nfrom skimage import measure\nfrom scipy.ndimage.morphology import distance_transform_edt\nfrom skimage.segmentation import find_boundaries\n\n\ndef compute_weight_map(label_mask, w_0, sigma, plot=False):\n    \"\"\"compute weight map for each ground truth segmentation to compensate\n    for the different class frequencies and to put additional\n    emphasis on small borders as proposed by Ronneberger et al.\n\n    Args:\n        label mask (torch tensor): true segmentation masks [batch_size, 1, 388, 388]\n        w_0 (float): hyperparameter in second term of weight map\n        sigma (float): hyperparameter in second term of weight map\n\n    Returns:\n        weight_map (torch tensor): computed weight map [batch_size, 1, 388, 388]\n\n    researchgate.net/post/creating_a_weight_map_from_a_binary_image_U-net_paper\n    \"\"\"\n    batch_size = label_mask.shape[0]\n    weight_map = torch.zeros_like(label_mask)\n    for i in range(batch_size):\n        # compute w_c to balance class frequencies\n        w_c = label_mask[i][0].clone()\n        class_freq_0 = (label_mask[i]==0).sum().item()\n        class_freq_1 = (label_mask[i]==1).sum().item()\n        w_c[label_mask[i][0]==0] = class_freq_1 / class_freq_0\n        # compute d_1, d_2, i.e., euclid. dist. to border of (1st/2nd) closest cell\n        d_1 = np.zeros(label_mask[i][0].shape)\n        d_2 = np.zeros(label_mask[i][0].shape)\n        # distinguish all cells (connected components of ones)\n        all_cells = measure.label(label_mask[i][0], background=0, connectivity=2)\n        num_cells = np.max(all_cells)\n        # initialize distances for all cells\n        dists = np.zeros([num_cells, d_2.shape[0], d_2.shape[1]])\n        # iterate over all zero components\n        for index, i_cell in enumerate(range(1, num_cells + 1)):\n            # cell segmentation (segmented cell 1, rest 0)\n            cell_segmentation = all_cells==i_cell\n            # find boundary (boundary 1, rest 0)\n            boundary = find_boundaries(cell_segmentation, mode='inner')\n            # compute distance to boundary (set boundary 0, rest -1)\n            bound_dists = distance_transform_edt(1 - boundary)\n            dists[index] = bound_dists\n        # sort dists along first axis (each pixel)\n        dists.sort(axis=0)\n        d_1 = dists[0]\n        d_2 = dists[1]\n        w = w_c + w_0 * np.exp(- (d_1 + d_2)**2/(2*sigma**2))\n        # save w to weight map\n        weight_map[i, 0] = w\n\n        # visualize weight map\n        if plot and i==0:\n            fig = plt.figure(figsize=(18, 14))\n\n            ax = plt.subplot(1, 3, 1)\n            plt.title('Segmenation Mask')\n            plt.imshow(label_mask[0, 0], cmap='gray')\n            divider = make_axes_locatable(ax)\n            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n            plt.colorbar(cax=cax)\n\n            ax = plt.subplot(1, 3, 2)\n            plt.title('w_c')\n            plt.imshow(w_c, cmap='jet')\n            divider = make_axes_locatable(ax)\n            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n            plt.colorbar(cax=cax)\n\n\n            ax = plt.subplot(1, 3, 3)\n            plt.title('w')\n            plt.imshow(w, cmap='jet')\n            divider = make_axes_locatable(ax)\n            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n            plt.colorbar(cax=cax)\n    return weight_map\n\n\nimg, label_mask = whole_dataset[0]\nweight_map = compute_weight_map(label_mask.unsqueeze(0), w_0=10, sigma=5, plot=True)\n\n\n\n\n\nTraining Procedure: A simple SGD (Stochastic Gradient Descent) optimizer with a high momentum (0.99) and a batch_size of 1 are choosen for training as proposed by Ronneberger et al. (2015), see code below. Note that we take the mean instead of the sum in the loss function calculation to avoid overflow (i.e., nans). This will only change the strength of a gradient step (which can be adjusted by the learning rate), but not its direction.\n\n\nCode\nfrom livelossplot import PlotLosses\nfrom torch.utils.data import DataLoader\n\n\ndef train(u_net, dataset, epochs):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    # hyperparameters weight map\n    w_0, sigma = 10, 5\n\n    print('Device: {}'.format(device))\n\n    data_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n\n    u_net.to(device)\n    optimizer = torch.optim.SGD(u_net.parameters(), lr=0.001, momentum=0.99)\n\n    losses_plot = PlotLosses()\n    for epoch in range(epochs):\n        avg_loss = 0\n        for counter, (imgs, label_masks) in enumerate(data_loader):\n            u_net.zero_grad()\n            # retrieve predictions of u_net [batch, 2, 388, 388]\n            pred_masks = u_net(imgs.to(device))\n            # compute weight map\n            weight_map = compute_weight_map(label_masks, w_0, sigma).to(device)\n            # put label_masks to device\n            label_masks = label_masks.to(device)\n            # compute weighted binary cross entropy loss\n            loss = -(weight_map*\n                    (pred_masks[:, 0:1].log() * label_masks +\n                      pred_masks[:, 1:2].log() * (1 - label_masks))\n                    ).mean()\n            loss.backward()\n            optimizer.step()\n\n            avg_loss += loss.item() / len(dataset)\n\n            losses_plot.update({'current weighted loss': loss.item()},\n                              current_step=epoch + counter/len(data_loader))\n            losses_plot.draw()\n        losses_plot.update({'avg weighted loss': avg_loss},\n                          current_step=epoch + 1)\n        losses_plot.draw()\n    trained_u_net = u_net\n    return trained_u_net\n\n\nBeware: Training for 30 epochs (i.e., the code below) takes about 2 hours with a NVIDIA Tesla K80 as GPU. The loss plot (see below avg weighted loss) indicates that training for more epochs might improve the model even more5. For people who are interested in using the model without waiting for 2 hours, I stored a trained version on nextjournal.\n\n\nCode\nu_net = Unet()\nepochs = 30\n# all image indexes\nidx = np.arange(30)\n# random inplace shuffling of indexes\nnp.random.seed(1)\nnp.random.shuffle(idx)\n# split data into training and test data\ntrain_imgs, train_labels = imgs[idx[0:25]], labels[idx[0:25]]\ntest_imgs, test_labels = imgs[idx[25:]], labels[idx[25:]]\n# generate datasets\nstride = 124\ntrain_dataset = EM_Dataset(train_imgs, train_labels, stride=stride,\n                          transformation=True, probability=0.7, alpha=50,\n                          sigma=5, kernel_dim=25)\ntest_dataset = EM_Dataset(test_imgs, test_labels, stride=stride,\n                          transformation=False)\n# start training procedure\ntrained_u_net = train(u_net, train_dataset, epochs)\n\n\n\n\n\n\nResults\nLet’s look at some image segmentations generated by the trained model on the unseen test set:\n\n\nCode\ndef visualize_results(trained_u_net, test_dataset, num_test_images=None):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    # take random tile from each test image\n    num_tiles = (1 + int((512 - 388)/test_dataset.stride))**2\n    num_images = int(len(test_dataset) / num_tiles)\n    if num_test_images:\n        # number of images &lt; number of images in test set\n        num_images = min(num_test_images, num_images)\n    random_tile_idx = np.random.choice(range(num_tiles), num_images,\n                                       replace=True)\n\n    fig = plt.figure(figsize=(num_images*6, 10))\n    # annotation plots\n    ax = plt.subplot(3, num_images + 1, 1)\n    ax.annotate('cell image\\n(input)', xy=(1, 0.5), xycoords='axes fraction',\n                 fontsize=14, va='center', ha='right')\n    ax.set_aspect('equal')\n    ax.axis('off')\n    ax = plt.subplot(3, num_images + 1, num_images + 2)\n    ax.annotate('true segmentation\\n(label)', xy=(1, 0.5),\n                xycoords='axes fraction', fontsize=14, va='center', ha='right')\n    ax.set_aspect('equal')\n    ax.axis('off')\n    ax = plt.subplot(3, num_images + 1, 2*(num_images + 1) + 1)\n    ax.annotate('U-net prediction', xy=(1, 0.5), xycoords='axes fraction',\n                 fontsize=14, va='center', ha='right')\n    ax.set_aspect('equal')\n    ax.axis('off')\n    # image, label, predicted label plots\n    for index in range(num_images):\n        img, label = test_dataset[index*num_tiles + random_tile_idx[index]]\n        label_pred = u_net(img.unsqueeze(0).to(device)).squeeze(0)[0] &gt; 0.5\n\n        # plot original image\n        plt.subplot(3, num_images + 1, index + 2)\n        plt.imshow(transforms.ToPILImage()(img), cmap='gray')\n        plt.plot([92, 480, 480, 92, 92], [92, 92, 480, 480, 92],\n                 'yellow', linewidth=2)\n        plt.xticks([])\n        plt.yticks([])\n        # plot original segmentation mask\n        plt.subplot(3, num_images + 1, index + num_images + 3)\n        plt.imshow(transforms.ToPILImage()(label), cmap='gray')\n        plt.xticks([])\n        plt.yticks([])\n        # plot prediction segmentation mask\n        plt.subplot(3, num_images + 1, index + 2*(num_images + 1) + 2)\n        plt.imshow(label_pred.detach().cpu().numpy(), cmap='gray')\n        plt.xticks([])\n        plt.yticks([])\n    return\n\n\nvisualize_results(trained_u_net, test_dataset, num_test_images=3)\n\n\n\n\n\nvisualize results\n\n\nThe predictions are pretty decent, though far from perfect. Bear in mind, that our model had only 25 example images to learn from and that training for more epochs might have led to even better predictions."
  },
  {
    "objectID": "paper_summaries/u_net/index.html#footnotes",
    "href": "paper_summaries/u_net/index.html#footnotes",
    "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA greater input image than output segmentation size makes sense since the network has no information about the surrounding of the input image.↩︎\nActually, CNNs should put more emphasis on the where or rather the local relation between context information, see Geoffrey Hinton’s comment about pooling.↩︎\nThe implementation intends to be easily understandable, while keeping the computational resources low. Thus, it is not aimed to generate the best training results or model performance.↩︎\nSince the separation borders are much smaller than the segmented objects, the network could be trapped into merging touching objects without being penalized enough.↩︎\nThe implementation intends to be easily understandable, while keeping the computational resources low. Thus, it is not aimed to generate the best training results or model performance.↩︎"
  },
  {
    "objectID": "paper_summaries/spatial_transformer/index.html",
    "href": "paper_summaries/spatial_transformer/index.html",
    "title": "Spatial Transformer Networks",
    "section": "",
    "text": "Jaderberg et al. (2015) introduced the learnable Spatial Transformer (ST) module that can be used to empower standard neural networks to actively spatially transform feature maps or input data. In essence, the ST can be understood as a black box that applies some spatial transformation (e.g., crop, scale, rotate) to a given input (or part of it) conditioned on the particular input during a single forward path. In general, STs can also be seen as a learnable attention mechanism (including spatial transformation on the region of interest). Notably, STs can be easily integrated in existing neural network architectures without any supervision or modification to the optimization, i.e., STs are differentiable plug-in modules. The authors could show that STs help the models to learn invariances to translation, scale, rotation and more generic warping which resulted in state-of-the-art performance on several benchmarks, see image below."
  },
  {
    "objectID": "paper_summaries/spatial_transformer/index.html#model-description",
    "href": "paper_summaries/spatial_transformer/index.html#model-description",
    "title": "Spatial Transformer Networks",
    "section": "Model Description",
    "text": "Model Description\nThe aim of STs is to provide neural networks with spatial transformation and attention capabilities in a reasonable and efficient way. Note that standard neural network architectures (e.g., CNNs) are limited in this regard1. Therefore, the ST constitutes parametrized transformations \\(\\mathcal{T}_{\\boldsymbol{\\theta}}\\) that transform the regular input grid to a new sampling grid, see image below. Then, some form of interpolation is used to compute the pixel values in the new sampling grid (i.e., interpolation between values of the old grid).\n\n\n\n\n\n\n\n\nTwo examples of applying the parametrised sampling grid to an image \\(\\textbf{U}\\) producing the output \\(\\textbf{V}\\). The green dots represent the new sampling grid which is obtained by transforming the regular grid \\(\\textbf{G}\\) (defined on \\(\\textbf{V}\\)) using the transformation \\(\\mathcal{T}\\).  (a) The sampling grid is the regular grid \\(\\textbf{G} = \\mathcal{T}_{\\textbf{I}} (\\textbf{G})\\), where \\(\\textbf{I}\\) is the identity transformation matrix.  (b) The sampling grid is the result of warping the regular grid with an affine transformation \\(\\mathcal{T}_{\\boldsymbol{\\theta}} (\\textbf{G})\\).  Taken from Jaderberg et al. (2015).\n\n\n\nTo this end, the ST is divided into three consecutive parts:\n\nLocalisation Network: Its purpose is to retrieve the parameters \\(\\boldsymbol{\\theta}\\) of the spatial transformation \\(\\mathcal{T}_{\\boldsymbol{\\theta}}\\) taking the current feature map \\(\\textbf{U}\\) as input, i.e., \\(\\boldsymbol{\\theta} = f_{\\text{loc}} \\left(\\textbf{U} \\right)\\). Thereby, the spatial transformation is conditioned on the input. Note that dimensionality of \\(\\boldsymbol{\\theta}\\) depends on the transformation type which needs to be defined beforehand, see some examples below. Furthermore, the localisation network can take any differentiable form, e.g., a CNN or FCN.\n\nExamples of Spatial Transformations\n\nThe following examples highlight how a regular grid\n\\[\n\\textbf{G} = \\left\\{ \\begin{bmatrix} x_i^t \\\\ y_i^t \\end{bmatrix}\n\\right\\}_{i=1}^{H^t \\cdot W^t}\n\\]\ndefined on the output/target map \\(\\textbf{V}\\) (i.e., \\(H^t\\) and \\(W^t\\) denote height and width of \\(\\textbf{V}\\)) can be transformed into a new sampling grid\n\\[\n\\widetilde{\\textbf{G}} = \\left\\{ \\begin{bmatrix} x_i^s \\\\ y_i^s \\end{bmatrix}\n\\right\\}_{i=1}^{H^s \\cdot W^s}\n\\]\ndefined on the input/source feature map \\(\\textbf{U}\\) using a parametrized transformation \\(\\mathcal{T}_{\\boldsymbol{\\theta}}\\), i.e., \\(\\widetilde{G} = T_{\\boldsymbol{\\theta}} (G)\\). Visualizations have bee created by me, interactive versions can be found here.\n\n\n\n\n\n\n\n\n\n\n\n\nThis transformation allows cropping, translation, rotation, scale and skew to be applied to the input feature map. It has 6 degrees of freedom (DoF).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis transformation is more constrained with only 3-DoF. Therefore it only allows cropping, translation and isotropic scaling to be applied to the input feature map.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis transformation has 8-DoF and can be seen as an extension to the affine transformation. The main difference is that affine transformations are constrained to preserve parallelism.\n\n\n\n\nGrid Generator: Its purpose to create the new sampling grid \\(\\widetilde{\\textbf{G}}\\) on the input feature map \\(\\textbf{U}\\) by applying the predefined parametrized transformation using the parameters \\(\\boldsymbol{\\theta}\\) obtained from the localisation network, see examples above.   \nSampler: Its purpose is to compute the warped version of the input feature map \\(\\textbf{U}\\) by computing the pixel values in the new sampling grid \\(\\widetilde{\\textbf{G}}\\) obtained from the grid generator. Note that the new sampling grid does not necessarily align with the input feature map grid, therefore some kind of interpolation is needed. Jaderberg et al. (2015) formulate this interpolation as the application of a sampling kernel centered at a particular location in the input feature map, i.e.,\n\\[\n  V_i^c = \\sum_{n=1}^{H^s} \\sum_{m=1}^{W^s} U_{n,m}^c \\cdot \\underbrace{k(x_i^s - x_m^t;\n  \\boldsymbol{\\Phi}_x)}_{k_{\\boldsymbol{\\Phi}_x}} \\cdot \\underbrace{k(y_i^s - y_n^t; \\boldsymbol{\\Phi}_y)}_{k_{\\boldsymbol{\\Phi}_y}},\n\\]\nwhere \\(V_i^c \\in \\mathbb{R}^{W^t \\times H^t}\\) denotes the new pixel value of the \\(c\\)-th channel at the \\(i\\)-th position of the new sampling grid coordinates2 \\(\\begin{bmatrix} x_i^s & y_i^s\\end{bmatrix}^{T}\\) and \\(\\boldsymbol{\\Phi}_x, \\boldsymbol{\\Phi}_y\\) are the parameters of a generic sampling kernel \\(k()\\) which defines the image interpolation. As the sampling grid coordinates are not channel-dependent, each channel is transformed in the same way resulting in spatial consistency between channels. Note that although in theory we need to sum over all input locations, in practice we can ignore this sum by just looking at the kernel support region for each \\(V_i^c\\) (similar to CNNs).\nThe sampling kernel can be chosen freely as long as (sub-)gradients can be defined with respect to \\(x_i^s\\) and \\(y_i^s\\). Some possible choices are shown below.\n\\[\n  \\begin{array}{lcc}\n  \\hline\n    \\textbf{Interpolation Method} & k_{\\boldsymbol{\\Phi}_x} &\n  k_{\\boldsymbol{\\Phi}_x} \\\\ \\hline\n    \\text{Nearest Neightbor} &  \\delta( \\lfloor x_i^s + 0.5\\rfloor -\n  x_m^t) &  \\delta( \\lfloor y_i^s + 0.5\\rfloor - y_n^t) \\\\\n    \\text{Bilinear} &  \\max \\left(0, 1 -  \\mid x_i^s - x_m^t \\mid\n  \\right) &  \\max (0, 1 - \\mid y_i^s - y_m^t\\mid ) \\\\ \\hline\n  \\end{array}\n\\]\n\nThe figure below summarizes the ST architecture and shows how the individual parts interact with each other.\n\n\n\n\n\n\n\n\nArchitecture of ST Module. Taken from Jaderberg et al. (2015).\n\n\n\nMotivation: With the introduction of GPUs, convolutional layers enabled computationally efficient training of feature detectors on patches due to their weight sharing and local connectivity concepts. Since then, CNNs have proven to be the most powerful framework when it comes to computer vision tasks such as image classification or segmentation.\nDespite their success, Jaderberg et al. (2015) note that CNNs are still lacking mechanisms to be spatially invariant to the input data in a computationally and parameter efficient manner. While convolutional layers are translation-equivariant to the input data and the use of max-pooling layers has helped to allow the network to be somewhat spatially invariant to the position of features, this invariance is limited to the (typically) small spatial support of max-pooling (e.g., \\(2\\times 2\\)). As a result, CNNs are typically not invariant to larger transformations, thus need to learn complicated functions to approximate these invariances.\n\n\nWhat if we could enable the network to learn transformations of the input data? This is the main idea of STs! Learning spatial invariances is much easier when you have spatial transformation capabilities. The second aim of STs is to be computationally and parameter efficient. This is done by using structured, parameterized transformations which can be seen as a weight sharing scheme."
  },
  {
    "objectID": "paper_summaries/spatial_transformer/index.html#implementation",
    "href": "paper_summaries/spatial_transformer/index.html#implementation",
    "title": "Spatial Transformer Networks",
    "section": "Implementation",
    "text": "Implementation\nJaderberg et al. (2015) performed several supervised learning tasks (distorted MNIST, Street View House Numbers, fine-grained bird classification) to test the performance of a standard architecture (FCN or CNN) against an architecture that includes one or several ST modules. They could emperically validate that including STs results in performance gains, i.e., higher accuracies across multiple tasks.\nThe following reimplementation aims to reproduce a subset of the distored MNIST experiment (RTS distorted MNIST) comparing a standard CNN with a ST-CNN architecture. A starting point for the implementation was this pytorch tutorial by Ghassen Hamrouni.\n\nRTS Distorted MNIST\nWhile Jaderberg et al. (2015) explored multiple distortions on the MNIST handwriting dataset, this reimplementation focuses on the rotation-translation-scale (RTS) distorted MNIST, see image below. As described in appendix A.4 of Jaderberg et al. (2015) this dataset can easily be generated by augmenting the standard MNIST dataset as follows: * randomly rotate by sampling the angle uniformly in \\([+45^{\\circ}, 45^{\\circ}]\\), * randomly scale by sampling the factor uniformly in \\([0.7, 1.2]\\), * translate by picking a random location on a \\(42\\times 42\\) image (MNIST digits are \\(28 \\times 28\\)).\n\n\n\n\n\n\n\n\n\n\n\nRTS Distorted MNIST Examples\n\n\n\nNote that this transformation could also be used as a data augmentation technique, as the resulting images remain (mostly) valid digit representations (humans could still assign correct labels).\nThe code below can be used to create this dataset:\nimport torch\nfrom torchvision import datasets, transforms\n\n\ndef load_data():\n    \"\"\"loads MNIST datasets with 'RTS' (rotation, translation, scale)\n    transformation\n\n    Returns:\n        train_dataset (torch dataset): training dataset\n        test_dataset (torch dataset): test dataset\n    \"\"\"\n    def place_digit_randomly(img):\n        new_img = torch.zeros([42, 42])\n        x_pos, y_pos = torch.randint(0, 42-28, (2,))\n        new_img[y_pos:y_pos+28, x_pos:x_pos+28] = img\n        return new_img\n\n    transform = transforms.Compose([\n        transforms.RandomAffine(degrees=(-45, 45),\n                                scale=(0.7, 1.2)),\n        transforms.ToTensor(),\n        transforms.Lambda(lambda img: place_digit_randomly(img)),\n        transforms.Lambda(lambda img: img.unsqueeze(0))\n    ])\n    train_dataset = datasets.MNIST('./data', transform=transform,\n                                   train=True, download=True)\n    test_dataset = datasets.MNIST('./data', transform=transform,\n                                   train=True, download=True)\n    return train_dataset, test_dataset\n\n\ntrain_dataset, test_dataset = load_data()\n\n\nModel Implementation\nThe model implementation can be divided into three tasks:\n\nNetwork Architectures: The network architectures are based upon the description in appendix A.4 of Jaderberg et al. (2015). Note that there is only one ST at the beginning of the network such that the resulting transformation is only applied over one channel (input channel). For the sake of simplicity, we only implement an affine transformation matrix. Clearly, including an ST increases the networks capacity due to the number of added trainable parameters. To allow for a fair comparison, we therefore increase the capacity of the convolutional and linear layers in the standard CNN.\nThe code below creates both architectures and counts their trainable parameters.\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\n\n\ndef get_number_of_trainable_parameters(model):\n  \"\"\"taken from\n  discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325\n  \"\"\"\n  model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n  params = sum([np.prod(p.size()) for p in model_parameters])\n  return params\n\n\nclass CNN(nn.Module):\n\n  def __init__(self, img_size=42, include_ST=False):\n      super(CNN, self).__init__()\n      self.ST = include_ST\n      self.name = 'ST-CNN Affine' if include_ST else 'CNN'\n      c_dim = 32 if include_ST else 36\n      self.convs = nn.Sequential(\n          nn.Conv2d(1, c_dim, kernel_size=9, stride=1, padding=0),\n          nn.MaxPool2d(kernel_size=(2,2), stride=2),\n          nn.ReLU(True),\n          nn.Conv2d(c_dim, c_dim, kernel_size=7, stride=1, padding=0),\n          nn.MaxPool2d(kernel_size=(2,2), stride=2),\n          nn.ReLU(True),\n      )\n      out_conv = int((int((img_size - 8)/2) - 6)/2)\n      self.classification = nn.Sequential(\n          nn.Linear(out_conv**2*c_dim, 50),\n          nn.ReLU(True),\n          nn.Linear(50, 10),\n          nn.LogSoftmax(dim=1),\n      )\n      if include_ST:\n          loc_conv_out_dim = int((int(img_size/2) - 4)/2) - 4\n          loc_regression_layer = nn.Linear(20, 6)\n          # initalize final regression layer to identity transform\n          loc_regression_layer.weight.data.fill_(0)\n          loc_regression_layer.bias = nn.Parameter(\n              torch.tensor([1., 0., 0., 0., 1., 0.]))\n          self.localisation_net = nn.Sequential(\n              nn.Conv2d(1, 20, kernel_size=5, stride=1, padding=0),\n              nn.MaxPool2d(kernel_size=(2,2), stride=2),\n              nn.ReLU(True),\n              nn.Conv2d(20, 20, kernel_size=5, stride=1, padding=0),\n              nn.ReLU(True),\n              nn.Flatten(),\n              nn.Linear(loc_conv_out_dim**2*20, 20),\n              nn.ReLU(True),\n              loc_regression_layer\n          )\n      return\n\n  def forward(self, img):\n      batch_size = img.shape[0]\n      if self.ST:\n          out_ST = self.ST_module(img)\n          img = out_ST\n      out_conv = self.convs(img)\n      out_classification = self.classification(out_conv.view(batch_size, -1))\n      return out_classification\n\n  def ST_module(self, inp):\n      # act on twice downsampled inp\n      down_inp = F.interpolate(inp, scale_factor=0.5, mode='bilinear',\n                                recompute_scale_factor=False, align_corners=False)\n      theta_vector = self.localisation_net(down_inp)\n      # affine transformation\n      theta_matrix = theta_vector.view(-1, 2, 3)\n      # grid generator\n      grid = F.affine_grid(theta_matrix, inp.size(), align_corners=False)\n      # sampler\n      out = F.grid_sample(inp, grid, align_corners=False)\n      return out\n\n  def get_attention_rectangle(self, inp):\n      assert inp.shape[0] == 1, 'batch size has to be one'\n      # act on twice downsampled inp\n      down_inp = F.interpolate(inp, scale_factor=0.5, mode='bilinear',\n                               recompute_scale_factor=False, align_corners=False)\n      theta_vector = self.localisation_net(down_inp)\n      # affine transformation matrix\n      theta_matrix = theta_vector.view(2, 3).detach()\n      # create normalized target rectangle input image\n      target_rectangle = torch.tensor([\n          [-1., -1., 1., 1., -1.],\n          [-1., 1., 1., -1, -1.],\n          [1., 1., 1., 1., 1.]]\n      ).to(inp.device)\n      # get source rectangle by transformation\n      source_rectangle = torch.matmul(theta_matrix, target_rectangle)\n      return source_rectangle\n\n\n# instantiate models\ncnn = CNN(img_size=42, include_ST=False)\nst_cnn = CNN(img_size=42, include_ST=True)\n# print trainable parameters\nfor model in [cnn, st_cnn]:\n  num_trainable_params = get_number_of_trainable_parameters(model)\n  print(f'{model.name} has {num_trainable_params} trainable parameters')\n\nTraining Procedure: As described in appendix A.4 of Jaderberg et al. (2015), the networks are trained with standard SGD, batch size of \\(256\\) and base learning rate of \\(0.01\\). To reduce computation time, the number of epochs is limited to \\(50\\).\nThe loss function is the multinomial cross entropy loss, i.e.,\n\\[\n  \\text{Loss} = - \\sum_{i=1}^N \\sum_{k=1}^C p_i^{(k)} \\cdot \\log\n  \\left( \\widehat{p}_i^{(k)} \\right),\n\\]\nwhere \\(k\\) enumerates the number of classes, \\(i\\) enumerates the number of images, \\(p_i^{k} \\in \\{0, 1\\}\\) denotes the true probability of image \\(i\\) and class \\(k\\) and \\(\\widehat{p}_i^{k} \\in [0, 1]\\) is the probability predicted by the network. Note that the true probability distribution is categorical (hard labels), i.e.,\n\\[\n  p_i^{(k)} = 1_{k = y_i} = \\begin{cases}1 & \\text{if } k = y_i \\\\ 0\n  & \\text{else}\\end{cases}\n\\]\nwhere \\(y_i \\in \\{0, 1, \\cdots, 9 \\}\\) is the label assigned to the \\(i\\)-th image \\(\\textbf{x}_i\\). Thus, we can rewrite the loss as follows\n\\[\n  \\text{Loss} = - \\sum_{i=1}^N \\log \\left( \\widehat{p}_{i, y_i}\n  \\right),\n\\]\nwhich is the definition of the negative log likelihood loss (NLLLoss) in Pytorch, when the logarithmized predictions \\(\\log \\left( \\widehat{p}_{i, y_i} \\right)\\) (matrix of size \\(N\\times C\\)) and class labels \\(y_i\\) (vector of size \\(N\\)) are given as input.\nThe code below summarizes the whole training procedure.\nfrom livelossplot import PlotLosses\nfrom torch.utils.data import DataLoader\n\n\ndef train(model, dataset):\n    # fix hyperparameters\n    epochs = 50\n    learning_rate = 0.01\n    batch_size = 256\n    step_size_scheduler = 50000\n    gamma_scheduler = 0.1\n    # set device\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    print(f'Device: {device}')\n\n    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True,\n                            num_workers=4)\n\n    model.to(device)\n    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=gamma_scheduler,\n                                                step_size=step_size_scheduler)\n\n    losses_plot = PlotLosses()\n    print(f'Start training with {model.name}')\n    for epoch in range(1, epochs+1):\n        avg_loss = 0\n        for data, label in data_loader:\n            model.zero_grad()\n\n            log_prop_pred = model(data.to(device))\n            # multinomial cross entropy loss\n            loss = F.nll_loss(log_prop_pred, label.to(device))\n\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            avg_loss += loss.item() / len(data_loader)\n\n        losses_plot.update({'log loss': np.log(avg_loss)})\n        losses_plot.send()\n    trained_model = model\n    return trained_model\nTest Procedure: A very simple test procedure to evaluate both models is shown below. It is basically the same as in the pytorch tutorial.\ndef test(trained_model, test_dataset):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=True,\n                            num_workers=4)\n    with torch.no_grad():\n        trained_model.eval()\n        test_loss = 0\n        correct = 0\n        for data, label in test_loader:\n            data, label = data.to(device), label.to(device)\n\n            log_prop_pred = trained_model(data)\n            class_pred = log_prop_pred.max(1, keepdim=True)[1]\n\n            test_loss += F.nll_loss(log_prop_pred, label).item()/len(test_loader)\n            correct += class_pred.eq(label.view_as(class_pred)).sum().item()\n\n        print(f'{trained_model.name}: avg loss: {np.round(test_loss, 2)},  ' +\n              f'avg acc {np.round(100*correct/len(test_dataset), 2)}%')\n    return\n\n\n\nResults\nLastly, the results can also divided into three sections:\n\nTraining Results: Firstly, we train our models on the training dataset and compare the logarithmized losses:\ntrained_cnn = train(cnn, train_dataset)\n\ntrained_st_cnn = train(st_cnn, train_dataset)\n\nThe logarithmized losses already indicate that the ST-CNN performs better than the standard CNN (at least, it decreases the loss faster). However, it can also be noted that training the ST-CNN seems less stable.\nTest Performance: While the performance on the training dataset may be a good indicator, test set performance is much more meaningful. Let’s compare the losses and accuracies between both trained models:\nfor trained_model in [trained_cnn, trained_st_cnn]:\n    test(trained_model, test_dataset)\n\nClearly, the ST-CNN performs much better than the standard CNN. Note that training for more epochs would probably result in even better accuracies in both models.\nVisualization of Learned Transformations: Lastly, it might be interesting to see what the ST module actually does after training:\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import ConnectionPatch\n\n\ndef visualize_learned_transformations(trained_st_cnn, test_dataset, digit_class=8):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    trained_st_cnn.to(device)\n    n_samples = 5\n\n    data_loader = DataLoader(test_dataset, batch_size=256, shuffle=True)\n    batch_img, batch_label = next(iter(data_loader))\n    i_samples = np.where(batch_label.numpy() == digit_class)[0][0:n_samples]\n\n    fig = plt.figure(figsize=(n_samples*2.5, 2.5*4))\n    for counter, i_sample in enumerate(i_samples):\n        img = batch_img[i_sample]\n        label = batch_label[i_sample]\n\n        # input image\n        ax1 = plt.subplot(4, n_samples, 1 + counter)\n        plt.imshow(transforms.ToPILImage()(img), cmap='gray')\n        plt.axis('off')\n        if counter == 0:\n            ax1.annotate('Input', xy=(-0.3, 0.5), xycoords='axes fraction',\n                        fontsize=14, va='center', ha='right')\n\n        # image including border of affine transformation\n        img_inp = img.unsqueeze(0).to(device)\n        source_normalized = trained_st_cnn.get_attention_rectangle(img_inp)\n        # remap into absolute values\n        source_absolute = 0 + 20.5*(source_normalized.cpu() + 1)\n        ax2 = plt.subplot(4, n_samples, 1 + counter + n_samples)\n        x = np.arange(42)\n        y = np.arange(42)\n        X, Y = np.meshgrid(x, y)\n        plt.pcolor(X, Y, img.squeeze(0), cmap='gray')\n        plt.plot(source_absolute[0], source_absolute[1], color='red')\n        plt.axis('off')\n        ax2.axes.set_aspect('equal')\n        ax2.set_ylim(41, 0)\n        ax2.set_xlim(0, 41)\n        if counter == 0:\n            ax2.annotate('ST', xy=(-0.3, 0.5), xycoords='axes fraction',\n                        fontsize=14, va='center', ha='right')\n        # add arrow between\n        con = ConnectionPatch(xyA=(21, 41), xyB=(21, 0), coordsA='data',\n                              coordsB='data', axesA=ax1, axesB=ax2,\n                              arrowstyle=\"-|&gt;\", shrinkB=5)\n        ax2.add_artist(con)\n\n        # ST module output\n        st_img = trained_st_cnn.ST_module(img.unsqueeze(0).to(device))\n\n        ax3 = plt.subplot(4, n_samples, 1 + counter + 2*n_samples)\n        plt.imshow(transforms.ToPILImage()(st_img.squeeze(0).cpu()), cmap='gray')\n        plt.axis('off')\n        if counter == 0:\n            ax3.annotate('ST Output', xy=(-0.3, 0.5), xycoords='axes fraction',\n                        fontsize=14, va='center', ha='right')\n        # add arrow between\n        con = ConnectionPatch(xyA=(21, 41), xyB=(21, 0), coordsA='data',\n                              coordsB='data', axesA=ax2, axesB=ax3,\n                              arrowstyle=\"-|&gt;\", shrinkB=5)\n        ax3.add_artist(con)\n\n        # predicted label\n        log_pred = trained_st_cnn(img.unsqueeze(0).to(device))\n        pred_label = log_pred.max(1)[1].item()\n\n        ax4 = plt.subplot(4, n_samples, 1 + counter + 3*n_samples)\n        plt.text(0.45, 0.43, str(pred_label), fontsize=22)\n        plt.axis('off')\n        #plt.title(f'Ground Truth {label.item()}', y=-0.1, fontsize=14)\n        if counter == 0:\n            ax4.annotate('Prediction', xy=(-0.3, 0.5), xycoords='axes fraction',\n                        fontsize=14, va='center', ha='right')\n        # add arrow between\n        con = ConnectionPatch(xyA=(21, 41), xyB=(0.5, 0.65), coordsA='data',\n                              coordsB='data', axesA=ax3, axesB=ax4,\n                              arrowstyle=\"-|&gt;\", shrinkB=5)\n        ax4.add_artist(con)\n    return\n\n\nvisualize_learned_transformations(st_cnn, test_dataset, 2)\n\nClearly, the ST module attends to the digits such that the ST output has much less variation in terms of rotation, translation and scale making the classification task for the follow up CNN easier.\nPretty cool, hugh?"
  },
  {
    "objectID": "paper_summaries/spatial_transformer/index.html#footnotes",
    "href": "paper_summaries/spatial_transformer/index.html#footnotes",
    "title": "Spatial Transformer Networks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nClearly, convolutional layers are not rotation or scale invariant. Even the translation-equivariance property does not necessarily make CNNs translation-invariant as typically some fully connected layers are added at the end. Max-pooling layers can introduce some translation invariance, however are limited by their size such that often large translation are not captured.↩︎\nJaderberg et al. (2015) define the transformation with normalized coordinates, i.e., \\(-1 \\le x_i^s, y_i^s \\le 1\\). However, in the sampling kernel equations it seems more likely that they assume unnormalized/absolute coordinates, e.g., in equation 4 of the paper normalized coordinates would be nonsensical.↩︎"
  }
]