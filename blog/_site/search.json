[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "mysite",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "paper_summaries.html",
    "href": "paper_summaries.html",
    "title": "Paper Summaries",
    "section": "",
    "text": "Auto-Encoding Variational Bayes\n\n\n\nvariational autoencoder\n\n\ndimensionality reduction\n\n\n\n\n\n\n\nborea17\n\n\nJul 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSchema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics\n\n\n\nreinforcement learning\n\n\n\n\n\n\n\nborea17\n\n\nJul 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nRelational Inductive Biases, Deep Learning, and Graph Networks\n\n\n\ninteraction network\n\n\ngraph networks\n\n\ngeneralization\n\n\n\n\n\n\n\nborea17\n\n\nJul 10, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInteraction Networks for Learning about Objects, Relations and Physics\n\n\n\nneural network\n\n\ngraph networks\n\n\n\n\n\n\n\nborea17\n\n\nJun 26, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "paper_summaries/2020-06-26-IN/index.html",
    "href": "paper_summaries/2020-06-26-IN/index.html",
    "title": "Interaction Networks for Learning about Objects, Relations and Physics",
    "section": "",
    "text": "Battaglia et al. (2016) introduce the Interaction Network (IN) as the first general-purpose learnable physics engine capable of zero-shot generalization in terms of varying configurations of objects and relations. The IN leverages object- and relation-based reasoning by defining a message passing scheme on a graph-structured representation of objects as nodes and relations as edges. As a proof of concept, they show that their model successfully learned to predict physical trajectories in gravitational systems, bouncing ball domains and mass string systems, and that it could also learn to estimate abstract properties such as the potential energy. Although its formulation is based on dynamical physical systems, it might also be applicable to other domains that can be abstracted into a graph-structured representation of objects and relations such as model-based reinforcment learning."
  },
  {
    "objectID": "paper_summaries/2020-06-26-IN/index.html#model-description",
    "href": "paper_summaries/2020-06-26-IN/index.html#model-description",
    "title": "Interaction Networks for Learning about Objects, Relations and Physics",
    "section": "Model Description",
    "text": "Model Description\nIn essence, the IN model can be understood as a graph-based simulator (i.e., state is represented as a graph) that predicts a future state (i.e., altered graph) using a message-passing scheme. Battaglia et al. (2016) used a handcrafted scene encoder/decoder to convert the physical scene into the corresponding graph structure and vice versa.\nDefinition: Let \\(G=\\langle O, R \\rangle\\) be an attributed, directed multigraph in which the set of nodes \\(O=\\\\{o_j\\\\}_{j=1 \\dots N_O}\\) represents objects and the set of edges \\(R = \\\\{ \\langle i, j , r_k \\rangle_k \\\\}\\_{1 \\dots N_R}\\) represents the relations between the objects, i.e., the triplet \\(\\langle i,j, r_k \\rangle_k\\) defines the \\(k^{\\text{th}}\\) relation from sender \\(o_i\\) to receiver \\(o_j\\) with relation attribute \\(r_k\\). Each object \\(o_i\\) may have several attributes1, an object state \\(o_i^{(t)}\\) at time \\(t\\) can be understood as a value assignment to all of its attributes. Additionally, let \\(X=\\\\{ x_j \\\\}\\_{1 \\dots N_O}\\) denote external effects (e.g., active control or gravitation) which are applied to each object separately."
  },
  {
    "objectID": "paper_summaries/2020-06-26-IN/index.html#footnotes",
    "href": "paper_summaries/2020-06-26-IN/index.html#footnotes",
    "title": "Interaction Networks for Learning about Objects, Relations and Physics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn their implementation, Battaglia et al. (2016) assume that all objects share the same attributes, i.e., are instances from the same class. 2: Schematic is taken from the original paper of Battaglia et al. (2016).↩︎"
  },
  {
    "objectID": "paper_summaries/2020-06-26-interaction_network/index.html",
    "href": "paper_summaries/2020-06-26-interaction_network/index.html",
    "title": "Interaction Networks for Learning about Objects, Relations and Physics",
    "section": "",
    "text": "Battaglia et al. (2016) introduce the Interaction Network (IN) as the first general-purpose learnable physics engine capable of zero-shot generalization in terms of varying configurations of objects and relations. The IN leverages object- and relation-based reasoning by defining a message passing scheme on a graph-structured representation of objects as nodes and relations as edges. As a proof of concept, they show that their model successfully learned to predict physical trajectories in gravitational systems, bouncing ball domains and mass string systems, and that it could also learn to estimate abstract properties such as the potential energy. Although its formulation is based on dynamical physical systems, it might also be applicable to other domains that can be abstracted into a graph-structured representation of objects and relations such as model-based reinforcment learning."
  },
  {
    "objectID": "paper_summaries/2020-06-26-interaction_network/index.html#model-description",
    "href": "paper_summaries/2020-06-26-interaction_network/index.html#model-description",
    "title": "Interaction Networks for Learning about Objects, Relations and Physics",
    "section": "Model Description",
    "text": "Model Description\nIn essence, the IN model can be understood as a graph-based simulator (i.e., state is represented as a graph) that predicts a future state (i.e., altered graph) using a message-passing scheme. Battaglia et al. (2016) used a handcrafted scene encoder/decoder to convert the physical scene into the corresponding graph structure and vice versa.\nDefinition: Let \\(G=\\langle O, R \\rangle\\) be an attributed, directed multigraph in which the set of nodes \\(O=\\\\{o_j\\\\}_{j=1 \\dots N_O}\\) represents objects and the set of edges \\(R = \\\\{ \\langle i, j , r_k \\rangle_k \\\\}\\_{1 \\dots N_R}\\) represents the relations between the objects, i.e., the triplet \\(\\langle i,j, r_k \\rangle_k\\) defines the \\(k^{\\text{th}}\\) relation from sender \\(o_i\\) to receiver \\(o_j\\) with relation attribute \\(r_k\\). Each object \\(o_i\\) may have several attributes1, an object state \\(o_i^{(t)}\\) at time \\(t\\) can be understood as a value assignment to all of its attributes. Additionally, let \\(X=\\\\{ x_j \\\\}\\_{1 \\dots N_O}\\) denote external effects (e.g., active control or gravitation) which are applied to each object separately."
  },
  {
    "objectID": "paper_summaries/2020-06-26-interaction_network/index.html#footnotes",
    "href": "paper_summaries/2020-06-26-interaction_network/index.html#footnotes",
    "title": "Interaction Networks for Learning about Objects, Relations and Physics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn their implementation, Battaglia et al. (2016) assume that all objects share the same attributes, i.e., are instances from the same class. 2: Schematic is taken from the original paper of Battaglia et al. (2016).↩︎"
  },
  {
    "objectID": "paper_summaries/interaction_network/index.html",
    "href": "paper_summaries/interaction_network/index.html",
    "title": "Interaction Networks for Learning about Objects, Relations and Physics",
    "section": "",
    "text": "Battaglia et al. (2016) introduce the Interaction Network (IN) as the first general-purpose learnable physics engine capable of zero-shot generalization in terms of varying configurations of objects and relations. The IN leverages object- and relation-based reasoning by defining a message passing scheme on a graph-structured representation of objects as nodes and relations as edges. As a proof of concept, they show that their model successfully learned to predict physical trajectories in gravitational systems, bouncing ball domains and mass string systems, and that it could also learn to estimate abstract properties such as the potential energy. Although its formulation is based on dynamical physical systems, it might also be applicable to other domains that can be abstracted into a graph-structured representation of objects and relations such as model-based reinforcment learning."
  },
  {
    "objectID": "paper_summaries/interaction_network/index.html#model-description",
    "href": "paper_summaries/interaction_network/index.html#model-description",
    "title": "Interaction Networks for Learning about Objects, Relations and Physics",
    "section": "Model Description",
    "text": "Model Description\nIn essence, the IN model can be understood as a graph-based simulator (i.e., state is represented as a graph) that predicts a future state (i.e., altered graph) using a message-passing scheme. Battaglia et al. (2016) used a handcrafted scene encoder/decoder to convert the physical scene into the corresponding graph structure and vice versa.\nDefinition: Let \\(G=\\langle O, R \\rangle\\) be an attributed, directed multigraph in which the set of nodes \\(O=\\\\{o_j\\\\}_{j=1 \\dots N_O}\\) represents objects and the set of edges \\(R = \\\\{ \\langle i, j , r_k \\rangle_k \\\\}\\_{1 \\dots N_R}\\) represents the relations between the objects, i.e., the triplet \\(\\langle i,j, r_k \\rangle_k\\) defines the \\(k^{\\text{th}}\\) relation from sender \\(o_i\\) to receiver \\(o_j\\) with relation attribute \\(r_k\\). Each object \\(o_i\\) may have several attributes1, an object state \\(o_i^{(t)}\\) at time \\(t\\) can be understood as a value assignment to all of its attributes. Additionally, let \\(X=\\\\{ x_j \\\\}\\_{1 \\dots N_O}\\) denote external effects (e.g., active control or gravitation) which are applied to each object separately.\nIntuition: The ultimate goal of the IN is to predict all future object states \\(o_i^{(t+1)}\\) based on the graph \\(G\\), the current external effects per object \\(x_i^{(t)}\\) and all current object states \\(o_i^{(t)}\\). A message passing scheme is defined to achieve this goal in which first effects resulting from interactions are computed (relational reasoning), then these effects (messages) together with the external effects are aggregated towards the objects, lastly the aggregated information is used to update the object states (object reasoning).\nFormally, the basic IN is defined as follows\n\\[\n\\begin{align}\n&\\text{IN}(G) = \\phi_O \\Bigg( a\\Big( G, X, \\phi_R \\big( m (G)\\big) \\Big)\\Bigg)\n\\end{align}\n\\]\n\\[\n\\begin{align}\n  \\begin{aligned}\n    & m(G) = B = \\{ b_k\\}_{k=1\\dots N_R} \\\\\n    & f_{R} (b_k) = e_k \\\\\n    & \\phi_{R} (B) = E = \\{e_k\\}_{k=1 \\dots N_R}\n  \\end{aligned}\n    &&\n       \\begin{aligned}\n         & a(G, X, E) = C = \\{c_j\\}_{j=1\\dots N_O} \\\\\n         & f_O (c_j) = p_j \\\\\n         & \\phi_O (C) = P = \\{p_j\\}_{j=1\\dots N_o}\n       \\end{aligned}\n\\end{align}\n\\]\nIn this definition \\(m\\) denotes the marshalling function which rearranges objects and relations into interaction terms \\(b_k= \\langle o_i, o_j, r_k \\rangle \\in B\\) on which the relational function \\(\\phi_R\\) can operate (element-wise by applying \\(f_R\\) on each interaction term) to predict the effects of each interaction \\(e_k\\in E\\). The aggregation function \\(a\\) builds a a set of of object model inputs \\(c_j \\in C\\) (one per object) by collecting and merging all incoming effects per object and combining the result with the object state \\(o_{j}^{(t)}\\) and the external effects for that object \\(x_j^{(t)}\\). Lastly, the object model \\(\\phi_O\\) predicts for all objects their result \\(p_j\\in P\\), i.e., future object states \\(o_{j}^{(t+1)}\\), by applying \\(f_O\\) to each \\(c_j\\). The figure below represents the described procedure of an (exemplary) IN.\n\n\n\n\n\n\n\n\nSchematic of the IN’s update procedure for an exemplary IN2:Firstly, the marshalling function \\(m\\) rearranges objects \\(o_i\\) based on the relations \\(r_j\\) into interaction terms \\(b_k = \\langle o_i, o_j, r_k \\rangle\\). Secondly, the function \\(f_R\\) is applied on each interaction term to compute the corresponding (directed) effects.Thirdly, the aggregation function \\(a\\) uses the graph structure to collect and merge the incoming effects, and to add the corresponding object state and external effects into a new object term \\(c_k = \\langle o_k, x_k, \\hat{e}_k \\rangle\\) (\\(\\hat{e}_k\\) denotes aggregated effect). Lastly, this representation is used to predict the results \\(p_k\\), i.e., future object states, by applying \\(f_O\\) to each \\(c_k\\).\n\n\n\nIntuition: Computing the trajectories of planets in a solar system may be a good example to motivate and understand the IN definition. Objects in the graph shall be the planets and relations the pairwise gravitational forces on each other, i.e., each object has an arrow pointing to all other objects. Object attributes could be the mass, acceleration, velocity and position. As external effects we could define the step size (necessary for approximate integration). Relational attributes are not needed then. The physics approach to compute the approximated trajacetories would be to first compute all gravitational forces per object which corresponds to computing the effects in the IN. Then, the net force would be computed as the sum of all forces per object, i.e., aggregation in the IN. Lastly, the object attributes would be updated (except mass) using the calculated net force, current object state and step size which corresponds to the object-centric update in the IN."
  },
  {
    "objectID": "paper_summaries/interaction_network/index.html#footnotes",
    "href": "paper_summaries/interaction_network/index.html#footnotes",
    "title": "Interaction Networks for Learning about Objects, Relations and Physics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn their implementation, Battaglia et al. (2016) assume that all objects share the same attributes, i.e., are instances from the same class. 2: Schematic is taken from the original paper of Battaglia et al. (2016).↩︎"
  },
  {
    "objectID": "paper_summaries/interaction_network/index.html#learning-the-model",
    "href": "paper_summaries/interaction_network/index.html#learning-the-model",
    "title": "Interaction Networks for Learning about Objects, Relations and Physics",
    "section": "Learning the Model",
    "text": "Learning the Model\nIn the IN defintion, there are no limitations whatsoever for the functions and how they operate over their inputs (e.g., objects itself could also be graphs). Thus, learning the underlying dynamics given the graph structured representation and the scene encoder/decoder (and assuming that there is some IN that can simulate the dynamics) without further assumptions remains an intractable quest. To overcome this problem, Battaglia et al. (2016) present a learnable implementation of the IN which uses deep neural networks as function approximators and a specific object and relation representation using matrices. Then, learning the model comes down to training from data using the standard deep learning framework.\nImplementation: Let each object \\(o_i^{(t)}\\in \\mathbb{R}^{D_s}\\) be represented by a \\(D_s\\)-dimensional vector where each entry corresponds to an attribute, i.e., attributes have a predefined order which is fix over all objects. Then, \\(O\\) is defined as a \\(D_S \\times N_O\\) matrix where each column represents an object. Similarly, \\(X\\) is defined as a \\(D_X \\times N_O\\) matrix where each \\(D_x\\)-dimensional column represents the external effects that correspond to the object defined in the same column of \\(O\\). Let each relation be formalized into a triple of three vectors \\(r_k = \\langle r_r, r_s, r_a \\rangle\\) where \\(r_a \\in \\mathbb{R}^{D_R}\\) represents the (ordered) relational attributes and \\(r_r, r_s \\in \\{0, 1\\}^{N_O}\\) are one-hot encodings of the receiver and sender object, respectively. Then, all relations can be represented by the triplet \\(R = \\langle R_r, R_s, R_a \\rangle\\) where the matrices \\(R_r, R_s \\in \\{0,1\\}^{N_O \\times N_R}\\) and \\(R_a \\in \\mathbb{R}^{D_R \\times N_R}\\) are generated by stacking the relations column-wise.\nIt follows that the interaction terms \\(b_k\\) can be vectorized by concatenation of the receiver and sender object attributes and the relational attributes into a \\((2 D_s + D_R)\\)-length vector. The marshalling function \\(m\\) can be stated as follows\n\\[\n\\begin{align}\n  m(G)\n  = \\begin{bmatrix}  O R_r \\\\ O R_s \\\\ R_a  \\end{bmatrix} = \\begin{bmatrix} b_1  &\n    \\dots & b_{N_R} \\end{bmatrix} = B.\n\\end{align}\n\\]\n\\(B\\) is the input to the relational model \\(\\phi_R\\) which is defined through the application of \\(f_R\\) on each column of \\(B\\) (each interaction term), i.e.,\n\\[\n\\begin{align}\n  \\phi_R (B) = \\begin{bmatrix} f_R \\big(b_1\\big) & \\dots & f_R \\big(b_{N_R}\\big) \\end{bmatrix}\n             = \\begin{bmatrix} e_1 & \\dots e_{N_R}\\end{bmatrix}  = E.\n\\end{align}\n\\]\n\\(f_R\\) shall be approximated by a neural network to estimate a \\(D_E\\)-length vector \\(e_k\\) that encodes the resulting effect. Similar to the marshalling function, the aggregation function \\(a\\) constructs vectorized object terms \\(c_k = \\begin{bmatrix} o_k^{(t)} & x_k^{(t)} & \\hat{e}_k^{(t)} \\end{bmatrix}^{\\text{T}}\\) by concatenation of the object attributes, the external effects and the aggregated effect (summation of all incoming effects per object):\n\\[\n\\begin{align}\n  a(O, R, X, E) = \\begin{bmatrix} O & X & E R_r^{\\text{T}}\n  \\end{bmatrix}^{\\text{T}} =\n  \\begin{bmatrix} c_1 & \\dots & c_{N_O} \\end{bmatrix} = C.\n\\end{align}\n\\]\nLastly, \\(C\\) is used as the input to the object model \\(\\phi_O\\) which is defined through the application of \\(f_O\\) on each column of \\(C\\), i.e.,\n\\[\n  \\phi_O (C) =\n  \\begin{bmatrix} f_O \\big( c_1 \\big) & \\dots & f_O \\big( c_{N_O} \\big) \\end{bmatrix}=\n                                                \\begin{bmatrix} o_1^{(t+1)} & \\dots & o_{N_O}^{(t+1)} \\end{bmatrix}.\n\\]\nThe result can be used to update the graph structured representation. The figure below summarizes the implementation of the IN.\n\n\n\n\n\n\n\n\nOne step roll out of the IN implementation. The physical scene is encoded (decoded) into (from) a graph structured representation using a handcrafted scene encoder (decoder). Battaglia et al. (2016) present a learnable implementation by using neural networks (blue boxes) as function approximators for the relational model (\\(\\phi_R\\)) and the object model (\\(\\phi_O\\))."
  },
  {
    "objectID": "paper_summaries/interaction_network/index.html#drawbacks-of-paper",
    "href": "paper_summaries/interaction_network/index.html#drawbacks-of-paper",
    "title": "Interaction Networks for Learning about Objects, Relations and Physics",
    "section": "Drawbacks of Paper",
    "text": "Drawbacks of Paper\n\nhandcrafted scene encoder/decoder \\(\\Rightarrow\\) not end-to-end\nobject states could blow up since all objects share the same attributes"
  },
  {
    "objectID": "paper_summaries/graph_networks/index.html",
    "href": "paper_summaries/graph_networks/index.html",
    "title": "Relational Inductive Biases, Deep Learning, and Graph Networks",
    "section": "",
    "text": "Few years after the IN paper, Battaglia et al. (2018) showed that the IN can be cast into a special case of a broader framework, termed Graph Networks (GNs). They hypothesize that despite the recent successes in deep learning with minimal representational biases, key ingredients of human-like intelligence such as combinatorial generalization1 remain out of reach. Arguing that solving this challenge should be a top priority of current research, they recommend2 the use of integrative approaches that build strong relational inductive biases3 into deep learning architectures. By presenting and formalizing a general GN framework for entity- and relation-based reasoning, they conclude that this framework could be a stepping stone towards combinatorial generalization."
  },
  {
    "objectID": "paper_summaries/graph_networks/index.html#model-description",
    "href": "paper_summaries/graph_networks/index.html#model-description",
    "title": "Relational Inductive Biases, Deep Learning, and Graph Networks",
    "section": "Model Description",
    "text": "Model Description\nSimiliar to the IN model, the GN block can be understood as a graph-to-graph module using a message passing scheme. In contrast to the IN, the GN block includes global attributes instead of external effects and uses these global attributes to update the edge attributes and node attributes instead of only updating the node attributes. Accordingly, the message passing scheme is slightly more complex.\nDefinition: Let \\(G=\\langle \\textbf{u}, V, E \\rangle\\) be an attributed, directed multigraph in which the global attribute (vector) \\(\\textbf{u}\\) represents system-level properties (e.g., gravitational field), the set of nodes \\(V=\\\\{\\textbf{v}_j\\\\}\\_{j=1 \\dots N_V}\\) represents entities4 and the set of edges \\(E = \\\\{ \\langle \\textbf{e}_k, r_k, s_k \\rangle \\\\}\\_{k=1\\dots N_E}\\) represents the attributed relations, i.e., the triplet \\(\\langle \\textbf{e}_k, r_k, s_k \\rangle\\) defines the \\(k^{\\text{th}}\\) relation from sender \\(o\\_{s_k}\\) to receiver \\(o\\_{r_k}\\) with relation attribute(s) \\(\\textbf{e}_k\\).\nFormally, the (full) GN block is defined as follows\n\\[\n\\begin{align}\n  \\begin{split}\n    \\text{GN}(G) &= \\text{GN} (\\langle \\textbf{u}, V, E \\rangle) = G^\\prime\\\\\n               &= \\left\\langle\n                 \\underbrace{\\phi^{u} \\Big(\\textbf{u}, \\rho^{v\\rightarrow u} \\big(V^\\prime\\big), \\rho^{e\\rightarrow u}\\big(E^{\\prime} \\big) \\Big)}_{\\textbf{u}^\\prime},\n   \\underbrace{\\phi^{v} \\Big(\\textbf{u}, V, \\rho^{e\\rightarrow v}\\big(E^\\prime \\big)\\Big)}_{V^\\prime},\n   \\underbrace{\\phi^{e} \\Big(\\textbf{u}, V, E\\Big)}_{E^\\prime} \\right\\rangle,\n   \\end{split}\n\\end{align}\n\\]\nwhere the updates within the graph triple \\(G=\\langle \\textbf{u}, V, E \\rangle\\) occur from right to left. More specifically, \\(\\phi^e\\) updates the edge attributes of all edges to compute the updated edge set \\(E^\\prime\\) as follows\n\\[\n\\begin{align}\n  E^\\prime  = \\phi^{e} (\\textbf{u}, \\textbf{V}, E)\n  = \\left\\{ f^e \\big(\\textbf{e}_1, \\textbf{v}_{r_1}, \\textbf{v}_{s_1}, \\textbf{u}\\big), \\dots,\n    f^{e} \\big(\\textbf{e}_{N_E}, \\textbf{v}_{r_{N_E}}, \\textbf{v}_{s_{N_E}}, \\textbf{u}\\big)\\right\\}.\n\\end{align}\n\\]\nThe updated edge set \\(E^\\prime\\) is used to compute the aggregated updated edge attributes per node \\(\\overline{\\textbf{e}}_i\\) using the aggregation function \\(\\rho^{e\\rightarrow v}\\), i.e.,\n\\[\n\\begin{align}\n  \\forall i \\in \\{1, \\dots, N_V\\}: \\overline{\\textbf{e}}_i = \\rho^{e\\rightarrow v} (E^\\prime)\n  = \\rho^{e\\rightarrow v} \\Big( \\left\\{  \\big(\\textbf{e}_k^\\prime, r_k, s_k\\big)  \\right\\}_{r_k=i, k=1:N_E}\\Big).\n\\end{align}\n\\]\nThe results are used to compute the updated node set \\(V^\\prime\\) using \\(\\phi^v\\) as follows\n\\[\n\\begin{align}\n  V^\\prime = \\phi^v \\Big(\\textbf{u}, V, \\rho^{e\\rightarrow v} \\big( E^\\prime \\big) \\Big)\n  = \\{ f^v \\big(\\overline{\\textbf{e}}_1, \\textbf{v}_1, \\textbf{u}\\big), \\dots,\n  f^v\\big(\\overline{\\textbf{e}}_{N_V}, \\textbf{v}_{N_V}, \\textbf{u}\\big)\\}.\n\\end{align}\n\\]\nLastly, the global attribute is updated towards \\(\\textbf{u}^\\prime\\) by aggregating the edge and node attributes globally, and then applying \\(\\phi^u\\). The figure below summarizes the internal structure within a (full) GN block and shows how different variants such as the relation network (Raposo et al., 2017) can be identified within the GN framework.\n\n\n\n\n\n\n\n\n(a) The internal GN block structure in its broadest formulation is shown including three update and three aggregation functions. (b) The relation network by Raposo et al. (2017) can be identified as a special case of the broader GN framework which only uses the edge predictions to predict global attributes. Taken from Battaglia et al. (2018)\n\n\n\nThe GN block can be understood as a building block to compose complex multi-block architectures, e.g., by stacking GN blocks similar to stacking layers in MLPs or reusing a GN block in a recurrent fashion. Additionally, the features inside the GN such as node attributes can be input to a standard MLP to infer abstract properties such as the potential energy (which was done in the IN paper (Battaglia et al., 2016))."
  },
  {
    "objectID": "paper_summaries/graph_networks/index.html#footnotes",
    "href": "paper_summaries/graph_networks/index.html#footnotes",
    "title": "Relational Inductive Biases, Deep Learning, and Graph Networks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBattaglia et al. (2018) define the principle of combinatorial generalization as the ability of constructing new inferences, predictions, and behaviors from known building blocks.↩︎\nThe paper was written by a large group of 27 researchers from DeepMind, GoogleBrain, MIT and University of Edinburgh. As directly stated in the abstract, it is part position paper, part review, and part unification.↩︎\nBattaglia et al. (2018) use the term relational inductive bias to refer generally to inductive biases which impose constraints on relationships and interactions among entities in a learning process. They motivate the use of relational inductive biases by human cognition which also uses (yet-to-understand) mechanisms for representing structure (e.g., world is understood as composition of objects) and relations (e.g., distance between objects).↩︎\nBattaglia et al. (2018) define an entity as an element with attributes. Thus, the term entity is more general than object capturing objects, parts of objects or any other attributed structure.↩︎"
  },
  {
    "objectID": "paper_summaries/schema_networks/index.html",
    "href": "paper_summaries/schema_networks/index.html",
    "title": "Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics",
    "section": "",
    "text": "Kansky et al. (2017) showed remarkable results of zero-shot transfer in several variations of Breakout by introducing Schema Networks as a generative model for object-oriented reinforcement learning and planning. This model incorporates objects as entities, represents local cause-effect relationships including one or more entities and is based on Probabilistic Graphical Models (PGMs). Due to its foundation in PGMs, Schema Networks support flexible inference and search strategies for planning."
  },
  {
    "objectID": "paper_summaries/schema_networks/index.html#model-description",
    "href": "paper_summaries/schema_networks/index.html#model-description",
    "title": "Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics",
    "section": "Model Description",
    "text": "Model Description\nBuilding upon the ideas of object-oriented Markov decision processes (OO-MDPs), states are represented as a list of entities where each entity can be understood as a different instantiation from the same class (i.e., all entities share the same attributes). Additionally, the attributes, actions and rewards are binarized using discretization and one-hot encoding, see image below. This representation comes from a handcrafted image parser with the handwavy argument that in practice a vision module could be responsible for this task.\n\n\n\n\n\n\n\n\n\n\n\nExemplary State Representation in a Schema Network. A handcrafted image parser converts an image (left) into the state representation (right), where filled green circles indicate that the binary variable is set to True.\n\n\n\nKansky et al. (2017) define 53 attributes for each entity in the Breakout domain (21 for bricks, 30 for the paddle, 1 for walls, 1 for the ball). However, they do not elaborate on what these attributes describe exactly. Furthermore, each pixel is identified as a part of an object and assigned the corresponding attributes. Accordingly, their representation could rather be understood as a 53-channel image where each entry can either be 0 or 1, e.g., one layer showing the walls. In this form, the entity-based state representation can also be provided to other algorithms such as A3C.\nSimiliar to OO-MDPs, state transitions are determined by a change of entity-attributes. However, due to the specific representation in Schema Networks, entity-attributes can only be active or inactive (with an associated probability). An attribute becomes activated if a grounded schema1 is active. Grounded schemas can include a variable size of entity attributes from a variable number of entities and may include one or more actions. Thus, these schemas can be interpreted as local cause-effect relationships. Formally, a grounded schema \\(\\phi^{k}\\) is a binary variable that becomes activated via a probabilistic AND over the binary variables \\(v_1, \\dots, v_n\\) that are included in it:\n\\[\n\\begin{align}\n  \\phi^{k} = \\text{AND} (v_1, \\dots, v_n) = \\prod_{i=1}^n P(v_i = 1).\n\\end{align}\n\\]\nThe binary variables \\(v_1, \\dots, v_n\\) may be entity-attributes2 or actions, see image below.\nMultiple grounded schemas can predict the same attribute which is formalized through an OR factor, e.g., let \\(\\alpha_{i, j}^{(t+1)}\\) denote the \\(j^{th}\\) attribute of the \\(i^{th}\\) entity at time \\(t+1\\) and assume there are \\(n\\) grounded schemas that predict this entity attribute. Then, this formalizes into\n\\[\n\\begin{align}\n  \\alpha_{i,j}^{(t+1)} = \\text{OR} (\\phi_{i,j}^{1}, \\dots, \\phi_{i, j}^{n}) = 1 - \\prod_{k=1}^n  \\big(1 - P(\\phi_{i,j}^k)\\big).\n\\end{align}\n\\]\nKansky et al. (2017) divide entity attributes into two classes: * Positional Attributes: These attributes correspond to discrete positions. * Non-Positional Attributes: The semantic meaning of those attributes is unknown to the model such that they may encode completely different things, e.g., color and shape.\nA self-transition variable is introduced for positional attributes which represents the probability that a position attribute will remain active in the next time step when no schema predicts a change from that position. Note that through this mechanism, they include the bias that an object cannot be at multiple positions at the same time.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransition dynamics in Schema Networks are governed by changes in entity-attributes due to activated grounded schemas. In this example all relevant gates are shown to illustrate the state transition dynamics via this mechanics. Note that there are two schemas that predict the same behavior, i.e., only one input in OR is necessary to activate \\(y=1\\).\n\n\n\nFormally, a self transition is a NOR factor over the grounded schemas that predict a change from a position attribute (i.e., the grounded schemas that predict towards a different position) combined with an AND factor over the NOR factor and the position attribute at the current time step. E.g., let \\(\\alpha_{i,j}^{t}\\) denote the \\(j^{th}\\) position attribute of the the \\(i^{th}\\) entity at time \\(t\\) and assume that the set \\(\\{\\phi^1, \\dots, \\phi^{n} \\}\\) includes all schemas predicting towards a different position of that entity. Then, the self-transition is formalized as follows\n\\[\n\\begin{align}\n  \\Lambda_{i,j}^{t+1}\n  = \\text{AND} \\big(\\lnot \\phi^{1}, \\dots, \\lnot \\phi^{n}, \\alpha_{i,j}^{t} \\big).\n\\end{align}\n\\]\nFinally, the transition function in this model can be factorized as\n\\[\n\\begin{align}\n  T\\left(s^{(t+1)} | s^{(t)}, a^{(t)}\\right) = \\prod_{i=1}^N \\prod_{j=1}^M T_{i, j} \\left(s_{i, j}^{(t+1)}|s^{(t)}, a^{(t)}\\right),\n\\end{align}\n\\]\nwhere \\(T_{i,j}\\) denotes the transition probability of the \\(j^{th}\\) attribute of the \\(i^{th}\\) entity towards its value defined in \\(s_{i,j}^{(t+1)}\\). The entity attribute \\(s_{i,j}^{(t+1)}\\) is by definition activated if one of its grounded schema is active or if a self-transition occured, thus the entity-attribute transition probability is defined as\n\\[\n\\begin{align}\n  T_{i,j} \\left( s_{i,j}^{(t+1)} | s^{(t)}, a^{(t)}\\right) = \\text{OR}\\left( \\phi^{k_1}, \\dots, \\phi^{k_Q}, \\Lambda_{i,j} \\right),\n\\end{align}\n\\]\nwhere \\(\\Lambda_{i,j}\\) denotes the self-transition variable of \\(s_{i,j}\\) and \\(k_1, \\dots, k_Q\\) are the indices of all grounded schemas that predict \\(s_{i,j}\\). Note that although all variables are defined as binary variables, this model could still be used for non-deterministic environments.\nTo increase the generality of their model such that the attribute change of two entities is described by the same schema, Kansky et al. (2017) introduce the term ungrounded schema or template. An ungrounded schema can be understood as template for specific grounded schemas, i.e., it describes a grounded schema where the included entity-attributes are assigned relative to the position of the entity-attribute that should be predicted."
  },
  {
    "objectID": "paper_summaries/schema_networks/index.html#learning-the-model",
    "href": "paper_summaries/schema_networks/index.html#learning-the-model",
    "title": "Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics",
    "section": "Learning the Model",
    "text": "Learning the Model\nThe Schema Network is essentially a factor graph that is aimed to be a probabilistic simulator of the game environment using the aforementioned representation. Assuming that the environment dynamics can be represented by some Schema Network, learning the model comes down to structure learning in graphical models. Kansky et al. (2017) preprocess a sequence of observations (dataset of state-action-reward-new state tuples over time) into a convenient representation, define a NP-hard optimization problem based on this representation as the optimal solution and retrieve the schemas (and Schema Network) through solving the optimization problem approximately using linear programming (LP) relaxations.\n\nNotation\nLet \\(\\alpha\\_{i,j}^{(t)}\\) denote the \\(j^{th}\\) attribute of the \\(i^{th}\\) entity at time \\(t\\) and let \\(\\textbf{e}\\_i^{(t)} \\in \\\\{0, 1\\\\}^{M}\\) be an\n\\(M\\)-dimensional binary vector representing all entity-attributes values of the \\(i^{th}\\) entity at time \\(t\\), i.e., \\(\\textbf{e}\\_i^{(t)} = \\begin{bmatrix}\\alpha\\_{i,1}^{(t)} & \\dots & \\alpha\\_{i,M}^{(t)} \\end{bmatrix}^{\\text{T}}\\) where \\(M\\) denotes the number of entity-attributes.\nLet \\(\\boldsymbol{\\beta}\\_{i}^{(t)}\\in \\\\{0,1\\\\}^E\\) be a row vector representing the attribute values of the \\(i^{th}\\) entity and the entity-attributes of the \\(R-1\\) (fixed radius) spatial neighbors, i.e., \\(\\boldsymbol{\\beta}_{i}^{(t)} = \\begin{bmatrix} \\textbf{e}\\_{i}^{(t)} & \\textbf{e}\\_{i+1}^{(t)} & \\dots & \\textbf{e}\\_{R-1}^{(t)} \\end{bmatrix}\\) has length \\(E=M(R-1) + M = MR\\).\nSuppose there are \\(N\\) entities observed for \\(\\tau\\) timesteps. Then, let \\(\\textbf{X}\\in\\\\{0,1\\\\}^{D\\times E}\\) be a binary matrix where each row consists of a \\(\\boldsymbol{\\beta}\\_{i}^{(t)}\\) and there are \\(D=N\\cdot \\tau\\) rows (all entities and time steps). Similarly, let \\(\\textbf{y}\\in\\\\{0, 1\\\\}^D\\) be a binary vector where each entry refers to the future attribute value \\(\\alpha\\_{i,j}^{(t+1)}\\) corresonding to the a row of \\(\\textbf{X}\\) with entity \\(i\\) and time \\(t\\), i.e.,\n\\[\n\\begin{align*}\n  \\textbf{X} &=\n  \\begin{bmatrix}\n    \\begin{bmatrix} \\boldsymbol{\\beta}_{1}^{(1)} & \\dots & \\boldsymbol{\\beta}_{N}^{(1)} \\end{bmatrix}^{\\text{T}} \\\\\n    \\vdots\\\\\n    \\begin{bmatrix} \\boldsymbol{\\beta}_{1}^{(\\tau)} & \\dots & \\boldsymbol{\\beta}_{N}^{(\\tau)} \\end{bmatrix}^{\\text{T}}\n  \\end{bmatrix} , \\quad \\textbf{y} =\n   \\begin{bmatrix}\n\\begin{bmatrix} \\alpha_{1,j}^{(2)} & \\dots & \\alpha_{N,j}^{(2)} \\end{bmatrix}^{\\text{T}}\n\\\\ \\vdots \\\\\n\\begin{bmatrix} \\alpha_{1,j}^{(\\tau+1)} & \\dots & \\alpha_{N,j}^{(\\tau+1)} \\end{bmatrix}^{\\text{T}}\n    \\end{bmatrix}\n\\end{align*}\n\\]\n\n\nLearning Problem\nThe goal is to predict \\(\\alpha\\_{i,j}^{(t+1)}\\) based on the entity-attributes of itself and its spatial neighbors. Using the introduced notation, the learning problem can be defined as follows\n\\[\n  \\textbf{y} = f_{\\textbf{W}} (\\textbf{X}) = \\overline{\\overline{\\textbf{X}} \\textbf{W}} \\textbf{1},\n\\]\nwhere \\(f\\_{\\textbf{W}}\\) denotes the desired function of ungrounded schemas which is applied row-wise to the argument \\(\\textbf{X}\\) to produce either active or inactive grounded schemas. \\(f\\_{\\textbf{W}}\\) is parametrized by a binary matrix \\(\\textbf{W} \\in \\\\{0, 1\\\\}^{E \\times L}\\) with each column representing one ungrounded schema for a maximum of \\(L\\) schemas. Each element that is set to 1 in a column of \\(\\textbf{W}\\) indicates that for this schema the corresponding input attribute (from \\(\\textbf{X}\\)) is necessary for an activated grounded schema. On the right-hand side of the equation above all variables and operations follow Boolean logic: addition corresponds to ORing and overlining to negation.\n\nE.g., let \\(\\textbf{w}\\_i = \\begin{bmatrix} w\\_{i_1} & \\dots & w\\_{i_ E}\\end{bmatrix}^{\\text{T}}\\) denote the \\(i^{th}\\) column of \\(\\hspace{0.1cm}\\textbf{W}\\) and \\(\\textbf{x}_r = \\begin{bmatrix} x\\_{r_1} & \\dots & x\\_{r_E} \\end{bmatrix}\\) be the \\(r^{th}\\) row of \\(\\hspace{0.1cm}\\textbf{X}\\). Then, the (dot) product of the two vectors leads to\n\\[\n\\begin{align}\n\\textbf{x}_r \\textbf{w}_i = \\sum_{k=1}^{E} x_{r_k} \\cdot w_{i_k} = \\text{OR} \\left( x_{r_1} w_{i_1}, \\dots, x_{r_E} w_{i_E} \\right),\n\\end{align}\n\\]\nin this form the corresponding grounded schema would be activated as soon as one precondition is satisfied, i.e., as soon as for one \\(w\\_{i_j} = 1\\) the corresponding attribute variable is also \\(x\\_{i_j}=1\\).\n\nA grounded schema \\(\\phi\\) is defined through a logical AND over the necessary attributes (i.e., all preconditions)3 \n\\[\n\\begin{align}\n  \\phi = \\text{AND}\\left( \\{x_{r_j} \\mid \\forall j: w_{i_j} = 1 \\}  \\right) = \\text{Not} \\left(\n  \\text{OR} \\left(\\text{Not } \\{x_{r_j} \\mid \\forall j: w_{i_j} = 1 \\} \\right) \\right)\n  = \\overline{\\overline{\\textbf{x}}_r \\textbf{w}_i}.\n\\end{align}\n\\]\nThis equation states how one individual schema (\\(\\textbf{w}_{i}\\)) is applied to one attribute vector \\(\\textbf{x}_R\\). The first equation of this section summarizes this result into a matrix-matrix multiplication.\nAt the end all outputs of each individual schema are ORed to produce the final prediction for each attribute (corresponding to the provided attribute vector). This is done through multiplication with the identity tensor \\(\\textbf{1} \\in \\\\{1\\\\}^{L \\times D}\\). Remind that this is in alignment with the entity-attribute transition probability definition for non-positional attributes\n\\[\n\\begin{align}\n  T_{i,j} \\left( s_{i,j}^{(t+1)} | s^{(t)}, a^{(t)}\\right) = \\text{OR}\\left( \\phi^{k_1}, \\dots, \\phi^{k_Q}\\right)\n\\end{align}\n\\]\nAs stated above, for positional attributes a self-transition \\(\\Lambda_{i,j}\\) is added to allow these attributes to remain active when no change is predicted. Unfortunately, Kansky et al. (2017) did not elaborate on self-transitions in the learning problem. Thus, we can only guess how they are included. My idea would be to preprocess the data such that only positional attributes that changed (either from 0 to 1 or vice versa) are included in the learning problem. In the prediction phase, we then simply group all positional grounded schemas and apply the self-transition as a post-processing step.\n\n\nObjective Function\nAs there might be multiple Schemas that explain certain behaviors, the objective function is aimed to minimize the prediction error while keeping ungrounded schemas as simple as possible4:\n\\[\n\\begin{align}\n  \\min_{\\textbf{W}} J(\\textbf{W}) = \\min_{\\textbf{W}} \\underbrace{\\frac {1}{D} \\cdot \\Bigg| \\textbf{y} -\n  f_{\\textbf{W}} (\\textbf{X}) \\Bigg|_1 }_{\\text{Prediction Error}}+\n  \\underbrace{C \\cdot \\Bigg| \\textbf{W} \\Bigg|_1}_{\\text{Model Complexity}},\n\\end{align}\n\\]\nwhere \\(C\\) is a hyperparameter that can be used to control the trade-off between the complexity of the model and the accuracy of the predictions. This is a NP-hard optimization problem, since \\(\\textbf{W}\\in \\\\{0,1\\\\}^{E\\times L}\\) is a binary matrix. Furthermore, the search space is combinatorially large, i.e., there are \\(2^{E \\cdot L}\\) possible realizations of \\(\\textbf{W}\\). Hence, finding the optimal solution \\(\\textbf{W}^{*}\\) is infeasible (for larger environments such as Breakout).\n\n\nSchema Learning\nKansky et al. (2017) search for an approximate solution with the desired features (low prediction error and low model complexity) using a greedy algorithm of linear programming (LP) relaxations. This algorithm works as follows\n\nStart with an empty set of schemas, i.e., \\(\\textbf{W} = \\textbf{0}\\).\nGreedily select a schema \\(\\textbf{w}\\) that perfectly predicts a cluster of input samples:\n\nRandomly select an input sample \\(\\textbf{x}\\_n\\) for which \\(y_n = 1\\) and \\(f\\_{\\textbf{W}} (\\textbf{x}\\_n) = 0\\).\nPut sample in the set solved, then solve the following LP\n\\[\n\\begin{align}\n   \\begin{split}\n   &\\max_{\\textbf{w}\\in \\{0,1\\}^D} \\sum_{n: y_n = 1} \\overline{ \\overline{\\textbf{x}}_n \\textbf{w}} =\n   \\min_{\\textbf{w} \\in \\{0, 1\\}^{D}} \\sum (1 - {\\textbf{x}}_n \\textbf{w}) \\\\\n   &\\quad \\quad \\text{s.t. } \\forall_{n:y_n=0} \\quad (1 - \\textbf{x}_n) \\textbf{w} &gt; 1 \\qquad \\text{(no false alarms)}\\\\\n   &\\qquad \\quad \\hspace{0.3cm} \\forall_{n\\in \\text{solved}} \\hspace{0.2cm} (1-\\textbf{x}_n) \\textbf{w} = 0  \\qquad \\text{(active grounded schema)}\n   \\end{split}\n\\end{align}\n\\]\nUpdate the solved set, i.e., put all samples in for which \\((1-\\textbf{x}_n) \\textbf{w} = 0\\) in the solved set.\n\nSimplify resulting schema by making \\(\\textbf{w}\\) as sparse as possible while keeping the predictions correct without introducing false alarms:\n\\[\\begin{align}\n   \\begin{split}\n     &\\min_{\\textbf{w} \\in \\{0, 1\\}^D} \\textbf{w}^{\\text{T}} \\textbf{1} \\\\\n     &\\quad \\quad \\text{s.t. } \\forall_{n:y_n=0} \\quad (1 - \\textbf{x}_n) \\textbf{w} &gt; 1\\\\\n     &\\qquad \\quad \\hspace{0.3cm} \\forall_{n\\in \\text{solved}} \\hspace{0.2cm} (1-\\textbf{x}_n) \\textbf{w} = 0\n   \\end{split}\n\\end{align}\\]\nBinarize \\(\\textbf{w}\\)"
  },
  {
    "objectID": "paper_summaries/schema_networks/index.html#planning",
    "href": "paper_summaries/schema_networks/index.html#planning",
    "title": "Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics",
    "section": "Planning",
    "text": "Planning\nTODO: Summarize planning"
  },
  {
    "objectID": "paper_summaries/schema_networks/index.html#drawbacks",
    "href": "paper_summaries/schema_networks/index.html#drawbacks",
    "title": "Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics",
    "section": "Drawbacks",
    "text": "Drawbacks\n\nbased on specific and exact representation \\(\\Rightarrow\\) not end-to-end\nblow up of entity attributes, since:\n\nbinarized attributes using discretization and one-hot encoding\nall entities share the same set of attributes\n\nimage parser needs to know the whole attribute space beforehand\nNote: also the reward space needs to be defined beforehand.\nlearning algorithm is only capable of learning deterministic environments"
  },
  {
    "objectID": "paper_summaries/schema_networks/index.html#footnotes",
    "href": "paper_summaries/schema_networks/index.html#footnotes",
    "title": "Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA grounded schema in Schema Networks is similar to a rule in OO-MDP terms. Each attribute may have several grounded schemas. When one of those schemas is active (active effect condition in OO-MDP terms), the corresponding attribute is actived (set to True) in the next step.↩︎\nKansky et al. (2017) refer to the entity attributes in the binary variables \\(v_1, \\dots, v_n\\) of a grounded schema as entity-attribute preconditions. This terminology relates attributes to preconditions, since the actual condition (i.e., grounded schema) is only active iff all preconditions are active.↩︎\nIn Boolean logic, De Morgan’s law states that \\(\\text{AND} \\Big(A, B\\Big) =\\text{NOT} \\Big( \\text{OR} \\big(\\text{NOT } A, \\text{NOT } B \\big)\\Big)\\).↩︎\nOccam’s razor (law of parsimony) states that simplest solution is most likely the right one.↩︎"
  },
  {
    "objectID": "paper_summaries/auto-encoding_variational_bayes/index.html",
    "href": "paper_summaries/auto-encoding_variational_bayes/index.html",
    "title": "Auto-Encoding Variational Bayes",
    "section": "",
    "text": "Kingma and Welling (2013) introduced the Variational Auto-Encoder (VAE) to showcase how their Auto-Encoding Variational Bayes (AEVB) algorithm can be used in practice. Assuming i.i.d. datasets and continuous latent variables, the AEVB algorithm learns an approximate probabilistic encoder \\(q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x})\\) jointly with the probabilisitc decoder \\(p_{\\boldsymbol{\\theta}} (\\textbf{x}|\\textbf{z})\\) (where \\(\\boldsymbol{\\phi}, \\boldsymbol{\\theta}\\) parametrize the corresponding distributions) by learning the optimal model parameters \\(\\boldsymbol{\\phi}, \\boldsymbol{\\theta}\\) through optimizing an objective function with standard gradient ascent methods. In summary, a VAE is probabilistic autoencoder which uses variational inference to regularize the coding space. Furthermore, a VAE is a deep generative model as sampling from the coding space is possible, i.e., new observations can be generated."
  },
  {
    "objectID": "paper_summaries/auto-encoding_variational_bayes/index.html#model-description",
    "href": "paper_summaries/auto-encoding_variational_bayes/index.html#model-description",
    "title": "Auto-Encoding Variational Bayes",
    "section": "Model Description",
    "text": "Model Description\nThe AEVB algorithm basically assumes a generative process, introduces a variational approximation (see figure below) and optimizes the model parameters by maximizing an objective function. The objective function consists of the (reparametrized) variational lower bound of each datapoint. Reparametrization is necessary to allow the explicit formulation of gradients with respect to the model parameters.\n\n\n\n\n\n\n\n\n\n\n\nThe directed graphical models represent the assumed generative process (a) and the variational approximation of the intractable posterior (b) in the AEVB algorithm.\n\n\n\nObjective Function Derivation: Let \\(\\textbf{X}=\\\\{\\textbf{x}^{(i)}\\\\}_{i=1}^{N}\\) denote the dataset consisting of \\(N\\) i.i.d. samples and let \\(\\textbf{z}\\) denote the unobserved continuous random variable (i.e., hidden or code variable). Kingma and Welling (2013) assume that each observed sample \\(\\textbf{x}^{(i)}\\) comes from a generative process in which: Firstly, a hidden variable \\(\\textbf{z}^{(i)}\\) is generated from a prior distribution \\(p_{\\boldsymbol{\\theta}} (\\textbf{z})\\). Secondly, \\(\\textbf{x}^{(i)}\\) is generated from the conditional distribution \\(p_{\\boldsymbol{\\theta}}(\\textbf{x}|\\textbf{z}^{(i)})\\). Note that we do not know \\(\\boldsymbol{\\theta}\\) nor do we have information about \\(\\textbf{z}^{(i)}\\). In order to recover this generative process, they introduce \\(q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x})\\) as an approximation to the intractable true posterior1 \\(p_{\\boldsymbol{\\theta}} (\\textbf{z}|\\textbf{x})\\). The marginal log likelihood of each individual datapoint \\(\\textbf{x}^{(i)}\\) can then be stated as follows (see Eric Jang’s amazing blog post for detailed derivation)\n\\[\n  \\log p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}^{(i)}\\right) =\n  \\underbrace{D_{KL} \\left(q_{\\boldsymbol{\\phi}}\\left(\\textbf{z} | \\textbf{x}^{(i)}\\right)\n  || p_{\\boldsymbol{\\theta}} \\left(\\textbf{z}|\\textbf{x}^{(i)}\\right)\\right)}_{\\ge 0} +\n  \\mathcal{L} \\left( \\boldsymbol{\\theta}, \\boldsymbol{\\phi}, \\textbf{x}^{(i)}\\right) \\ge\n  \\mathcal{L} \\left( \\boldsymbol{\\theta}, \\boldsymbol{\\phi}, \\textbf{x}^{(i)}\\right),\n\\]\nwhere \\(D_{KL}(\\cdot)\\) denotes the KL divergence of the approximate from the true posterior (this quantity remains unknown since the true posterior \\(p_{\\boldsymbol{\\theta}} (\\textbf{z}|\\textbf{x}^{(i)})\\) is intractable). \\(\\mathcal{L} \\left(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}, \\textbf{x}^{(i)}\\right)\\) is called the variational lower bound or evidence lower bound (ELBO). The goal is to optimize \\(\\boldsymbol{\\phi}, \\boldsymbol{\\theta}\\) such that variational lower bound is maximized, thereby we indirectly maximize the marginal log likelihood. The variational lower bound can rewritten such that the objective function is obtained (also derived in Eric Jang’s blog post)\n\\[\n\\begin{align}\n  \\mathcal{L} \\left(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}, \\textbf{x}^{(i)}\\right) &=\n  \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x}^{(i)})}\n  \\left[ -\\log\n  q_{\\boldsymbol{\\phi}} (\\textbf{z} | \\textbf{x}^{(i)} ) +\n  \\log p_{\\boldsymbol{\\theta}} (\\textbf{z}) + \\log\n  p_{\\boldsymbol{\\theta}}\n  (\\textbf{x}^{(i)}|\\textbf{z}) \\right] \\\\\n  &= \\underbrace{-D_{KL} \\left(  q_{\\boldsymbol{\\phi}} \\left(\n  \\textbf{z} | \\textbf{x}^{(i)} \\right), p_{\\boldsymbol{\\theta}}\n  (\\textbf{z}) \\right)}_{\\text{Regularization Term}} +\n    \\underbrace{\n    \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x}^{(i)})}\n    \\left[ \\log p_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)}| \\textbf{z} \\right) \\right]}_{\\text{Reconstruction Accuracy}}\n    .\n\\end{align}\n\\]\nThe two terms have an associated interpretation in autoencoder language:\n\nReconstruction Accuracy (opposite of Reconstruction Error): The expectation can be interpreted using Monte Carlo integration,i.e.,\n\\[\n    \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x}^{(i)})}\n    \\left[ \\log p_{\\boldsymbol{\\theta}} \\left(\n    \\textbf{x}^{(i)} | \\textbf{z}\n    \\right) \\right] \\approx \\frac {1}{N} \\sum_{k=1}^{N} \\log p_{\\boldsymbol{\\theta}}\n    \\left( \\textbf{x}^{(i)} | \\textbf{z}^{(k)} \\right) \\qquad \\textbf{z}^{(k)} \\sim\n    q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x}^{(i)}),\n  \\]\nwhich results in an unbiased estimate. Sampling \\(\\textbf{z}^{(k)}\\sim q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x}^{(i)})\\) can be understood as encoding the observed input \\(\\textbf{x}^{(i)}\\) into a code \\(\\textbf{z}^{(k)}\\) using the probabilistic encoder \\(q_{\\boldsymbol{\\phi}}\\). Clearly, the expectation is maximized when the decoder \\(p_{\\boldsymbol{\\theta}}\\) maps the encoded input \\(\\textbf{z}^{(k)}\\) back the original input \\(\\textbf{x}^{(i)}\\), i.e., assigns high probability to \\(p_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)} | \\textbf{z}^{(i)} \\right)\\).\nRegularization Term: The KL divergence is non-negative and only zero if both distributions are identical. Thus, maximizing this term forces the encoder distribution \\(q_{\\boldsymbol{\\phi}}\\) to be close to the prior \\(p_{\\boldsymbol{\\theta}}(\\textbf{z})\\). In VAEs, the prior is typically set to be an isotropic normal distribution resulting in a regularized code space, i.e., encouraging a code space that is close to a normal distribution.\n\nReparametrization Trick: While the KL-divergence \\(D_{KL} \\left(  q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}^{(i)} \\right),  p_{\\boldsymbol{\\theta}} (\\textbf{z})\\right)\\) (i.e., the regularization term) can often be integrated analytically, the second term \\(\\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x}^{(i)})} \\left[ \\log  p_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)}| \\textbf{z} \\right) \\right]\\) (i.e., the reconstruction accuracy) requires sampling from \\(q_{\\boldsymbol{\\phi}}\\). There are two downsides associated wih sampling from \\(q_{\\boldsymbol{\\phi}}\\) approaches:\n\nBackpropagation does not work with a sampling operation, i.e., the implementation of VAEs would be more difficult.\nThe usual Monte Carlo gradient estimator (which relies on sampling from \\(q_{\\boldsymbol{\\phi}}\\)) w.r.t. \\(\\boldsymbol{\\phi}\\) exhibits very high variance.\n\nTo overcome these problems, Kingma and Welling (2013) use the reparametrization trick:\n\nSubstitute sampling \\(\\textbf{z} \\sim q_{\\boldsymbol{\\phi}}\\) by using a deterministic mapping \\(\\textbf{z} = g_{\\boldsymbol{\\phi}} (\\boldsymbol{\\epsilon}, \\textbf{x})\\) with the differential transformation \\(g_{\\boldsymbol{\\phi}}\\) of an auxiliary noise variable \\(\\boldsymbol{\\epsilon}\\) with \\(\\boldsymbol{\\epsilon}\\sim p(\\boldsymbol{\\epsilon})\\).\n\n As a result, the reparametrized objective function can be written as follows\n\\[\n  \\mathcal{L} \\left(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\textbf{x}^{(i)}\\right) =\n  -D_{KL} \\left(  q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}^{(i)} \\right),\n  p_{\\boldsymbol{\\theta}} (\\textbf{z}) \\right) +\n  \\mathbb{E}_{p(\\boldsymbol{\\epsilon})} \\left[ \\log\n  p_{\\boldsymbol{\\theta}}\n  \\left( \\textbf{x}^{(i)}| g_{\\boldsymbol{\\phi}} \\left(\n  \\boldsymbol{\\epsilon},\n  \\textbf{x}^{(i)} \\right) \\right) \\right]\n\\]\nin which the second term can be approximated with Monte Carlo integration yielding\n\\[\n  \\widetilde{\\mathcal{L}} \\left(\\boldsymbol{\\theta},\n  \\boldsymbol{\\phi};\n  \\textbf{x}^{(i)}\\right) =\n  -D_{KL} \\left(  q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}^{(i)} \\right),\n  p_{\\boldsymbol{\\theta}} (\\textbf{z}) \\right) +\n  \\frac {1}{L} \\sum_{l=1}^{L} \\log p_{\\boldsymbol{\\theta}}\\left(\n  \\textbf{x}^{(i)}| g_{\\boldsymbol{\\phi}}\n\\left( \\boldsymbol{\\epsilon}^{(i, l)}, \\textbf{x}^{(i)} \\right)\\right)\n\\]\nwith \\(\\boldsymbol{\\epsilon} \\sim p(\\boldsymbol{\\epsilon})\\). Note that Kingma and Welling denote this estimator as the second version of the Stochastic Gradient Variational Bayes (SGVB) estimator. Assuming that the KL-divergence can be integrated analytically, the derivatives \\(\\nabla_{\\boldsymbol{\\theta},\\boldsymbol{\\phi}} \\widetilde{L}\\) can be taken (see figure below), i.e., this estimator can be optimized using standard stochastic gradient methods.\n\n\n\n\n\n\n\n\n\n\n\nThe computation graphs summarize the difference between the computation of the reconstruction accuracy in the original objective (a) and the reparametrized objective (b). Circles indicate a sampling operation through which backpropagation is not allowed.\n\n\n\nTo increase stability and performance, Kingma and Welling introduce a minibatch estimator of the lower bound:\n\\[\n  \\widetilde{\\mathcal{L}}^{M} (\\boldsymbol{\\theta},\n  \\boldsymbol{\\phi}; \\textbf{X}^{M})  =  \\frac {N}{M}\n  \\sum_{i=1}^{M}\\widetilde{\\mathcal{L}} \\left(\n  \\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\textbf{x}^{(i)}\\right),\n\\]\nwhere \\(\\textbf{X}^{M} = \\left\\{ \\textbf{x}^{(i)} \\right\\}_{i=1}^{M}\\) denotes a minibatch of \\(M\\) datapoints from the full dataset \\(\\textbf{X}\\) of \\(N\\) datapoints."
  },
  {
    "objectID": "paper_summaries/auto-encoding_variational_bayes/index.html#learning-the-model",
    "href": "paper_summaries/auto-encoding_variational_bayes/index.html#learning-the-model",
    "title": "Auto-Encoding Variational Bayes",
    "section": "Learning the Model",
    "text": "Learning the Model\nLearning the probabilistic encoder \\(q_{\\boldsymbol{\\phi}}\\) and decoder \\(p_{\\boldsymbol{\\theta}}\\) comes down to learning the optimal model parameters \\(\\boldsymbol{\\phi}, \\boldsymbol{\\theta}\\) using the AEVB algorithm which can be summarized in 5 steps:\n\nInitialize model parameters \\(\\boldsymbol{\\phi}, \\boldsymbol{\\theta}\\) randomly.\nSample random minibatch \\(\\textbf{X}^{M} = \\left\\{ \\textbf{x}^{(i)} \\right\\}_{i=1}^{M}\\).\nCompute gradients \\(\\nabla_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}}  \\widetilde{\\mathcal{L}}^{M} \\left(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\textbf{X}^{M} \\right)\\).\nUpdate model parameters \\(\\boldsymbol{\\phi}, \\boldsymbol{\\theta}\\) by taking a gradient ascent step.\nRepeat steps 2-4 until model parameters converged"
  },
  {
    "objectID": "paper_summaries/auto-encoding_variational_bayes/index.html#vae-implementation",
    "href": "paper_summaries/auto-encoding_variational_bayes/index.html#vae-implementation",
    "title": "Auto-Encoding Variational Bayes",
    "section": "VAE Implementation",
    "text": "VAE Implementation\nA VAE simply uses deep neural networks (DNNs) as function approximators to parametrize the probabilistic encoder \\(q_{\\boldsymbol{\\phi}}\\) and decoder \\(p_{\\boldsymbol{\\theta}}\\). The optimal parameters \\(\\boldsymbol{\\phi}, \\boldsymbol{\\theta}\\) are learned jointly by training the VAE using the AEVB algorithm.\n\n\n\n\n\n\n\n\n\n\n\nSchematic of a standard VAE\n\n\n\nRegularization Term: Typically, the prior over the latent variables is set to be the centered isotropic Gaussian, i.e., \\(p_{\\boldsymbol{\\theta}} (\\textbf{z}) \\sim \\mathcal{N} (\\textbf{0}, \\textbf{I})\\). Note that this prior is needed to compute the regularization term in the objective function. Furthermore, it is commonly assumed that the true posterior \\(p\\_{\\boldsymbol{\\theta}}\\left(\\textbf{z} | \\textbf{x}^{(i)}\\right)\\) may be approximated by \\(q\\_{\\boldsymbol{\\phi}} \\left(\\textbf{z} | \\textbf{x}^{(i)} \\right) \\sim \\mathcal{N}\\left(\\boldsymbol{\\mu}_E^{(i)}, \\boldsymbol{\\sigma}_E^{2 (i)} \\textbf{I} \\right)\\) (subscripts denote that these parameters come from the encoder network). As a result, the regularization term can be integrated analytically leading to a term that only depends on \\(\\boldsymbol{\\mu}_E^{(i)}, \\boldsymbol{\\sigma}_E^{2 (i)}\\) (see Appendix B of Kingma and Welling)\n\\[\n-D_{KL} \\left(  q_{\\boldsymbol{\\phi}} \\left(\n  \\textbf{z} | \\textbf{x}^{(i)} \\right), p_{\\boldsymbol{\\theta}}\n  (\\textbf{z}) \\right) = \\frac {1}{2} \\sum_{j=1}^{J} \\left(\n  1 + \\log \\left( \\left( \\sigma_{E_j}^{(i)} \\right)^2 \\right)\n  - \\left( \\mu_{E_j}^{(i)}  \\right)^2 -  \\left( \\sigma_{E_j}^{(i)} \\right)^2\n  \\right),\n\\]\nwhere \\(J\\) denotes the latent space dimension.\nEncoder/Decoder Network: Kingma and Welling (2013) use simple neural networks with only one hidden layer to approximate the parameters of the probabilistic encoder and decoder. As stated above, the encoder network is fixed to compute the parameters \\(\\boldsymbol{\\mu}^{(i)}_E, \\boldsymbol{\\sigma}_E^{(i)} \\in \\mathbb{R}^{L}\\) of the Gaussian distribution \\(\\mathcal{N}\\left(\\boldsymbol{\\mu}_E^{(i)}, \\boldsymbol{\\sigma}_E^{2 (i)} \\textbf{I} \\right)\\). In fact, the encoder network takes a sample \\(\\textbf{x}^{(i)}\\) and outputs the mean \\(\\boldsymbol{\\mu}_E^{(i)}\\) and logarithmized variance, i.e.,\n\\[\n\\begin{bmatrix} \\boldsymbol{\\mu}_E^{(i)} & \\log\n\\boldsymbol{\\sigma}^{2(i)} \\end{bmatrix} = f_{\\boldsymbol{\\phi}} \\left( \\textbf{x}^{(i)} \\right).\n\\]\nNote that using the logarithmized version of the variance increases stability and simplifies the training2.\nIn principle, the encoder and decoder network are very similar only that the dimension of the input and output are reversed. While the encoder network is fixed to approximate a multivariate Gaussian with diagonal covariance structure, the decoder network can approximate a multivariate Gaussian (real-valued data) or Bernoulli (binary data) distribution.\nBelow is a simple Python class that can be used to instantiate the encoder or decoder network as described in appendix C of Kingma and Welling (2013).\nimport torch.nn as nn\nfrom collections import OrderedDict\n\n\nclass CoderNetwork(nn.Module):\n    r\"\"\"Encoder/Decoder for use in VAE based on Kingma and Welling\n    \n    Args:\n        input_dim: input dimension (int)\n        output_dim: output dimension (int)\n        hidden_dim: hidden layer dimension (int)\n        coder_type: encoder/decoder type can be \n                   'Gaussian'   - Gaussian with diagonal covariance structure\n                   'I-Gaussian' - Gaussian with identity as covariance matrix \n                   'Bernoulli'  - Bernoulli distribution       \n    \"\"\"\n    \n    def __init__(self, input_dim, hidden_dim, output_dim, coder_type='Gaussian'):\n        super().__init__()\n        \n        assert coder_type in  ['Gaussian', 'I-Gaussian' ,'Bernoulli'], \\\n            'unknown coder_type'\n        \n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.coder_type = coder_type\n        \n        self.coder = nn.Sequential(OrderedDict([\n            ('h', nn.Linear(input_dim, hidden_dim)),\n            ('ReLU', nn.ReLU()) # ReLU instead of Tanh proposed by K. and W.       \n        ]))\n        self.fc_mu = nn.Linear(hidden_dim, output_dim)\n        \n        if coder_type == 'Gaussian':\n            self.fc_log_var = nn.Linear(hidden_dim, output_dim)\n        elif coder_type == 'Bernoulli':\n            self.sigmoid_mu = nn.Sigmoid()\n        return\n    \n    def forward(self, inp):\n        out = self.coder(inp)\n        mu = self.fc_mu(out)\n        \n        if self.coder_type == 'Gaussian':\n            log_var = self.fc_log_var(out)\n            return [mu, log_var]\n        elif self.coder_type == 'I-Gaussian':\n            return mu\n        elif self.coder_type == 'Bernoulli':\n            return self.sigmoid_mu(mu)\n        return\nReconstruction Accuracy: Sampling from the encoder distribution is avoided by using the reparameterization trick, i.e., the latent variable \\(\\textbf{z}^{(i)}\\) is expressed as a deterministic variable\n\\[\n  \\textbf{z}^{(i, l)}=g_{\\boldsymbol{\\phi}} (\\textbf{x}^{(i)},\n  \\boldsymbol{\\epsilon}) = \\boldsymbol{\\mu}_E^{(i)} +\n  \\boldsymbol{\\sigma}_E^{(i)} \\odot \\boldsymbol{\\epsilon}^{(l)} \\quad\n  \\text{where}\n  \\quad \\boldsymbol{\\epsilon} \\sim\n  \\mathcal{N} (\\textbf{0}, \\textbf{I}),\n\\]\nand \\(\\odot\\) denotes element-wise multiplication.\nNote that we do not need to sample from the decoder distribution, since during training the reconstruction accuracy in the objective function only sums the log-likelihood of each sample \\(\\textbf{z}^{(i, l)}\\) and during test time we are mostly interested in the reconstructed \\(\\textbf{x}^{\\prime}\\) with highest probability, i.e., the mean.\nThe reconstruction accuracy in the reparametrized form is given by\n\\[\n\\text{Reconstruction Accuracy} =\n   \\frac {1}{L}\n\\sum_{l=1}^{L} \\log p_{\\boldsymbol{\\theta}}\\left(\n  \\textbf{x}^{(i)}| \\textbf{z}^{(i,l)}\\right),\n\\]\nwhere \\(L\\) denotes the number of samples used during the reparameterization trick. Depending on the chosen decoder distribution, the log-likelihood can be stated in terms of the estimated distribution parameters:\n\nGaussian distribution with diagonal covariance structure $ p_{} ( ^| _D^{(i)} , ( _D^{2(i)} ) ) $\n\\[\n\\log_e p_{\\boldsymbol{\\theta}}\\left(\n\\textbf{x}^{(i)}| \\textbf{z}^{(i,l)}\\right) = \\underbrace{- \\frac {D}{2} \\ln\n2\\pi}_{\\text{const}} - \\frac {1}{2} \\ln \\left( \\prod_{k=1}^{D} \\sigma_{D_k}^{2(i)} \\right)\n- \\frac {1}{2}\\sum_{k=1}^{D} \\frac {1}{\\sigma_{D_k}^{2(i)}}\\left( x_k^{(i)}  - \\mu_{D_k}^{(i)}\\right)^2\n\\]\nwith the original observation \\(\\textbf{x}^{(i)} \\in \\mathbb{R}^{D}\\). In this form, the objective function is ill-posed since there are no limitations on the form of the normal distribution. As a result the objective function is unbounded, i.e., the VAE could learn the true mean \\(\\boldsymbol{\\mu}\\_D^{(i)} = \\textbf{x}^{(i)}\\) with arbitrary variance \\(\\boldsymbol{\\sigma}\\_D^{2(i)}\\) or huge variances with arbitrary means to maximize the log-likelihood (see this post). Note that in the encoder network, the prior \\(p_{\\boldsymbol{\\theta}}(\\textbf{z})\\) is used to constrain the encoder distribution (i.e., the mean and variance).\nGaussian distribution with identity as covariance variance $ p_{} ( ^| _D^{(i)} , ) $\n\\[\n\\log_e p_{\\boldsymbol{\\theta}}\\left(\n\\textbf{x}^{(i)}| \\textbf{z}^{(i,l)}\\right) =\n- \\frac {1}{2}\\sum_{k=1}^{D} \\left( x_k^{(i)}  -\n\\mu_{D_k}^{(i)}\\right)^2 + \\text{const}\n\\]\nwith the original observation \\(\\textbf{x}^{(i)} \\in \\mathbb{R}^{D}\\). In this case the reconstruction accuracy is proportional to the negative mean squarred error which is typically used as the loss function in standard autoencoders.\n\nBernoulli distribution $ p_{} (^| _D^{(i)} ) = _{k=1}^{D} ( _{D_k}{(i)}){x_k^} ( 1 - _{D_k}{(i)}){1 - x_k^} $\n\\[\n   \\log_e p_{\\boldsymbol{\\theta}}\\left(\n\\textbf{x}^{(i)}| \\textbf{z}^{(i,l)}\\right) = \\sum_{k=1}^{D} \\left(\n   x_k^{(i)} \\ln \\left( \\mu_{D_k}^{(i)} \\right) + \\left(1 - x_k^{(i)}\n   \\right) \\ln \\left( 1 - \\mu_{D_k}^{(i)} \\right) \\right)\n\\]\nwith the original observation \\(\\textbf{x}^{(i)} \\in \\\\{0, 1\\\\}^{D}\\). In this case the reconstruction accuracy equals the negative binary cross entropy loss. Note that there are plenty of VAE implementations that use the binary cross entropy loss on non-binary observations, see discussions in this thread.\n\nTo put this into practice, below is a simple VAE Python class which will be used to compare the different decoder distributions.\nimport torch\nfrom torch.distributions.multivariate_normal import MultivariateNormal\n\n\nclass VAE(nn.Module):\n    r\"\"\"A simple VAE class based on Kingma and Welling\n        \n    Args:\n        encoder_network:  instance of CoderNetwork class\n        decoder_network:  instance of CoderNetwork class\n        L:                number of samples used during reparameterization trick\n    \"\"\"\n    \n    def __init__(self, encoder_network, decoder_network, L=1):\n        super().__init__()\n        self.encoder = encoder_network\n        self.decoder = decoder_network\n        self.L = L\n        \n        latent_dim = encoder_network.output_dim\n                \n        self.normal_dist = MultivariateNormal(torch.zeros(latent_dim), \n                                              torch.eye(latent_dim))\n        return\n    \n    def forward(self, x):\n        L = self.L\n        \n        z, mu_E, log_var_E = self.encode(x, L)\n        # regularization term per batch, i.e., size: (batch_size)\n        regularization_term = (1/2) * (1 + log_var_E - mu_E**2\n                                       - torch.exp(log_var_E)).sum(axis=1)\n        \n        # upsample x and reshape\n        batch_size = x.shape[0]\n        x_ups = x.repeat(L, 1).view(batch_size, L, -1)    \n        if self.decoder.coder_type == 'Gaussian':\n            # mu_D, log_var_D have shape (batch_size, L, output_dim)\n            mu_D, log_var_D = self.decode(z)\n            # reconstruction accuracy per batch, i.e., size: (batch_size)\n            recons_acc = (1/L) * (-(0.5)*(log_var_D.sum(axis=2)).sum(axis=1)\n               -(0.5) * ((1/torch.exp(log_var_D))*((x_ups - mu_D)**2)\n                         ).sum(axis=2).sum(axis=1))\n        elif self.decoder.coder_type == 'I-Gaussian':\n            # mu_D has shape (batch_size, L, output_dim)\n            mu_D = self.decode(z)\n            # reconstruction accuracy per batch, i.e., size: (batch_size)\n            recons_acc = (1/L) * (-(0.5) * ((x_ups - mu_D)**2\n                                            ).sum(axis=2).sum(axis=1))\n        elif self.decoder.coder_type == 'Bernoulli':\n            # mu_D has shape (batch_size, L, output_dim)\n            mu_D = self.decode(z)     \n            # reconstruction accuracy per batch, i.e., size: (batch_size)\n            # corresponds to the negative binary cross entropy loss (BCELoss)\n            recons_acc = (1/L) * (x_ups * torch.log(mu_D) + \n                                  (1 - x_ups) * torch.log(1 - mu_D)\n                                  ).sum(axis=2).sum(axis=1)\n        loss = - regularization_term.sum() - recons_acc.sum()\n        return loss\n    \n    def encode(self, x, L=1):\n        # get encoder distribution parameters\n        mu_E, log_var_E = self.encoder(x)\n        # sample noise variable L times for each batch\n        batch_size = x.shape[0]\n        epsilon = self.normal_dist.sample(sample_shape=(batch_size, L, ))\n        # upsample mu_E, log_var_E and reshape\n        mu_E_ups = mu_E.repeat(L, 1).view(batch_size, L, -1) \n        log_var_E_ups = log_var_E.repeat(L, 1).view(batch_size, L, -1)\n        # get latent variable by reparametrization trick\n        z = mu_E_ups + torch.sqrt(torch.exp(log_var_E_ups)) * epsilon\n        return z, mu_E, log_var_E\n    \n    def decode(self, z):\n        # get decoder distribution parameters\n        if self.decoder.coder_type == 'Gaussian':\n            mu_D, log_var_D = self.decoder(z)\n            return mu_D, log_var_D\n        elif self.decoder.coder_type == 'I-Gaussian':\n            mu_D = self.decoder(z)\n            return mu_D\n        elif self.decoder.coder_type == 'Bernoulli':\n            mu_D = self.decoder(z)\n            return mu_D\n        return\nLet’s train the three different VAEs on the MNIST digits dataset\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n\ndef train(decoder_type, dataset, x_dim, hid_dim, z_dim, batch_size, L, epochs):\n    encoder_network = CoderNetwork(input_dim=x_dim, \n                                   hidden_dim=hid_dim, \n                                   output_dim=z_dim,\n                                   coder_type='Gaussian')\n    decoder_network = CoderNetwork(input_dim=z_dim, \n                                   hidden_dim=hid_dim, \n                                   output_dim=x_dim,\n                                   coder_type=decoder_type)\n    \n    model = VAE(encoder_network, decoder_network, L=L)\n    data_loader = DataLoader(dataset, batch_size, shuffle=True)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n    \n    print('Start training with {} decoder distribution\\n'.format(decoder_type))\n    for epoch in range(1, epochs + 1):\n        print('Epoch {}/{}'.format(epoch, epochs))\n        avg_loss = 0\n        for counter, (mini_batch_data, label) in enumerate(data_loader):\n            \n            model.zero_grad()\n            \n            loss = model(mini_batch_data.view(-1, x_dim))\n            loss.backward()\n            optimizer.step()\n            \n            avg_loss += loss.item() / len(dataset)\n            \n            if counter % 20 == 0 or (counter + 1)==len(data_loader):\n                batch_loss = loss.item() / len(mini_batch_data)\n                print('\\r[{}/{}] batch loss: {:.2f}'.format(counter + 1,\n                                                            len(data_loader),\n                                                            batch_loss),\n                      end='', flush=True)\n        print('\\nAverage loss: {:.3f}'.format(avg_loss)) \n    print('Done!\\n')\n    trained_VAE = model\n    return trained_VAE\n\ndataset = datasets.MNIST('data/', transform=transforms.ToTensor(), download=True)\nx_dim, hid_dim, z_dim = 28*28, 400, 20\nbatch_size, L, epochs = 128, 5, 3\n\nBernoulli_VAE = train('Bernoulli', dataset, x_dim, hid_dim, z_dim, \n                      batch_size, L, epochs)\nGaussian_VAE = train('Gaussian', dataset, x_dim, hid_dim, z_dim, \n                     batch_size, L, epochs)\nI_Gaussian_VAE = train('I-Gaussian', dataset, x_dim, hid_dim, z_dim, \n                       batch_size, L, epochs)\nStart training with Bernoulli decoder distribution\n\nEpoch 1/3\n[469/469] batch loss: 129.51\nAverage loss: 148.437\nEpoch 2/3\n[469/469] batch loss: 112.28\nAverage loss: 122.225\nEpoch 3/3\n[469/469] batch loss: 116.99\nAverage loss: 119.317\nDone!\n\nStart training with Gaussian decoder distribution\n\nEpoch 1/3\n[469/469] batch loss: -1403.16\nAverage loss: -1227.643\nEpoch 2/3\n[469/469] batch loss: -1289.26\nAverage loss: -1281.867\nEpoch 3/3\n[469/469] batch loss: -1582.44\nAverage loss: -1342.641\nDone!\n\nStart training with I-Gaussian decoder distribution\n\nEpoch 1/3\n[469/469] batch loss: 22.13\nAverage loss: 27.051\nEpoch 2/3\n[469/469] batch loss: 21.58\nAverage loss: 21.879\nEpoch 3/3\n[469/469] batch loss: 21.88\nAverage loss: 21.301\nDone!\n\n\n\n\n\nLet’s look at the differences in the reconstructions:\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\ndef plot_results(trained_model, dataset, n_samples):\n    decoder_type = trained_model.decoder.coder_type\n    \n    fig = plt.figure(figsize=(14, 3))\n    fig.suptitle(decoder_type + ' Distribution: Observations (top row) and ' +\n                 'their reconstructions (bottom row)')\n    for i_sample in range(n_samples):\n        x_sample = dataset[i_sample][0].view(-1, 28*28)\n        \n        z, mu_E, log_var_E = trained_model.encode(x_sample, L=1)\n        if decoder_type in ['Bernoulli', 'I-Gaussian']:\n            x_prime = trained_model.decode(z)\n        else:\n            x_prime = trained_model.decode(z)[0]\n    \n        plt.subplot(2, n_samples, i_sample + 1)\n        plt.imshow(x_sample.view(28, 28).data.numpy())\n        plt.axis('off')\n        plt.subplot(2, n_samples, i_sample + 1 + n_samples)\n        plt.imshow(x_prime.view(28, 28).data.numpy())\n        plt.axis('off')\n    return\n\n\nn_samples = 10\n\nplot_results(Bernoulli_VAE, dataset, n_samples)\nplot_results(Gaussian_VAE, dataset, n_samples)\nplot_results(I_Gaussian_VAE, dataset, n_samples)\n\n\n\nplot results"
  },
  {
    "objectID": "paper_summaries/auto-encoding_variational_bayes/index.html#acknowledgement",
    "href": "paper_summaries/auto-encoding_variational_bayes/index.html#acknowledgement",
    "title": "Auto-Encoding Variational Bayes",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nDaniel Daza’s blog was really helpful and the presented code is highly inspired by his summary on VAEs."
  },
  {
    "objectID": "paper_summaries/auto-encoding_variational_bayes/index.html#footnotes",
    "href": "paper_summaries/auto-encoding_variational_bayes/index.html#footnotes",
    "title": "Auto-Encoding Variational Bayes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe true posterior could be calculated via Bayes theorem \\(\\displaystyle p_{\\boldsymbol{\\theta}} (\\textbf{z}|\\textbf{x}) = \\frac {p_{\\boldsymbol{\\theta}} (\\textbf{x}|\\textbf{z}) p_{\\boldsymbol{\\theta}} (\\textbf{z})} {\\int p_{\\boldsymbol{\\theta}} (\\textbf{x}|\\textbf{z}) p_{\\boldsymbol{\\theta}} (\\textbf{z}) d\\textbf{z}}\\). However, the integral in the denominator is intractable in practice.↩︎\nNote that the variance is by definition greater than zero. Furthermore, the variance is typically relatively small. Thus, using the logarithmized variance as network output increases stability and performance (see this answer for details).↩︎"
  }
]