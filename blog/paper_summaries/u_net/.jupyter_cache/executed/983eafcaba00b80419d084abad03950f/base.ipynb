{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "262bd3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# imports\n",
    "import os\n",
    "import sys\n",
    "import types\n",
    "import json\n",
    "\n",
    "# figure size/format\n",
    "fig_width = 7\n",
    "fig_height = 5\n",
    "fig_format = 'retina'\n",
    "fig_dpi = 96\n",
    "\n",
    "# matplotlib defaults / format\n",
    "try:\n",
    "  import matplotlib.pyplot as plt\n",
    "  plt.rcParams['figure.figsize'] = (fig_width, fig_height)\n",
    "  plt.rcParams['figure.dpi'] = fig_dpi\n",
    "  plt.rcParams['savefig.dpi'] = fig_dpi\n",
    "  from IPython.display import set_matplotlib_formats\n",
    "  set_matplotlib_formats(fig_format)\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# plotly use connected mode\n",
    "try:\n",
    "  import plotly.io as pio\n",
    "  pio.renderers.default = \"notebook_connected\"\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# enable pandas latex repr when targeting pdfs\n",
    "try:\n",
    "  import pandas as pd\n",
    "  if fig_format == 'pdf':\n",
    "    pd.set_option('display.latex.repr', True)\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "\n",
    "\n",
    "# output kernel dependencies\n",
    "kernel_deps = dict()\n",
    "for module in list(sys.modules.values()):\n",
    "  # Some modules play games with sys.modules (e.g. email/__init__.py\n",
    "  # in the standard library), and occasionally this can cause strange\n",
    "  # failures in getattr.  Just ignore anything that's not an ordinary\n",
    "  # module.\n",
    "  if not isinstance(module, types.ModuleType):\n",
    "    continue\n",
    "  path = getattr(module, \"__file__\", None)\n",
    "  if not path:\n",
    "    continue\n",
    "  if path.endswith(\".pyc\") or path.endswith(\".pyo\"):\n",
    "    path = path[:-1]\n",
    "  if not os.path.exists(path):\n",
    "    continue\n",
    "  kernel_deps[path] = os.stat(path).st_mtime\n",
    "print(json.dumps(kernel_deps))\n",
    "\n",
    "# set run_path if requested\n",
    "if r'/home/borea17/GIT/borea17.github.io/blog/paper_summaries/u_net':\n",
    "  os.chdir(r'/home/borea17/GIT/borea17.github.io/blog/paper_summaries/u_net')\n",
    "\n",
    "# reset state\n",
    "%reset\n",
    "\n",
    "def ojs_define(**kwargs):\n",
    "  import json\n",
    "  try:\n",
    "    # IPython 7.14 preferred import\n",
    "    from IPython.display import display, HTML\n",
    "  except:\n",
    "    from IPython.core.display import display, HTML\n",
    "\n",
    "  # do some minor magic for convenience when handling pandas\n",
    "  # dataframes\n",
    "  def convert(v):\n",
    "    try:\n",
    "      import pandas as pd\n",
    "    except ModuleNotFoundError: # don't do the magic when pandas is not available\n",
    "      return v\n",
    "    if type(v) == pd.Series:\n",
    "      v = pd.DataFrame(v)\n",
    "    if type(v) == pd.DataFrame:\n",
    "      j = json.loads(v.T.to_json(orient='split'))\n",
    "      return dict((k,v) for (k,v) in zip(j[\"index\"], j[\"data\"]))\n",
    "    else:\n",
    "      return v\n",
    "  \n",
    "  v = dict(contents=list(dict(name=key, value=convert(value)) for (key, value) in kwargs.items()))\n",
    "  display(HTML('<script type=\"ojs-define\">' + json.dumps(v) + '</script>'), metadata=dict(ojs_define = True))\n",
    "globals()[\"ojs_define\"] = ojs_define\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3474b444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    num_img, img_size = 30, 512\n",
    "    # initialize\n",
    "    imgs = torch.zeros(num_img, 1, img_size, img_size)\n",
    "    labels = torch.zeros(num_img, 1, img_size, img_size)\n",
    "    # fill tensors with data\n",
    "    for index in range(num_img):\n",
    "        cur_name = str(index) + '.png'\n",
    "\n",
    "        img_frame = Image.open('./Dataset/train/image/' + cur_name)\n",
    "        label_frame = Image.open('./Dataset/train/label/' + cur_name)\n",
    "\n",
    "        imgs[index] = transforms.ToTensor()(img_frame).type(torch.float32)\n",
    "        labels[index] = transforms.ToTensor()(label_frame).type(torch.float32)\n",
    "    return imgs, labels\n",
    "\n",
    "\n",
    "imgs, labels = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6d0dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.signal import convolve2d\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "class EM_Dataset(Dataset):\n",
    "    \"\"\"EM Dataset (from ISBI 2012) to train U-Net on including data\n",
    "    augmentation as proposed by Ronneberger et al. (2015)\n",
    "\n",
    "    Args:\n",
    "        imgs (tensor): torch tensor containing input images [1, 512, 512]\n",
    "        labels (tensor): torch tensor containing segmented images [1, 512, 512]\n",
    "        stride (int): stride that is used for overlap-tile strategy,\n",
    "            Note: stride must be chosen such that all labels are retrieved\n",
    "        transformation (bool): transform should be applied (True) or not (False)\n",
    "    ------- transformation related -------\n",
    "        probability (float): probability that transformation is applied\n",
    "        alpha (float): intensity of elastic deformation\n",
    "        sigma (float): std dev. of Gaussian kernel, i.e., smoothing parameter\n",
    "        kernel dim (int): kernel size is [kernel_dim, kernel_dim]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, imgs, labels, stride, transformation=False,\n",
    "                 probability=None, alpha=None, sigma=None, kernel_dim=None):\n",
    "        super().__init__()\n",
    "        assert isinstance(stride, int) and stride <= 124 and \\\n",
    "          round((512-388)/stride) == (512-388)/stride\n",
    "        self.orig_imgs = imgs\n",
    "        self.imgs = EM_Dataset._extrapolate_by_mirroring(imgs)\n",
    "        self.labels = labels\n",
    "        self.stride = stride\n",
    "        self.transformation = transformation\n",
    "        if transformation:\n",
    "            assert 0 <= probability <= 1\n",
    "            self.probability = probability\n",
    "            self.alpha = alpha\n",
    "            self.kernel = EM_Dataset._create_gaussian_kernel(kernel_dim, sigma)\n",
    "        return\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"images and labels are divided into several overlaping parts using the\n",
    "        overlap-tile strategy\n",
    "        \"\"\"\n",
    "        number_of_tiles_1D = (1 + int((512 - 388)/self.stride))\n",
    "        number_of_tiles_2D = number_of_tiles_1D**2\n",
    "\n",
    "        img_index = int(index/number_of_tiles_2D)\n",
    "        # tile indexes of image\n",
    "        tile_index = (index % number_of_tiles_2D)\n",
    "        tile_index_x = (tile_index % number_of_tiles_1D) * self.stride\n",
    "        tile_index_y = int(tile_index / number_of_tiles_1D) * self.stride\n",
    "\n",
    "        img = self.imgs[img_index, :,\n",
    "                        tile_index_y:tile_index_y + 572,\n",
    "                        tile_index_x:tile_index_x + 572]\n",
    "        label = self.labels[img_index, :,\n",
    "                            tile_index_y: tile_index_y + 388,\n",
    "                            tile_index_x: tile_index_x + 388]\n",
    "        if self.transformation:\n",
    "            if np.random.random() > 1 - self.probability:\n",
    "                img, label = EM_Dataset.elastic_deform(img, label, self.alpha,\n",
    "                                                       self.kernel)\n",
    "        return (img, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        number_of_imgs = len(self.imgs)\n",
    "        number_of_tiles = (1 + int((512 - 388)/self.stride))**2\n",
    "        return number_of_imgs * number_of_tiles\n",
    "\n",
    "    @staticmethod\n",
    "    def gray_value_variations(image, sigma):\n",
    "        \"\"\"applies gray value variations by adding Gaussian noise\n",
    "\n",
    "        Args:\n",
    "            image (torch tensor): extrapolated image tensor [1, 572, 572]\n",
    "            sigma (float): std. dev. of Gaussian distribution\n",
    "\n",
    "        Returns:\n",
    "            image (torch tensor): image tensor w. gray value var. [1, 572, 572]\n",
    "        \"\"\"\n",
    "        # see https://stats.stackexchange.com/a/383976\n",
    "        noise = torch.randn(image.shape, dtype=torch.float32) * sigma\n",
    "        return image + noise\n",
    "\n",
    "    @staticmethod\n",
    "    def affine_transform(image, label, angle, translate):\n",
    "        \"\"\"applies random affine translations and rotation on image and label\n",
    "\n",
    "        Args:\n",
    "            image (torch tensor): extrapolated image tensor [1, 572, 572]\n",
    "            label (torch tensor): label tensor [1, 388, 388]\n",
    "            angle (float): rotation angle\n",
    "            translate (list): entries correspond to horizontal and vertical shift\n",
    "\n",
    "        Returns:\n",
    "            image (torch tensor): transformed image tensor [1, 572, 572]\n",
    "            label (torch tensor): transformed label tensor [1, 388, 388]\n",
    "        \"\"\"\n",
    "        # transform to PIL\n",
    "        image = transforms.ToPILImage()(image[0])\n",
    "        label = transforms.ToPILImage()(label[0])\n",
    "        # apply affine transformation\n",
    "        image = TF.affine(image, angle=angle, translate=translate,\n",
    "                          scale=1, shear=0)\n",
    "        label = TF.affine(label, angle=angle, translate=translate,\n",
    "                          scale=1, shear=0)\n",
    "        # transform back to tensor\n",
    "        image = transforms.ToTensor()(np.array(image))\n",
    "        label = transforms.ToTensor()(np.array(label))\n",
    "        return image, label\n",
    "\n",
    "    @staticmethod\n",
    "    def elastic_deform(image, label, alpha, gaussian_kernel):\n",
    "        \"\"\"apply smooth elastic deformation on image and label data as\n",
    "        described in\n",
    "\n",
    "        [Simard2003] \"Best Practices for Convolutional Neural Networks applied\n",
    "        to Visual Document Analysis\"\n",
    "\n",
    "        Args:\n",
    "            image (torch tensor): extrapolated image tensor [1, 572, 572]\n",
    "            label (torch tensor): label tensor [1, 388, 388]\n",
    "            alpha (float): intensity of transformation\n",
    "            gaussian_kernel (np array): gaussian kernel used for smoothing\n",
    "\n",
    "        Returns:\n",
    "            deformed_img (torch tensor): deformed image tensor [1, 572, 572]\n",
    "            deformed_label (torch tensor): deformed label tensor [1, 388, 388]\n",
    "\n",
    "        code is adapted from https://github.com/vsvinayak/mnist-helper\n",
    "        \"\"\"\n",
    "        # generate standard coordinate grids\n",
    "        x_i, y_i = np.meshgrid(np.arange(572), np.arange(572))\n",
    "        x_l, y_l = np.meshgrid(np.arange(388), np.arange(388))\n",
    "        # generate random displacement fields (uniform distribution [-1, 1])\n",
    "        dx = 2*np.random.rand(*x_i.shape) - 1\n",
    "        dy = 2*np.random.rand(*y_i.shape) - 1\n",
    "        # smooth by convolving with gaussian kernel\n",
    "        dx = alpha * convolve2d(dx, gaussian_kernel, mode='same')\n",
    "        dy = alpha * convolve2d(dy, gaussian_kernel, mode='same')\n",
    "        # one dimensional coordinates (neccessary for map_coordinates)\n",
    "        x_img = np.reshape(x_i + dx, (-1, 1))\n",
    "        y_img = np.reshape(y_i + dy, (-1, 1))\n",
    "        x_label = np.reshape(x_l + dx[92:480, 92:480], (-1, 1))\n",
    "        y_label = np.reshape(y_l + dy[92:480, 92:480], (-1, 1))\n",
    "        # deformation using map_coordinates interpolation (spline not bicubic)\n",
    "        deformed_img = map_coordinates(image[0], [y_img, x_img], order=1,\n",
    "                                       mode='reflect')\n",
    "        deformed_label = map_coordinates(label[0], [y_label, x_label], order=1,\n",
    "                                         mode='reflect')\n",
    "        # reshape into desired shape and cast to tensor\n",
    "        deformed_img = torch.from_numpy(deformed_img.reshape(image.shape))\n",
    "        deformed_label = torch.from_numpy(deformed_label.reshape(label.shape))\n",
    "        return deformed_img, deformed_label\n",
    "\n",
    "    @staticmethod\n",
    "    def _extrapolate_by_mirroring(data):\n",
    "        \"\"\"increase data by mirroring (needed for overlap-tile strategy)\n",
    "\n",
    "        Args:\n",
    "            data (torch tensor): shape [num_samples, 1, 512, 512]\n",
    "\n",
    "        Returns:\n",
    "            extrapol_data (torch tensor): shape [num_samples, 1, 696, 696]\n",
    "        \"\"\"\n",
    "        num_samples = len(data)\n",
    "        extrapol_data = torch.zeros(num_samples, 1, 696, 696)\n",
    "\n",
    "        # put data into center of extrapol data\n",
    "        extrapol_data[:,:, 92:92+512, 92:92+512] = data\n",
    "        # mirror left\n",
    "        extrapol_data[:,:, 92:92+512, 0:92] = data[:,:,:,0:92].flip(3)\n",
    "        # mirror right\n",
    "        extrapol_data[:,:, 92:92+512, 92+512::] = data[:,:,:,-92::].flip(3)\n",
    "        # mirror top\n",
    "        extrapol_data[:,:, 0:92,:] = extrapol_data[:,:,92:92+92,:].flip(2)\n",
    "        # mirror buttom\n",
    "        extrapol_data[:,:, 92+512::,:] = extrapol_data[:,:, 512:512+92,:].flip(2)\n",
    "        return extrapol_data\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_gaussian_kernel(kernel_dim, sigma):\n",
    "        \"\"\"returns a 2D Gaussian kernel with the standard deviation\n",
    "        denoted by sigma\n",
    "\n",
    "        Args:\n",
    "            kernel_dim (int): kernel size will be [kernel_dim, kernel_dim]\n",
    "            sigma (float): std dev of Gaussian (smoothing parameter)\n",
    "\n",
    "        Returns:\n",
    "            gaussian_kernel (numpy array): centered gaussian kernel\n",
    "\n",
    "        code is adapted from https://github.com/vsvinayak/mnist-helper\n",
    "        \"\"\"\n",
    "        # check if the dimension is odd\n",
    "        if kernel_dim % 2 == 0:\n",
    "            raise ValueError(\"Kernel dimension should be odd\")\n",
    "        # initialize the kernel\n",
    "        kernel = np.zeros((kernel_dim, kernel_dim), dtype=np.float16)\n",
    "        # calculate the center point\n",
    "        center = kernel_dim/2\n",
    "        # calculate the variance\n",
    "        variance = sigma ** 2\n",
    "        # calculate the normalization coefficeint\n",
    "        coeff = 1. / (2 * variance)\n",
    "        # create the kernel\n",
    "        for x in range(0, kernel_dim):\n",
    "            for y in range(0, kernel_dim):\n",
    "                x_val = abs(x - center)\n",
    "                y_val = abs(y - center)\n",
    "                numerator = x_val**2 + y_val**2\n",
    "                denom = 2*variance\n",
    "\n",
    "                kernel[x,y] = coeff * np.exp(-1. * numerator/denom)\n",
    "        # normalise it\n",
    "        return kernel/sum(sum(kernel))\n",
    "\n",
    "\n",
    "# generate datasets\n",
    "stride = 124\n",
    "whole_dataset = EM_Dataset(imgs, labels, stride=stride,\n",
    "                           transformation=True, probability=0.5, alpha=50,\n",
    "                           sigma=5, kernel_dim=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a676f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualize_overlap_tile_strategy(dataset, img_index, tile_indexes):\n",
    "    # compute tiling data\n",
    "    number_of_tiles_1D = (1 + int((512 - 388)/dataset.stride))\n",
    "    number_of_tiles_2D = number_of_tiles_1D**2\n",
    "    # original image [1, 512, 512]\n",
    "    orig_img = dataset.orig_imgs[img_index]\n",
    "    # extrapolated image [1, 696, 696]\n",
    "    extrapol_img = dataset.imgs[img_index]\n",
    "\n",
    "\n",
    "    # start plotting\n",
    "    fig = plt.figure(figsize=(14, 7))\n",
    "    # original image\n",
    "    plt.subplot(1, len(tile_indexes) + 1, 1)\n",
    "    plt.imshow(transforms.ToPILImage()(orig_img), cmap='gray')\n",
    "    plt.title('Original Image')\n",
    "    # extrapolated image with bounding boxes and mirror lines for tile_indexes\n",
    "    for index, tile_index in enumerate(tile_indexes):\n",
    "        plt.subplot(1, len(tile_indexes) + 1, 2 + index)\n",
    "        plt.imshow(transforms.ToPILImage()(extrapol_img), cmap='gray')\n",
    "        # calculate tile index x and y\n",
    "        tile_ix = (tile_index % number_of_tiles_1D) * dataset.stride\n",
    "        tile_iy = int(tile_index / number_of_tiles_1D) * dataset.stride\n",
    "        # add focus of current input tile\n",
    "        plt.plot([tile_ix, tile_ix + 572, tile_ix + 572, tile_ix, tile_ix],\n",
    "                 [tile_iy, tile_iy, tile_iy + 572, tile_iy + 572, tile_iy],\n",
    "                 'blue', linewidth=2)\n",
    "        # add focus of current segmentation mask\n",
    "        tile_ix, tile_iy = tile_ix + 92, tile_iy + 92\n",
    "        plt.plot([tile_ix, tile_ix + 388, tile_ix + 388, tile_ix, tile_ix],\n",
    "                 [tile_iy, tile_iy, tile_iy + 388, tile_iy + 388, tile_iy],\n",
    "                 'yellow', linewidth=2)\n",
    "        # add mirror lines\n",
    "        plt.vlines([92, 604], 0, 696, 'white', linewidth=1)\n",
    "        plt.hlines([92, 604], 0, 696, 'white', linewidth=1)\n",
    "        plt.title('Extrapolated Image, Tile '+ str(tile_index + 1) + '/' +\n",
    "                  str(number_of_tiles_2D))\n",
    "        plt.xlim(0, 696)\n",
    "        plt.ylim(696, 0)\n",
    "    return\n",
    "\n",
    "\n",
    "def visualize_data_augmentation(dataset, index, show_grid, kind):\n",
    "    # get untransformed img, label\n",
    "    dataset.transformation = False\n",
    "    img, label = dataset[index]\n",
    "    # copy image (since it may be modified)\n",
    "    cur_img = img.clone().numpy()\n",
    "    cur_label = label.clone().numpy()\n",
    "    if show_grid:\n",
    "        # modify image to include outer grid (outside of label)\n",
    "        cur_img[0, 0:91:25] = 10.0\n",
    "        cur_img[0, 480::25] = 10.0\n",
    "        cur_img[0, :, 0:91:25] = 10.0\n",
    "        cur_img[0, :, 480::25] = 10.0\n",
    "        # modify image to include label grid\n",
    "        cur_img[0, 92:480:20, 92:480] = -5\n",
    "        cur_img[0,  92:480, 92:480:20] = -5\n",
    "        # modify label to include label grid\n",
    "        cur_label[0, ::20] = -5\n",
    "        cur_label[0, :, ::20] = -5\n",
    "    if kind == 'elastic deformation':\n",
    "        # set transformation\n",
    "        kernel = dataset.kernel\n",
    "        alpha = dataset.alpha\n",
    "        new_img, new_label = EM_Dataset.elastic_deform(cur_img, cur_label,\n",
    "                                                       alpha, kernel)\n",
    "    elif kind == 'affine transformation':\n",
    "        angle = np.random.randint(-3, 3)\n",
    "        translate = list(np.random.randint(-3, 3, size=2))\n",
    "        new_img, new_label = EM_Dataset.affine_transform(cur_img, cur_label,\n",
    "                                                         angle, translate)\n",
    "    elif kind == 'gray value variation':\n",
    "        sigma = 0.2\n",
    "        new_img = EM_Dataset.gray_value_variations(img, sigma)\n",
    "        new_label = label\n",
    "    else:\n",
    "        raise NameError('Unknown `kind`, can only be `elastic deformation`, ' +\n",
    "                        '`affine transformation` or `gray value variation`')\n",
    "    # start plotting\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.title('Before ' + kind)\n",
    "    plt.imshow(cur_img[0], cmap='gray', aspect='equal',\n",
    "               interpolation='gaussian', vmax=1, vmin=0)\n",
    "    # focus of current segmentation mask\n",
    "    plt.plot([92, 480, 480, 92, 92], [92, 92, 480, 480, 92],\n",
    "            'yellow', linewidth=2)\n",
    "    plt.subplots_adjust(hspace=0.01)\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.imshow(cur_label[0], cmap='gray', aspect='equal',\n",
    "               interpolation='gaussian', vmax=1, vmin=0)\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.title('After ' + kind)\n",
    "    plt.imshow(new_img[0], cmap='gray', aspect='equal',\n",
    "               interpolation='gaussian', vmax=1, vmin=0)\n",
    "    # focus of current segmentation mask\n",
    "    plt.plot([92, 480, 480, 92, 92], [92, 92, 480, 480, 92],\n",
    "            'yellow', linewidth=2)\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.imshow(new_label[0], cmap='gray', aspect='equal',\n",
    "               interpolation='gaussian', vmax=1, vmin=0)\n",
    "    return\n",
    "\n",
    "\n",
    "# generate images in order of appearance\n",
    "visualize_overlap_tile_strategy(whole_dataset, img_index=0,\n",
    "                                tile_indexes=[0, 1])\n",
    "visualize_data_augmentation(whole_dataset, index=0, show_grid=True,\n",
    "                            kind='affine transformation')\n",
    "visualize_data_augmentation(whole_dataset, index=0, show_grid=True,\n",
    "                            kind='elastic deformation')\n",
    "visualize_data_augmentation(whole_dataset, index=0, show_grid=False,\n",
    "                            kind='gray value variation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d8973cc",
   "metadata": {
    "md-indent": "  "
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    \"\"\"original U-Net architecture proposed by Ronneberger et al. (2015)\n",
    "\n",
    "    Attributes:\n",
    "        encoder_blocks (list):  four u_net blocks of encoder path\n",
    "        bottleneck_bock: block that mediates between encoder and decoder\n",
    "        decoder_blocks (list):  four u_net blocks of decoder path\n",
    "        cropped_img_size (list): cropped images size in order of encoder blocks\n",
    "        up_convs (list): upsampling (transposed convolutional) layers (decoder)\n",
    "        max_pool: max pool operation used in encoder path\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder_blocks = nn.ModuleList([\n",
    "            Unet._block(1, 64),\n",
    "            Unet._block(64, 128),\n",
    "            Unet._block(128, 256),\n",
    "            Unet._block(256, 512)\n",
    "        ])\n",
    "        self.bottleneck_block = Unet._block(512, 1024)\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Unet._block(1024, 512),\n",
    "            Unet._block(512, 256),\n",
    "            Unet._block(256, 128),\n",
    "            Unet._block(128, 64)\n",
    "        ])\n",
    "        self.cropped_img_sizes = [392, 200, 104, 56]\n",
    "        self.up_convs = nn.ModuleList([\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=(2,2), stride=2),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=(2,2), stride=2),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=(2,2), stride=2),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=(2,2), stride=2),\n",
    "        ])\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        self.prediction = nn.Conv2d(64, 2, kernel_size=(1,1), stride=1)\n",
    "        return\n",
    "\n",
    "    def forward(self, x):\n",
    "        # go through encoder path and store cropped images\n",
    "        cropped_imgs = []\n",
    "        for index, encoder_block in enumerate(self.encoder_blocks):\n",
    "            out = encoder_block(x)\n",
    "            # center crop and add to cropped image list\n",
    "            cropped_img = Unet._center_crop(out, self.cropped_img_sizes[index])\n",
    "            cropped_imgs.append(cropped_img)\n",
    "            # max pool output of encoder block\n",
    "            x = self.max_pool(out)\n",
    "        # bottleneck block (no max pool)\n",
    "        x = self.bottleneck_block(x)  # [batch_size, 1024, 28, 28]\n",
    "        # go through decoder path with stored cropped images\n",
    "        for index, decoder_block in enumerate(self.decoder_blocks):\n",
    "            x = self.up_convs[index](x)\n",
    "            # concatenate x and cropped img along channel dimension\n",
    "            x = torch.cat((cropped_imgs[-1-index], x), 1)\n",
    "            # feed through decoder_block\n",
    "            x = decoder_block(x)\n",
    "        # feed through prediction layer [batch_size, 2, 388, 388]\n",
    "        x_pred_unnormalized = self.prediction(x)\n",
    "        # normalize prediction for each pixel\n",
    "        x_pred = torch.softmax(x_pred_unnormalized, 1)\n",
    "        return x_pred\n",
    "\n",
    "    @staticmethod\n",
    "    def _center_crop(x, new_size):\n",
    "        \"\"\"center croping of a square input tensor\n",
    "\n",
    "        Args:\n",
    "            x: input tensor shape [batch_size, channels, resolution, resolution]\n",
    "            new_size: the desired output resolution (taking center of input)\n",
    "\n",
    "        Returns:\n",
    "            x_cropped: tensor shape [batch_size, channels, new_size, new_size]\n",
    "        \"\"\"\n",
    "        img_size = x.shape[-1]\n",
    "        i_start = int((img_size - new_size)/2)\n",
    "        i_end = int((img_size + new_size)/2)\n",
    "        x_cropped = x[:, :, i_start:i_end, i_start:i_end]\n",
    "        return x_cropped\n",
    "\n",
    "    @staticmethod\n",
    "    def _block(in_channels, out_channels):\n",
    "        \"\"\"block for use in U-Net architecture,\n",
    "        consists of two conv 3x3, ReLU layers\n",
    "\n",
    "        Args:\n",
    "            in_channels: number of input channels for first convolution\n",
    "            out_channels: number of output channels for both convolutions\n",
    "\n",
    "        Returns:\n",
    "            u_net_block: Sequential U net block\n",
    "        \"\"\"\n",
    "        conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(3,3), stride=1)\n",
    "        conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(3,3), stride=1)\n",
    "\n",
    "        N_1, N_2 = 9*in_channels, 9*out_channels\n",
    "        # initialize by drawing weights from Gaussian distribution\n",
    "        conv1.weight.data.normal_(mean=0, std=np.sqrt(2/N_1))\n",
    "        conv2.weight.data.normal_(mean=0, std=np.sqrt(2/N_2))\n",
    "        # define u_net_block\n",
    "        u_net_block = nn.Sequential(\n",
    "            conv1,\n",
    "            nn.ReLU(),\n",
    "            conv2,\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        return u_net_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e7fb04f",
   "metadata": {
    "md-indent": "  "
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from skimage import measure\n",
    "from scipy.ndimage.morphology import distance_transform_edt\n",
    "from skimage.segmentation import find_boundaries\n",
    "\n",
    "\n",
    "def compute_weight_map(label_mask, w_0, sigma, plot=False):\n",
    "    \"\"\"compute weight map for each ground truth segmentation to compensate\n",
    "    for the different class frequencies and to put additional\n",
    "    emphasis on small borders as proposed by Ronneberger et al.\n",
    "\n",
    "    Args:\n",
    "        label mask (torch tensor): true segmentation masks [batch_size, 1, 388, 388]\n",
    "        w_0 (float): hyperparameter in second term of weight map\n",
    "        sigma (float): hyperparameter in second term of weight map\n",
    "\n",
    "    Returns:\n",
    "        weight_map (torch tensor): computed weight map [batch_size, 1, 388, 388]\n",
    "\n",
    "    researchgate.net/post/creating_a_weight_map_from_a_binary_image_U-net_paper\n",
    "    \"\"\"\n",
    "    batch_size = label_mask.shape[0]\n",
    "    weight_map = torch.zeros_like(label_mask)\n",
    "    for i in range(batch_size):\n",
    "        # compute w_c to balance class frequencies\n",
    "        w_c = label_mask[i][0].clone()\n",
    "        class_freq_0 = (label_mask[i]==0).sum().item()\n",
    "        class_freq_1 = (label_mask[i]==1).sum().item()\n",
    "        w_c[label_mask[i][0]==0] = class_freq_1 / class_freq_0\n",
    "        # compute d_1, d_2, i.e., euclid. dist. to border of (1st/2nd) closest cell\n",
    "        d_1 = np.zeros(label_mask[i][0].shape)\n",
    "        d_2 = np.zeros(label_mask[i][0].shape)\n",
    "        # distinguish all cells (connected components of ones)\n",
    "        all_cells = measure.label(label_mask[i][0], background=0, connectivity=2)\n",
    "        num_cells = np.max(all_cells)\n",
    "        # initialize distances for all cells\n",
    "        dists = np.zeros([num_cells, d_2.shape[0], d_2.shape[1]])\n",
    "        # iterate over all zero components\n",
    "        for index, i_cell in enumerate(range(1, num_cells + 1)):\n",
    "            # cell segmentation (segmented cell 1, rest 0)\n",
    "            cell_segmentation = all_cells==i_cell\n",
    "            # find boundary (boundary 1, rest 0)\n",
    "            boundary = find_boundaries(cell_segmentation, mode='inner')\n",
    "            # compute distance to boundary (set boundary 0, rest -1)\n",
    "            bound_dists = distance_transform_edt(1 - boundary)\n",
    "            dists[index] = bound_dists\n",
    "        # sort dists along first axis (each pixel)\n",
    "        dists.sort(axis=0)\n",
    "        d_1 = dists[0]\n",
    "        d_2 = dists[1]\n",
    "        w = w_c + w_0 * np.exp(- (d_1 + d_2)**2/(2*sigma**2))\n",
    "        # save w to weight map\n",
    "        weight_map[i, 0] = w\n",
    "\n",
    "        # visualize weight map\n",
    "        if plot and i==0:\n",
    "            fig = plt.figure(figsize=(18, 14))\n",
    "\n",
    "            ax = plt.subplot(1, 3, 1)\n",
    "            plt.title('Segmenation Mask')\n",
    "            plt.imshow(label_mask[0, 0], cmap='gray')\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "            plt.colorbar(cax=cax)\n",
    "\n",
    "            ax = plt.subplot(1, 3, 2)\n",
    "            plt.title('w_c')\n",
    "            plt.imshow(w_c, cmap='jet')\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "            plt.colorbar(cax=cax)\n",
    "\n",
    "\n",
    "            ax = plt.subplot(1, 3, 3)\n",
    "            plt.title('w')\n",
    "            plt.imshow(w, cmap='jet')\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "            plt.colorbar(cax=cax)\n",
    "    return weight_map\n",
    "\n",
    "\n",
    "img, label_mask = whole_dataset[0]\n",
    "weight_map = compute_weight_map(label_mask.unsqueeze(0), w_0=10, sigma=5, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cdda492",
   "metadata": {
    "md-indent": "  "
   },
   "outputs": [],
   "source": [
    "from livelossplot import PlotLosses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def train(u_net, dataset, epochs):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    # hyperparameters weight map\n",
    "    w_0, sigma = 10, 5\n",
    "\n",
    "    print('Device: {}'.format(device))\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    u_net.to(device)\n",
    "    optimizer = torch.optim.SGD(u_net.parameters(), lr=0.001, momentum=0.99)\n",
    "\n",
    "    losses_plot = PlotLosses()\n",
    "    for epoch in range(epochs):\n",
    "        avg_loss = 0\n",
    "        for counter, (imgs, label_masks) in enumerate(data_loader):\n",
    "            u_net.zero_grad()\n",
    "            # retrieve predictions of u_net [batch, 2, 388, 388]\n",
    "            pred_masks = u_net(imgs.to(device))\n",
    "            # compute weight map\n",
    "            weight_map = compute_weight_map(label_masks, w_0, sigma).to(device)\n",
    "            # put label_masks to device\n",
    "            label_masks = label_masks.to(device)\n",
    "            # compute weighted binary cross entropy loss\n",
    "            loss = -(weight_map*\n",
    "                    (pred_masks[:, 0:1].log() * label_masks +\n",
    "                      pred_masks[:, 1:2].log() * (1 - label_masks))\n",
    "                    ).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss.item() / len(dataset)\n",
    "\n",
    "            losses_plot.update({'current weighted loss': loss.item()},\n",
    "                              current_step=epoch + counter/len(data_loader))\n",
    "            losses_plot.draw()\n",
    "        losses_plot.update({'avg weighted loss': avg_loss},\n",
    "                          current_step=epoch + 1)\n",
    "        losses_plot.draw()\n",
    "    trained_u_net = u_net\n",
    "    return trained_u_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "973b5138",
   "metadata": {
    "md-indent": "  "
   },
   "outputs": [],
   "source": [
    "u_net = Unet()\n",
    "epochs = 30\n",
    "# all image indexes\n",
    "idx = np.arange(30)\n",
    "# random inplace shuffling of indexes\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(idx)\n",
    "# split data into training and test data\n",
    "train_imgs, train_labels = imgs[idx[0:25]], labels[idx[0:25]]\n",
    "test_imgs, test_labels = imgs[idx[25:]], labels[idx[25:]]\n",
    "# generate datasets\n",
    "stride = 124\n",
    "train_dataset = EM_Dataset(train_imgs, train_labels, stride=stride,\n",
    "                          transformation=True, probability=0.7, alpha=50,\n",
    "                          sigma=5, kernel_dim=25)\n",
    "test_dataset = EM_Dataset(test_imgs, test_labels, stride=stride,\n",
    "                          transformation=False)\n",
    "# start training procedure\n",
    "trained_u_net = train(u_net, train_dataset, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}