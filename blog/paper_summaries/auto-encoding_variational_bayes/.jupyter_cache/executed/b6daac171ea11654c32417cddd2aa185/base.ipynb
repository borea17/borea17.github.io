{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9308bb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# imports\n",
    "import os\n",
    "import sys\n",
    "import types\n",
    "import json\n",
    "\n",
    "# figure size/format\n",
    "fig_width = 7\n",
    "fig_height = 5\n",
    "fig_format = 'retina'\n",
    "fig_dpi = 96\n",
    "\n",
    "# matplotlib defaults / format\n",
    "try:\n",
    "  import matplotlib.pyplot as plt\n",
    "  plt.rcParams['figure.figsize'] = (fig_width, fig_height)\n",
    "  plt.rcParams['figure.dpi'] = fig_dpi\n",
    "  plt.rcParams['savefig.dpi'] = fig_dpi\n",
    "  from IPython.display import set_matplotlib_formats\n",
    "  set_matplotlib_formats(fig_format)\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# plotly use connected mode\n",
    "try:\n",
    "  import plotly.io as pio\n",
    "  pio.renderers.default = \"notebook_connected\"\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# enable pandas latex repr when targeting pdfs\n",
    "try:\n",
    "  import pandas as pd\n",
    "  if fig_format == 'pdf':\n",
    "    pd.set_option('display.latex.repr', True)\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "\n",
    "\n",
    "# output kernel dependencies\n",
    "kernel_deps = dict()\n",
    "for module in list(sys.modules.values()):\n",
    "  # Some modules play games with sys.modules (e.g. email/__init__.py\n",
    "  # in the standard library), and occasionally this can cause strange\n",
    "  # failures in getattr.  Just ignore anything that's not an ordinary\n",
    "  # module.\n",
    "  if not isinstance(module, types.ModuleType):\n",
    "    continue\n",
    "  path = getattr(module, \"__file__\", None)\n",
    "  if not path:\n",
    "    continue\n",
    "  if path.endswith(\".pyc\") or path.endswith(\".pyo\"):\n",
    "    path = path[:-1]\n",
    "  if not os.path.exists(path):\n",
    "    continue\n",
    "  kernel_deps[path] = os.stat(path).st_mtime\n",
    "print(json.dumps(kernel_deps))\n",
    "\n",
    "# set run_path if requested\n",
    "if r'/home/borea17/GIT/borea17.github.io/blog/paper_summaries/auto-encoding_variational_bayes':\n",
    "  os.chdir(r'/home/borea17/GIT/borea17.github.io/blog/paper_summaries/auto-encoding_variational_bayes')\n",
    "\n",
    "# reset state\n",
    "%reset\n",
    "\n",
    "def ojs_define(**kwargs):\n",
    "  import json\n",
    "  try:\n",
    "    # IPython 7.14 preferred import\n",
    "    from IPython.display import display, HTML\n",
    "  except:\n",
    "    from IPython.core.display import display, HTML\n",
    "\n",
    "  # do some minor magic for convenience when handling pandas\n",
    "  # dataframes\n",
    "  def convert(v):\n",
    "    try:\n",
    "      import pandas as pd\n",
    "    except ModuleNotFoundError: # don't do the magic when pandas is not available\n",
    "      return v\n",
    "    if type(v) == pd.Series:\n",
    "      v = pd.DataFrame(v)\n",
    "    if type(v) == pd.DataFrame:\n",
    "      j = json.loads(v.T.to_json(orient='split'))\n",
    "      return dict((k,v) for (k,v) in zip(j[\"index\"], j[\"data\"]))\n",
    "    else:\n",
    "      return v\n",
    "  \n",
    "  v = dict(contents=list(dict(name=key, value=convert(value)) for (key, value) in kwargs.items()))\n",
    "  display(HTML('<script type=\"ojs-define\">' + json.dumps(v) + '</script>'), metadata=dict(ojs_define = True))\n",
    "globals()[\"ojs_define\"] = ojs_define\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ff9482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class CoderNetwork(nn.Module):\n",
    "    r\"\"\"Encoder/Decoder for use in VAE based on Kingma and Welling\n",
    "    \n",
    "    Args:\n",
    "        input_dim: input dimension (int)\n",
    "        output_dim: output dimension (int)\n",
    "        hidden_dim: hidden layer dimension (int)\n",
    "        coder_type: encoder/decoder type can be \n",
    "                   'Gaussian'   - Gaussian with diagonal covariance structure\n",
    "                   'I-Gaussian' - Gaussian with identity as covariance matrix \n",
    "                   'Bernoulli'  - Bernoulli distribution       \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, coder_type='Gaussian'):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert coder_type in  ['Gaussian', 'I-Gaussian' ,'Bernoulli'], \\\n",
    "            'unknown coder_type'\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.coder_type = coder_type\n",
    "        \n",
    "        self.coder = nn.Sequential(OrderedDict([\n",
    "            ('h', nn.Linear(input_dim, hidden_dim)),\n",
    "            ('ReLU', nn.ReLU()) # ReLU instead of Tanh proposed by K. and W.       \n",
    "        ]))\n",
    "        self.fc_mu = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        if coder_type == 'Gaussian':\n",
    "            self.fc_log_var = nn.Linear(hidden_dim, output_dim)\n",
    "        elif coder_type == 'Bernoulli':\n",
    "            self.sigmoid_mu = nn.Sigmoid()\n",
    "        return\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        out = self.coder(inp)\n",
    "        mu = self.fc_mu(out)\n",
    "        \n",
    "        if self.coder_type == 'Gaussian':\n",
    "            log_var = self.fc_log_var(out)\n",
    "            return [mu, log_var]\n",
    "        elif self.coder_type == 'I-Gaussian':\n",
    "            return mu\n",
    "        elif self.coder_type == 'Bernoulli':\n",
    "            return self.sigmoid_mu(mu)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1f0e161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    r\"\"\"A simple VAE class based on Kingma and Welling\n",
    "        \n",
    "    Args:\n",
    "        encoder_network:  instance of CoderNetwork class\n",
    "        decoder_network:  instance of CoderNetwork class\n",
    "        L:                number of samples used during reparameterization trick\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, encoder_network, decoder_network, L=1):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder_network\n",
    "        self.decoder = decoder_network\n",
    "        self.L = L\n",
    "        \n",
    "        latent_dim = encoder_network.output_dim\n",
    "                \n",
    "        self.normal_dist = MultivariateNormal(torch.zeros(latent_dim), \n",
    "                                              torch.eye(latent_dim))\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        L = self.L\n",
    "        \n",
    "        z, mu_E, log_var_E = self.encode(x, L)\n",
    "        # regularization term per batch, i.e., size: (batch_size)\n",
    "        regularization_term = (1/2) * (1 + log_var_E - mu_E**2\n",
    "                                       - torch.exp(log_var_E)).sum(axis=1)\n",
    "        \n",
    "        # upsample x and reshape\n",
    "        batch_size = x.shape[0]\n",
    "        x_ups = x.repeat(L, 1).view(batch_size, L, -1)    \n",
    "        if self.decoder.coder_type == 'Gaussian':\n",
    "            # mu_D, log_var_D have shape (batch_size, L, output_dim)\n",
    "            mu_D, log_var_D = self.decode(z)\n",
    "            # reconstruction accuracy per batch, i.e., size: (batch_size)\n",
    "            recons_acc = (1/L) * (-(0.5)*(log_var_D.sum(axis=2)).sum(axis=1)\n",
    "               -(0.5) * ((1/torch.exp(log_var_D))*((x_ups - mu_D)**2)\n",
    "                         ).sum(axis=2).sum(axis=1))\n",
    "        elif self.decoder.coder_type == 'I-Gaussian':\n",
    "            # mu_D has shape (batch_size, L, output_dim)\n",
    "            mu_D = self.decode(z)\n",
    "            # reconstruction accuracy per batch, i.e., size: (batch_size)\n",
    "            recons_acc = (1/L) * (-(0.5) * ((x_ups - mu_D)**2\n",
    "                                            ).sum(axis=2).sum(axis=1))\n",
    "        elif self.decoder.coder_type == 'Bernoulli':\n",
    "            # mu_D has shape (batch_size, L, output_dim)\n",
    "            mu_D = self.decode(z)     \n",
    "            # reconstruction accuracy per batch, i.e., size: (batch_size)\n",
    "            # corresponds to the negative binary cross entropy loss (BCELoss)\n",
    "            recons_acc = (1/L) * (x_ups * torch.log(mu_D) + \n",
    "                                  (1 - x_ups) * torch.log(1 - mu_D)\n",
    "                                  ).sum(axis=2).sum(axis=1)\n",
    "        loss = - regularization_term.sum() - recons_acc.sum()\n",
    "        return loss\n",
    "    \n",
    "    def encode(self, x, L=1):\n",
    "        # get encoder distribution parameters\n",
    "        mu_E, log_var_E = self.encoder(x)\n",
    "        # sample noise variable L times for each batch\n",
    "        batch_size = x.shape[0]\n",
    "        epsilon = self.normal_dist.sample(sample_shape=(batch_size, L, ))\n",
    "        # upsample mu_E, log_var_E and reshape\n",
    "        mu_E_ups = mu_E.repeat(L, 1).view(batch_size, L, -1) \n",
    "        log_var_E_ups = log_var_E.repeat(L, 1).view(batch_size, L, -1)\n",
    "        # get latent variable by reparametrization trick\n",
    "        z = mu_E_ups + torch.sqrt(torch.exp(log_var_E_ups)) * epsilon\n",
    "        return z, mu_E, log_var_E\n",
    "    \n",
    "    def decode(self, z):\n",
    "        # get decoder distribution parameters\n",
    "        if self.decoder.coder_type == 'Gaussian':\n",
    "            mu_D, log_var_D = self.decoder(z)\n",
    "            return mu_D, log_var_D\n",
    "        elif self.decoder.coder_type == 'I-Gaussian':\n",
    "            mu_D = self.decoder(z)\n",
    "            return mu_D\n",
    "        elif self.decoder.coder_type == 'Bernoulli':\n",
    "            mu_D = self.decoder(z)\n",
    "            return mu_D\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64697549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def train(decoder_type, dataset, x_dim, hid_dim, z_dim, batch_size, L, epochs):\n",
    "    encoder_network = CoderNetwork(input_dim=x_dim, \n",
    "                                   hidden_dim=hid_dim, \n",
    "                                   output_dim=z_dim,\n",
    "                                   coder_type='Gaussian')\n",
    "    decoder_network = CoderNetwork(input_dim=z_dim, \n",
    "                                   hidden_dim=hid_dim, \n",
    "                                   output_dim=x_dim,\n",
    "                                   coder_type=decoder_type)\n",
    "    \n",
    "    model = VAE(encoder_network, decoder_network, L=L)\n",
    "    data_loader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    print('Start training with {} decoder distribution\\n'.format(decoder_type))\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print('Epoch {}/{}'.format(epoch, epochs))\n",
    "        avg_loss = 0\n",
    "        for counter, (mini_batch_data, label) in enumerate(data_loader):\n",
    "            \n",
    "            model.zero_grad()\n",
    "            \n",
    "            loss = model(mini_batch_data.view(-1, x_dim))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_loss += loss.item() / len(dataset)\n",
    "            \n",
    "            if counter % 20 == 0 or (counter + 1)==len(data_loader):\n",
    "                batch_loss = loss.item() / len(mini_batch_data)\n",
    "                print('\\r[{}/{}] batch loss: {:.2f}'.format(counter + 1,\n",
    "                                                            len(data_loader),\n",
    "                                                            batch_loss),\n",
    "                      end='', flush=True)\n",
    "        print('\\nAverage loss: {:.3f}'.format(avg_loss)) \n",
    "    print('Done!\\n')\n",
    "    trained_VAE = model\n",
    "    return trained_VAE\n",
    "\n",
    "dataset = datasets.MNIST('data/', transform=transforms.ToTensor(), download=True)\n",
    "x_dim, hid_dim, z_dim = 28*28, 400, 20\n",
    "batch_size, L, epochs = 128, 5, 3\n",
    "\n",
    "Bernoulli_VAE = train('Bernoulli', dataset, x_dim, hid_dim, z_dim, \n",
    "                      batch_size, L, epochs)\n",
    "Gaussian_VAE = train('Gaussian', dataset, x_dim, hid_dim, z_dim, \n",
    "                     batch_size, L, epochs)\n",
    "I_Gaussian_VAE = train('I-Gaussian', dataset, x_dim, hid_dim, z_dim, \n",
    "                       batch_size, L, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3767524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def plot_results(trained_model, dataset, n_samples):\n",
    "    decoder_type = trained_model.decoder.coder_type\n",
    "    \n",
    "    fig = plt.figure(figsize=(14, 3))\n",
    "    fig.suptitle(decoder_type + ' Distribution: Observations (top row) and ' +\n",
    "                 'their reconstructions (bottom row)')\n",
    "    for i_sample in range(n_samples):\n",
    "        x_sample = dataset[i_sample][0].view(-1, 28*28)\n",
    "        \n",
    "        z, mu_E, log_var_E = trained_model.encode(x_sample, L=1)\n",
    "        if decoder_type in ['Bernoulli', 'I-Gaussian']:\n",
    "            x_prime = trained_model.decode(z)\n",
    "        else:\n",
    "            x_prime = trained_model.decode(z)[0]\n",
    "    \n",
    "        plt.subplot(2, n_samples, i_sample + 1)\n",
    "        plt.imshow(x_sample.view(28, 28).data.numpy())\n",
    "        plt.axis('off')\n",
    "        plt.subplot(2, n_samples, i_sample + 1 + n_samples)\n",
    "        plt.imshow(x_prime.view(28, 28).data.numpy())\n",
    "        plt.axis('off')\n",
    "    return\n",
    "\n",
    "\n",
    "n_samples = 10\n",
    "\n",
    "plot_results(Bernoulli_VAE, dataset, n_samples)\n",
    "plot_results(Gaussian_VAE, dataset, n_samples)\n",
    "plot_results(I_Gaussian_VAE, dataset, n_samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}