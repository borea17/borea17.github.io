{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a387fcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# imports\n",
    "import os\n",
    "import sys\n",
    "import types\n",
    "import json\n",
    "\n",
    "# figure size/format\n",
    "fig_width = 7\n",
    "fig_height = 5\n",
    "fig_format = 'retina'\n",
    "fig_dpi = 96\n",
    "\n",
    "# matplotlib defaults / format\n",
    "try:\n",
    "  import matplotlib.pyplot as plt\n",
    "  plt.rcParams['figure.figsize'] = (fig_width, fig_height)\n",
    "  plt.rcParams['figure.dpi'] = fig_dpi\n",
    "  plt.rcParams['savefig.dpi'] = fig_dpi\n",
    "  from IPython.display import set_matplotlib_formats\n",
    "  set_matplotlib_formats(fig_format)\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# plotly use connected mode\n",
    "try:\n",
    "  import plotly.io as pio\n",
    "  pio.renderers.default = \"notebook_connected\"\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# enable pandas latex repr when targeting pdfs\n",
    "try:\n",
    "  import pandas as pd\n",
    "  if fig_format == 'pdf':\n",
    "    pd.set_option('display.latex.repr', True)\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "\n",
    "\n",
    "# output kernel dependencies\n",
    "kernel_deps = dict()\n",
    "for module in list(sys.modules.values()):\n",
    "  # Some modules play games with sys.modules (e.g. email/__init__.py\n",
    "  # in the standard library), and occasionally this can cause strange\n",
    "  # failures in getattr.  Just ignore anything that's not an ordinary\n",
    "  # module.\n",
    "  if not isinstance(module, types.ModuleType):\n",
    "    continue\n",
    "  path = getattr(module, \"__file__\", None)\n",
    "  if not path:\n",
    "    continue\n",
    "  if path.endswith(\".pyc\") or path.endswith(\".pyo\"):\n",
    "    path = path[:-1]\n",
    "  if not os.path.exists(path):\n",
    "    continue\n",
    "  kernel_deps[path] = os.stat(path).st_mtime\n",
    "print(json.dumps(kernel_deps))\n",
    "\n",
    "# set run_path if requested\n",
    "if r'/home/borea17/GIT/borea17.github.io/blog/paper_summaries/spatial_broadcast_decoder':\n",
    "  os.chdir(r'/home/borea17/GIT/borea17.github.io/blog/paper_summaries/spatial_broadcast_decoder')\n",
    "\n",
    "# reset state\n",
    "%reset\n",
    "\n",
    "def ojs_define(**kwargs):\n",
    "  import json\n",
    "  try:\n",
    "    # IPython 7.14 preferred import\n",
    "    from IPython.display import display, HTML\n",
    "  except:\n",
    "    from IPython.core.display import display, HTML\n",
    "\n",
    "  # do some minor magic for convenience when handling pandas\n",
    "  # dataframes\n",
    "  def convert(v):\n",
    "    try:\n",
    "      import pandas as pd\n",
    "    except ModuleNotFoundError: # don't do the magic when pandas is not available\n",
    "      return v\n",
    "    if type(v) == pd.Series:\n",
    "      v = pd.DataFrame(v)\n",
    "    if type(v) == pd.DataFrame:\n",
    "      j = json.loads(v.T.to_json(orient='split'))\n",
    "      return dict((k,v) for (k,v) in zip(j[\"index\"], j[\"data\"]))\n",
    "    else:\n",
    "      return v\n",
    "  \n",
    "  v = dict(contents=list(dict(name=key, value=convert(value)) for (key, value) in kwargs.items()))\n",
    "  display(HTML('<script type=\"ojs-define\">' + json.dumps(v) + '</script>'), metadata=dict(ojs_define = True))\n",
    "globals()[\"ojs_define\"] = ojs_define\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccc9a669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "def generate_img(x_position, y_position, shape, color, img_size, size=20):\n",
    "    \"\"\"Generate an RGB image from the provided latent factors\n",
    "\n",
    "    Args:\n",
    "        x_position (float): normalized x position\n",
    "        y_position (float): normalized y position\n",
    "        shape (string): can only be 'circle' or 'square'\n",
    "        color (string): color name or rgb string\n",
    "        img_size (int): describing the image size (img_size, img_size)\n",
    "        size (int): size of shape\n",
    "\n",
    "    Returns:\n",
    "        torch tensor [3, img_size, img_size] (dtype=torch.float32)\n",
    "    \"\"\"\n",
    "    # creation of image\n",
    "    img = Image.new('RGB', (img_size, img_size), color='black')\n",
    "    # map (x, y) position to pixel coordinates\n",
    "    x_position = (img_size - 2 - size) * x_position\n",
    "    y_position = (img_size - 2 - size) * y_position\n",
    "    # define coordinates\n",
    "    x_0, y_0 = x_position, y_position\n",
    "    x_1, y_1 = x_position + size, y_position + size\n",
    "    # draw shapes\n",
    "    img1 = ImageDraw.Draw(img)\n",
    "    if shape == 'square':\n",
    "        img1.rectangle([(x_0, y_0), (x_1, y_1)], fill=color)\n",
    "    elif shape == 'circle':\n",
    "        img1.ellipse([(x_0, y_0), (x_1, y_1)], fill=color)\n",
    "    return transforms.ToTensor()(img).type(torch.float32)\n",
    "\n",
    "\n",
    "def generate_dataset(img_size, shape_sizes, num_pos, shapes, colors):\n",
    "    \"\"\"procedurally generated from 4 ground truth independent latent factors,\n",
    "       these factors are/can be\n",
    "           Position X: num_pos values in [0, 1]\n",
    "           Poistion Y: num_pos values in [0, 1]\n",
    "           Shape: square, circle\n",
    "           Color: standard HTML color name or 'rgb(x, y, z)'\n",
    "\n",
    "    Args:\n",
    "           img_size (int): describing the image size (img_size, img_size)\n",
    "           shape_sizes (list): sizes of shapes\n",
    "           num_pos (int): discretized positions\n",
    "           shapes (list): shapes (can only be 'circle', 'square')\n",
    "           colors (list): colors\n",
    "\n",
    "    Returns:\n",
    "           data: torch tensor [n_samples, 3, img_size, img_size]\n",
    "           latents: each entry describes the latents of corresp. data entry\n",
    "    \"\"\"\n",
    "    num_shapes, num_colors, sizes = len(shapes), len(colors), len(shape_sizes)\n",
    "\n",
    "    n_samples = num_pos*num_pos*num_shapes*num_colors*sizes\n",
    "    data = torch.empty([n_samples, 3, img_size, img_size])\n",
    "    latents = np.empty([n_samples], dtype=object)\n",
    "\n",
    "    index = 0\n",
    "    for x_pos in np.linspace(0, 1, num_pos):\n",
    "        for y_pos in np.linspace(0, 1, num_pos):\n",
    "            for shape in shapes:\n",
    "                for size in shape_sizes:\n",
    "                    for color in colors:\n",
    "                        img = generate_img(x_pos, y_pos, shape, color,\n",
    "                                           img_size, size)\n",
    "                        data[index] = img\n",
    "                        latents[index] = [x_pos, y_pos, shape, color]\n",
    "\n",
    "                        index += 1\n",
    "    return data, latents\n",
    "\n",
    "\n",
    "circles_data, latents = generate_dataset(img_size=64, shape_sizes=[16],\n",
    "                                         num_pos=35,\n",
    "                                         shapes=['circle'],\n",
    "                                         colors=['red', 'green', 'blue'])\n",
    "sprites_dataset = TensorDataset(circles_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f096238",
   "metadata": {
    "md-indent": "  "
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\"Encoder class for use in convolutional VAE\n",
    "\n",
    "    Args:\n",
    "        latent_dim: dimensionality of latent distribution\n",
    "\n",
    "    Attributes:\n",
    "        encoder_conv: convolution layers of encoder\n",
    "        fc_mu: fully connected layer for mean in latent space\n",
    "        fc_log_var: fully connceted layers for log variance in latent space\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=6):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.encoder_conv = nn.Sequential(\n",
    "            # shape: [batch_size, 3, 64, 64]\n",
    "            nn.Conv2d(3,  64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # shape: [batch_size, 64, 32, 32]\n",
    "            nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # shape: [batch_size, 64, 4, 4],\n",
    "            nn.Flatten(),\n",
    "            # shape: [batch_size, 1024]\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            # shape: [batch_size, 256]\n",
    "        )\n",
    "        self.fc_mu = nn.Sequential(\n",
    "            nn.Linear(in_features=256, out_features=self.latent_dim),\n",
    "        )\n",
    "        self.fc_log_var = nn.Sequential(\n",
    "            nn.Linear(in_features=256, out_features=self.latent_dim),\n",
    "        )\n",
    "        return\n",
    "\n",
    "    def forward(self, inp):\n",
    "        out = self.encoder_conv(inp)\n",
    "        mu = self.fc_mu(out)\n",
    "        log_var = self.fc_log_var(out)\n",
    "        return [mu, log_var]\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"(standard) Decoder class for use in convolutional VAE,\n",
    "    a Gaussian distribution with fixed variance (identity times fixed variance\n",
    "    as covariance matrix) used as the decoder distribution\n",
    "\n",
    "    Args:\n",
    "        latent_dim: dimensionality of latent distribution\n",
    "        fixed_variance: variance of distribution\n",
    "\n",
    "    Attributes:\n",
    "        decoder_upsampling: linear upsampling layer(s)\n",
    "        decoder_deconv: deconvolution layers of decoder (also upsampling)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim, fixed_variance):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.coder_type = 'Gaussian with fixed variance'\n",
    "        self.fixed_variance = fixed_variance\n",
    "\n",
    "        self.decoder_upsampling = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            # reshaped into [batch_size, 64, 2, 2]\n",
    "        )\n",
    "        self.decoder_deconv = nn.Sequential(\n",
    "            # shape: [batch_size, 64, 2, 2]\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # shape: [batch_size, 64, 4, 4]\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64,  3, kernel_size=4, stride=2, padding=1),\n",
    "            # shape: [batch_size, 3, 64, 64]\n",
    "        )\n",
    "        return\n",
    "\n",
    "    def forward(self, inp):\n",
    "        ups_inp = self.decoder_upsampling(inp)\n",
    "        ups_inp = ups_inp.view(-1, 64, 2, 2)\n",
    "        mu = self.decoder_deconv(ups_inp)\n",
    "        return mu\n",
    "\n",
    "\n",
    "class SpatialBroadcastDecoder(nn.Module):\n",
    "    \"\"\"SBD class for use in convolutional VAE,\n",
    "      a Gaussian distribution with fixed variance (identity times fixed\n",
    "      variance as covariance matrix) used as the decoder distribution\n",
    "\n",
    "    Args:\n",
    "        latent_dim: dimensionality of latent distribution\n",
    "        fixed_variance: variance of distribution\n",
    "\n",
    "    Attributes:\n",
    "        img_size: image size (necessary for tiling)\n",
    "        decoder_convs: convolution layers of decoder (also upsampling)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim, fixed_variance):\n",
    "        super().__init__()\n",
    "        self.img_size = 64\n",
    "        self.coder_type = 'Gaussian with fixed variance'\n",
    "        self.latent_dim = latent_dim\n",
    "        self.fixed_variance = fixed_variance\n",
    "\n",
    "        x = torch.linspace(-1, 1, self.img_size)\n",
    "        y = torch.linspace(-1, 1, self.img_size)\n",
    "        x_grid, y_grid = torch.meshgrid(x, y, indexing=\"ij\")\n",
    "        # reshape into [1, 1, img_size, img_size] and save in state_dict\n",
    "        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n",
    "        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n",
    "\n",
    "        self.decoder_convs = nn.Sequential(\n",
    "            # shape [batch_size, latent_dim + 2, 64, 64]\n",
    "            nn.Conv2d(in_channels=self.latent_dim+2, out_channels=64,\n",
    "                      stride=(1, 1), kernel_size=(3,3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            # shape [batch_size, 64, 64, 64]\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, stride=(1,1),\n",
    "                      kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            # shape [batch_size, 64, 64, 64]\n",
    "            nn.Conv2d(in_channels=64, out_channels=3, stride=(1,1),\n",
    "                      kernel_size=(3, 3), padding=1),\n",
    "            # shape [batch_size, 3, 64, 64]\n",
    "        )\n",
    "        return\n",
    "\n",
    "    def forward(self, z):\n",
    "        batch_size = z.shape[0]\n",
    "        # reshape z into [batch_size, latent_dim, 1, 1]\n",
    "        z = z.view(z.shape + (1, 1))\n",
    "        # tile across image [batch_size, latent_im, img_size, img_size]\n",
    "        z_b = z.expand(-1, -1, self.img_size, self.img_size)\n",
    "        # upsample x_grid and y_grid to [batch_size, 1, img_size, img_size]\n",
    "        x_b = self.x_grid.expand(batch_size, -1, -1, -1)\n",
    "        y_b = self.y_grid.expand(batch_size, -1, -1, -1)\n",
    "        # concatenate vectors [batch_size, latent_dim+2, img_size, img_size]\n",
    "        z_sb = torch.cat((z_b, x_b, y_b), dim=1)\n",
    "        # apply convolutional layers\n",
    "        mu_D = self.decoder_convs(z_sb)\n",
    "        return mu_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e975e3f5",
   "metadata": {
    "md-indent": "  "
   },
   "outputs": [],
   "source": [
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    \"\"\"A simple VAE class\n",
    "\n",
    "    Args:\n",
    "        vae_tpe: type of VAE either 'Standard' or 'SBD'\n",
    "        latent_dim: dimensionality of latent distribution\n",
    "        fixed_var: fixed variance of decoder distribution\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vae_type, latent_dim, fixed_var):\n",
    "        super().__init__()\n",
    "        self.vae_type = vae_type\n",
    "\n",
    "        if self.vae_type == 'Standard':\n",
    "            self.decoder = Decoder(latent_dim=latent_dim,\n",
    "                                  fixed_variance=fixed_var)\n",
    "        else:\n",
    "            self.decoder = SpatialBroadcastDecoder(latent_dim=latent_dim,\n",
    "                                                   fixed_variance=fixed_var)\n",
    "\n",
    "        self.encoder = Encoder(latent_dim=latent_dim)\n",
    "        self.normal_dist = MultivariateNormal(torch.zeros(latent_dim),\n",
    "                                              torch.eye(latent_dim))\n",
    "        return\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu_E, log_var_E = self.encode(x)\n",
    "        # regularization term per batch, i.e., size: (batch_size)\n",
    "        regularization_term = 0.5 * (1 + log_var_E - mu_E**2\n",
    "                                      - torch.exp(log_var_E)).sum(axis=1)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        if self.decoder.coder_type == 'Gaussian with fixed variance':\n",
    "            # x_rec has shape (batch_size, 3, 64, 64)\n",
    "            x_rec = self.decode(z)\n",
    "            # reconstruction accuracy per batch, i.e., size: (batch_size)\n",
    "            factor = 0.5 * (1/self.decoder.fixed_variance)\n",
    "            recons_acc = - factor * ((x.view(batch_size, -1) -\n",
    "                                    x_rec.view(batch_size, -1))**2\n",
    "                                  ).sum(axis=1)\n",
    "        return -regularization_term.mean(), -recons_acc.mean()\n",
    "\n",
    "    def reconstruct(self, x):\n",
    "        mu_E, log_var_E = self.encoder(x)\n",
    "        x_rec = self.decoder(mu_E)\n",
    "        return x_rec\n",
    "\n",
    "    def encode(self, x):\n",
    "        # get encoder distribution parameters\n",
    "        mu_E, log_var_E = self.encoder(x)\n",
    "        # sample noise variable for each batch\n",
    "        batch_size = x.shape[0]\n",
    "        epsilon = self.normal_dist.sample(sample_shape=(batch_size, )\n",
    "                                          ).to(x.device)\n",
    "        # get latent variable by reparametrization trick\n",
    "        z = mu_E + torch.exp(0.5*log_var_E) * epsilon\n",
    "        return z, mu_E, log_var_E\n",
    "\n",
    "    def decode(self, z):\n",
    "        # get decoder distribution parameters\n",
    "        mu_D = self.decoder(z)\n",
    "        return mu_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2d568ef",
   "metadata": {
    "md-indent": "  "
   },
   "outputs": [],
   "source": [
    "from livelossplot import PlotLosses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def train(dataset, epochs, VAE):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    print('Device: {}'.format(device))\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    VAE.to(device)\n",
    "    optimizer = torch.optim.Adam(VAE.parameters(), lr=3e-4)\n",
    "\n",
    "    losses_plot = PlotLosses(groups={'avg log loss':\n",
    "                                    ['kl loss', 'reconstruction loss']})\n",
    "    print('Start training with {} decoder\\n'.format(VAE.vae_type))\n",
    "    for epoch in range(1, epochs +1):\n",
    "        avg_kl = 0\n",
    "        avg_recons_err = 0\n",
    "        for counter, mini_batch_data in enumerate(data_loader):\n",
    "            VAE.zero_grad()\n",
    "\n",
    "            kl_div, recons_err = VAE(mini_batch_data[0].to(device))\n",
    "            loss = kl_div + recons_err\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_kl += kl_div.item() / len(dataset)\n",
    "            avg_recons_err += recons_err.item() / len(dataset)\n",
    "\n",
    "        losses_plot.update({'kl loss': np.log(avg_kl),\n",
    "                            'reconstruction loss': np.log(avg_recons_err)})\n",
    "        losses_plot.send()\n",
    "    trained_VAE = VAE\n",
    "    return trained_VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d5260ea",
   "metadata": {
    "md-indent": "  "
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def reconstructions_and_latent_traversals(STD_VAE, SBD_VAE, dataset, SEED=1):\n",
    "    np.random.seed(SEED)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    latent_dims = STD_VAE.encoder.latent_dim\n",
    "\n",
    "    n_samples = 7\n",
    "    i_samples = np.random.choice(range(len(dataset)), n_samples, replace=False)\n",
    "\n",
    "    # preperation for latent traversal\n",
    "    i_latent = i_samples[n_samples//2]\n",
    "    lat_image = dataset[i_latent][0]\n",
    "    sweep = np.linspace(-2, 2, n_samples)\n",
    "\n",
    "    fig = plt.figure(constrained_layout=False, figsize=(2*n_samples, 2+latent_dims))\n",
    "    grid = plt.GridSpec(latent_dims + 5, n_samples*2 + 3,\n",
    "                        hspace=0.2, wspace=0.02, figure=fig)\n",
    "    # standard VAE\n",
    "    for counter, i_sample in enumerate(i_samples):\n",
    "        orig_image = dataset[i_sample][0]\n",
    "        # original\n",
    "        main_ax = fig.add_subplot(grid[1, counter + 1])\n",
    "        main_ax.imshow(transforms.ToPILImage()(orig_image))\n",
    "        main_ax.axis('off')\n",
    "        main_ax.set_aspect('equal')\n",
    "\n",
    "        # reconstruction\n",
    "        x_rec = STD_VAE.reconstruct(orig_image.unsqueeze(0).to(device))\n",
    "        # clamp output into [0, 1] and prepare for plotting\n",
    "        recons_image =  torch.clamp(x_rec, 0, 1).squeeze(0).cpu()\n",
    "\n",
    "        main_ax = fig.add_subplot(grid[2, counter + 1])\n",
    "        main_ax.imshow(transforms.ToPILImage()(recons_image))\n",
    "        main_ax.axis('off')\n",
    "        main_ax.set_aspect('equal')\n",
    "    # latent dimension traversal\n",
    "    z, mu_E, log_var_E = STD_VAE.encode(lat_image.unsqueeze(0).to(device))\n",
    "    for latent_dim in range(latent_dims):\n",
    "        for counter, z_replaced in enumerate(sweep):\n",
    "            z_new = z.detach().clone()\n",
    "            z_new[0][latent_dim] = z_replaced\n",
    "\n",
    "            # clamp output into [0, 1] and prepare for plotting\n",
    "            img_rec = torch.clamp(STD_VAE.decode(z_new), 0, 1).squeeze(0).cpu()\n",
    "\n",
    "            main_ax = fig.add_subplot(grid[4 + latent_dim, counter + 1])\n",
    "            main_ax.imshow(transforms.ToPILImage()(img_rec))\n",
    "            main_ax.axis('off')\n",
    "    # SBD VAE\n",
    "    for counter, i_sample in enumerate(i_samples):\n",
    "        orig_image = dataset[i_sample][0]\n",
    "        # original\n",
    "        main_ax = fig.add_subplot(grid[1, counter + n_samples + 2])\n",
    "        main_ax.imshow(transforms.ToPILImage()(orig_image))\n",
    "        main_ax.axis('off')\n",
    "        main_ax.set_aspect('equal')\n",
    "        # reconstruction\n",
    "        x_rec = SBD_VAE.reconstruct(orig_image.unsqueeze(0).to(device))\n",
    "        # clamp output into [0, 1] and prepare for plotting\n",
    "        recons_image = torch.clamp(x_rec, 0, 1).squeeze(0).cpu()\n",
    "\n",
    "        main_ax = fig.add_subplot(grid[2, counter + n_samples + 2])\n",
    "        main_ax.imshow(transforms.ToPILImage()(recons_image))\n",
    "        main_ax.axis('off')\n",
    "        main_ax.set_aspect('equal')\n",
    "    # latent dimension traversal\n",
    "    z, mu_E, log_var_E = SBD_VAE.encode(lat_image.unsqueeze(0).to(device))\n",
    "    for latent_dim in range(latent_dims):\n",
    "        for counter, z_replaced in enumerate(sweep):\n",
    "            z_new = z.detach().clone()\n",
    "            z_new[0][latent_dim] = z_replaced\n",
    "            # clamp output into [0, 1] and prepare for plotting\n",
    "            img_rec = torch.clamp(SBD_VAE.decode(z_new), 0, 1).squeeze(0).cpu()\n",
    "\n",
    "            main_ax = fig.add_subplot(grid[4+latent_dim, counter+n_samples+2])\n",
    "            main_ax.imshow(transforms.ToPILImage()(img_rec))\n",
    "            main_ax.axis('off')\n",
    "    # prettify by adding annotation texts\n",
    "    fig = prettify_with_annotation_texts(fig, grid, n_samples, latent_dims)\n",
    "    return fig\n",
    "\n",
    "def prettify_with_annotation_texts(fig, grid, n_samples, latent_dims):\n",
    "    # figure titles\n",
    "    titles = ['Deconv Reconstructions', 'Spatial Broadcast Reconstructions',\n",
    "              'Deconv Traversals', 'Spatial Broadcast Traversals']\n",
    "    idx_title_pos = [[0, 1, n_samples+1], [0, n_samples+2, n_samples*2+2],\n",
    "                    [3, 1, n_samples+1], [3, n_samples+2, n_samples*2+2]]\n",
    "    for title, idx_pos in zip(titles, idx_title_pos):\n",
    "        fig_ax = fig.add_subplot(grid[idx_pos[0], idx_pos[1]:idx_pos[2]])\n",
    "        fig_ax.annotate(title, xy=(0.5, 0), xycoords='axes fraction',\n",
    "                        fontsize=14, va='bottom', ha='center')\n",
    "        fig_ax.axis('off')\n",
    "    # left annotations\n",
    "    fig_ax = fig.add_subplot(grid[1, 0])\n",
    "    fig_ax.annotate('input', xy=(1, 0.5), xycoords='axes fraction',\n",
    "                    fontsize=12,  va='center', ha='right')\n",
    "    fig_ax.axis('off')\n",
    "    fig_ax = fig.add_subplot(grid[2, 0])\n",
    "    fig_ax.annotate('recons', xy=(1, 0.5), xycoords='axes fraction',\n",
    "                    fontsize=12, va='center', ha='right')\n",
    "    fig_ax.axis('off')\n",
    "    fig_ax = fig.add_subplot(grid[4:latent_dims + 4, 0])\n",
    "    fig_ax.annotate('latent coordinate traversed', xy=(0.9, 0.5),\n",
    "                    xycoords='axes fraction', fontsize=12,\n",
    "                    va='center', ha='center', rotation=90)\n",
    "    fig_ax.axis('off')\n",
    "    # pertubation magnitude\n",
    "    for i_y_grid in [[1, n_samples+1], [n_samples+2, n_samples*2+2]]:\n",
    "        fig_ax = fig.add_subplot(grid[latent_dims + 4, i_y_grid[0]:i_y_grid[1]])\n",
    "        fig_ax.annotate('pertubation magnitude', xy=(0.5, 0),\n",
    "                        xycoords='axes fraction', fontsize=12,\n",
    "                        va='bottom', ha='center')\n",
    "        fig_ax.set_frame_on(False)\n",
    "        fig_ax.axes.set_xlim([-2.5, 2.5])\n",
    "        fig_ax.xaxis.set_ticks([-2, 0, 2])\n",
    "        fig_ax.xaxis.set_ticks_position('top')\n",
    "        fig_ax.xaxis.set_tick_params(direction='inout', pad=-16)\n",
    "        fig_ax.get_yaxis().set_ticks([])\n",
    "    # latent dim\n",
    "    for latent_dim in range(latent_dims):\n",
    "        fig_ax = fig.add_subplot(grid[4 + latent_dim, n_samples*2 + 2])\n",
    "        fig_ax.annotate('lat dim ' + str(latent_dim + 1), xy=(0, 0.5),\n",
    "                        xycoords='axes fraction',\n",
    "                        fontsize=12, va='center', ha='left')\n",
    "        fig_ax.axis('off')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dabed8b4",
   "metadata": {
    "md-indent": "  "
   },
   "outputs": [],
   "source": [
    "def latent_space_geometry(STD_VAE, SBD_VAE):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    # x,y position grid in [0.2, 0.8] (generative factors)\n",
    "    equi = np.linspace(0.2, 0.8, 31)\n",
    "    equi_without_vert = np.setdiff1d(equi, np.linspace(0.2, 0.8, 6))\n",
    "\n",
    "    x_pos = np.append(np.repeat(np.linspace(0.2, 0.8, 6), len(equi)),\n",
    "                      np.tile(equi_without_vert, 6))\n",
    "    y_pos = np.append(np.tile(equi, 6),\n",
    "                      np.repeat(np.linspace(0.8, 0.2, 6), len(equi_without_vert)))\n",
    "    labels = np.append(np.repeat(np.arange(6), 31),\n",
    "                      np.repeat(np.arange(6)+10, 25))\n",
    "    # plot generative factor geometry\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.scatter(x_pos, y_pos, c=labels, cmap=plt.cm.get_cmap('rainbow', 10))\n",
    "    plt.gca().set_title('Ground Truth Factors', fontsize=16)\n",
    "    plt.xlabel('X-Position')\n",
    "    plt.ylabel('Y-Position')\n",
    "\n",
    "    # generate images\n",
    "    img_size = 64\n",
    "    shape_size = 16\n",
    "    images = torch.empty([len(x_pos), 3, img_size, img_size]).to(device)\n",
    "    for counter, (x, y) in enumerate(zip(x_pos, y_pos)):\n",
    "        images[counter] = generate_img(x, y, 'circle', 'red',\n",
    "                                      img_size, shape_size)\n",
    "\n",
    "    # STD VAE\n",
    "    [all_mu, all_log_var] = STD_VAE.encoder(images)\n",
    "    # most informative latent variable\n",
    "    lat_1, lat_2 = all_log_var.mean(axis=0).sort()[1][:2]\n",
    "    # latent coordinates\n",
    "    x_lat = all_mu[:, lat_1].detach().cpu().numpy()\n",
    "    y_lat = all_mu[:, lat_2].detach().cpu().numpy()\n",
    "    # plot latent space geometry\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.scatter(x_lat, y_lat, c=labels, cmap=plt.cm.get_cmap('rainbow', 10))\n",
    "    plt.gca().set_title('DeConv', fontsize=16)\n",
    "    plt.xlabel('latent 1 value')\n",
    "    plt.ylabel('latent 2 value')\n",
    "\n",
    "    # SBD VAE\n",
    "    [all_mu, all_log_var] = SBD_VAE.encoder(images)\n",
    "    # most informative latent variable\n",
    "    lat_1, lat_2 = all_log_var.mean(axis=0).sort()[1][:2]\n",
    "    # latent coordinates\n",
    "    x_lat = all_mu[:, lat_1].detach().cpu().numpy()\n",
    "    y_lat = all_mu[:, lat_2].detach().cpu().numpy()\n",
    "    # plot latent space geometry\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.scatter(x_lat, y_lat, c=labels, cmap=plt.cm.get_cmap('rainbow', 10))\n",
    "    plt.gca().set_title('Spatial Broadcast', fontsize=16)\n",
    "    plt.xlabel('latent 1 value')\n",
    "    plt.ylabel('latent 2 value')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f35bc002",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 150\n",
    "latent_dims = 5 # x position, y position, color, extra slots\n",
    "fixed_variance = 0.3\n",
    "\n",
    "standard_VAE = VAE(vae_type='Standard', latent_dim=latent_dims,\n",
    "                   fixed_var=fixed_variance)\n",
    "SBD_VAE = VAE(vae_type='SBD', latent_dim=latent_dims,\n",
    "              fixed_var=fixed_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "404ba3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_standard_VAE  = train(sprites_dataset, epochs, standard_VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e46936a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_SBD_VAE = train(sprites_dataset, epochs, SBD_VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f32a802d",
   "metadata": {
    "md-indent": "  "
   },
   "outputs": [],
   "source": [
    "reconstructions_and_latent_traversals(trained_standard_VAE,\n",
    "                                      trained_SBD_VAE, sprites_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "305475e0",
   "metadata": {
    "md-indent": "  "
   },
   "outputs": [],
   "source": [
    "latent_space_geometry(trained_standard_VAE, trained_SBD_VAE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}