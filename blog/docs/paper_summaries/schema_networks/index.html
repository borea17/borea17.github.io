<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.340">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="borea17">
<meta name="dcterms.date" content="2020-07-15">

<title>borea17 - Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../logo.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">borea17</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../paper_summaries.html" rel="" target="">
 <span class="menu-text">Paper Summaries</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../ml101.html" rel="" target="">
 <span class="menu-text">ML101</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">RL</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>borea17 </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 15, 2020</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#model-description" id="toc-model-description" class="nav-link active" data-scroll-target="#model-description">Model Description</a></li>
  <li><a href="#learning-the-model" id="toc-learning-the-model" class="nav-link" data-scroll-target="#learning-the-model">Learning the Model</a>
  <ul class="collapse">
  <li><a href="#notation" id="toc-notation" class="nav-link" data-scroll-target="#notation">Notation</a></li>
  <li><a href="#learning-problem" id="toc-learning-problem" class="nav-link" data-scroll-target="#learning-problem">Learning Problem</a></li>
  <li><a href="#objective-function" id="toc-objective-function" class="nav-link" data-scroll-target="#objective-function">Objective Function</a></li>
  <li><a href="#schema-learning" id="toc-schema-learning" class="nav-link" data-scroll-target="#schema-learning">Schema Learning</a></li>
  </ul></li>
  <li><a href="#planning" id="toc-planning" class="nav-link" data-scroll-target="#planning">Planning</a></li>
  <li><a href="#drawbacks" id="toc-drawbacks" class="nav-link" data-scroll-target="#drawbacks">Drawbacks</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p><a href="https://arxiv.org/abs/1706.04317">Kansky et al.&nbsp;(2017)</a> showed remarkable results of zero-shot transfer in several variations of Breakout by introducing Schema Networks as a generative model for object-oriented reinforcement learning and planning. This model incorporates objects as <em>entities</em>, represents local cause-effect relationships including one or more entities and is based on Probabilistic Graphical Models (PGMs). Due to its foundation in PGMs, Schema Networks support flexible inference and search strategies for planning.</p>
<section id="model-description" class="level2">
<h2 class="anchored" data-anchor-id="model-description">Model Description</h2>
<p>Building upon the ideas of object-oriented Markov decision processes (OO-MDPs), states are represented as a list of entities where each entity can be understood as a different instantiation from the same class (i.e., all entities share the same attributes). Additionally, the attributes, actions and rewards are binarized using discretization and one-hot encoding, see image below. This representation comes from a handcrafted image parser with the handwavy argument that in practice a vision module could be responsible for this task.</p>
<table class="table">
<colgroup>
<col style="width: 100%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><img src="./img/exemplary_state_representation.png" title="Exemplary State Representation" class="img-fluid" alt="Exemplary State Representation"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Exemplary State Representation in a Schema Network</strong>. A handcrafted image parser converts an image (left) into the state representation (right), where filled green circles indicate that the binary variable is set to <em>True</em>.</td>
</tr>
</tbody>
</table>
<p><a href="https://arxiv.org/abs/1706.04317">Kansky et al.&nbsp;(2017)</a> define 53 attributes for each entity in the Breakout domain (21 for bricks, 30 for the paddle, 1 for walls, 1 for the ball). However, they do not elaborate on what these attributes describe exactly. Furthermore, each pixel is identified as a part of an object and assigned the corresponding attributes. Accordingly, their representation could rather be understood as a 53-channel image where each entry can either be 0 or 1, e.g., one layer showing the walls. In this form, the entity-based state representation can also be provided to other algorithms such as A3C.</p>
<p>Similiar to OO-MDPs, state transitions are determined by a change of entity-attributes. However, due to the specific representation in Schema Networks, entity-attributes can only be active or inactive (with an associated probability). An attribute becomes activated if a <em>grounded schema</em><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> is active. Grounded schemas can include a variable size of entity attributes from a variable number of entities and may include one or more actions. Thus, these schemas can be interpreted as local cause-effect relationships. Formally, a grounded schema <span class="math inline">\(\phi^{k}\)</span> is a binary variable that becomes activated via a probabilistic AND over the binary variables <span class="math inline">\(v_1, \dots, v_n\)</span> that are included in it:</p>
<p><span class="math display">\[
\begin{align}
  \phi^{k} = \text{AND} (v_1, \dots, v_n) = \prod_{i=1}^n P(v_i = 1).
\end{align}
\]</span></p>
<p>The binary variables <span class="math inline">\(v_1, \dots, v_n\)</span> may be entity-attributes<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> or actions, see image below.</p>
<p>Multiple grounded schemas can predict the same attribute which is formalized through an OR factor, e.g., let <span class="math inline">\(\alpha_{i, j}^{(t+1)}\)</span> denote the <span class="math inline">\(j^{th}\)</span> attribute of the <span class="math inline">\(i^{th}\)</span> entity at time <span class="math inline">\(t+1\)</span> and assume there are <span class="math inline">\(n\)</span> grounded schemas that predict this entity attribute. Then, this formalizes into</p>
<p><span class="math display">\[
\begin{align}
  \alpha_{i,j}^{(t+1)} = \text{OR} (\phi_{i,j}^{1}, \dots, \phi_{i, j}^{n}) = 1 - \prod_{k=1}^n  \big(1 - P(\phi_{i,j}^k)\big).
\end{align}
\]</span></p>
<p><a href="https://arxiv.org/abs/1706.04317">Kansky et al.&nbsp;(2017)</a> divide entity attributes into two classes: * <em>Positional Attributes</em>: These attributes correspond to discrete positions. * <em>Non-Positional Attributes</em>: The semantic meaning of those attributes is unknown to the model such that they may encode completely different things, e.g., color and shape.</p>
<p>A <strong>self-transition</strong> variable is introduced for <em>positional attributes</em> which represents the probability that a position attribute will remain active in the next time step when no schema predicts a change from that position. Note that through this mechanism, they include the bias that an object cannot be at multiple positions at the same time.</p>
<!-- A problem of ambiguity arises, when entity-attributes are categorical and should -->
<!-- remain active without a grounded schema[^3]. To circumvent this problem, -->
<!-- [Kansky et al. (2017)](https://arxiv.org/abs/1706.04317) introduce a -->
<!-- *self-transition* gate that allows an entity-attribute to remain -->
<!-- active in the next time step when no schema predicts a change from -->
<!-- that entity-attribute, see image below. -->
<table class="table">
<colgroup>
<col style="width: 100%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><img src="./img/transition_dynamics.png" title="Transition Dynamics" class="img-fluid" alt="Transition Dynamics"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Transition dynamics in Schema Networks are governed by changes in entity-attributes due to activated grounded schemas. In this example all relevant gates are shown to illustrate the state transition dynamics via this mechanics. Note that there are two schemas that predict the same behavior, i.e., only one input in OR is necessary to activate <span class="math inline">\(y=1\)</span>.</td>
</tr>
</tbody>
</table>
<p>Formally, a self transition is a NOR factor over the grounded schemas that predict a change from a position attribute (i.e., the grounded schemas that predict towards a different position) combined with an AND factor over the NOR factor and the position attribute at the current time step. E.g., let <span class="math inline">\(\alpha_{i,j}^{t}\)</span> denote the <span class="math inline">\(j^{th}\)</span> position attribute of the the <span class="math inline">\(i^{th}\)</span> entity at time <span class="math inline">\(t\)</span> and assume that the set <span class="math inline">\(\{\phi^1, \dots, \phi^{n} \}\)</span> includes all schemas predicting towards a different position of that entity. Then, the self-transition is formalized as follows</p>
<p><span class="math display">\[
\begin{align}
  \Lambda_{i,j}^{t+1}
  = \text{AND} \big(\lnot \phi^{1}, \dots, \lnot \phi^{n}, \alpha_{i,j}^{t} \big).
\end{align}
\]</span></p>
<p>Finally, the transition function in this model can be factorized as</p>
<p><span class="math display">\[
\begin{align}
  T\left(s^{(t+1)} | s^{(t)}, a^{(t)}\right) = \prod_{i=1}^N \prod_{j=1}^M T_{i, j} \left(s_{i, j}^{(t+1)}|s^{(t)}, a^{(t)}\right),
\end{align}
\]</span></p>
<p>where <span class="math inline">\(T_{i,j}\)</span> denotes the transition probability of the <span class="math inline">\(j^{th}\)</span> attribute of the <span class="math inline">\(i^{th}\)</span> entity towards its value defined in <span class="math inline">\(s_{i,j}^{(t+1)}\)</span>. The entity attribute <span class="math inline">\(s_{i,j}^{(t+1)}\)</span> is by definition activated if one of its grounded schema is active or if a self-transition occured, thus the entity-attribute transition probability is defined as</p>
<p><span class="math display">\[
\begin{align}
  T_{i,j} \left( s_{i,j}^{(t+1)} | s^{(t)}, a^{(t)}\right) = \text{OR}\left( \phi^{k_1}, \dots, \phi^{k_Q}, \Lambda_{i,j} \right),
\end{align}
\]</span></p>
<p>where <span class="math inline">\(\Lambda_{i,j}\)</span> denotes the self-transition variable of <span class="math inline">\(s_{i,j}\)</span> and <span class="math inline">\(k_1, \dots, k_Q\)</span> are the indices of all grounded schemas that predict <span class="math inline">\(s_{i,j}\)</span>. Note that although all variables are defined as binary variables, this model could still be used for non-deterministic environments.</p>
<p>To increase the generality of their model such that the attribute change of two entities is described by the same schema, <a href="https://arxiv.org/abs/1706.04317">Kansky et al. (2017)</a> introduce the term <em>ungrounded schema</em> or <em>template</em>. An ungrounded schema can be understood as template for specific grounded schemas, i.e., it describes a grounded schema where the included entity-attributes are assigned relative to the position of the entity-attribute that should be predicted.</p>
</section>
<section id="learning-the-model" class="level2">
<h2 class="anchored" data-anchor-id="learning-the-model">Learning the Model</h2>
<p>The Schema Network is essentially a factor graph that is aimed to be a probabilistic simulator of the game environment using the aforementioned representation. Assuming that the environment dynamics can be represented by some Schema Network, learning the model comes down to structure learning in graphical models. <a href="https://arxiv.org/abs/1706.04317">Kansky et al.&nbsp;(2017)</a> preprocess a sequence of observations (dataset of <em>state-action-reward-new state</em> tuples over time) into a convenient representation, define a NP-hard optimization problem based on this representation as the optimal solution and retrieve the schemas (and Schema Network) through solving the optimization problem approximately using linear programming (LP) relaxations.</p>
<section id="notation" class="level3">
<h3 class="anchored" data-anchor-id="notation">Notation</h3>
<p>Let <span class="math inline">\(\alpha\_{i,j}^{(t)}\)</span> denote the <span class="math inline">\(j^{th}\)</span> attribute of the <span class="math inline">\(i^{th}\)</span> entity at time <span class="math inline">\(t\)</span> and let <span class="math inline">\(\textbf{e}\_i^{(t)} \in \\{0, 1\\}^{M}\)</span> be an<br>
<span class="math inline">\(M\)</span>-dimensional binary vector representing all entity-attributes values of the <span class="math inline">\(i^{th}\)</span> entity at time <span class="math inline">\(t\)</span>, i.e., <span class="math inline">\(\textbf{e}\_i^{(t)} = \begin{bmatrix}\alpha\_{i,1}^{(t)} &amp; \dots &amp; \alpha\_{i,M}^{(t)} \end{bmatrix}^{\text{T}}\)</span> where <span class="math inline">\(M\)</span> denotes the number of entity-attributes.</p>
<p>Let <span class="math inline">\(\boldsymbol{\beta}\_{i}^{(t)}\in \\{0,1\\}^E\)</span> be a row vector representing the attribute values of the <span class="math inline">\(i^{th}\)</span> entity and the entity-attributes of the <span class="math inline">\(R-1\)</span> (fixed radius) spatial neighbors, i.e., <span class="math inline">\(\boldsymbol{\beta}_{i}^{(t)} = \begin{bmatrix} \textbf{e}\_{i}^{(t)} &amp; \textbf{e}\_{i+1}^{(t)} &amp; \dots &amp; \textbf{e}\_{R-1}^{(t)} \end{bmatrix}\)</span> has length <span class="math inline">\(E=M(R-1) + M = MR\)</span>.</p>
<p>Suppose there are <span class="math inline">\(N\)</span> entities observed for <span class="math inline">\(\tau\)</span> timesteps. Then, let <span class="math inline">\(\textbf{X}\in\\{0,1\\}^{D\times E}\)</span> be a binary matrix where each row consists of a <span class="math inline">\(\boldsymbol{\beta}\_{i}^{(t)}\)</span> and there are <span class="math inline">\(D=N\cdot \tau\)</span> rows (all entities and time steps). Similarly, let <span class="math inline">\(\textbf{y}\in\\{0, 1\\}^D\)</span> be a binary vector where each entry refers to the future attribute value <span class="math inline">\(\alpha\_{i,j}^{(t+1)}\)</span> corresonding to the a row of <span class="math inline">\(\textbf{X}\)</span> with entity <span class="math inline">\(i\)</span> and time <span class="math inline">\(t\)</span>, i.e.,</p>
<p><span class="math display">\[
\begin{align*}
  \textbf{X} &amp;=
  \begin{bmatrix}
    \begin{bmatrix} \boldsymbol{\beta}_{1}^{(1)} &amp; \dots &amp; \boldsymbol{\beta}_{N}^{(1)} \end{bmatrix}^{\text{T}} \\
    \vdots\\
    \begin{bmatrix} \boldsymbol{\beta}_{1}^{(\tau)} &amp; \dots &amp; \boldsymbol{\beta}_{N}^{(\tau)} \end{bmatrix}^{\text{T}}
  \end{bmatrix} , \quad \textbf{y} =
   \begin{bmatrix}
\begin{bmatrix} \alpha_{1,j}^{(2)} &amp; \dots &amp; \alpha_{N,j}^{(2)} \end{bmatrix}^{\text{T}}
\\ \vdots \\
\begin{bmatrix} \alpha_{1,j}^{(\tau+1)} &amp; \dots &amp; \alpha_{N,j}^{(\tau+1)} \end{bmatrix}^{\text{T}}
    \end{bmatrix}
\end{align*}
\]</span></p>
</section>
<section id="learning-problem" class="level3">
<h3 class="anchored" data-anchor-id="learning-problem">Learning Problem</h3>
<p>The goal is to predict <span class="math inline">\(\alpha\_{i,j}^{(t+1)}\)</span> based on the entity-attributes of itself and its spatial neighbors. Using the introduced notation, the learning problem can be defined as follows</p>
<p><span class="math display">\[
  \textbf{y} = f_{\textbf{W}} (\textbf{X}) = \overline{\overline{\textbf{X}} \textbf{W}} \textbf{1},
\]</span></p>
<p>where <span class="math inline">\(f\_{\textbf{W}}\)</span> denotes the desired function of <em>ungrounded schemas</em> which is applied row-wise to the argument <span class="math inline">\(\textbf{X}\)</span> to produce either active or inactive grounded schemas. <span class="math inline">\(f\_{\textbf{W}}\)</span> is parametrized by a binary matrix <span class="math inline">\(\textbf{W} \in \\{0, 1\\}^{E \times L}\)</span> with each column representing one ungrounded schema for a maximum of <span class="math inline">\(L\)</span> schemas. Each element that is set to 1 in a column of <span class="math inline">\(\textbf{W}\)</span> indicates that for this schema the corresponding input attribute (from <span class="math inline">\(\textbf{X}\)</span>) is necessary for an activated grounded schema. On the right-hand side of the equation above all variables and operations follow Boolean logic: addition corresponds to <code>OR</code>ing and overlining to negation.</p>
<blockquote class="blockquote">
<p>E.g., let <span class="math inline">\(\textbf{w}\_i = \begin{bmatrix} w\_{i_1} &amp; \dots &amp; w\_{i_ E}\end{bmatrix}^{\text{T}}\)</span> denote the <span class="math inline">\(i^{th}\)</span> column of <span class="math inline">\(\hspace{0.1cm}\textbf{W}\)</span> and <span class="math inline">\(\textbf{x}_r = \begin{bmatrix} x\_{r_1} &amp; \dots &amp; x\_{r_E} \end{bmatrix}\)</span> be the <span class="math inline">\(r^{th}\)</span> row of <span class="math inline">\(\hspace{0.1cm}\textbf{X}\)</span>. Then, the (dot) product of the two vectors leads to</p>
<p><span class="math display">\[
\begin{align}
\textbf{x}_r \textbf{w}_i = \sum_{k=1}^{E} x_{r_k} \cdot w_{i_k} = \text{OR} \left( x_{r_1} w_{i_1}, \dots, x_{r_E} w_{i_E} \right),
\end{align}
\]</span></p>
<p>in this form the corresponding grounded schema would be activated as soon as one <em>precondition</em> is satisfied, i.e., as soon as for one <span class="math inline">\(w\_{i_j} = 1\)</span> the corresponding attribute variable is also <span class="math inline">\(x\_{i_j}=1\)</span>.</p>
</blockquote>
<p>A <strong>grounded schema</strong> <span class="math inline">\(\phi\)</span> is defined through a logical AND over the necessary attributes (i.e., all preconditions)<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> <!-- , thus the product needs to be rewritten as follows --></p>
<p><span class="math display">\[
\begin{align}
  \phi = \text{AND}\left( \{x_{r_j} \mid \forall j: w_{i_j} = 1 \}  \right) = \text{Not} \left(
  \text{OR} \left(\text{Not } \{x_{r_j} \mid \forall j: w_{i_j} = 1 \} \right) \right)
  = \overline{\overline{\textbf{x}}_r \textbf{w}_i}.
\end{align}
\]</span></p>
<p>This equation states how one individual schema (<span class="math inline">\(\textbf{w}_{i}\)</span>) is applied to one attribute vector <span class="math inline">\(\textbf{x}_R\)</span>. The first equation of this section summarizes this result into a matrix-matrix multiplication.</p>
<p>At the end all outputs of each individual schema are <code>OR</code>ed to produce the final prediction for each attribute (corresponding to the provided attribute vector). This is done through multiplication with the identity tensor <span class="math inline">\(\textbf{1} \in \\{1\\}^{L \times D}\)</span>. Remind that this is in alignment with the entity-attribute transition probability definition for <em>non-positional</em> attributes</p>
<p><span class="math display">\[
\begin{align}
  T_{i,j} \left( s_{i,j}^{(t+1)} | s^{(t)}, a^{(t)}\right) = \text{OR}\left( \phi^{k_1}, \dots, \phi^{k_Q}\right)
\end{align}
\]</span></p>
<p>As stated above, for <em>positional attributes</em> a self-transition <span class="math inline">\(\Lambda_{i,j}\)</span> is added to allow these attributes to remain active when no change is predicted. Unfortunately, <a href="https://arxiv.org/abs/1706.04317">Kansky et al.&nbsp;(2017)</a> did not elaborate on self-transitions in the learning problem. Thus, we can only guess how they are included. My idea would be to preprocess the data such that only positional attributes that changed (either from 0 to 1 or vice versa) are included in the learning problem. In the prediction phase, we then simply group all positional grounded schemas and apply the self-transition as a post-processing step.</p>
</section>
<section id="objective-function" class="level3">
<h3 class="anchored" data-anchor-id="objective-function">Objective Function</h3>
<p>As there might be multiple Schemas that explain certain behaviors, the objective function is aimed to minimize the prediction error while keeping ungrounded schemas as simple as possible<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>:</p>
<p><span class="math display">\[
\begin{align}
  \min_{\textbf{W}} J(\textbf{W}) = \min_{\textbf{W}} \underbrace{\frac {1}{D} \cdot \Bigg| \textbf{y} -
  f_{\textbf{W}} (\textbf{X}) \Bigg|_1 }_{\text{Prediction Error}}+
  \underbrace{C \cdot \Bigg| \textbf{W} \Bigg|_1}_{\text{Model Complexity}},
\end{align}
\]</span></p>
<p>where <span class="math inline">\(C\)</span> is a hyperparameter that can be used to control the trade-off between the complexity of the model and the accuracy of the predictions. This is a NP-hard optimization problem, since <span class="math inline">\(\textbf{W}\in \\{0,1\\}^{E\times L}\)</span> is a binary matrix. Furthermore, the search space is combinatorially large, i.e., there are <span class="math inline">\(2^{E \cdot L}\)</span> possible realizations of <span class="math inline">\(\textbf{W}\)</span>. Hence, finding the optimal solution <span class="math inline">\(\textbf{W}^{*}\)</span> is infeasible (for larger environments such as Breakout).</p>
</section>
<section id="schema-learning" class="level3">
<h3 class="anchored" data-anchor-id="schema-learning">Schema Learning</h3>
<p><a href="https://arxiv.org/abs/1706.04317">Kansky et al.&nbsp;(2017)</a> search for an approximate solution with the desired features (low prediction error and low model complexity) using a greedy algorithm of linear programming (LP) relaxations. This algorithm works as follows</p>
<ol type="1">
<li><p>Start with an empty set of schemas, i.e., <span class="math inline">\(\textbf{W} = \textbf{0}\)</span>.</p></li>
<li><p>Greedily select a schema <span class="math inline">\(\textbf{w}\)</span> that perfectly predicts a cluster of input samples:</p>
<ol type="a">
<li><p>Randomly select an input sample <span class="math inline">\(\textbf{x}\_n\)</span> for which <span class="math inline">\(y_n = 1\)</span> and <span class="math inline">\(f\_{\textbf{W}} (\textbf{x}\_n) = 0\)</span>.</p></li>
<li><p>Put sample in the set <code>solved</code>, then solve the following LP</p>
<p><span class="math display">\[
\begin{align}
   \begin{split}
   &amp;\max_{\textbf{w}\in \{0,1\}^D} \sum_{n: y_n = 1} \overline{ \overline{\textbf{x}}_n \textbf{w}} =
   \min_{\textbf{w} \in \{0, 1\}^{D}} \sum (1 - {\textbf{x}}_n \textbf{w}) \\
   &amp;\quad \quad \text{s.t. } \forall_{n:y_n=0} \quad (1 - \textbf{x}_n) \textbf{w} &gt; 1 \qquad \text{(no false alarms)}\\
   &amp;\qquad \quad \hspace{0.3cm} \forall_{n\in \text{solved}} \hspace{0.2cm} (1-\textbf{x}_n) \textbf{w} = 0  \qquad \text{(active grounded schema)}
   \end{split}
\end{align}
\]</span></p></li>
<li><p>Update the <code>solved</code> set, i.e., put all samples in for which <span class="math inline">\((1-\textbf{x}_n) \textbf{w} = 0\)</span> in the <code>solved</code> set.</p></li>
</ol></li>
<li><p>Simplify resulting schema by making <span class="math inline">\(\textbf{w}\)</span> as sparse as possible while keeping the predictions correct without introducing false alarms:</p>
<p><span class="math display">\[\begin{align}
   \begin{split}
     &amp;\min_{\textbf{w} \in \{0, 1\}^D} \textbf{w}^{\text{T}} \textbf{1} \\
     &amp;\quad \quad \text{s.t. } \forall_{n:y_n=0} \quad (1 - \textbf{x}_n) \textbf{w} &gt; 1\\
     &amp;\qquad \quad \hspace{0.3cm} \forall_{n\in \text{solved}} \hspace{0.2cm} (1-\textbf{x}_n) \textbf{w} = 0
   \end{split}
\end{align}\]</span></p></li>
<li><p>Binarize <span class="math inline">\(\textbf{w}\)</span></p></li>
</ol>
</section>
</section>
<section id="planning" class="level2">
<h2 class="anchored" data-anchor-id="planning">Planning</h2>
<p>TODO: Summarize planning</p>
</section>
<section id="drawbacks" class="level2">
<h2 class="anchored" data-anchor-id="drawbacks">Drawbacks</h2>
<ul>
<li>based on specific and exact representation <span class="math inline">\(\Rightarrow\)</span> not end-to-end</li>
<li>blow up of entity attributes, since:
<ul>
<li>binarized attributes using discretization and one-hot encoding</li>
<li>all entities share the same set of attributes</li>
</ul></li>
<li>image parser needs to know the whole attribute space beforehand<br>
Note: also the reward space needs to be defined beforehand.</li>
<li>learning algorithm is only capable of learning deterministic environments</li>
</ul>
<hr>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>A grounded schema in Schema Networks is similar to a rule in OO-MDP terms. Each attribute may have several grounded schemas. When one of those schemas is active (active effect condition in OO-MDP terms), the corresponding attribute is actived (set to <em>True</em>) in the next step.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><a href="https://arxiv.org/abs/1706.04317">Kansky et al.&nbsp;(2017)</a> refer to the entity attributes in the binary variables <span class="math inline">\(v_1, \dots, v_n\)</span> of a grounded schema as <em>entity-attribute preconditions</em>. This terminology relates attributes to preconditions, since the actual condition (i.e., grounded schema) is only active iff all preconditions are active.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>In Boolean logic, De Morgan’s law states that <span class="math inline">\(\text{AND} \Big(A, B\Big) =\text{NOT} \Big( \text{OR} \big(\text{NOT } A, \text{NOT } B \big)\Big)\)</span>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Occam’s razor (<em>law of parsimony</em>) states that simplest solution is most likely the right one.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>