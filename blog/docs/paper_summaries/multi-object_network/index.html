<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="borea17">
<meta name="dcterms.date" content="2021-03-27">

<title>borea17 - MONet: Unsupervised Scene Decomposition and Representation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../logo.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">borea17</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../paper_summaries.html"> 
<span class="menu-text">Paper Summaries</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../ml101.html"> 
<span class="menu-text">ML101</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">MONet: Unsupervised Scene Decomposition and Representation</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button" data-quarto-source-url="https://github.com/borea17/Notebooks/blob/master/07_Concrete_Distribution.ipynb">View Source</a></li></ul></div></div>
                                <div class="quarto-categories">
                <div class="quarto-category">reimplementation</div>
                <div class="quarto-category">unsupervised</div>
                <div class="quarto-category">VAE</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>borea17 </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 27, 2021</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#model-description" id="toc-model-description" class="nav-link active" data-scroll-target="#model-description">Model Description</a></li>
  <li><a href="#implementation" id="toc-implementation" class="nav-link" data-scroll-target="#implementation">Implementation</a>
  <ul class="collapse">
  <li><a href="#data-generation" id="toc-data-generation" class="nav-link" data-scroll-target="#data-generation">Data Generation</a></li>
  <li><a href="#model-implementation" id="toc-model-implementation" class="nav-link" data-scroll-target="#model-implementation">Model Implementation</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a></li>
  </ul></li>
  <li><a href="#drawbacks-of-paper" id="toc-drawbacks-of-paper" class="nav-link" data-scroll-target="#drawbacks-of-paper">Drawbacks of Paper</a></li>
  <li><a href="#acknowledgment" id="toc-acknowledgment" class="nav-link" data-scroll-target="#acknowledgment">Acknowledgment</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p><a href="https://arxiv.org/abs/1901.11390">Burgess et al.&nbsp;(2019)</a> developed the <strong>Multi-Object Network (MONet)</strong> as an end-to-end trainable model to decompose images into meaningful entities such as objects. Similar to <a href="https://borea17.github.io/paper_summaries/AIR">AIR</a>, the whole training process is unsupervised, i.e., there are no labeled segmentations, handcrafted bounding boxes or whatsoever. In essence, their model combines a Variational Auto-Encoder (<a href="https://borea17.github.io/paper_summaries/auto-encoding_variational_bayes">VAE</a>) with a recurrent attention network (<a href="https://borea17.github.io/paper_summaries/u_net">U-Net</a> <em>segmentation network</em>) to spatially decompose scenes into attention masks (over which the VAE needs to reconstruct masked regions) and latent representations of each masked region. In contrast to AIR, MONet does not contain a fully generative model and its latent space is less structured. As a proof of concept, they show that their model could learn disentangled representations in a common latent code (i.e., representations of object features in latent space) and object segmentations (i.e., attention masks on the original image) on non-trivial 3D scenes.</p>
<section id="model-description" class="level2">
<h2 class="anchored" data-anchor-id="model-description">Model Description</h2>
<p>MONet builds upon the inductive bias that the world (or rather <em>simple images</em> of the world) can often be approximated as a composition of individual objects with the same underlying structure (i.e., different instantiations of the same class). To put this into practice, <a href="https://arxiv.org/abs/1901.11390">Burgess et al.&nbsp;(2019)</a> developed a conditional generative sampling scheme in which scenes are spatially decomposed into parts that have to be individually modelled through a common representation code. The architecture incorporates two kind of neural networks that are trained in tandem:</p>
<ul>
<li><p><strong>Attention Network</strong>: Its purpose is to deliver attention masks <span class="math inline">\(\textbf{m}_k\)</span> for the image such that the whole image is completely spatially decomposed into <span class="math inline">\(K\)</span> parts, i.e., <span class="math inline">\(\sum_{k=1}^K
\textbf{m}_k = \textbf{1}\)</span>. Ideally, after training each mask focuses on a semantically meaningful element/segment of the image. Thus, it may also be understood as a <em>segmentation network</em>.</p>
<p>To allow for a variable number of attention masks, <a href="https://arxiv.org/abs/1901.11390">Burgess et al. (2019)</a> use a recurrent neural network <span class="math inline">\(\alpha_{\boldsymbol{\psi}}\)</span> for the decomposition. Therein, an auto-regressive process is defined for the ongoing state. This state is called <em>scope</em> <span class="math inline">\(\textbf{s}_k \in [0, 1]^{W\times
H}\)</span> (image width <span class="math inline">\(W\)</span> and height <span class="math inline">\(H\)</span>) as it is used to track the image parts that remain to be explained, i.e., the scope for the next state is given by</p>
<p><span class="math display">\[
   \textbf{s}_{k+1} = \textbf{s}_k \odot \left(\textbf{1} -
\underbrace{\alpha_{\boldsymbol{\psi}} \left( \textbf{x};
\textbf{s}_{k} \right)}_{[0,1]^{W \times H}} \right)
\]</span></p>
<p>with the first scope <span class="math inline">\(\textbf{s}_0 = \textbf{1}\)</span> (<span class="math inline">\(\odot\)</span> denotes element-wise multiplication). The attention masks are given by</p>
<p><span class="math display">\[
  \textbf{m}_k  = \begin{cases} \textbf{s}_{k-1} \odot
  \alpha_{\boldsymbol{\psi}} \left( \textbf{x}; \textbf{s}_{k-1}
  \right) &amp; \forall k &lt; K \\
  \textbf{s}_{k-1} &amp; k=K \end{cases}
\]</span></p>
<p>By construction, we get that</p>
<p><span class="math display">\[
\begin{align}
  &amp;\textbf{s}_{k} = \textbf{s}_{k+1} + \textbf{m}_{k + 1} =
  \textbf{s}_{k+2} + \textbf{m}_{k+2} + \textbf{m}_{k+1} \\
  \textbf{1}=&amp;\textbf{s}_0 =
  \textbf{s}_{K-1} + \sum_{k=1}^{K-1} \textbf{m}_{k} = \sum_{k=1}^K \textbf{m}_k,
\end{align}
\]</span></p>
<p>i.e., at each recursion the remaining part to be explained <span class="math inline">\(\textbf{s}_{k}\)</span> is divided into a segmentation mask <span class="math inline">\(\textbf{m}_{k+1}\)</span> and a new scope <span class="math inline">\(\textbf{s}_{k+1}\)</span> such that with <span class="math inline">\(\textbf{s}_0=\textbf{1}\)</span> the entire image is explained by the resulting segmentation masks, i.e., <span class="math inline">\(\sum_{k=1}^K \textbf{m}_k = \textbf{1}\)</span>.</p></li>
<li><p><strong>Component VAE</strong>: Its purpose is to represent each masked region in a common latent code, i.e., each segment is encoded by the same VAE<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. The encoder distribution <span class="math inline">\(q_{\boldsymbol{\phi}}
\left(\textbf{z}_k | \textbf{x}, \textbf{m}_k\right)\)</span> is conditioned both on the input image <span class="math inline">\(\textbf{x}\)</span> and the corresponding attention mask <span class="math inline">\(\textbf{m}_k\)</span>. I.e., instead of feeding each masked region into the network, <a href="https://arxiv.org/abs/1901.11390">Burgess et al.&nbsp;(2019)</a> use the whole image <span class="math inline">\(\textbf{x}\)</span> concatenated with the corresponding attention mask <span class="math inline">\(\textbf{m}_k\)</span>. As a result, we get <span class="math inline">\(K\)</span> different latent codes <span class="math inline">\(\textbf{z}_k\)</span> (termed “slots”) which represent the features of each object (masked region) in a common latent/feature space across all objects.</p>
<p>The decoder distribution <span class="math inline">\(p_{\boldsymbol{\theta}}\)</span> is required to reconstruct the image component <span class="math inline">\(\widetilde{\textbf{x}}_k \sim p_{\boldsymbol{\theta}} \left( \textbf{x} | \textbf{z}_k \right)\)</span> and the attention masks<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> <span class="math inline">\(\widetilde{\textbf{m}}_k \sim p_{\boldsymbol{\theta}}
\left(\textbf{c} | \textbf{z}_k \right)\)</span> from these latent codes. Note that <span class="math inline">\(p_{\boldsymbol{\theta}} \left(\textbf{c} | \textbf{z}_k
\right)\)</span> defines the mask distribution of the Component VAE, whereas <span class="math inline">\(q_{\boldsymbol{\psi}} \left(\textbf{c} | \textbf{x}\right)\)</span> denotes the mask distribution of the attention network<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<p>Importantly, each of the <span class="math inline">\(k\)</span> component reconstruction distributions is multiplied with the corresponding attention mask <span class="math inline">\(\textbf{m}_k\)</span>, i.e.,</p>
<p><span class="math display">\[
\text{Reconstruction Distribution}_k = \textbf{m}_k \odot
   p_{\boldsymbol{\theta}} \left(\textbf{x} | \textbf{z}_k \right).
\]</span></p>
<p>The negative (decoder) log likelihood <em>NLL</em> (can be interpreted as the <em>reconstruction error</em>, see my post on <a href="https://borea17.github.io/paper_summaries/auto-encoding_variational_bayes#model-description">VAEs</a>) of the whole image is given by</p>
<p><span class="math display">\[
\text{NLL} = - \log \left( \sum_{k=1}^K \textbf{m}_k \odot p_{\boldsymbol{\theta}} \left(\textbf{x} | \textbf{z}_k \right)\right),
\]</span></p>
<p>where the sum can be understood as the reconstruction distribution of the whole image (mixture of components) conditioned on the latent codes <span class="math inline">\(\textbf{z}_k\)</span> and the attention masks <span class="math inline">\(\textbf{m}_k\)</span>. Clearly, the reconstructions <span class="math inline">\(\widetilde{\textbf{x}}_k \sim p_{\boldsymbol{\theta}} \left(
\textbf{x} | \textbf{z}_k\right)\)</span> are unconstrained outside of the masked regions (i.e., where <span class="math inline">\(m_{k,i} = 0\)</span>).</p>
<p>Note that they use a prior for the latent codes <span class="math inline">\(\textbf{z}_k\)</span>, but not for the attentions masks <span class="math inline">\(\textbf{m}_k\)</span>. Thus, the model is not fully generative, but rather a conditional generative model.</p></li>
</ul>
<p>The figure below summarizes the whole architecture of the model by showing the individual components (attention network, component VAE) and their interaction.</p>
<table class="table">
<colgroup>
<col style="width: 100%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><img src="./img/MONet_schematic.png" title="Schematic of MONet" class="img-fluid" alt="Schematic of MONet"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Schematic of MONet</strong>. A recurrent attention network is used to obtain the attention masks <span class="math inline">\(\textbf{m}^{(i)}\)</span>. Afterwards, a group structured representation is retrieved by feeding each concatenation of <span class="math inline">\(\textbf{m}^{(i)}, \textbf{x}\)</span> through the same VAE with encoder parameters <span class="math inline">\(\boldsymbol{\phi}\)</span> and decoder parameters <span class="math inline">\(\boldsymbol{\theta}\)</span>. The outputs of the VAE are the unmasked image reconstructions <span class="math inline">\(\widetilde{\textbf{x}}^{(i)}\)</span> and mask reconstructions <span class="math inline">\(\widetilde{\textbf{m}}^{(i)}\)</span>. Lastly, the reconstructed image is assembled using the deterministic attention masks <span class="math inline">\(\textbf{m}^{(i)}\)</span> and the sampled unmasked image reconstructions <span class="math inline">\(\widetilde{\textbf{x}}^{(i)}\)</span>.</td>
</tr>
</tbody>
</table>
<p>The whole model is end-to-end trainable with the following loss function</p>
<p><span class="math display">\[
\begin{align}
\mathcal{L}\left(\boldsymbol{\phi}; \boldsymbol{\theta};
\boldsymbol{\psi}; \textbf{x} \right) &amp;= \underbrace{- \log \left( \sum_{k=1}^K \textbf{m}_k \odot p_{\boldsymbol{\theta}} \left(\textbf{x} |
\textbf{z}_k \right)\right)}_{\text{Reconstruction Error between }
\widetilde{\textbf{x}} \text{ and } \textbf{x}} + \beta
\underbrace{D_{KL} \left( \prod_{k=1}^K q_{\boldsymbol{\phi}} \left(\textbf{z}_k |
\textbf{x}, \textbf{m}_k\right) || p(\textbf{z})
\right)}_{\text{Regularization Term for Distribution of }\textbf{z}_k}\\
&amp;+ \gamma \underbrace{D_{KL} \left( q_{\boldsymbol{\psi}} \left( \textbf{c} |
\textbf{x} \right) || p_{\boldsymbol{\theta}} \left( \textbf{c} | \{
\textbf{z}_k \} \right) \right)}_{\text{Reconstruction Error between }
\widetilde{\textbf{m}}_k \text{ and } \textbf{m}_k},
\end{align}
\]</span></p>
<p>where the first term measures the reconstruction error of the fully reconstructed image (sum) as mentioned above. The second term is the KL divergence between the variational posterior approximation factorized across slots, i.e., <span class="math inline">\(q_{\boldsymbol{\phi}}
\left( \textbf{z} | \textbf{x} \right) = \prod_{k=1}^K
q_{\boldsymbol{\phi}} \left(\textbf{z}_k| \textbf{x},
\textbf{m}_k\right)\)</span>, and the prior of the latent distribution <span class="math inline">\(p(\textbf{z})\)</span>. As this term pushes the encoder distribution to be close to the prior distribution, it is commonly referred to as <em>regularization term</em>. It is weighted by the tuneable hyperparameter <span class="math inline">\(\beta\)</span> to encourage learning of disentanglement latent representions, see <a href="https://deepmind.com/research/publications/beta-VAE-Learning-Basic-Visual-Concepts-with-a-Constrained-Variational-Framework">Higgins et al. (2017)</a>. Note that the first two terms are derived from the standard VAE loss. The third term is the KL divergence between the attention mask distribution generated by the attention network <span class="math inline">\(q_{\boldsymbol{\psi}} \left( \textbf{c} | \textbf{x} \right)\)</span> and the component VAE <span class="math inline">\(p_{\boldsymbol{\theta}}
\left(\textbf{c} |\{\textbf{z}_k\} \right)\)</span>, i.e., it forces these distributions to lie close to each other. It could be understood as the reconstructions error of the VAE’s attention masks <span class="math inline">\(\widetilde{\textbf{m}}_k\)</span>, as it forces them to lie close to the attention masks <span class="math inline">\(\textbf{m}_k\)</span> of the attention network. Note however that the attention network itself is trainable, thus the network could also react by pushing the attention mask distribution towards the reconstructed mask distribution of the VAE. <span class="math inline">\(\gamma\)</span> is a tuneable hypeterparameter to modulate the importance of this term, i.e., increasing <span class="math inline">\(\gamma\)</span> results in close distributions.</p>
<p><strong>Motivation</strong>: The model aims to produce semantically meaningful decompositions in terms of segmentation and latent space attributes. Previous work such as the <a href="https://borea17.github.io/paper_summaries/spatial_broadcast_decoder">Spatial Broadcast decoder</a> has shown that VAEs are extensively capable of decomposing <em>simple</em> single-object scenes into disentangled latent space representations. However, even <em>simple</em> multi-object scenes are far more challenging to encode due to their complexity. <a href="https://arxiv.org/abs/1901.11390">Burgess et al. (2019)</a> hypothesize that exploiting the compositional structure of scenes (inductive bias) may help to reduce this complexity. Instead of decomposing the entire multi-object scene in one sweep, MONet breaks the image in multiple (<span class="math inline">\(K\)</span>) tasks which it decomposes with the same VAE<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>. Restricting the model complexity of the decoder (e.g., by using few layers), forces the model to produce segmentation with similar tasks, i.e., segmentations over structurally similar scene elements such that the VAE is capable of solving all tasks (note that this is a hypothesis). The authors argue that optimization should push towards a meaningful decomposition. Furthermore, they empirically validate their hypothesis by showing that for the <em>Objects Room</em> dataset the reconstruction error is much lower when the ground truth attention masks are given compared to an <em>all-in-one</em> (single sweep) or a <em>wrong</em> masks situation.</p>
<p>Adding some more motivation: It might be helpful to think about the data-generating process: Commonly, <em>artificial</em> multi-object scenes are created by adding each object successively to the image. Assuming that each of these objects is generated from the same class with different instantiations (i.e., different color/shape/size/…), it seems most natural to recover this process by decomposing the image and then decoding each part.</p>
</section>
<section id="implementation" class="level2">
<h2 class="anchored" data-anchor-id="implementation">Implementation</h2>
<p><a href="https://arxiv.org/abs/1901.11390">Burgess et al.&nbsp;(2019)</a> tested MONet on three different multi-object scene datasets (<em>Objects Room</em>, <em>CLEVR</em>, <em>Multi-dSprites</em>) and showed that their model could successively learn to <!-- decompose scenes into semantically meaningful parts (i.e., produce --> <!-- meaningful segmentation masks), to represent each segmented object in a --> <!-- common (nearly disentangled) latent code, and to generalize to unseen --> <!-- scene configurations without any supervision.  --></p>
<ul>
<li>decompose scenes into semantically meaningful parts, i.e., produce meaningful segmentation masks,</li>
<li>represent each segmented object in a common (nearly disentangled) latent code, and</li>
<li>generalize to unseen scene configurations</li>
</ul>
<p>without any supervision. Notably, MONet can handle a variable number of objects by producing latent codes that map to an empty scene, see image below. Furthermore, it turned out that MONet is also able to deal with occlusions: In the CLEVR dataset the unmasked reconstructions could even recover occluded objects, see image below. <a href="https://arxiv.org/abs/1901.11390">Burgess et al.&nbsp;(2019)</a> argue that this indicates how <code>MONet is learning from and constrained by the structure of the data</code>.</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;"><img src="./img/paper_results.png" title="MONet Paper Results" class="img-fluid" alt="MONet Paper Results"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>MONet Paper Results</strong>: Decomposition on <em>Multi-dSprties</em> and <em>CLEVR</em> images. First row shows the input image, second and third row the corresponding reconstructions and segmentations by MONet (trained for 1,000,000 iterations). The last three rows show the unmasked component reconstructions from some chosen slots (indicated by <span class="math inline">\(S\)</span>). Red arrows highlight occluded regions of shapes that are completed as full shapes. Taken from <a href="https://arxiv.org/abs/1901.11390">Burgess et al.&nbsp;(2019)</a>.</td>
</tr>
</tbody>
</table>
<p>The following reimplementation aims to reproduce some of these results while providing an in-depth understanding of the model architecture. Therefore, a dataset that is similar to the <em>Multi-dSprites</em> dataset is created, then the whole model (as faithfully as possible close to the original architecture) is reimplemented and trained in Pytorch and lastly, some useful visualizations of the trained model are created.</p>
<section id="data-generation" class="level3">
<h3 class="anchored" data-anchor-id="data-generation">Data Generation</h3>
<p>A dataset that is similar in spirit to the <em>Multi-dSprites</em> will be generated. <a href="https://arxiv.org/abs/1901.11390">Burgess et al.&nbsp;(2019)</a> generated this dataset by sampling <span class="math inline">\(1-4\)</span> images randomly from the binary <a href="https://github.com/deepmind/dsprites-dataset">dsprites dataset</a>(consisting of <span class="math inline">\(737,280\)</span> images), colorizing these by sampling from a uniform random RGB color and compositing those (with occlusion) onto a uniform random RGB background.</p>
<p>To reduce training time, we are going to generate a much simpler dataset of <span class="math inline">\(x\)</span> images with two non-overlaping objects (<code>square</code> or <code>circle</code>) and a fixed color space (<code>red</code>, <code>green</code> or <code>aqua</code>) for these objects, see image below. The dataset is generated by sampling uniformly random from possible latent factors, i.e., random non-overlaping positions for the two objects, random object constellations and random colors from color space, see code below image.</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;"><img src="./img/self_dataset.png" title="Examples of Dataset" class="img-fluid" alt="Examples of Dataset"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Visualization of self-written dataset.</td>
</tr>
</tbody>
</table>
<div class="cell" data-execution_count="2">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image, ImageDraw</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms <span class="im">as</span> transforms</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> TensorDataset</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_dataset(n_samples, SEED<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### CONFIGURATION ###############</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    canvas_size<span class="op">=</span><span class="dv">64</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    min_obj_size, max_obj_size <span class="op">=</span> <span class="dv">12</span>, <span class="dv">20</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    min_num_obj, max_num_obj <span class="op">=</span> <span class="dv">0</span>, <span class="dv">2</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    shapes <span class="op">=</span> [<span class="st">"circle"</span>, <span class="st">"square"</span>]</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    colors <span class="op">=</span> [<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"aqua"</span>]</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="co">#############################################</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> torch.empty([n_samples, <span class="dv">3</span>, canvas_size, canvas_size])</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> torch.empty([n_samples, <span class="dv">1</span>, canvas_size, canvas_size])</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    pos_positions <span class="op">=</span> np.arange(canvas_size <span class="op">-</span> max_obj_size <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    np.random.seed(SEED)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    rnd_positions <span class="op">=</span> np.random.choice(pos_positions, size<span class="op">=</span>(n_samples, <span class="dv">2</span>), replace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    rnd_num_objs <span class="op">=</span> np.random.randint(min_num_obj, max_num_obj <span class="op">+</span> <span class="dv">1</span>, size<span class="op">=</span>(n_samples))</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    rnd_obj_sizes <span class="op">=</span> np.random.randint(min_obj_size, max_obj_size <span class="op">+</span> <span class="dv">1</span>, </span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>                                      size<span class="op">=</span>(n_samples, max_num_obj))</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    rnd_shapes <span class="op">=</span> np.random.choice(shapes, size<span class="op">=</span>(n_samples, max_num_obj), replace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    rnd_colors <span class="op">=</span> np.random.choice(colors, size<span class="op">=</span>(n_samples, max_num_obj), replace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i_data <span class="kw">in</span> <span class="bu">range</span>(n_samples):</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        x_0, y_0 <span class="op">=</span> rnd_positions[i_data]</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>        num_objs <span class="op">=</span> rnd_num_objs[i_data]</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> num_objs <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>            <span class="co"># make sure that there is no overlap</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>            max_obj_size <span class="op">=</span> <span class="bu">max</span>(rnd_obj_sizes[i_data])</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>            impos_x_pos <span class="op">=</span> np.arange(x_0 <span class="op">-</span> max_obj_size, x_0 <span class="op">+</span> max_obj_size <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>            impos_y_pos <span class="op">=</span> np.arange(y_0 <span class="op">-</span> max_obj_size, y_0 <span class="op">+</span> max_obj_size <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>            x_1 <span class="op">=</span> np.random.choice(np.setdiff1d(pos_positions, impos_x_pos), size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>            y_1 <span class="op">=</span> np.random.choice(np.setdiff1d(pos_positions, impos_y_pos), size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>            x_1 <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>            y_1 <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># current latent factors</span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>        num_objs <span class="op">=</span> rnd_num_objs[i_data]</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>        x_positions, y_positions <span class="op">=</span> [x_0, x_1], [y_0, y_1]</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>        obj_sizes <span class="op">=</span> rnd_obj_sizes[i_data]</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>        shapes <span class="op">=</span> rnd_shapes[i_data]</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>        colors <span class="op">=</span> rnd_colors[i_data]</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># create img and label tensors</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>        img, label <span class="op">=</span> generate_img_and_label(</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>            x_pos<span class="op">=</span>x_positions[:num_objs],</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>            y_pos<span class="op">=</span>y_positions[:num_objs],</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>            shapes<span class="op">=</span>shapes[:num_objs],</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>            colors<span class="op">=</span>colors[:num_objs],</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>            sizes<span class="op">=</span>obj_sizes[:num_objs],</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>            img_size<span class="op">=</span>canvas_size</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>        data[i_data] <span class="op">=</span> img</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>        labels[i_data] <span class="op">=</span> label</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>    dataset <span class="op">=</span> TensorDataset(data, labels)</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dataset</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_img_and_label(x_pos, y_pos, shapes, colors, sizes, img_size):</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""generates a img and corresponding segmentation label mask</span></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a><span class="co">    from the provided latent factors</span></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a><span class="co">        x_pos (list): x positions of objects</span></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a><span class="co">        y_post (list): y positions of objects</span></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a><span class="co">        shapes (list): shape can only be `circle` or `square`</span></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a><span class="co">        colors (list): colors of object</span></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a><span class="co">        sizes (list): object sizes</span></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a><span class="co">        img (torch tensor): generated image represented as tensor</span></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a><span class="co">        label (torch tensor): corresponding semantic segmentation mask</span></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>    out_img <span class="op">=</span> Image.new(<span class="st">"RGB"</span>, (img_size, img_size), color<span class="op">=</span><span class="st">"black"</span>)</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> []</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>    <span class="co"># add objects</span></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x, y, shape, color, size <span class="kw">in</span> <span class="bu">zip</span>(x_pos, y_pos, shapes, colors, sizes):</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>        img <span class="op">=</span> Image.new(<span class="st">"RGB"</span>, (img_size, img_size), color<span class="op">=</span><span class="st">"black"</span>)</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>        <span class="co"># define end coordinates</span></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>        x_1, y_1 <span class="op">=</span> x <span class="op">+</span> size, y <span class="op">+</span> size</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>        <span class="co"># draw new image onto black image</span></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>        img1 <span class="op">=</span> ImageDraw.Draw(img)</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>        img2 <span class="op">=</span> ImageDraw.Draw(out_img)</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> shape <span class="op">==</span> <span class="st">"square"</span>:</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>            img1.rectangle([(x, y), (x_1, y_1)], fill<span class="op">=</span>color)</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>            img2.rectangle([(x, y), (x_1, y_1)], fill<span class="op">=</span>color)</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> shape <span class="op">==</span> <span class="st">"circle"</span>:</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>            img1.ellipse([(x, y), (x_1, y_1)], fill<span class="op">=</span>color)</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>            img2.ellipse([(x, y), (x_1, y_1)], fill<span class="op">=</span>color)</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>        labels.append((transforms.ToTensor()(img).<span class="bu">sum</span>(<span class="dv">0</span>) <span class="op">&gt;</span> <span class="dv">0</span>).unsqueeze(<span class="dv">0</span>))</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>    out_image <span class="op">=</span> transforms.ToTensor()(out_img).<span class="bu">type</span>(torch.float32)</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>    out_label <span class="op">=</span> torch.zeros(<span class="dv">1</span>, img_size, img_size)</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i_object <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(labels)):</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>        out_label[labels[i_object]] <span class="op">=</span> i_object <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> out_image, out_label</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="model-implementation" class="level3">
<h3 class="anchored" data-anchor-id="model-implementation">Model Implementation</h3>
<p>MONet is a rather sophisticated model composing two powerful neural network architectures in a reasonable way. One major downside of such complex models is that they comprise lots of hyperparamters from which much remains unknown such as sensitivity to small pertubations (e.g., changing layers within network architectures or parameters <span class="math inline">\(\beta\)</span>, <span class="math inline">\(\gamma\)</span>). Therefore, the model implementation aims to be as close as possible to the original model. Note that <a href="https://arxiv.org/abs/1901.11390">Burgess et al.&nbsp;(2019)</a> did not publish their implementation.</p>
<p>For the sake of simplicity, this section is divided into four parts:</p>
<ul>
<li><p><strong>Attention Network</strong>: The architecture of the recurrent neural network <span class="math inline">\(\boldsymbol{\alpha}_{\psi}\)</span> is described in appendix B.2 of <a href="https://arxiv.org/abs/1901.11390">Burgess et al.&nbsp;(2019)</a>. Basically, it consists of a slightly modified <a href="https://borea17.github.io/paper_summaries/u_net">U-Net</a> architecture that (at the <span class="math inline">\(k\)</span>th step) takes as input the concatenation of the image <span class="math inline">\(\textbf{x}\)</span> and the current scope mask in log units <span class="math inline">\(\log \textbf{s}_k\)</span>. The output of the modified U-Net is a one channel image <span class="math inline">\(\textbf{o} \in ]-\infty, + \infty[^{W\times
H}\)</span> in which each entry can be interpreted as the logits probability <span class="math inline">\(\text{logits }\boldsymbol{\alpha}_k\)</span>. A sigmoid layer can be used to transform these logits into probabilities, i.e.,</p>
<p><span class="math display">\[
\begin{align}
  \boldsymbol{\alpha}_k &amp;= \text{Sigmoid} \left(\text{logits }
  \boldsymbol{\alpha}_k \right) = \frac {1} {1 + \exp\left(- \text{logits }
  \boldsymbol{\alpha}_k \right)}\\
  1 - \boldsymbol{\alpha}_k &amp;= 1 - \text{Sigmoid} \left(\text{logits }
  \boldsymbol{\alpha}_k \right) = \frac {\exp\left(- \text{logits }
  \boldsymbol{\alpha}_k \right) } { 1 + \exp\left(- \text{logits }
  \boldsymbol{\alpha}_k \right)}
\end{align}
\]</span></p>
<p>Additionally, <a href="https://arxiv.org/abs/1901.11390">Burgess et al. (2019)</a> transform these probabilties into logaritmic units, i.e.,</p>
<p><span class="math display">\[
\begin{align}
   \log \boldsymbol{\alpha}_k &amp;= - \log \left( 1 + \exp\left(- \text{logits }
  \boldsymbol{\alpha}_k \right)\right)=\text{LogSigmoid }\left(
\text{logits } \boldsymbol{\alpha}_k \right)\\
   \log \left(1 - \boldsymbol{\alpha}_k\right) &amp;= - \text{logits }
\boldsymbol{\alpha}_k + \log \boldsymbol{\alpha}_k,
\end{align}
\]</span></p>
<p>i.e., a LogSigmoid layer can be used (instead of a sigmoid layer with applying logarithm to both outputs) to speed up the computations. From the model description above, it follows</p>
<p><span class="math display">\[
\begin{align}
  \textbf{s}_{k+1} &amp;= \textbf{s}_k \odot \left( 1 -
  \boldsymbol{\alpha}_k \right) \quad &amp;&amp;\Leftrightarrow  \quad \log
  \textbf{s}_{k+1} = \log \textbf{s}_k + \log \left(1 - \boldsymbol{\alpha}_k \right)\\
  \textbf{m}_{k+1} &amp;= \textbf{s}_{k} \odot \boldsymbol{\alpha}_k \quad
&amp;&amp;\Leftrightarrow \quad \log \textbf{m}_{k+1} = \log \textbf{s}_{k} + \log \boldsymbol{\alpha}_k,
\end{align}
\]</span></p>
<p>i.e., the output of the <span class="math inline">\(k\)</span>th step can be computed by simply adding the log current scope <span class="math inline">\(\log \textbf{s}_k\)</span> to each log probability. As a result, the next log attention mask <span class="math inline">\(\log \textbf{m}_{k+1}\)</span> and next log scope <span class="math inline">\(\log \textbf{s}_{k+1}\)</span> can be retrieved. Note that using log units instead of standard units is beneficial as it ensures numerical stability while simplifying the optimization due to an increased learning signal. <!-- or simpliyfing the loss function computation --></p>
<p>The code below summarizes the network architecture, <a href="https://arxiv.org/abs/1901.11390">Burgess et al. (2019)</a> did not state the channel dimensionality within the U-Net blocks explicitely. However, as they mentioned to use a <code>U-Net blueprint</code>, it is assumed that they use the same dimensionality as in the original <a href="https://borea17.github.io/paper_summaries/u_net">U-Net paper</a>. To reduce training time and memory capacity, the following implementation caps the channel dimensionality in the encoder to 64 output channels.</p></li>
</ul>
<div class="cell" data-execution_count="3">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> UNet(nn.Module):</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""U-Net architecture with blocks proposed by Burgess et al. (2019)</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Attributes:</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">        encoder_blocks (list): u_net blocks of encoder path</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co">        decoder_blocks (list): u_net blocks of decoder path</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co">        bottleneck_MLP (list): bottleneck is a 3-layer MLP with ReLUs</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co">        out_conv (nn.Conv2d): convolutional classification layer</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder_blocks <span class="op">=</span> nn.ModuleList(</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>                UNet._block(<span class="dv">4</span>, <span class="dv">16</span>),              <span class="co"># [batch_size, 16, 64, 64]</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>                UNet._block(<span class="dv">16</span>, <span class="dv">32</span>),             <span class="co"># [batch_size, 32, 32, 32]</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>                UNet._block(<span class="dv">32</span>, <span class="dv">64</span>),             <span class="co"># [batch_size, 64, 16, 16]</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>                UNet._block(<span class="dv">64</span>, <span class="dv">64</span>),             <span class="co"># [batch_size, 64, 8, 8]</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>                UNet._block(<span class="dv">64</span>, <span class="dv">64</span>),             <span class="co"># [batch_size, 75, 4, 4]</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bottleneck_MLP <span class="op">=</span> nn.Sequential(</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>            nn.Flatten(),                        <span class="co"># [batch_size, 512*4*4]</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">64</span><span class="op">*</span><span class="dv">4</span><span class="op">*</span><span class="dv">4</span>, <span class="dv">128</span>),</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">128</span>, <span class="dv">128</span>),                 <span class="co"># [batch_size, 512*4*4]</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">128</span>, <span class="dv">64</span><span class="op">*</span><span class="dv">4</span><span class="op">*</span><span class="dv">4</span>),              <span class="co"># [batch_size, 512*4*4]</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),             <span class="co"># reshaped into [batch_size, 512, 4, 4]</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder_blocks <span class="op">=</span> nn.ModuleList(</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>                UNet._block(<span class="dv">128</span>, <span class="dv">64</span>),             <span class="co"># [batch_size, 64, 4, 4]</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>                UNet._block(<span class="dv">128</span>, <span class="dv">64</span>),             <span class="co"># [batch_size, 64, 8, 8]</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>                UNet._block(<span class="dv">128</span>, <span class="dv">32</span>),             <span class="co"># [batch_size, 32, 16, 16]</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>                UNet._block(<span class="dv">64</span>, <span class="dv">16</span>),              <span class="co"># [batch_size, 32, 32, 32]</span></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>                UNet._block(<span class="dv">32</span>, <span class="dv">16</span>),              <span class="co"># [batch_size, 64, 64, 64]</span></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.out_conv <span class="op">=</span> nn.Conv2d(<span class="dv">16</span>, <span class="dv">1</span>, kernel_size<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">1</span>), stride<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># go through encoder path and store intermediate results</span></span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>        skip_tensors <span class="op">=</span> []</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> index, encoder_block <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.encoder_blocks):</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> encoder_block(x)</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>            skip_tensors.append(out)</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>            <span class="co"># no resizing in the last block</span></span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> index <span class="op">&lt;</span> <span class="bu">len</span>(<span class="va">self</span>.encoder_blocks) <span class="op">-</span> <span class="dv">1</span>:  <span class="co"># downsample</span></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>                x <span class="op">=</span> F.interpolate(</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>                    out, scale_factor<span class="op">=</span><span class="fl">0.5</span>, mode<span class="op">=</span><span class="st">"nearest"</span>, </span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>                    recompute_scale_factor<span class="op">=</span><span class="va">False</span></span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>        last_skip <span class="op">=</span> out</span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># feed last skip tensor through bottleneck</span></span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>        out_MLP <span class="op">=</span> <span class="va">self</span>.bottleneck_MLP(last_skip)</span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># reshape output to match last skip tensor</span></span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> out_MLP.view(last_skip.shape)</span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>        <span class="co"># go through decoder path and use skip tensors</span></span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> index, decoder_block <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.decoder_blocks):</span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>            inp <span class="op">=</span> torch.cat((skip_tensors[<span class="op">-</span><span class="dv">1</span> <span class="op">-</span> index], out), <span class="dv">1</span>)</span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> decoder_block(inp)</span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a>            <span class="co"># no resizing in the last block</span></span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> index <span class="op">&lt;</span> <span class="bu">len</span>(<span class="va">self</span>.decoder_blocks) <span class="op">-</span> <span class="dv">1</span>:  <span class="co"># upsample</span></span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>                out <span class="op">=</span> F.interpolate(out, scale_factor<span class="op">=</span><span class="dv">2</span>, mode<span class="op">=</span><span class="st">"nearest"</span>)</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>        prediction <span class="op">=</span> <span class="va">self</span>.out_conv(out)</span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> prediction</span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _block(in_channels, out_channels):</span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""U-Net block as described by Burgess et al. (2019)"""</span></span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>        u_net_block <span class="op">=</span> nn.Sequential(</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(</span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a>                in_channels,</span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a>                out_channels,</span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a>                kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>),</span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a>                stride<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a>                padding<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a>                bias<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a>            ),</span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a>            nn.InstanceNorm2d(num_features<span class="op">=</span>out_channels, affine<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> u_net_block</span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AttentionNetwork(nn.Module):</span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.unet <span class="op">=</span> UNet()</span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, num_slots):</span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a>        log_s_k <span class="op">=</span> torch.zeros_like(x[:, <span class="dv">0</span>:<span class="dv">1</span>, :, :])</span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a>        <span class="co"># initialize list to store intermediate results</span></span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a>        log_m <span class="op">=</span> []</span>
<span id="cb2-103"><a href="#cb2-103" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> slot <span class="kw">in</span> <span class="bu">range</span>(num_slots <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb2-104"><a href="#cb2-104" aria-hidden="true" tabindex="-1"></a>            inp <span class="op">=</span> torch.cat((x, log_s_k), <span class="dv">1</span>)</span>
<span id="cb2-105"><a href="#cb2-105" aria-hidden="true" tabindex="-1"></a>            alpha_logits <span class="op">=</span> <span class="va">self</span>.unet(inp)  <span class="co"># [batch_size, 1, image_dim, image_dim]</span></span>
<span id="cb2-106"><a href="#cb2-106" aria-hidden="true" tabindex="-1"></a>            <span class="co"># transform into log probabilties log (alpha_k) and log (1 - alpha_k)</span></span>
<span id="cb2-107"><a href="#cb2-107" aria-hidden="true" tabindex="-1"></a>            log_alpha_k <span class="op">=</span> F.logsigmoid(alpha_logits)</span>
<span id="cb2-108"><a href="#cb2-108" aria-hidden="true" tabindex="-1"></a>            log_1_m_alpha_k <span class="op">=</span> <span class="op">-</span>alpha_logits <span class="op">+</span> log_alpha_k</span>
<span id="cb2-109"><a href="#cb2-109" aria-hidden="true" tabindex="-1"></a>            <span class="co"># compute log_new_mask, log_new_scope</span></span>
<span id="cb2-110"><a href="#cb2-110" aria-hidden="true" tabindex="-1"></a>            log_new_mask <span class="op">=</span> log_s_k <span class="op">+</span> log_alpha_k</span>
<span id="cb2-111"><a href="#cb2-111" aria-hidden="true" tabindex="-1"></a>            log_new_scope <span class="op">=</span> log_s_k <span class="op">+</span> log_1_m_alpha_k</span>
<span id="cb2-112"><a href="#cb2-112" aria-hidden="true" tabindex="-1"></a>            <span class="co"># store intermediate results in list</span></span>
<span id="cb2-113"><a href="#cb2-113" aria-hidden="true" tabindex="-1"></a>            log_m.append(log_new_mask)</span>
<span id="cb2-114"><a href="#cb2-114" aria-hidden="true" tabindex="-1"></a>            <span class="co"># update log scope</span></span>
<span id="cb2-115"><a href="#cb2-115" aria-hidden="true" tabindex="-1"></a>            log_s_k <span class="op">=</span> log_new_scope</span>
<span id="cb2-116"><a href="#cb2-116" aria-hidden="true" tabindex="-1"></a>        log_m.append(log_s_k)</span>
<span id="cb2-117"><a href="#cb2-117" aria-hidden="true" tabindex="-1"></a>        <span class="co"># convert list to tensor [batch_size, num_slots, 1, image_dim, image_dim]</span></span>
<span id="cb2-118"><a href="#cb2-118" aria-hidden="true" tabindex="-1"></a>        log_m <span class="op">=</span> torch.cat(log_m, dim<span class="op">=</span><span class="dv">1</span>).unsqueeze(<span class="dv">2</span>)</span>
<span id="cb2-119"><a href="#cb2-119" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> log_m</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<!-- [^5]: [Burgess et al. (2019)](https://arxiv.org/abs/1901.11390) state -->
<!--     that they use a log softmax layer, however this would only be -->
<!--     possible if there were two channels at the output. Note that in -->
<!--     binary classification, a pixel-wise softmax layer (two channel) can be -->
<!--     transformed into a sigmoid layer (one channel) by using the -->
<!--     difference between the two channels as input:  -->
<!--     $$ -->
<!--     \begin{align} -->
<!--       \text{Softmax} (x_1) &= \frac {1} {1 + \exp(x_2 - x_1)} = \frac -->
<!--     {1} {1 + \exp(t)} = \text{Sigmoid} (t), \\ -->
<!--       \text{Softmax} (x_2) &= \frac {1}{1 + \exp(x_1 - x_2)} = \frac -->
<!--     {1}{1+\exp(-t)} = 1 - \text{Sigmoid}(t). -->
<!--     \end{align} -->
<!--     $$ -->
<ul>
<li><p><strong>Component VAE</strong>: The architectures for the encoder <span class="math inline">\(q_{\boldsymbol{\phi}}\)</span> and decoder <span class="math inline">\(p_{\boldsymbol{\theta}}\)</span> neural networks are described in appendix B.1 of <a href="https://arxiv.org/abs/1901.11390">Burgess et al. (2019)</a>. Basically, the encoder <span class="math inline">\(q_{\boldsymbol{\phi}}(\textbf{z}_k | \textbf{x}, \textbf{m}_k)\)</span> is a typical CNN that takes the concatentation of an image <span class="math inline">\(\textbf{x}\)</span> and a segmentation mask in logaritmic units <span class="math inline">\(\log \textbf{m}_k\)</span> as input to compute the mean <span class="math inline">\(\boldsymbol{\mu}_{E, k}\)</span> and logarithmed variance <span class="math inline">\(\boldsymbol{\sigma}^2_{E,k}\)</span> of the Gaussian latent space distribution <span class="math inline">\(\mathcal{N} \left(
\boldsymbol{\mu}_{E, k}, \text{diag}\left(\boldsymbol{\sigma}^2_{E,k} \right)
\right)\)</span>. Sampling from this distribution is avoided by using the reparametrization trick, i.e., the latent variable <span class="math inline">\(\textbf{z}_k\)</span> is expressed as a deterministic variable<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<p><span class="math display">\[
  \textbf{z}_k = \boldsymbol{\mu}_{E, k} +
  \boldsymbol{\sigma}^2_{E,k} \odot \boldsymbol{\epsilon} \quad
  \text{where} \quad \boldsymbol{\epsilon} \sim \mathcal{N}\left(
  \textbf{0}, \textbf{I}
  \right).
\]</span></p>
<p>The component VAE uses a <a href="https://borea17.github.io/paper_summaries/spatial_broadcast_decoder">Spatial Broadcast decoder</a> <span class="math inline">\(p_{\boldsymbol{\theta}}\)</span> to transform the latent vector <span class="math inline">\(\textbf{z}_k\)</span> into the reconstructed image component <span class="math inline">\(\widetilde{\textbf{x}}_k \sim p_{\boldsymbol{\theta}}
\left(\textbf{x} | \textbf{z}_k \right)\)</span> and mask <span class="math inline">\(\widetilde{\textbf{m}}_k \sim p_{\boldsymbol{\theta}} \left(
\textbf{c}|\textbf{z}_k \right)\)</span>. <a href="https://arxiv.org/abs/1901.11390">Burgess et al. (2019)</a> chose independent Gaussian distributions with fixed variances for each pixel as the reconstructed image component distributions <span class="math inline">\(p_{\boldsymbol{\theta}} \left( x_i | \textbf{z}_k \right) \sim
\mathcal{N} \left(\mu_{k,i} (\boldsymbol{\theta}), \sigma_k^2
\right)\)</span> and independent Bernoulli distributions for each pixel as the reconstructed mask distributions <span class="math inline">\(p\left(c_{k, i}| \textbf{z}_k
\right) \sim \text{Bern} \left( p_{k,i}
(\boldsymbol{\theta})\right)\)</span>. I.e., the decoder output is a 4 channel image from which the first three channels correspond to the 3 RGB channels for the means of the image components <span class="math inline">\(\boldsymbol{\mu}_k\)</span> and the last channel corresponds to the logits probabilities of the Bernoulli distribution <span class="math inline">\(\text{logits }\textbf{p}_k\)</span>.</p></li>
</ul>
<div class="cell" data-execution_count="4">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CNN_VAE(nn.Module):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""simple CNN-VAE class with a Gaussian encoder (mean and diagonal variance</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">    structure) and a Gaussian decoder with fixed variance </span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co">    (decoder is implemented as a Spatial Broadcast decoder) </span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Attributes</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co">        latent_dim (int): dimension of latent space</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">        encoder (nn.Sequential): encoder network for mean and log_var</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">        decoder (nn.Sequential): spatial broadcast decoder  for mean (fixed var)</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co">        x_grid (torch tensor): appended x coordinates for spatial broadcast decoder</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co">        y_grid (torch tensor): appended x coordinates for spatial broadcast decoder</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(CNN_VAE, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.latent_dim <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> nn.Sequential(</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>            <span class="co"># shape: [batch_size, 4, 64, 64]</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">4</span>, <span class="dv">32</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>,<span class="dv">3</span>), stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>            <span class="co"># shape: [batch_size, 32, 32, 32]</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">32</span>, <span class="dv">32</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>,<span class="dv">3</span>), stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>            <span class="co"># shape: [batch_size, 32, 16, 16]</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">32</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>,<span class="dv">3</span>), stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>            <span class="co"># shape: [batch_size, 64, 8, 8]</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>,<span class="dv">3</span>), stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>            <span class="co"># shape: [batch_size, 64, 4, 4],</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>            nn.Flatten(),</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>            <span class="co"># shape: [batch_size, 1024]</span></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.MLP <span class="op">=</span> nn.Sequential(</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">64</span><span class="op">*</span><span class="dv">4</span><span class="op">*</span><span class="dv">4</span>, <span class="dv">256</span>),</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">256</span>, <span class="dv">2</span><span class="op">*</span><span class="va">self</span>.latent_dim),</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># spatial broadcast decoder configuration</span></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>        img_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># "input width and height of CNN both 8 larger than target output"</span></span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, img_size <span class="op">+</span> <span class="dv">8</span>)</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, img_size <span class="op">+</span> <span class="dv">8</span>)</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>        x_grid, y_grid <span class="op">=</span> torch.meshgrid(x, y)</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># reshape into [1, 1, img_size, img_size] and save in state_dict</span></span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'x_grid'</span>, x_grid.view((<span class="dv">1</span>, <span class="dv">1</span>) <span class="op">+</span> x_grid.shape).clone())</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'y_grid'</span>, y_grid.view((<span class="dv">1</span>, <span class="dv">1</span>) <span class="op">+</span> y_grid.shape).clone())</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> nn.Sequential(</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>             <span class="co"># shape [batch_size, latent_dim + 2, 72, 72]</span></span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(in_channels<span class="op">=</span><span class="va">self</span>.latent_dim<span class="op">+</span><span class="dv">2</span>, out_channels<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>                      stride<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), kernel_size<span class="op">=</span>(<span class="dv">3</span>,<span class="dv">3</span>)),</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>            <span class="co"># shape [batch_size, 16, 70, 70]</span></span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(in_channels<span class="op">=</span><span class="dv">16</span>, out_channels<span class="op">=</span><span class="dv">16</span>, stride<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">1</span>),</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>                      kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>)),</span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>            <span class="co"># shape [batch_size, 16, 68, 68]</span></span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(in_channels<span class="op">=</span><span class="dv">16</span>, out_channels<span class="op">=</span><span class="dv">16</span>, stride<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">1</span>),</span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>                      kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>)),</span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>            <span class="co"># shape [batch_size, 16, 66, 66]</span></span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(in_channels<span class="op">=</span><span class="dv">16</span>, out_channels<span class="op">=</span><span class="dv">16</span>, stride<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">1</span>),</span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a>                      kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>)),</span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a>            <span class="co"># shape [batch_size, 4, 64, 64]</span></span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(in_channels<span class="op">=</span><span class="dv">16</span>, out_channels<span class="op">=</span><span class="dv">4</span>, stride<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">1</span>),</span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a>                      kernel_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>)),</span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a>        [z, mu_E, log_var_E] <span class="op">=</span> <span class="va">self</span>.encode(x)</span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a>        x_rec <span class="op">=</span> <span class="va">self</span>.decode(z)</span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x_rec, z, mu_E, log_var_E</span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-76"><a href="#cb3-76" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> encode(<span class="va">self</span>, x):</span>
<span id="cb3-77"><a href="#cb3-77" aria-hidden="true" tabindex="-1"></a>        out_encoder <span class="op">=</span> <span class="va">self</span>.MLP(<span class="va">self</span>.encoder(x))</span>
<span id="cb3-78"><a href="#cb3-78" aria-hidden="true" tabindex="-1"></a>        mu_E, log_var_E <span class="op">=</span> torch.chunk(out_encoder, <span class="dv">2</span>, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-79"><a href="#cb3-79" aria-hidden="true" tabindex="-1"></a>        <span class="co"># sample noise variable for each batch</span></span>
<span id="cb3-80"><a href="#cb3-80" aria-hidden="true" tabindex="-1"></a>        epsilon <span class="op">=</span> torch.randn_like(log_var_E)</span>
<span id="cb3-81"><a href="#cb3-81" aria-hidden="true" tabindex="-1"></a>        <span class="co"># get latent variable by reparametrization trick</span></span>
<span id="cb3-82"><a href="#cb3-82" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> mu_E <span class="op">+</span> torch.exp(<span class="fl">0.5</span> <span class="op">*</span> log_var_E) <span class="op">*</span> epsilon</span>
<span id="cb3-83"><a href="#cb3-83" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [z, mu_E, log_var_E]</span>
<span id="cb3-84"><a href="#cb3-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-85"><a href="#cb3-85" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> decode(<span class="va">self</span>, z):</span>
<span id="cb3-86"><a href="#cb3-86" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> z.shape[<span class="dv">0</span>]</span>
<span id="cb3-87"><a href="#cb3-87" aria-hidden="true" tabindex="-1"></a>        <span class="co"># reshape z into [batch_size, latent_dim, 1, 1]</span></span>
<span id="cb3-88"><a href="#cb3-88" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> z.view(z.shape <span class="op">+</span> (<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb3-89"><a href="#cb3-89" aria-hidden="true" tabindex="-1"></a>        <span class="co"># tile across image [batch_size, latent_im, 64+8, 64+8]</span></span>
<span id="cb3-90"><a href="#cb3-90" aria-hidden="true" tabindex="-1"></a>        z_b <span class="op">=</span> z.repeat(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">64</span> <span class="op">+</span> <span class="dv">8</span>, <span class="dv">64</span> <span class="op">+</span> <span class="dv">8</span>)</span>
<span id="cb3-91"><a href="#cb3-91" aria-hidden="true" tabindex="-1"></a>        <span class="co"># upsample x_grid and y_grid to [batch_size, 1, 64+8, 64+8]</span></span>
<span id="cb3-92"><a href="#cb3-92" aria-hidden="true" tabindex="-1"></a>        x_b <span class="op">=</span> <span class="va">self</span>.x_grid.repeat(batch_size, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb3-93"><a href="#cb3-93" aria-hidden="true" tabindex="-1"></a>        y_b <span class="op">=</span> <span class="va">self</span>.y_grid.repeat(batch_size, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb3-94"><a href="#cb3-94" aria-hidden="true" tabindex="-1"></a>        <span class="co"># concatenate vectors [batch_size, latent_dim+2, 64+8, 64+8]</span></span>
<span id="cb3-95"><a href="#cb3-95" aria-hidden="true" tabindex="-1"></a>        z_sb <span class="op">=</span> torch.cat((z_b, x_b, y_b), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-96"><a href="#cb3-96" aria-hidden="true" tabindex="-1"></a>        <span class="co"># apply convolutional layers mu_D</span></span>
<span id="cb3-97"><a href="#cb3-97" aria-hidden="true" tabindex="-1"></a>        mu_D <span class="op">=</span> <span class="va">self</span>.decoder(z_sb)</span>
<span id="cb3-98"><a href="#cb3-98" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> mu_D</span>
<span id="cb3-99"><a href="#cb3-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-100"><a href="#cb3-100" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-101"><a href="#cb3-101" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ComponentVAE(CNN_VAE):</span>
<span id="cb3-102"><a href="#cb3-102" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Component VAE class for use in MONet as proposed by Burgess et al. (2019)</span></span>
<span id="cb3-103"><a href="#cb3-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-104"><a href="#cb3-104" aria-hidden="true" tabindex="-1"></a><span class="co">    Attributes:</span></span>
<span id="cb3-105"><a href="#cb3-105" aria-hidden="true" tabindex="-1"></a><span class="co">        #################### CNN_VAE ########################</span></span>
<span id="cb3-106"><a href="#cb3-106" aria-hidden="true" tabindex="-1"></a><span class="co">        encoder (nn.Sequential): encoder network for mean and log_var</span></span>
<span id="cb3-107"><a href="#cb3-107" aria-hidden="true" tabindex="-1"></a><span class="co">        decoder (nn.Sequential): decoder network for mean (fixed var)</span></span>
<span id="cb3-108"><a href="#cb3-108" aria-hidden="true" tabindex="-1"></a><span class="co">        img_dim (int): image dimension along one axis</span></span>
<span id="cb3-109"><a href="#cb3-109" aria-hidden="true" tabindex="-1"></a><span class="co">        expand_dim (int): expansion of latent image to accomodate for lack of padding</span></span>
<span id="cb3-110"><a href="#cb3-110" aria-hidden="true" tabindex="-1"></a><span class="co">        x_grid (torch tensor): appended x coordinates for spatial broadcast decoder</span></span>
<span id="cb3-111"><a href="#cb3-111" aria-hidden="true" tabindex="-1"></a><span class="co">        y_grid (torch tensor): appended x coordinates for spatial broadcast decoder</span></span>
<span id="cb3-112"><a href="#cb3-112" aria-hidden="true" tabindex="-1"></a><span class="co">        #####################################################</span></span>
<span id="cb3-113"><a href="#cb3-113" aria-hidden="true" tabindex="-1"></a><span class="co">        img_channels (int): number of channels in image</span></span>
<span id="cb3-114"><a href="#cb3-114" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-115"><a href="#cb3-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-116"><a href="#cb3-116" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,):</span>
<span id="cb3-117"><a href="#cb3-117" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-118"><a href="#cb3-118" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.img_channels <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb3-119"><a href="#cb3-119" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb3-120"><a href="#cb3-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-121"><a href="#cb3-121" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image, log_mask, deterministic<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb3-122"><a href="#cb3-122" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb3-123"><a href="#cb3-123" aria-hidden="true" tabindex="-1"></a><span class="co">        parellize computation of reconstructions</span></span>
<span id="cb3-124"><a href="#cb3-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-125"><a href="#cb3-125" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb3-126"><a href="#cb3-126" aria-hidden="true" tabindex="-1"></a><span class="co">            image (torch.tensor): input image [batch, img_channels, img_dim, img_dim]</span></span>
<span id="cb3-127"><a href="#cb3-127" aria-hidden="true" tabindex="-1"></a><span class="co">            log_mask (torch.tensor): all seg masks [batch, slots, 1, img_dim, img_dim]</span></span>
<span id="cb3-128"><a href="#cb3-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-129"><a href="#cb3-129" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb3-130"><a href="#cb3-130" aria-hidden="true" tabindex="-1"></a><span class="co">            mu_z_k (torch.tensor): latent mean [batch, slot, latent_dim]</span></span>
<span id="cb3-131"><a href="#cb3-131" aria-hidden="true" tabindex="-1"></a><span class="co">            log_var_z_k (torch.tensor): latent log_var [batch, slot, latent_dim]</span></span>
<span id="cb3-132"><a href="#cb3-132" aria-hidden="true" tabindex="-1"></a><span class="co">            z_k (torch.tensor): latent log_var [batch, slot, latent_dim]</span></span>
<span id="cb3-133"><a href="#cb3-133" aria-hidden="true" tabindex="-1"></a><span class="co">            x_r_k (torch.tensor): img reconstruction </span></span>
<span id="cb3-134"><a href="#cb3-134" aria-hidden="true" tabindex="-1"></a><span class="co">                [batch, slot, img_chan, img_dim, img_dim]</span></span>
<span id="cb3-135"><a href="#cb3-135" aria-hidden="true" tabindex="-1"></a><span class="co">            logits_m_r_k (torch.tensor): mask recons. [batch, slot, 1, img_dim, img_dim]</span></span>
<span id="cb3-136"><a href="#cb3-136" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb3-137"><a href="#cb3-137" aria-hidden="true" tabindex="-1"></a>        num_slots <span class="op">=</span> log_mask.shape[<span class="dv">1</span>]</span>
<span id="cb3-138"><a href="#cb3-138" aria-hidden="true" tabindex="-1"></a>        <span class="co"># create input [batch_size*num_slots, image_channels+1, img_dim, img_dim]</span></span>
<span id="cb3-139"><a href="#cb3-139" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> ComponentVAE._prepare_input(image, log_mask, num_slots)</span>
<span id="cb3-140"><a href="#cb3-140" aria-hidden="true" tabindex="-1"></a>        <span class="co"># get encoder distribution parameters [batch*slots, latent_dim]</span></span>
<span id="cb3-141"><a href="#cb3-141" aria-hidden="true" tabindex="-1"></a>        [z_k, mu_z_k, log_var_z_k] <span class="op">=</span> <span class="va">self</span>.encode(x)</span>
<span id="cb3-142"><a href="#cb3-142" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> deterministic:</span>
<span id="cb3-143"><a href="#cb3-143" aria-hidden="true" tabindex="-1"></a>            z_k <span class="op">=</span> mu_z_k</span>
<span id="cb3-144"><a href="#cb3-144" aria-hidden="true" tabindex="-1"></a>        <span class="co"># get decoder dist. parameters [batch*slots, image_channels, img_dim, img_dim]</span></span>
<span id="cb3-145"><a href="#cb3-145" aria-hidden="true" tabindex="-1"></a>        [x_r_k, logits_m_r_k] <span class="op">=</span> <span class="va">self</span>.decode(z_k)</span>
<span id="cb3-146"><a href="#cb3-146" aria-hidden="true" tabindex="-1"></a>        <span class="co"># convert outputs into easier understandable shapes</span></span>
<span id="cb3-147"><a href="#cb3-147" aria-hidden="true" tabindex="-1"></a>        [mu_z_k, log_var_z_k, z_k, x_r_k, logits_m_r_k] <span class="op">=</span> ComponentVAE._prepare_output(</span>
<span id="cb3-148"><a href="#cb3-148" aria-hidden="true" tabindex="-1"></a>            mu_z_k, log_var_z_k, z_k, x_r_k, logits_m_r_k, num_slots</span>
<span id="cb3-149"><a href="#cb3-149" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-150"><a href="#cb3-150" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [mu_z_k, log_var_z_k, z_k, x_r_k, logits_m_r_k]</span>
<span id="cb3-151"><a href="#cb3-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-152"><a href="#cb3-152" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> decode(<span class="va">self</span>, z):</span>
<span id="cb3-153"><a href="#cb3-153" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb3-154"><a href="#cb3-154" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb3-155"><a href="#cb3-155" aria-hidden="true" tabindex="-1"></a><span class="co">            z (torch.tensor): [batch_size*num_slots, latent_dim]</span></span>
<span id="cb3-156"><a href="#cb3-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-157"><a href="#cb3-157" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb3-158"><a href="#cb3-158" aria-hidden="true" tabindex="-1"></a><span class="co">            mu_x (torch.tensor): [batch*slots, img_channels, img_dim, img_dim]</span></span>
<span id="cb3-159"><a href="#cb3-159" aria-hidden="true" tabindex="-1"></a><span class="co">            logits_m (torch.tensor): [batch*slots, 1, img_dim, img_dim]</span></span>
<span id="cb3-160"><a href="#cb3-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-161"><a href="#cb3-161" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb3-162"><a href="#cb3-162" aria-hidden="true" tabindex="-1"></a>        mu_D <span class="op">=</span> <span class="bu">super</span>().decode(z)</span>
<span id="cb3-163"><a href="#cb3-163" aria-hidden="true" tabindex="-1"></a>        <span class="co"># split into means of x and logits of m</span></span>
<span id="cb3-164"><a href="#cb3-164" aria-hidden="true" tabindex="-1"></a>        mu_x, logits_m <span class="op">=</span> torch.split(mu_D, <span class="va">self</span>.img_channels, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-165"><a href="#cb3-165" aria-hidden="true" tabindex="-1"></a>        <span class="co"># enforce positivity of mu_x</span></span>
<span id="cb3-166"><a href="#cb3-166" aria-hidden="true" tabindex="-1"></a>        mu_x <span class="op">=</span> mu_x.<span class="bu">abs</span>()</span>
<span id="cb3-167"><a href="#cb3-167" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [mu_x, logits_m]</span>
<span id="cb3-168"><a href="#cb3-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-169"><a href="#cb3-169" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb3-170"><a href="#cb3-170" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _prepare_input(image, log_mask, num_slots):</span>
<span id="cb3-171"><a href="#cb3-171" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb3-172"><a href="#cb3-172" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb3-173"><a href="#cb3-173" aria-hidden="true" tabindex="-1"></a><span class="co">            image (torch.tensor): input image [batch, img_channels, img_dim, img_dim]</span></span>
<span id="cb3-174"><a href="#cb3-174" aria-hidden="true" tabindex="-1"></a><span class="co">            log_mask (torch.tensor): all seg masks [batch, slots, 1, img_dim, img_dim]</span></span>
<span id="cb3-175"><a href="#cb3-175" aria-hidden="true" tabindex="-1"></a><span class="co">            num_slots (int): number of slots (log_mask.shape[1])</span></span>
<span id="cb3-176"><a href="#cb3-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-177"><a href="#cb3-177" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb3-178"><a href="#cb3-178" aria-hidden="true" tabindex="-1"></a><span class="co">            x (torch.tensor): input image [batch*slots, img_channels+1, img_dim, img_dim]</span></span>
<span id="cb3-179"><a href="#cb3-179" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb3-180"><a href="#cb3-180" aria-hidden="true" tabindex="-1"></a>        <span class="co"># prepare image [batch_size*num_slots, image_channels, img_dim, img_dim]</span></span>
<span id="cb3-181"><a href="#cb3-181" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> image.repeat(num_slots, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb3-182"><a href="#cb3-182" aria-hidden="true" tabindex="-1"></a>        <span class="co"># prepare log_mask [batch_size*num_slots, 1, img_dim, img_dim]</span></span>
<span id="cb3-183"><a href="#cb3-183" aria-hidden="true" tabindex="-1"></a>        log_mask <span class="op">=</span> torch.cat(log_mask.squeeze(<span class="dv">2</span>).chunk(num_slots, <span class="dv">1</span>), <span class="dv">0</span>)</span>
<span id="cb3-184"><a href="#cb3-184" aria-hidden="true" tabindex="-1"></a>        <span class="co"># concatenate along color channel</span></span>
<span id="cb3-185"><a href="#cb3-185" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.cat((image, log_mask), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-186"><a href="#cb3-186" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb3-187"><a href="#cb3-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-188"><a href="#cb3-188" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb3-189"><a href="#cb3-189" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _prepare_output(mu_z_k, log_var_z_k, z_k, x_r_k, logits_m_r_k, num_slots):</span>
<span id="cb3-190"><a href="#cb3-190" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb3-191"><a href="#cb3-191" aria-hidden="true" tabindex="-1"></a><span class="co">        convert output into an easier understandable format</span></span>
<span id="cb3-192"><a href="#cb3-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-193"><a href="#cb3-193" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb3-194"><a href="#cb3-194" aria-hidden="true" tabindex="-1"></a><span class="co">            mu_z_k (torch.tensor): [batch_size*num_slots, latent_dim]</span></span>
<span id="cb3-195"><a href="#cb3-195" aria-hidden="true" tabindex="-1"></a><span class="co">            log_var_z_k (torch.tensor): [batch_size*num_slots, latent_dim]</span></span>
<span id="cb3-196"><a href="#cb3-196" aria-hidden="true" tabindex="-1"></a><span class="co">            z_k (torch.tensor): [batch_size*num_slots, latent_dim]</span></span>
<span id="cb3-197"><a href="#cb3-197" aria-hidden="true" tabindex="-1"></a><span class="co">            x_r_k (torch.tensor): [batch_size*num_slots, img_channels, img_dim, img_dim]</span></span>
<span id="cb3-198"><a href="#cb3-198" aria-hidden="true" tabindex="-1"></a><span class="co">            logits_m_r_k (torch.tensor): [batch_size*num_slots, 1, img_dim, img_dim]</span></span>
<span id="cb3-199"><a href="#cb3-199" aria-hidden="true" tabindex="-1"></a><span class="co">            num_slots (int): number of slots (log_mask.shape[1])</span></span>
<span id="cb3-200"><a href="#cb3-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-201"><a href="#cb3-201" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb3-202"><a href="#cb3-202" aria-hidden="true" tabindex="-1"></a><span class="co">            mu_z_k (torch.tensor): [batch, slot, latent_dim]</span></span>
<span id="cb3-203"><a href="#cb3-203" aria-hidden="true" tabindex="-1"></a><span class="co">            log_var_z_k (torch.tensor): [batch, slot, latent_dim]</span></span>
<span id="cb3-204"><a href="#cb3-204" aria-hidden="true" tabindex="-1"></a><span class="co">            z_k (torch.tensor): [batch, slot, latent_dim]</span></span>
<span id="cb3-205"><a href="#cb3-205" aria-hidden="true" tabindex="-1"></a><span class="co">            x_r_k (torch.tensor): [batch, slots, img_channels, img_dim, img_dim]</span></span>
<span id="cb3-206"><a href="#cb3-206" aria-hidden="true" tabindex="-1"></a><span class="co">            logits_m_r_k (torch.tensor): [batch, slots, 1, img_dim, img_dim]</span></span>
<span id="cb3-207"><a href="#cb3-207" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb3-208"><a href="#cb3-208" aria-hidden="true" tabindex="-1"></a>        mu_z_k <span class="op">=</span> torch.stack(mu_z_k.chunk(num_slots, dim<span class="op">=</span><span class="dv">0</span>), <span class="dv">1</span>)</span>
<span id="cb3-209"><a href="#cb3-209" aria-hidden="true" tabindex="-1"></a>        log_var_z_k <span class="op">=</span> torch.stack(log_var_z_k.chunk(num_slots, dim<span class="op">=</span><span class="dv">0</span>), <span class="dv">1</span>)</span>
<span id="cb3-210"><a href="#cb3-210" aria-hidden="true" tabindex="-1"></a>        z_k <span class="op">=</span> torch.stack(z_k.chunk(num_slots, dim<span class="op">=</span><span class="dv">0</span>), <span class="dv">1</span>)</span>
<span id="cb3-211"><a href="#cb3-211" aria-hidden="true" tabindex="-1"></a>        x_r_k <span class="op">=</span> torch.stack(x_r_k.chunk(num_slots, dim<span class="op">=</span><span class="dv">0</span>), <span class="dv">1</span>)</span>
<span id="cb3-212"><a href="#cb3-212" aria-hidden="true" tabindex="-1"></a>        logits_m_r_k <span class="op">=</span> torch.stack(logits_m_r_k.chunk(num_slots, dim<span class="op">=</span><span class="dv">0</span>), <span class="dv">1</span>)</span>
<span id="cb3-213"><a href="#cb3-213" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [mu_z_k, log_var_z_k, z_k, x_r_k, logits_m_r_k]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><p><strong>MONet Implementation</strong>: The compositional structure is achieved by looping for <span class="math inline">\(K\)</span> steps over the image and combining the attention network with the component VAE. While attention masks and latent codes can be generated easily (during test time), computing the loss <span class="math inline">\(\mathcal{L}\)</span> is more complicated. Remind that the loss function is given by</p>
<p><span class="math display">\[
\begin{align}
\mathcal{L}\left(\boldsymbol{\phi}; \boldsymbol{\theta};
\boldsymbol{\psi}; \textbf{x} \right) &amp;= \underbrace{- \log \left( \sum_{k=1}^K \textbf{m}_k \odot p_{\boldsymbol{\theta}} \left(\textbf{x} |
\textbf{z}_k \right)\right)}_{\text{Reconstruction Error between }
\widetilde{\textbf{x}} \text{ and } \textbf{x}} + \beta
\underbrace{D_{KL} \left( \prod_{k=1}^K q_{\boldsymbol{\phi}} \left(\textbf{z}_k |
\textbf{x}, \textbf{m}_k\right) || p(\textbf{z})
\right)}_{\text{Regularization Term for Distribution of }\textbf{z}_k}\\
&amp;+ \gamma \underbrace{D_{KL} \left( q_{\boldsymbol{\psi}} \left( \textbf{c} |
\textbf{x} \right) || p_{\boldsymbol{\theta}} \left( \textbf{c} | \{
\textbf{z}_k \} \right) \right)}_{\text{Reconstruction Error between }
\widetilde{\textbf{m}}_k \text{ and } \textbf{m}_k}.
\end{align}
\]</span></p>
<p>Each of these three terms can be written in a more explicit form such that the implementation becomes trivial:</p>
<ol type="1">
<li><p><em>Reconstruction Error between <span class="math inline">\(\widetilde{\textbf{x}}\)</span> and <span class="math inline">\(\textbf{x}\)</span></em>: This term is also known as the negative log likelihood (NLL) of the whole reconstructed image. <a href="https://arxiv.org/abs/1901.11390">Burgess et al.&nbsp;(2019)</a> chose independent Gaussian distributions with fixed variance for each pixel as the decoder distribution <span class="math inline">\(p_{\boldsymbol{\theta}} \left(
x_{i} | \textbf{z}_k \right) \sim \mathcal{N} \left(\mu_{k,
i}(\boldsymbol{\theta}), \sigma_k^2 \right)\)</span>.</p></li>
<li><p><em>Regularization Term for Distribution of <span class="math inline">\(\textbf{z}_k\)</span></em>: The coding space is regularized using the KL divergence between the latent (posterior) distribution <span class="math inline">\(q_{\boldsymbol{\phi}} \left( \textbf{z}_k \right) \sim
\mathcal{N} \left( \boldsymbol{\mu}_k, \left(\boldsymbol{\sigma}_k^2\right)^{\text{T}}
\textbf{I} \right)\)</span> factorized across slots and the latent prior distribution weighted with the hyperparameter <span class="math inline">\(\beta\)</span>. The product of multiple Gaussians is itself a Gaussian, however it is rather complicated to compute the new mean and covariance matrix of this Gaussian. Fortunately, each <span class="math inline">\(\textbf{z}_k\)</span> is sampled independently from the corresponding latent distribution <span class="math inline">\(q_{\boldsymbol{\phi}}(\textbf{z}_k)\)</span>, thus we can generate the new mean and covariance by concatenation (see <a href="https://stats.stackexchange.com/a/308137">this post</a>), i.e.,</p>
<p><span class="math display">\[
  q(\textbf{z}_1, \dots, \textbf{z}_K) = \prod_{k=1}^K q_{\boldsymbol{\phi}} \left(\textbf{z}_k
\right)  = q\left( \begin{bmatrix} \textbf{z}_1 \\ \vdots
  \\ \textbf{z}_K  \end{bmatrix}\right) = \mathcal{N} \left(
  \underbrace{
  \begin{bmatrix} \boldsymbol{\mu}_1 \\ \vdots
   \\ \boldsymbol{\mu}_K \end{bmatrix}}_{
   \widehat{\boldsymbol{\mu}}}, \underbrace{\text{diag}\left(
  \begin{bmatrix} \boldsymbol{\sigma}_1^2 \\  \vdots\\
  \boldsymbol{\sigma}_K^2  \end{bmatrix}
  \right)}_{ \left(\widehat{\boldsymbol{\sigma}}^2\right)^{\text{T}} \textbf{I}}\right)
\]</span></p>
<p><a href="https://arxiv.org/abs/1901.11390">Burgess et al.&nbsp;(2019)</a> chose a unit Gaussian distribution as the latent prior <span class="math inline">\(p(\textbf{z})
\sim \mathcal{N} \left(\textbf{0}, \textbf{I} \right)\)</span> with <span class="math inline">\(\text{dim}(\textbf{0}) = \text{dim}(\hat{\boldsymbol{\mu}})\)</span>. The KL divergence between those two Gaussian distributions can be calculated in closed form (see Appendix B of <a href="https://arxiv.org/abs/1312.6114">Kingma and Welling (2013)</a>)</p>
<p><span class="math display">\[
\begin{align}
   D_{KL} \left( \prod_{k=1}^K q_{\boldsymbol{\phi}}
\left(\textbf{z}_k \right) || p(\textbf{z}) \right) &amp;= -\frac
{1}{2} \sum_{j=1}^{K \cdot L} \left(1 + \log \left(
\widehat{\sigma}_j^2 \right) - \widehat{\mu}_j^2 - \widehat{\sigma}_j^2 \right),
\end{align}
\]</span></p>
<p>where <span class="math inline">\(L\)</span> denotes the dimensionality of the latent space.</p></li>
<li><p><em>Reconstruction Error between <span class="math inline">\(\widetilde{\textbf{m}}_k\)</span> and <span class="math inline">\(\textbf{m}_k\)</span></em>: Remind that the attention network <span class="math inline">\(\boldsymbol{\alpha}_{\boldsymbol{\psi}}\)</span> produces <span class="math inline">\(K\)</span> segmentation masks in logaritmic units, i.e., <span class="math inline">\(\log
\textbf{m}_k\)</span>. By construction <span class="math inline">\(\sum_{k=1}^K \textbf{m}_k =
\textbf{1}\)</span>, i.e., concatentation of the attention masks <span class="math inline">\(\textbf{m} = \begin{bmatrix} \textbf{m}_1 &amp; \dots &amp;
\textbf{m}_K \end{bmatrix}^{\text{T}}\)</span> can be interpreted as a pixel-wise categorical distribution<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>. Similarly, concatenating the logits probabilties of the component VAE and applying a pixel-wise softmax, i.e.,</p>
<p><span class="math display">\[
\widetilde{\textbf{m}} = \begin{bmatrix} \widetilde{\textbf{m}}_1 \\ \vdots \\
\widetilde{\textbf{m}}_K \end{bmatrix} = \text{Softmax}\left(\begin{bmatrix} \text{logits }\textbf{p}_1 \\ \vdots \\
\text{logits }\textbf{p}_K \end{bmatrix}\right),
\]</span></p>
<p>transforms the logits outputs of the component VAE into a pixel-wise categorical distribution. Thus, the KL-divergence can be calculated as follows</p>
<p><span class="math display">\[
\begin{align}
  D_{KL} \left( q_{\boldsymbol{\psi}} \left( \textbf{c} |
\textbf{x} \right) || p_{\boldsymbol{\theta}} \left( \textbf{c} | \{
\textbf{z}_k \} \right) \right) &amp;=
  \sum_{i=1}^{H\cdot W} D_{KL} \left( {\textbf{m}}_i || \widetilde{\textbf{m}}_i \right) \\
  &amp;= \sum_{i=1}^{H\cdot W} \textbf{m}_i \odot \left(\log \textbf{m}_i - \log \widetilde{\textbf{m}}_i \right),
\end{align}
\]</span></p>
<p>where <span class="math inline">\(i\)</span> denotes the pixel space, i.e., <span class="math inline">\(\textbf{m}_i \in [0,
1]^{K}\)</span>. To make the computation more efficient, we directly compute the reconstructed segmentations in logaritmic units using pixel-wise logsoftmax, i.e.,</p>
<p><span class="math display">\[
\log \widetilde{\textbf{m}} = \text{LogSoftmax}\left(\begin{bmatrix} \text{logits }\textbf{p}_1 \\ \vdots \\
\text{logits }\textbf{p}_K \end{bmatrix}\right).
\]</span></p></li>
</ol></li>
</ul>
<div class="cell" data-execution_count="5">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pytorch_lightning <span class="im">as</span> pl</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MONet(pl.LightningModule):</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Multi-Object Network class as described by Burgess et al. (2019)</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Atributes:</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co">        n_samples (int): number of samples in training dataset</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co">        attention_network (AttentionNetwork)</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co">        component_VAE (ComponentVAE)</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co">        ############## loss specific ##############</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co">        bg_var (float): background variance</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co">        fg_var (float): foreground variance</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="co">        beta (float): hyperparamater for loss</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co">        gamma (float): hyperparameter for loss</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="co">        ###########################################</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co">        ############ training specific ############</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="co">        num_slots_train (int): number of slots used during training time</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="co">        lr (float): learning rate</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="co">        batch_size (int): batch size used during training</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="co">        log_every_k_epochs (int): how often current result img should be logged</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="co">        ###########################################</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_samples):</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(MONet, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_samples <span class="op">=</span> n_samples</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention_network <span class="op">=</span> AttentionNetwork()</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.component_VAE <span class="op">=</span> ComponentVAE()</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># initialize all biases to zero</span></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention_network.<span class="bu">apply</span>(MONet.weight_init)</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.component_VAE.<span class="bu">apply</span>(MONet.weight_init)</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>        <span class="co">############## loss specific ##############</span></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_slots_train <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bg_var, <span class="va">self</span>.fg_var <span class="op">=</span> <span class="fl">0.09</span><span class="op">**</span><span class="dv">2</span>, <span class="fl">0.11</span><span class="op">**</span><span class="dv">2</span></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.beta <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gamma <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>        <span class="co">###########################################</span></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>        <span class="co">############ training specific ############</span></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lr, <span class="va">self</span>.batch_size <span class="op">=</span> <span class="fl">0.0001</span>, <span class="dv">64</span></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.log_every_k_epochs <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialise pixel output standard deviations (NLL calculation)</span></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>        var <span class="op">=</span> <span class="va">self</span>.fg_var <span class="op">*</span> torch.ones(<span class="dv">1</span>, <span class="va">self</span>.num_slots_train, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>        var[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>] <span class="op">=</span> <span class="va">self</span>.bg_var  <span class="co"># first step</span></span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">"var"</span>, var)</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.save_hyperparameters()</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, num_slots):</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a><span class="co">        defines the inference procedure of MONet, i.e., computes the latent</span></span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a><span class="co">        space and keeps track of useful metrics</span></span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a><span class="co">            x (torch.tensor): image [batch_size, img_channels, img_dim, img_dim]</span></span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a><span class="co">            num_slots (int): number of slots</span></span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a><span class="co">            out (dict): output dictionary containing</span></span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a><span class="co">                log_m_k (torch.tensor) [batch, slots, 1, img_dim, img_dim]</span></span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a><span class="co">                    (logarithmized attention masks of attention_network)</span></span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a><span class="co">                mu_k (torch.tensor) [batch, slots, latent_dim]</span></span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a><span class="co">                    (means of component VAE latent space)</span></span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a><span class="co">                log_var_k (torch.tensor) [batch, slots, latent_dim]</span></span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a><span class="co">                    (logarithmized variances of component VAE latent space)</span></span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a><span class="co">                x_r_k (torch.tensor) [batch, slots, img_channels, img_dim, img_dim]</span></span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a><span class="co">                    (slot-wise VAE image reconstructions)</span></span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a><span class="co">                logits_m_r_k (torch.tensor) [batch, slots, 1, img_dim, img_dim]</span></span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a><span class="co">                    (slot-wise VAE mask reconstructions in logits)</span></span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a><span class="co">                x_tilde (torch.tensor) [batch, img_channels, img_dim, img_dim]</span></span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a><span class="co">                    (reconstructed image using x_r_k and log_m_k)</span></span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute all logarithmized masks (iteratively)</span></span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>        log_m_k <span class="op">=</span> <span class="va">self</span>.attention_network(x, num_slots)</span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute all VAE reconstructions (parallel)</span></span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a>        [mu_z_k, log_var_z_k, z_k, x_r_k, logits_m_r_k] <span class="op">=</span> <span class="va">self</span>.component_VAE(x, </span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a>                                                                             log_m_k.exp())</span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a>        <span class="co"># store output in dict</span></span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="bu">dict</span>()</span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a>        output[<span class="st">"log_m_k"</span>] <span class="op">=</span> log_m_k</span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a>        output[<span class="st">"mu_z_k"</span>] <span class="op">=</span> mu_z_k</span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a>        output[<span class="st">"log_var_z_k"</span>] <span class="op">=</span> log_var_z_k</span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a>        output[<span class="st">"z_k"</span>] <span class="op">=</span> z_k</span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a>        output[<span class="st">"x_r_k"</span>] <span class="op">=</span> x_r_k</span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a>        output[<span class="st">"logits_m_r_k"</span>] <span class="op">=</span> logits_m_r_k</span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a>        output[<span class="st">"x_tilde"</span>] <span class="op">=</span> (log_m_k.exp() <span class="op">*</span> x_r_k).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a>    <span class="co">########################################</span></span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a>    <span class="co">#########  TRAINING FUNCTIONS  #########</span></span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a>    <span class="co">########################################</span></span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> training_step(<span class="va">self</span>, batch, batch_idx):</span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a>        x, labels <span class="op">=</span> batch  <span class="co"># labels are not used here (unsupervised)</span></span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.forward(x, <span class="va">self</span>.num_slots_train)        </span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a>        <span class="co">############ NLL \sum_k m_k log p(x_k) #############################</span></span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a>        NLL <span class="op">=</span> (</span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a>            output[<span class="st">"log_m_k"</span>].exp() <span class="op">*</span> </span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a>            (((x.unsqueeze(<span class="dv">1</span>) <span class="op">-</span> output[<span class="st">"x_r_k"</span>]) <span class="op">**</span> <span class="dv">2</span> <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> <span class="va">self</span>.var)))</span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a>        ).<span class="bu">sum</span>(axis<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>))</span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute KL divergence of latent space (component VAE) per batch</span></span>
<span id="cb4-105"><a href="#cb4-105" aria-hidden="true" tabindex="-1"></a>        KL_div_VAE <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> (</span>
<span id="cb4-106"><a href="#cb4-106" aria-hidden="true" tabindex="-1"></a>            <span class="dv">1</span> <span class="op">+</span> output[<span class="st">"log_var_z_k"</span>] <span class="op">-</span> output[<span class="st">"mu_z_k"</span>] <span class="op">**</span> <span class="dv">2</span> </span>
<span id="cb4-107"><a href="#cb4-107" aria-hidden="true" tabindex="-1"></a>            <span class="op">-</span> output[<span class="st">"log_var_z_k"</span>].exp()</span>
<span id="cb4-108"><a href="#cb4-108" aria-hidden="true" tabindex="-1"></a>        ).<span class="bu">sum</span>(axis<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb4-109"><a href="#cb4-109" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute KL divergence between masks</span></span>
<span id="cb4-110"><a href="#cb4-110" aria-hidden="true" tabindex="-1"></a>        log_m_r_k <span class="op">=</span> output[<span class="st">"logits_m_r_k"</span>].log_softmax(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-111"><a href="#cb4-111" aria-hidden="true" tabindex="-1"></a>        KL_div_masks <span class="op">=</span> (output[<span class="st">"log_m_k"</span>].exp() <span class="op">*</span> (output[<span class="st">"log_m_k"</span>] <span class="op">-</span> log_m_r_k)).<span class="bu">sum</span>(</span>
<span id="cb4-112"><a href="#cb4-112" aria-hidden="true" tabindex="-1"></a>            axis<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>)</span>
<span id="cb4-113"><a href="#cb4-113" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-114"><a href="#cb4-114" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute loss</span></span>
<span id="cb4-115"><a href="#cb4-115" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> (NLL.mean() <span class="op">+</span> <span class="va">self</span>.beta <span class="op">*</span> KL_div_VAE.mean() </span>
<span id="cb4-116"><a href="#cb4-116" aria-hidden="true" tabindex="-1"></a>                <span class="op">+</span> <span class="va">self</span>.gamma <span class="op">*</span> KL_div_masks.mean())</span>
<span id="cb4-117"><a href="#cb4-117" aria-hidden="true" tabindex="-1"></a>        <span class="co"># log results in TensorBoard</span></span>
<span id="cb4-118"><a href="#cb4-118" aria-hidden="true" tabindex="-1"></a>        step <span class="op">=</span> <span class="va">self</span>.global_step</span>
<span id="cb4-119"><a href="#cb4-119" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.experiment.add_scalar(<span class="st">"loss/NLL"</span>, NLL.mean(), step)</span>
<span id="cb4-120"><a href="#cb4-120" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.experiment.add_scalar(<span class="st">"loss/KL VAE"</span>, KL_div_VAE.mean(), step)</span>
<span id="cb4-121"><a href="#cb4-121" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.experiment.add_scalar(<span class="st">"loss/KL masks"</span>, KL_div_masks.mean(), step)</span>
<span id="cb4-122"><a href="#cb4-122" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.logger.experiment.add_scalar(<span class="st">"loss/loss"</span>, loss, step)</span>
<span id="cb4-123"><a href="#cb4-123" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {<span class="st">"loss"</span>:loss, <span class="st">"x"</span>: x}</span>
<span id="cb4-124"><a href="#cb4-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-125"><a href="#cb4-125" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> training_epoch_end(<span class="va">self</span>, outputs):</span>
<span id="cb4-126"><a href="#cb4-126" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""this function is called after each epoch"""</span></span>
<span id="cb4-127"><a href="#cb4-127" aria-hidden="true" tabindex="-1"></a>        step <span class="op">=</span> <span class="bu">int</span>(<span class="va">self</span>.current_epoch)</span>
<span id="cb4-128"><a href="#cb4-128" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (step <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="va">self</span>.log_every_k_epochs <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb4-129"><a href="#cb4-129" aria-hidden="true" tabindex="-1"></a>            <span class="co"># log some images, their segmentations and reconstructions</span></span>
<span id="cb4-130"><a href="#cb4-130" aria-hidden="true" tabindex="-1"></a>            n_samples <span class="op">=</span> <span class="dv">7</span></span>
<span id="cb4-131"><a href="#cb4-131" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-132"><a href="#cb4-132" aria-hidden="true" tabindex="-1"></a>            last_x <span class="op">=</span> outputs[<span class="op">-</span><span class="dv">1</span>][<span class="st">"x"</span>]</span>
<span id="cb4-133"><a href="#cb4-133" aria-hidden="true" tabindex="-1"></a>            i_samples <span class="op">=</span> np.random.choice(<span class="bu">range</span>(<span class="bu">len</span>(last_x)), n_samples, <span class="va">False</span>)</span>
<span id="cb4-134"><a href="#cb4-134" aria-hidden="true" tabindex="-1"></a>            images <span class="op">=</span> last_x[i_samples]</span>
<span id="cb4-135"><a href="#cb4-135" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-136"><a href="#cb4-136" aria-hidden="true" tabindex="-1"></a>            fig_rec <span class="op">=</span> <span class="va">self</span>.plot_reconstructions_and_decompositions(images, </span>
<span id="cb4-137"><a href="#cb4-137" aria-hidden="true" tabindex="-1"></a>                                                                   <span class="va">self</span>.num_slots_train)</span>
<span id="cb4-138"><a href="#cb4-138" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logger.experiment.add_figure(<span class="st">"image and reconstructions"</span>, </span>
<span id="cb4-139"><a href="#cb4-139" aria-hidden="true" tabindex="-1"></a>                                              fig_rec, global_step<span class="op">=</span>step)</span>
<span id="cb4-140"><a href="#cb4-140" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb4-141"><a href="#cb4-141" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-142"><a href="#cb4-142" aria-hidden="true" tabindex="-1"></a>    <span class="co">########################################</span></span>
<span id="cb4-143"><a href="#cb4-143" aria-hidden="true" tabindex="-1"></a>    <span class="co">######### TRAINING SETUP HOOKS #########</span></span>
<span id="cb4-144"><a href="#cb4-144" aria-hidden="true" tabindex="-1"></a>    <span class="co">########################################</span></span>
<span id="cb4-145"><a href="#cb4-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-146"><a href="#cb4-146" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> configure_optimizers(<span class="va">self</span>):</span>
<span id="cb4-147"><a href="#cb4-147" aria-hidden="true" tabindex="-1"></a>        optimizer <span class="op">=</span> torch.optim.RMSprop(<span class="va">self</span>.parameters(), lr<span class="op">=</span><span class="va">self</span>.lr)</span>
<span id="cb4-148"><a href="#cb4-148" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> optimizer</span>
<span id="cb4-149"><a href="#cb4-149" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-150"><a href="#cb4-150" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb4-151"><a href="#cb4-151" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> weight_init(m):</span>
<span id="cb4-152"><a href="#cb4-152" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""initialize all bias to zero"""</span></span>
<span id="cb4-153"><a href="#cb4-153" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(m, nn.Conv2d) <span class="kw">or</span> <span class="bu">isinstance</span>(m, nn.Linear):</span>
<span id="cb4-154"><a href="#cb4-154" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> m.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-155"><a href="#cb4-155" aria-hidden="true" tabindex="-1"></a>                torch.nn.init.zeros_(m.bias)</span>
<span id="cb4-156"><a href="#cb4-156" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb4-157"><a href="#cb4-157" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-158"><a href="#cb4-158" aria-hidden="true" tabindex="-1"></a>    <span class="co">########################################</span></span>
<span id="cb4-159"><a href="#cb4-159" aria-hidden="true" tabindex="-1"></a>    <span class="co">####### PLOT AND HELPER FUNCTIONS ######</span></span>
<span id="cb4-160"><a href="#cb4-160" aria-hidden="true" tabindex="-1"></a>    <span class="co">########################################</span></span>
<span id="cb4-161"><a href="#cb4-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-162"><a href="#cb4-162" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb4-163"><a href="#cb4-163" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> convert_masks_indices_to_mask_rgb(masks_ind, slots):</span>
<span id="cb4-164"><a href="#cb4-164" aria-hidden="true" tabindex="-1"></a>        colors <span class="op">=</span> plt.cm.get_cmap(<span class="st">"hsv"</span>, slots <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb4-165"><a href="#cb4-165" aria-hidden="true" tabindex="-1"></a>        cmap_rgb <span class="op">=</span> colors(np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, slots <span class="op">+</span> <span class="dv">1</span>))[:, <span class="dv">0</span>:<span class="dv">3</span>]</span>
<span id="cb4-166"><a href="#cb4-166" aria-hidden="true" tabindex="-1"></a>        masks_RGB <span class="op">=</span> cmap_rgb[masks_ind].squeeze(<span class="dv">1</span>)</span>
<span id="cb4-167"><a href="#cb4-167" aria-hidden="true" tabindex="-1"></a>        masks_RGB_tensor <span class="op">=</span> torch.from_numpy(masks_RGB)</span>
<span id="cb4-168"><a href="#cb4-168" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> masks_RGB_tensor</span>
<span id="cb4-169"><a href="#cb4-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-170"><a href="#cb4-170" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> plot_reconstructions_and_decompositions(<span class="va">self</span>, images, num_slots):</span>
<span id="cb4-171"><a href="#cb4-171" aria-hidden="true" tabindex="-1"></a>        monet_output <span class="op">=</span> <span class="va">self</span>.forward(images, num_slots)</span>
<span id="cb4-172"><a href="#cb4-172" aria-hidden="true" tabindex="-1"></a>        batch_size, img_channels <span class="op">=</span> images.shape[<span class="dv">0</span>:<span class="dv">2</span>]</span>
<span id="cb4-173"><a href="#cb4-173" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-174"><a href="#cb4-174" aria-hidden="true" tabindex="-1"></a>        colors <span class="op">=</span> plt.cm.get_cmap(<span class="st">"hsv"</span>, num_slots <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb4-175"><a href="#cb4-175" aria-hidden="true" tabindex="-1"></a>        cmap <span class="op">=</span> colors(np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, num_slots <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb4-176"><a href="#cb4-176" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-177"><a href="#cb4-177" aria-hidden="true" tabindex="-1"></a>        <span class="co"># get mask indices using argmax [batch_size, 1, 64, 64]</span></span>
<span id="cb4-178"><a href="#cb4-178" aria-hidden="true" tabindex="-1"></a>        masks_ind <span class="op">=</span> monet_output[<span class="st">"log_m_k"</span>].exp().argmax(<span class="dv">1</span>).detach().cpu()</span>
<span id="cb4-179"><a href="#cb4-179" aria-hidden="true" tabindex="-1"></a>        <span class="co"># convert into RGB values  [batch_size, 64, 64, 3]</span></span>
<span id="cb4-180"><a href="#cb4-180" aria-hidden="true" tabindex="-1"></a>        masks_RGB <span class="op">=</span> MONet.convert_masks_indices_to_mask_rgb(masks_ind, num_slots)              </span>
<span id="cb4-181"><a href="#cb4-181" aria-hidden="true" tabindex="-1"></a>        fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">10</span>))</span>
<span id="cb4-182"><a href="#cb4-182" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> counter <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb4-183"><a href="#cb4-183" aria-hidden="true" tabindex="-1"></a>            orig_img <span class="op">=</span> images[counter]</span>
<span id="cb4-184"><a href="#cb4-184" aria-hidden="true" tabindex="-1"></a>            <span class="co"># data</span></span>
<span id="cb4-185"><a href="#cb4-185" aria-hidden="true" tabindex="-1"></a>            plt.subplot(<span class="dv">3</span> <span class="op">+</span> num_slots, batch_size <span class="op">+</span> <span class="dv">1</span>, counter <span class="op">+</span> <span class="dv">2</span>)</span>
<span id="cb4-186"><a href="#cb4-186" aria-hidden="true" tabindex="-1"></a>            plt.imshow(transforms.ToPILImage()(orig_img))</span>
<span id="cb4-187"><a href="#cb4-187" aria-hidden="true" tabindex="-1"></a>            plt.axis(<span class="st">'off'</span>)</span>
<span id="cb4-188"><a href="#cb4-188" aria-hidden="true" tabindex="-1"></a>            <span class="co"># reconstruction mixture</span></span>
<span id="cb4-189"><a href="#cb4-189" aria-hidden="true" tabindex="-1"></a>            x_tilde <span class="op">=</span> monet_output[<span class="st">"x_tilde"</span>][counter].clamp(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb4-190"><a href="#cb4-190" aria-hidden="true" tabindex="-1"></a>            plt.subplot(<span class="dv">3</span> <span class="op">+</span> num_slots, batch_size <span class="op">+</span> <span class="dv">1</span>, counter <span class="op">+</span> <span class="dv">2</span> <span class="op">+</span> (batch_size <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb4-191"><a href="#cb4-191" aria-hidden="true" tabindex="-1"></a>            plt.imshow(transforms.ToPILImage()(x_tilde))</span>
<span id="cb4-192"><a href="#cb4-192" aria-hidden="true" tabindex="-1"></a>            plt.axis(<span class="st">'off'</span>)</span>
<span id="cb4-193"><a href="#cb4-193" aria-hidden="true" tabindex="-1"></a>            <span class="co"># segmentation (binary) from attention network</span></span>
<span id="cb4-194"><a href="#cb4-194" aria-hidden="true" tabindex="-1"></a>            plt.subplot(<span class="dv">3</span> <span class="op">+</span> num_slots, batch_size <span class="op">+</span> <span class="dv">1</span>, counter <span class="op">+</span> <span class="dv">2</span> <span class="op">+</span> (batch_size <span class="op">+</span> <span class="dv">1</span>)<span class="op">*</span><span class="dv">2</span>)</span>
<span id="cb4-195"><a href="#cb4-195" aria-hidden="true" tabindex="-1"></a>            plt.imshow(masks_RGB[counter])</span>
<span id="cb4-196"><a href="#cb4-196" aria-hidden="true" tabindex="-1"></a>            plt.axis(<span class="st">'off'</span>)</span>
<span id="cb4-197"><a href="#cb4-197" aria-hidden="true" tabindex="-1"></a>            <span class="co"># unmasked component reconstructions</span></span>
<span id="cb4-198"><a href="#cb4-198" aria-hidden="true" tabindex="-1"></a>            x_r_k <span class="op">=</span> monet_output[<span class="st">"x_r_k"</span>][counter].clamp(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb4-199"><a href="#cb4-199" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> slot <span class="kw">in</span> <span class="bu">range</span>(num_slots):</span>
<span id="cb4-200"><a href="#cb4-200" aria-hidden="true" tabindex="-1"></a>                x_rec <span class="op">=</span> x_r_k[slot]</span>
<span id="cb4-201"><a href="#cb4-201" aria-hidden="true" tabindex="-1"></a>                plot_idx <span class="op">=</span>  counter <span class="op">+</span> <span class="dv">2</span> <span class="op">+</span> (batch_size <span class="op">+</span> <span class="dv">1</span>)<span class="op">*</span>(slot<span class="op">+</span><span class="dv">3</span>)</span>
<span id="cb4-202"><a href="#cb4-202" aria-hidden="true" tabindex="-1"></a>                plt.subplot(<span class="dv">3</span> <span class="op">+</span> num_slots, batch_size <span class="op">+</span> <span class="dv">1</span>, plot_idx)</span>
<span id="cb4-203"><a href="#cb4-203" aria-hidden="true" tabindex="-1"></a>                plt.imshow(transforms.ToPILImage()(x_rec))</span>
<span id="cb4-204"><a href="#cb4-204" aria-hidden="true" tabindex="-1"></a>                plt.axis(<span class="st">'off'</span>)</span>
<span id="cb4-205"><a href="#cb4-205" aria-hidden="true" tabindex="-1"></a>        <span class="co"># annotation plots</span></span>
<span id="cb4-206"><a href="#cb4-206" aria-hidden="true" tabindex="-1"></a>        ax <span class="op">=</span> plt.subplot(<span class="dv">3</span> <span class="op">+</span> num_slots, batch_size <span class="op">+</span> <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb4-207"><a href="#cb4-207" aria-hidden="true" tabindex="-1"></a>        ax.annotate(<span class="st">'Data'</span>, xy<span class="op">=</span>(<span class="dv">1</span>, <span class="fl">0.5</span>), xycoords<span class="op">=</span><span class="st">'axes fraction'</span>,</span>
<span id="cb4-208"><a href="#cb4-208" aria-hidden="true" tabindex="-1"></a>                    fontsize<span class="op">=</span><span class="dv">14</span>, va<span class="op">=</span><span class="st">'center'</span>, ha<span class="op">=</span><span class="st">'right'</span>)</span>
<span id="cb4-209"><a href="#cb4-209" aria-hidden="true" tabindex="-1"></a>        ax.set_aspect(<span class="st">'equal'</span>)</span>
<span id="cb4-210"><a href="#cb4-210" aria-hidden="true" tabindex="-1"></a>        ax.axis(<span class="st">'off'</span>)</span>
<span id="cb4-211"><a href="#cb4-211" aria-hidden="true" tabindex="-1"></a>        ax <span class="op">=</span> plt.subplot(<span class="dv">3</span> <span class="op">+</span> num_slots, batch_size <span class="op">+</span> <span class="dv">1</span>, batch_size <span class="op">+</span> <span class="dv">2</span>)</span>
<span id="cb4-212"><a href="#cb4-212" aria-hidden="true" tabindex="-1"></a>        ax.annotate(<span class="st">'Reconstruction</span><span class="ch">\n</span><span class="st">mixture'</span>, xy<span class="op">=</span>(<span class="dv">1</span>, <span class="fl">0.5</span>), xycoords<span class="op">=</span><span class="st">'axes fraction'</span>,</span>
<span id="cb4-213"><a href="#cb4-213" aria-hidden="true" tabindex="-1"></a>                    fontsize<span class="op">=</span><span class="dv">14</span>, va<span class="op">=</span><span class="st">'center'</span>, ha<span class="op">=</span><span class="st">'right'</span>)</span>
<span id="cb4-214"><a href="#cb4-214" aria-hidden="true" tabindex="-1"></a>        ax.set_aspect(<span class="st">'equal'</span>)</span>
<span id="cb4-215"><a href="#cb4-215" aria-hidden="true" tabindex="-1"></a>        ax.axis(<span class="st">'off'</span>)</span>
<span id="cb4-216"><a href="#cb4-216" aria-hidden="true" tabindex="-1"></a>        ax <span class="op">=</span> plt.subplot(<span class="dv">3</span> <span class="op">+</span> num_slots, batch_size <span class="op">+</span> <span class="dv">1</span>, <span class="dv">2</span><span class="op">*</span>batch_size <span class="op">+</span> <span class="dv">3</span>)</span>
<span id="cb4-217"><a href="#cb4-217" aria-hidden="true" tabindex="-1"></a>        ax.annotate(<span class="st">'Segmentation'</span>, xy<span class="op">=</span>(<span class="dv">1</span>, <span class="fl">0.5</span>), xycoords<span class="op">=</span><span class="st">'axes fraction'</span>,</span>
<span id="cb4-218"><a href="#cb4-218" aria-hidden="true" tabindex="-1"></a>                    fontsize<span class="op">=</span><span class="dv">14</span>, va<span class="op">=</span><span class="st">'center'</span>, ha<span class="op">=</span><span class="st">'right'</span>)</span>
<span id="cb4-219"><a href="#cb4-219" aria-hidden="true" tabindex="-1"></a>        ax.set_aspect(<span class="st">'equal'</span>)</span>
<span id="cb4-220"><a href="#cb4-220" aria-hidden="true" tabindex="-1"></a>        ax.axis(<span class="st">'off'</span>)</span>
<span id="cb4-221"><a href="#cb4-221" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> slot <span class="kw">in</span> <span class="bu">range</span>(num_slots):</span>
<span id="cb4-222"><a href="#cb4-222" aria-hidden="true" tabindex="-1"></a>            ax <span class="op">=</span> plt.subplot(<span class="dv">3</span> <span class="op">+</span> num_slots, batch_size <span class="op">+</span> <span class="dv">1</span>, </span>
<span id="cb4-223"><a href="#cb4-223" aria-hidden="true" tabindex="-1"></a>                             <span class="dv">1</span> <span class="op">+</span> (batch_size <span class="op">+</span> <span class="dv">1</span>)<span class="op">*</span>(slot<span class="op">+</span><span class="dv">3</span>))</span>
<span id="cb4-224"><a href="#cb4-224" aria-hidden="true" tabindex="-1"></a>            ax.annotate(<span class="ss">f'S</span><span class="sc">{</span>slot<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>, xy<span class="op">=</span>(<span class="dv">1</span>, <span class="fl">0.5</span>), xycoords<span class="op">=</span><span class="st">'axes fraction'</span>,</span>
<span id="cb4-225"><a href="#cb4-225" aria-hidden="true" tabindex="-1"></a>                        fontsize<span class="op">=</span><span class="dv">14</span>, va<span class="op">=</span><span class="st">'center'</span>, ha<span class="op">=</span><span class="st">'right'</span>, weight<span class="op">=</span><span class="st">'bold'</span>,</span>
<span id="cb4-226"><a href="#cb4-226" aria-hidden="true" tabindex="-1"></a>                        color<span class="op">=</span>cmap[slot])</span>
<span id="cb4-227"><a href="#cb4-227" aria-hidden="true" tabindex="-1"></a>            ax.set_aspect(<span class="st">'equal'</span>)</span>
<span id="cb4-228"><a href="#cb4-228" aria-hidden="true" tabindex="-1"></a>            ax.axis(<span class="st">'off'</span>)</span>
<span id="cb4-229"><a href="#cb4-229" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> fig</span>
<span id="cb4-230"><a href="#cb4-230" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-231"><a href="#cb4-231" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> plot_ComponentVAE_results(<span class="va">self</span>, images, num_slots):</span>
<span id="cb4-232"><a href="#cb4-232" aria-hidden="true" tabindex="-1"></a>        monet_output <span class="op">=</span> <span class="va">self</span>.forward(images, num_slots)</span>
<span id="cb4-233"><a href="#cb4-233" aria-hidden="true" tabindex="-1"></a>        x_r_k <span class="op">=</span> monet_output[<span class="st">"x_r_k"</span>]</span>
<span id="cb4-234"><a href="#cb4-234" aria-hidden="true" tabindex="-1"></a>        masks <span class="op">=</span> monet_output[<span class="st">"log_m_k"</span>].exp()</span>
<span id="cb4-235"><a href="#cb4-235" aria-hidden="true" tabindex="-1"></a>        <span class="co"># get mask indices using argmax [batch_size, 1, 64, 64]</span></span>
<span id="cb4-236"><a href="#cb4-236" aria-hidden="true" tabindex="-1"></a>        masks_ind <span class="op">=</span> masks.argmax(<span class="dv">1</span>).detach().cpu()</span>
<span id="cb4-237"><a href="#cb4-237" aria-hidden="true" tabindex="-1"></a>        <span class="co"># convert into RGB values  [batch_size, 64, 64, 3]</span></span>
<span id="cb4-238"><a href="#cb4-238" aria-hidden="true" tabindex="-1"></a>        masks_RGB <span class="op">=</span> MONet.convert_masks_indices_to_mask_rgb(masks_ind, num_slots) </span>
<span id="cb4-239"><a href="#cb4-239" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-240"><a href="#cb4-240" aria-hidden="true" tabindex="-1"></a>        colors <span class="op">=</span> plt.cm.get_cmap(<span class="st">'hsv'</span>, num_slots <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb4-241"><a href="#cb4-241" aria-hidden="true" tabindex="-1"></a>        cmap <span class="op">=</span> colors(np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, num_slots <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb4-242"><a href="#cb4-242" aria-hidden="true" tabindex="-1"></a>        n_samples, img_channels <span class="op">=</span> images.shape[<span class="dv">0</span>:<span class="dv">2</span>]</span>
<span id="cb4-243"><a href="#cb4-243" aria-hidden="true" tabindex="-1"></a>        fig <span class="op">=</span> plt.figure(constrained_layout<span class="op">=</span><span class="va">False</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">14</span>))</span>
<span id="cb4-244"><a href="#cb4-244" aria-hidden="true" tabindex="-1"></a>        grid_spec <span class="op">=</span> fig.add_gridspec(<span class="dv">2</span>, n_samples, hspace<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb4-245"><a href="#cb4-245" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-246"><a href="#cb4-246" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> counter <span class="kw">in</span> <span class="bu">range</span>(n_samples):</span>
<span id="cb4-247"><a href="#cb4-247" aria-hidden="true" tabindex="-1"></a>            orig_img <span class="op">=</span> images[counter]</span>
<span id="cb4-248"><a href="#cb4-248" aria-hidden="true" tabindex="-1"></a>            x_tilde <span class="op">=</span> monet_output[<span class="st">"x_tilde"</span>][counter].clamp(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb4-249"><a href="#cb4-249" aria-hidden="true" tabindex="-1"></a>            segmentation_mask <span class="op">=</span> masks_RGB[counter]</span>
<span id="cb4-250"><a href="#cb4-250" aria-hidden="true" tabindex="-1"></a>            <span class="co"># upper plot: Data, Reconstruction Mixture, Segmentation</span></span>
<span id="cb4-251"><a href="#cb4-251" aria-hidden="true" tabindex="-1"></a>            upper_grid <span class="op">=</span> grid_spec[<span class="dv">0</span>, counter].subgridspec(<span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb4-252"><a href="#cb4-252" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> upper_plot_index <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb4-253"><a href="#cb4-253" aria-hidden="true" tabindex="-1"></a>                ax <span class="op">=</span> fig.add_subplot(upper_grid[upper_plot_index])</span>
<span id="cb4-254"><a href="#cb4-254" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> upper_plot_index <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb4-255"><a href="#cb4-255" aria-hidden="true" tabindex="-1"></a>                    plt.imshow(transforms.ToPILImage()(orig_img))</span>
<span id="cb4-256"><a href="#cb4-256" aria-hidden="true" tabindex="-1"></a>                <span class="cf">elif</span> upper_plot_index <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb4-257"><a href="#cb4-257" aria-hidden="true" tabindex="-1"></a>                    plt.imshow(transforms.ToPILImage()(x_tilde))   </span>
<span id="cb4-258"><a href="#cb4-258" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb4-259"><a href="#cb4-259" aria-hidden="true" tabindex="-1"></a>                    plt.imshow(segmentation_mask)</span>
<span id="cb4-260"><a href="#cb4-260" aria-hidden="true" tabindex="-1"></a>                plt.axis(<span class="st">'off'</span>)</span>
<span id="cb4-261"><a href="#cb4-261" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> counter <span class="op">==</span> <span class="dv">0</span>:  <span class="co"># annotations</span></span>
<span id="cb4-262"><a href="#cb4-262" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> upper_plot_index <span class="op">==</span> <span class="dv">0</span>:  <span class="co"># Data</span></span>
<span id="cb4-263"><a href="#cb4-263" aria-hidden="true" tabindex="-1"></a>                        ax.annotate(<span class="st">'Data'</span>, xy<span class="op">=</span>(<span class="op">-</span><span class="fl">0.1</span>, <span class="fl">0.5</span>), </span>
<span id="cb4-264"><a href="#cb4-264" aria-hidden="true" tabindex="-1"></a>                                    xycoords<span class="op">=</span><span class="st">'axes fraction'</span>, ha<span class="op">=</span><span class="st">'right'</span>,</span>
<span id="cb4-265"><a href="#cb4-265" aria-hidden="true" tabindex="-1"></a>                                    fontsize<span class="op">=</span><span class="dv">14</span>, va<span class="op">=</span><span class="st">'center'</span>,)</span>
<span id="cb4-266"><a href="#cb4-266" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">elif</span> upper_plot_index <span class="op">==</span> <span class="dv">1</span>:  <span class="co"># Reconstruction mixture</span></span>
<span id="cb4-267"><a href="#cb4-267" aria-hidden="true" tabindex="-1"></a>                        ax.annotate(<span class="st">'Reconstruction</span><span class="ch">\n</span><span class="st">mixture'</span>, xy<span class="op">=</span>(<span class="op">-</span><span class="fl">0.1</span>, <span class="fl">0.5</span>), </span>
<span id="cb4-268"><a href="#cb4-268" aria-hidden="true" tabindex="-1"></a>                                     va<span class="op">=</span><span class="st">'center'</span>,</span>
<span id="cb4-269"><a href="#cb4-269" aria-hidden="true" tabindex="-1"></a>                                     xycoords<span class="op">=</span><span class="st">'axes fraction'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, ha<span class="op">=</span><span class="st">'right'</span>)</span>
<span id="cb4-270"><a href="#cb4-270" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">else</span>:  <span class="co"># Segmentation</span></span>
<span id="cb4-271"><a href="#cb4-271" aria-hidden="true" tabindex="-1"></a>                        ax.annotate(<span class="st">'Segmentation'</span>, xy<span class="op">=</span>(<span class="op">-</span><span class="fl">0.1</span>, <span class="fl">0.5</span>), va<span class="op">=</span><span class="st">'center'</span>,</span>
<span id="cb4-272"><a href="#cb4-272" aria-hidden="true" tabindex="-1"></a>                                     xycoords<span class="op">=</span><span class="st">'axes fraction'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, ha<span class="op">=</span><span class="st">'right'</span>)</span>
<span id="cb4-273"><a href="#cb4-273" aria-hidden="true" tabindex="-1"></a>            <span class="co"># lower plot: Component VAE reconstructions</span></span>
<span id="cb4-274"><a href="#cb4-274" aria-hidden="true" tabindex="-1"></a>            lower_grid <span class="op">=</span> grid_spec[<span class="dv">1</span>, counter].subgridspec(num_slots, <span class="dv">2</span>, </span>
<span id="cb4-275"><a href="#cb4-275" aria-hidden="true" tabindex="-1"></a>                                                           wspace<span class="op">=</span><span class="fl">0.1</span>, hspace<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb4-276"><a href="#cb4-276" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> row_index <span class="kw">in</span> <span class="bu">range</span>(num_slots):</span>
<span id="cb4-277"><a href="#cb4-277" aria-hidden="true" tabindex="-1"></a>                x_slot_r <span class="op">=</span> x_r_k[counter][row_index]</span>
<span id="cb4-278"><a href="#cb4-278" aria-hidden="true" tabindex="-1"></a>                m_slot_r <span class="op">=</span> masks[counter][row_index]</span>
<span id="cb4-279"><a href="#cb4-279" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> col_index <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb4-280"><a href="#cb4-280" aria-hidden="true" tabindex="-1"></a>                    ax <span class="op">=</span> fig.add_subplot(lower_grid[row_index, col_index])</span>
<span id="cb4-281"><a href="#cb4-281" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> col_index <span class="op">==</span> <span class="dv">0</span>:  <span class="co"># unmasked</span></span>
<span id="cb4-282"><a href="#cb4-282" aria-hidden="true" tabindex="-1"></a>                        plt.imshow(transforms.ToPILImage()(x_slot_r.clamp(<span class="dv">0</span>, <span class="dv">1</span>)))</span>
<span id="cb4-283"><a href="#cb4-283" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">if</span> row_index <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb4-284"><a href="#cb4-284" aria-hidden="true" tabindex="-1"></a>                            plt.title(<span class="st">'Unmasked'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb4-285"><a href="#cb4-285" aria-hidden="true" tabindex="-1"></a>                        plt.axis(<span class="st">'off'</span>)</span>
<span id="cb4-286"><a href="#cb4-286" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">else</span>:  <span class="co"># masked</span></span>
<span id="cb4-287"><a href="#cb4-287" aria-hidden="true" tabindex="-1"></a>                        masked <span class="op">=</span> ((<span class="dv">1</span> <span class="op">-</span> m_slot_r)<span class="op">*</span>torch.ones_like(x_slot_r) </span>
<span id="cb4-288"><a href="#cb4-288" aria-hidden="true" tabindex="-1"></a>                                  <span class="op">+</span> m_slot_r<span class="op">*</span>x_slot_r)</span>
<span id="cb4-289"><a href="#cb4-289" aria-hidden="true" tabindex="-1"></a>                        <span class="co">#masked = m_slot_r*x_slot_r</span></span>
<span id="cb4-290"><a href="#cb4-290" aria-hidden="true" tabindex="-1"></a>                        plt.imshow(transforms.ToPILImage()(masked.clamp(<span class="dv">0</span>, <span class="dv">1</span>)))</span>
<span id="cb4-291"><a href="#cb4-291" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">if</span> row_index <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb4-292"><a href="#cb4-292" aria-hidden="true" tabindex="-1"></a>                            plt.title(<span class="st">'Masked'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb4-293"><a href="#cb4-293" aria-hidden="true" tabindex="-1"></a>                        plt.axis(<span class="st">'off'</span>)</span>
<span id="cb4-294"><a href="#cb4-294" aria-hidden="true" tabindex="-1"></a>                    ax.set_aspect(<span class="st">'equal'</span>)</span>
<span id="cb4-295"><a href="#cb4-295" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> counter <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> col_index <span class="op">==</span> <span class="dv">0</span>:  <span class="co"># annotations</span></span>
<span id="cb4-296"><a href="#cb4-296" aria-hidden="true" tabindex="-1"></a>                        ax.annotate(<span class="ss">f'S</span><span class="sc">{</span>row_index<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>, xy<span class="op">=</span>(<span class="op">-</span><span class="fl">0.1</span>, <span class="fl">0.5</span>), </span>
<span id="cb4-297"><a href="#cb4-297" aria-hidden="true" tabindex="-1"></a>                                    xycoords<span class="op">=</span><span class="st">'axes fraction'</span>, ha<span class="op">=</span><span class="st">'right'</span>,</span>
<span id="cb4-298"><a href="#cb4-298" aria-hidden="true" tabindex="-1"></a>                                    fontsize<span class="op">=</span><span class="dv">14</span>, va<span class="op">=</span><span class="st">'center'</span>, weight<span class="op">=</span><span class="st">'bold'</span>,</span>
<span id="cb4-299"><a href="#cb4-299" aria-hidden="true" tabindex="-1"></a>                                    color<span class="op">=</span>cmap[row_index])</span>
<span id="cb4-300"><a href="#cb4-300" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb4-301"><a href="#cb4-301" aria-hidden="true" tabindex="-1"></a>                                   </span>
<span id="cb4-302"><a href="#cb4-302" aria-hidden="true" tabindex="-1"></a>    <span class="co">########################################</span></span>
<span id="cb4-303"><a href="#cb4-303" aria-hidden="true" tabindex="-1"></a>    <span class="co">########## DATA RELATED HOOKS ##########</span></span>
<span id="cb4-304"><a href="#cb4-304" aria-hidden="true" tabindex="-1"></a>    <span class="co">########################################</span></span>
<span id="cb4-305"><a href="#cb4-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-306"><a href="#cb4-306" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> prepare_data(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb4-307"><a href="#cb4-307" aria-hidden="true" tabindex="-1"></a>        n_samples <span class="op">=</span> <span class="va">self</span>.n_samples</span>
<span id="cb4-308"><a href="#cb4-308" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dataset <span class="op">=</span> generate_dataset(n_samples<span class="op">=</span>n_samples)</span>
<span id="cb4-309"><a href="#cb4-309" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb4-310"><a href="#cb4-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-311"><a href="#cb4-311" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> train_dataloader(<span class="va">self</span>):</span>
<span id="cb4-312"><a href="#cb4-312" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> DataLoader(<span class="va">self</span>.dataset, batch_size<span class="op">=</span><span class="va">self</span>.batch_size, </span>
<span id="cb4-313"><a href="#cb4-313" aria-hidden="true" tabindex="-1"></a>                          num_workers<span class="op">=</span><span class="dv">12</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li><strong>Training Procedure</strong>: <a href="https://arxiv.org/abs/1901.11390">Burgess et al. (2019)</a> chose <code>RMSProp</code> for the optimization with a learning rate of <code>0.0001</code> and a batch size of <code>64</code>, see Appendix B.3. Thanks to the <a href="https://pytorch-lightning.readthedocs.io/en/latest/">PyTorch-Lightning</a> framework, these paramters are already defined in the model and we can easily integrate tensorboard into our training procedure:</li>
</ul>
<div class="cell" data-execution_count="6">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pytorch_lightning <span class="im">import</span> seed_everything</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pytorch_lightning.loggers <span class="im">import</span> TensorBoardLogger</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(n_samples, num_epochs, SEED<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    seed_everything(SEED)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    monet <span class="op">=</span> MONet(n_samples)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    logger <span class="op">=</span> TensorBoardLogger(<span class="st">'./results'</span>, name<span class="op">=</span><span class="st">"SimplifiedMultiSprites"</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialize pytorch lightning trainer</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    num_gpus <span class="op">=</span> <span class="dv">1</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    trainer <span class="op">=</span> pl.Trainer(</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        deterministic<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        gpus<span class="op">=</span>num_gpus,</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        track_grad_norm<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        gradient_clip_val<span class="op">=</span><span class="dv">2</span>,  <span class="co"># don't clip</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        max_epochs<span class="op">=</span>num_epochs,</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        progress_bar_refresh_rate<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        logger<span class="op">=</span>logger,</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>     <span class="co"># train model</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    trainer.fit(monet)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    trained_monet <span class="op">=</span> monet</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> trained_monet</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>trained_monet <span class="op">=</span> train(n_samples<span class="op">=</span><span class="dv">50000</span>, num_epochs<span class="op">=</span><span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./img/MONET_train.png" title="Training" class="img-fluid figure-img"></p>
<figcaption>Training</figcaption>
</figure>
</div>
</section>
<section id="results" class="level3">
<h3 class="anchored" data-anchor-id="results">Results</h3>
<p>The following visualization are inspired by Figure 3 and 7 of <a href="https://arxiv.org/abs/1901.11390">Burgess et al.&nbsp;(2019)</a> and mainly serve to evaluate the representation quality of the trained model.</p>
<ul>
<li><p><strong>MONet Reconstructions and Decompositions</strong>: The most intuitive visualization is to show some (arbitrarly chosen) fully reconstructed images (i.e, <code>Reconstruction mixture</code> <span class="math inline">\(\widetilde{\textbf{x}} = \sum_{k=1}^K
\textbf{m}_k \odot \widetilde{\textbf{x}}_k\)</span>) compared to the original input <span class="math inline">\(\textbf{x}\)</span> (<code>Data</code>) together with the learned segmentation masks (i.e., <code>Segmentation</code> <span class="math inline">\(\{ \textbf{m}_k \}\)</span>) of the attention network. Note that in order to visualize the segmentations in one plot, we cast the attenion masks into binary attention masks by applying <code>arg max</code> pixel-wise over all <span class="math inline">\(K\)</span> attention masks. In addition, all umasked component VAE reconstructions (i.e., <code>S(k)</code> <span class="math inline">\(\widetilde{\textbf{x}}_k\)</span>) are shown, see figure below.</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;"><img src="./img/reconstruction_and_decompositions.png" title="MONet Reconstructions and Decompositions" class="img-fluid" alt="MONet Reconstruction and Decompositions"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Figure 7 of</strong> <a href="https://arxiv.org/abs/1901.11390">Burgess et al.&nbsp;(2019)</a>: Each example shows the image fed as input data to the model, with corresponding outputs from the model. Reconstruction mixtures show sum of components from all slots, weighted by the learned masks from the attention network. Colour-coded segmentation maps summarize the attention masks <span class="math inline">\(\{\textbf{m}_k \}\)</span>. Rows labeld S1-5 show the reconstruction components of each slot.</td>
</tr>
</tbody>
</table></li>
</ul>
<div class="cell" data-execution_count="7">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>dataloader <span class="op">=</span> trained_monet.train_dataloader()</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>random_batch <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(dataloader))[<span class="dv">0</span>]</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> trained_monet.plot_reconstructions_and_decompositions(batch[<span class="dv">0</span>: <span class="dv">4</span>], <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./img/MONET_rec.png" title="Reconstructions and Decompositions" class="img-fluid figure-img"></p>
<figcaption>MONet Reconstructions and Decompositions after Train</figcaption>
</figure>
</div>
<ul>
<li><p><strong>Component VAE Results</strong>: In order to evaluate the perfomance of the component VAE, we are interested in the unmasked slot-wise reconstructions (i.e., <code>unmasked</code> refers to <span class="math inline">\(\widetilde{\textbf{x}}_k\)</span> for each slot <span class="math inline">\(k\)</span>) and the slot-wise reconstructions masked by the VAE’s reconstructed masks (i.e., <code>masked</code> refers to <span class="math inline">\(\widetilde{\textbf{m}}_k \odot
\widetilde{\textbf{x}}_k\)</span>). Ideally, masked versions capture either a single object, the background or nothing at all (representing no object), see figure below. In addition, we are going to plot the ground truth masked reconstructions (i.e., <code>gt masked</code> refers to <span class="math inline">\(\textbf{m}_k \odot \widetilde{\textbf{x}}_k\)</span>) such that the difference between <code>gt masked</code> and <code>masked</code> indicates the reconstruction error of the attention masks.</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;"><img src="./img/component_VAE_results.png" title="Component VAE Results" class="img-fluid" alt="Component VAE Results"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Figure 3 of</strong> <a href="https://arxiv.org/abs/1901.11390">Burgess et al.&nbsp;(2019)</a>: Each example shows the image fet as input data to the model, with corresponding outputs from the model. Reconstruction mixtures show sum of components from all slots, weighted by the learned masks from the attention network. Color-coded segmentation maps summarise the attention masks <span class="math inline">\(\{\textbf{m}_k\}\)</span>. Rows labeled S1-7 show the reconstruction components of each slot. Unmasked version are shown side-by-side with corresponding versions that are masked with the VAE’s reconstructed masks <span class="math inline">\(\widetilde{\textbf{m}}_k\)</span>.</td>
</tr>
</tbody>
</table></li>
</ul>
<div class="cell" data-execution_count="8">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>dataloader <span class="op">=</span> trained_monet.train_dataloader()</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>random_batch <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(dataloader))[<span class="dv">0</span>]</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> trained_monet.plot_ComponentVAE_results(batch[<span class="dv">0</span>: <span class="dv">4</span>], <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./img/MONET_CompVAE.png" title="MONet Component VAE" class="img-fluid figure-img"></p>
<figcaption>MONet Component VAE</figcaption>
</figure>
</div>
</section>
</section>
<section id="drawbacks-of-paper" class="level2">
<h2 class="anchored" data-anchor-id="drawbacks-of-paper">Drawbacks of Paper</h2>
<ul>
<li>deterministic attention mechanism implying that objective function is not a valid lower bound on the marginal likelihood (as mentioned by <a href="https://arxiv.org/abs/1907.13052">Engelcke et al.&nbsp;(2020)</a>)</li>
<li>image generation suffers from discrepancy between inferred and reconstructed masks <!-- * only works on simple images in which multiple objects of the same class occur --> <!-- * even simple images require high training times --></li>
<li>lots of hyperparameters (network architectures, <span class="math inline">\(\beta\)</span>, <span class="math inline">\(\gamma\)</span>, optimization)</li>
</ul>
</section>
<section id="acknowledgment" class="level2">
<h2 class="anchored" data-anchor-id="acknowledgment">Acknowledgment</h2>
<p>There are a lot of implementations out there that helped me very much in understanding the paper:</p>
<ul>
<li><a href="https://github.com/baudm/MONet-pytorch">Darwin Bautista’s implementation</a> includes derivation of the NLL (which in the end, I did not use for simplicity).</li>
<li><a href="https://github.com/stelzner/monet/">Karl Stelzner’s implementation</a> is kept more simplisitic and is therefore easier to understand.</li>
<li><a href="https://github.com/applied-ai-lab/genesis">Martin Engelcke, Claas Voelcker and Max Morrison</a> included an implementation of MONet in the Genesis repository.</li>
</ul>
<hr>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Encoding each segment through the same VAE can be understood as an architectural prior on common structure within individual objects.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><a href="https://arxiv.org/abs/1901.11390">Burgess et al.&nbsp;(2019)</a> do not explain why the Component VAE should also model the attention masks. Note however that this allows for better generalization, e.g., shape/class variation depends on attention mask.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>For completeness <span class="math inline">\(\textbf{c} \in \{1, \dots, K\}\)</span> denotes a categorical variable to indicate the probability that pixels belong to a particular component <span class="math inline">\(k\)</span>, i.e., <span class="math inline">\(\textbf{m}_k =
p(\textbf{c} = k)\)</span>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Philosophical note: Humans also tend to work better when focusing on one task at a time.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>This is explained in more detail in my <a href="https://borea17.github.io/paper_summaries/auto-encoding_variational_bayes">VAE</a> post. For simplicity, we are setting the number of (noise variable) samples <span class="math inline">\(L\)</span> per datapoint to 1 (see equation <span class="math inline">\(\displaystyle \widetilde{\mathcal{L}}\)</span> in <a href="https://borea17.github.io/paper_summaries/auto-encoding_variational_bayes#model-description"><em>Reparametrization Trick</em></a> paragraph). Note that <a href="https://arxiv.org/abs/1312.6114">Kingma and Welling (2013)</a> stated that in their experiments setting <span class="math inline">\(L=1\)</span> sufficed as long as the minibatch size was large enough.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Note that concatenation of masks leads to a three dimensional tensor.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/borea17\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>