<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="borea17">
<meta name="dcterms.date" content="2020-08-21">

<title>borea17 - U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../logo.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">borea17</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../paper_summaries.html" rel="" target="">
 <span class="menu-text">Paper Summaries</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../ml101.html" rel="" target="">
 <span class="menu-text">ML101</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">U-Net: Convolutional Networks for Biomedical Image Segmentation</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button" data-quarto-source-url="https://github.com/borea17/Notebooks/blob/master/03_U-Net.ipynb">View Source</a></li></ul></div></div>
                                <div class="quarto-categories">
                <div class="quarto-category">reimplementation</div>
                <div class="quarto-category">CNN</div>
                <div class="quarto-category">segmentation</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>borea17 </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">August 21, 2020</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#model-description" id="toc-model-description" class="nav-link active" data-scroll-target="#model-description">Model Description</a></li>
  <li><a href="#implementatation" id="toc-implementatation" class="nav-link" data-scroll-target="#implementatation">Implementatation</a>
  <ul class="collapse">
  <li><a href="#em-dataset" id="toc-em-dataset" class="nav-link" data-scroll-target="#em-dataset">EM Dataset</a></li>
  <li><a href="#data-augmentation" id="toc-data-augmentation" class="nav-link" data-scroll-target="#data-augmentation">Data Augmentation</a></li>
  <li><a href="#model-implementation" id="toc-model-implementation" class="nav-link" data-scroll-target="#model-implementation">Model Implementation</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<!-- nextjournal_link: "https://nextjournal.com/borea17/u-net/" -->
<p><a href="https://arxiv.org/abs/1505.04597">Ronneberger et al.&nbsp;(2015)</a> introduced a novel neural network architecture to generate better semantic segmentations (i.e., class label assigend to each pixel) in limited datasets which is a typical challenge in the area of biomedical image processing (see figure below for an example). In essence, their model consists of a U-shaped convolutional neural network (CNN) with skip connections between blocks to capture context information, while allowing for precise localizations. In addition to the network architecture, they describe some data augmentation methods to use available data more efficiently. By the time the paper was published, the proposed architecture won several segmentation challenges in the field of biomedical engineering, outperforming state-of-the-art models by a large margin. Due to its success and efficiency, U-Net has become a standard architecture when it comes to image segmentations tasks even in the non-biomedical area (e.g., <a href="https://arxiv.org/abs/1611.07004">image-to-image translation</a>, <a href="https://arxiv.org/abs/1706.03319">neural style transfer</a>, <a href="https://arxiv.org/abs/1901.11390">Multi-Objetct Network</a>).</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;"><img src="./img/semantic_segmentation.png" title="Semantic Segmentation Example" class="img-fluid" alt="Semantic Segmentation Example"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Example of a biomedical image segmentation task in which dental x-ray images should be segmented: <br> (<strong>Left</strong>) Raw dental image. <br> (<strong>Right</strong>) Ground truth segmentation, each color represents some class (e.g., red=pulp, blue=caries). <br>Taken from <a href="http://www-o.ntust.edu.tw/~cweiwang/ISBI2015/challenge2/index.html">ISBI 2015 Challenge on Computer-Automated Detection of Caries in Bitewing Radiography</a></td>
</tr>
</tbody>
</table>
<section id="model-description" class="level2">
<h2 class="anchored" data-anchor-id="model-description">Model Description</h2>
<p>U-Net builds upon the ideas of <code>Fully Convolutional Networks (FCNs) for Semantic Segmentation</code> by <a href="https://arxiv.org/abs/1411.4038">Long et al.&nbsp;(2015)</a> who successfully trained FCNs (including convolutional prediction, upsampling layers and skip connections) end-to-end (pixels-to-pixels) on semantic segmentation tasks. U-Net is basically a modified version of the FCN by making the architecture more symmetric, i.e., adding a more powerful expansive path. <a href="https://arxiv.org/abs/1505.04597">Ronneberger et al. (2015)</a> argue that this modification yields more precise segmentations due to its capacity to better propagate context information to higher resolution layers.</p>
<p><strong>FCN architecture</strong>: The main idea of the FCN architecture is to take a standard classification network (such as VGG-16), discard the final classifier layer, convert fully connected layers into convolutions (i.e., prediction layers) and add skip connections to (some) pooling layers, see figure below. The skip connections consist of a prediction (<span class="math inline">\(1 \times 1\)</span> convolutional layer with channel dimension equal to number of possible classes) and a deconvolutional (upsampling) layer.</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;"><img src="./img/FCN_example2.png" title="Example FCN Architecture" class="img-fluid" alt="Example FCN Architecture"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Example of FCN Architecture. VGG-16 net is used as feature learning part. Numbers under the cubes indicate the number of output channels. The prediction layer is itself a <span class="math inline">\(1 \times 1\)</span> convolutional layer (the final output consists only of 6 possible classes). A final softmax layer is added to output a normalized classification per pixel. Taken from <a href="https://arxiv.org/abs/1610.01732">Tai et al.&nbsp;(2017)</a></td>
</tr>
</tbody>
</table>
<p><strong>U-Net architecture</strong>: The main idea of the U-Net architecture is to build an encoder-decoder FCN with skip connections between corresponding blocks, see figure below. The left side of U-Net, i.e., <em>contractive path</em> or <em>encoder</em>, is very similar to the left side of the FC architecture above. The right side of U-Net, i.e., <em>expansive path</em> or <em>decoder</em>, differs due to its number of feature channels and the convolutional + ReLu layers. Note that the input image size is greater than the output segmentation size, i.e., the network only segments the inner part of the image<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;"><img src="./img/u_net_architecture.png" title="U-Net Architecture" class="img-fluid" alt="U-Net Architecture"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">U-Net architecture as proposed by <a href="https://arxiv.org/abs/1505.04597">Ronneberger et al.&nbsp;(2015)</a>.</td>
</tr>
</tbody>
</table>
<p><strong>Motivation</strong>: Semantic segmentation of images can be divided into two tasks</p>
<ul>
<li><strong>Context Information Retrieval</strong>: Global information about the different parts of the image, e.g., in a CNN classification network after training there might be some feature representation for <em>nose</em>, <em>eyes</em> and <em>mouth</em>. Depending on the feature combination at hand, the network may classify the image as <em>human</em> or <em>not human</em>.</li>
<li><strong>Localization of Context Information</strong>: In addition to <code>what</code>, localization ensures <code>where</code>. Semantic segmentation is only possible when content information can be localized. Note: In image classification, we are often not interested in <code>where</code><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</li>
</ul>
<p><a href="https://arxiv.org/abs/1411.4038">Long et al.&nbsp;(2015)</a> argue that CNNs during classification tasks must learn useful feature representations, i.e., classification nets are capable to solve the <em>context information retrieval</em> task. Fully connected layers are inappropriate for semantic segmentation as they throw away the principle of localization. These two arguments motivate the use of FCNs that take the feature representation part of classification nets and convert fully connected layers into convolutions. During the <em>contractive</em> path, information gets compressed into coarse appearance/context information. However, in this process the dimensionality of the input is reduced massively. Skip connections are introduced to combine coarse, semantic information of deeper layers with finer, appearance information of early layers. Thereby, the <em>localization</em> task is addressed.</p>
<p><a href="https://arxiv.org/abs/1505.04597">Ronneberger et al.&nbsp;(2015)</a> extend these ideas by essentially increasing the capacity of the decoder path. The symmetric architecture allows to combine low level feature maps (left side, fine information) with high level feature maps (right side, coarse information) more effectively such that context information can be better propagated to higher resolution layers (top right). As a result, more precise segmentations can be retrieved even with few training examples, indicating that the optimization problem is better posed in U-Nets.</p>
</section>
<section id="implementatation" class="level2">
<h2 class="anchored" data-anchor-id="implementatation">Implementatation</h2>
<p><a href="https://arxiv.org/abs/1505.04597">Ronneberger et al.&nbsp;(2015)</a> demonstrated U-Net application results for three different segmentation tasks and open-sourced their original <a href="https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/">U-Net implementation</a> (or rather the ready trained network). The whole training process and data augmentation procedures are not provided (except for overlap-tile segmentation). The following reimplementation aims to give an understanding of the whole paper (data augmentation and training process included), while being as simple as possible. Note that there are lots of open-source U-Net reimplementations out there, however most of them are already modified versions.</p>
<section id="em-dataset" class="level3">
<h3 class="anchored" data-anchor-id="em-dataset">EM Dataset</h3>
<p>Only the first task of the three different U-Net applications is reimplemented: The segmentation of neuronal structures in electron microscopic (EM) recordings. The training data consists of 30 images (<span class="math inline">\(512 \times 512\)</span> pixels with 8-bit grayscale) from the ventral nerve cord of some species of fruit flies together with the corresponding 30 binary segmentation masks (white pixels for segmented objects, black for the rest), see gif below. The dataset formed part of the 2D EM segmentation challenge at the ISBI 2012 conference. Although the workshop competition is done, the challenge remains open for new contributions. Further details about the data can be found at the <a href="http://brainiac2.mit.edu/isbi_challenge/">ISBI Challenge website</a>, where also the training and test data can be downloaded (after registration).</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;"><img src="./img/EM_dataset.gif" title="EM training data" class="img-fluid" alt="EM training data."></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">EM training data. Taken from <a href="http://brainiac2.mit.edu/isbi_challenge/">ISBI Challenge</a>.</td>
</tr>
</tbody>
</table>
<p>The following function can be used to load the training dataset.</p>
<div class="cell" data-execution_count="2">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms <span class="im">as</span> transforms</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> load_dataset():</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    num_img, img_size <span class="op">=</span> <span class="dv">30</span>, <span class="dv">512</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialize</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    imgs <span class="op">=</span> torch.zeros(num_img, <span class="dv">1</span>, img_size, img_size)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> torch.zeros(num_img, <span class="dv">1</span>, img_size, img_size)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># fill tensors with data</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> index <span class="kw">in</span> <span class="bu">range</span>(num_img):</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        cur_name <span class="op">=</span> <span class="bu">str</span>(index) <span class="op">+</span> <span class="st">'.png'</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        img_frame <span class="op">=</span> Image.<span class="bu">open</span>(<span class="st">'./Dataset/train/image/'</span> <span class="op">+</span> cur_name)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        label_frame <span class="op">=</span> Image.<span class="bu">open</span>(<span class="st">'./Dataset/train/label/'</span> <span class="op">+</span> cur_name)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        imgs[index] <span class="op">=</span> transforms.ToTensor()(img_frame).<span class="bu">type</span>(torch.float32)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        labels[index] <span class="op">=</span> transforms.ToTensor()(label_frame).<span class="bu">type</span>(torch.float32)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> imgs, labels</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>imgs, labels <span class="op">=</span> load_dataset()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="data-augmentation" class="level3">
<h3 class="anchored" data-anchor-id="data-augmentation">Data Augmentation</h3>
<p>Training neural networks on image data typically requires large amounts of data to make the model robust (i.e., avoid overfitting) and accurate (i.e., avoid underfitting). However, data scarcity is a common problem in biomedical segmentation tasks, since labeling is expensive and time consuming. In such cases, <strong>data augmentation</strong> offers a solution by generating additional data (using plausible transformations) to expand the training dataset. In most image segmentation tasks the function to be learned has some transformation-invariance properties (e.g., translating the input should result in a translated output). The data augmentation applied by <a href="https://arxiv.org/abs/1505.04597">Ronneberger et al.&nbsp;(2015)</a> can be divided into four parts:</p>
<ul>
<li><p><strong>Overlap-tile strategy</strong> is used to divide an arbitrary large image into several overlaping parts (each forming an input and label to the training algorithm). Remind that the input to the neural network is greater than the output, in case of the EM dataset the input is even greater than the whole image. Therefore, <a href="https://arxiv.org/abs/1505.04597">Ronneberger et al. (2015)</a> expand the images by mirroring at the sides. The overlap-tile strategy is shown below. Depending on the <code>stride</code> (i.e., how much the next rectangle is shifted to the right), the training dataset is enlarged by a factor greater than 4.</p>
<table class="table">
<colgroup>
<col style="width: 100%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><img src="./img/overlap_tile_self.png" title="Overlap-Tile Strategy" class="img-fluid" alt="Overlap-Tile Strategy"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Overlap-Tile Strategy for seamless segmentation of arbitrary large images. Blue area depicts input to neural network, yellow area corresponds to the prediction area. Missing input is extrapolated by mirroring (white lines). The number of tiles depends on the <code>stride</code> length (here: <code>stride=124</code>). Image created with <code>visualize_overlap_tile_strategy</code> (code presented at the end of this section).</td>
</tr>
</tbody>
</table></li>
<li><p><strong>Affine transformations</strong> are mathematically defined as geometric transformations preserving lines and parallelisms, e.g., scaling, translation, rotation, reflection or any mix of them. <a href="https://arxiv.org/abs/1505.04597">Ronneberger et al.&nbsp;(2015)</a> state that in case of microscopical image data mainly translation and rotation invariance (as affine transformation invariances) are desired properties of the resulting function. Note that the overlap-tile strategy itself leads to some translation invariance.</p>
<table class="table">
<colgroup>
<col style="width: 100%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><img src="./img/affine_transformation_with_grid.png" title="Affine Transformation Visualization" class="img-fluid" alt="Affine Transformation Visualization"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Affine transformation visualization. Left side shows input and label data before transformation is applied. Right side shows the corresponding data after random affine transformation (random rotation and shifting). The grid is artificially added to emphasize that image and label are transformed in the same way. Image created with <code>visualize_data_augmentation</code> (code presented at the end of this section).</td>
</tr>
</tbody>
</table></li>
<li><p><strong>Elastic deformations</strong> are basically distinct affine transformations for each pixel. The term is probably derived from physics in which an elastic deformation describes a temporary change in shape of an elastic material (due to induced force). The transformation result looks similar to the physics phenomenon, see image below. <a href="https://arxiv.org/abs/1505.04597">Ronneberger et al. (2015)</a> noted that elastic deformations seem to be a key concept for successfully training with few samples. A possible reason may be that the modelâ€™s generalization capabilities improve more by elastic deformations since the resulting images have more variability than with coherent affine transformations.</p>
<table class="table">
<colgroup>
<col style="width: 100%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><img src="./img/deformation_with_grid2.png" title="Elastic Deformation Visualization" class="img-fluid" alt="Elastic Deformation Visualization"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Elastic deformation visualization. Left side shows input and label data before deformation is applied. Right side shows the corresponding data after deformation. The grid is artificially added to emphasize that image and label are deformed in the same way. Image created with <code>visualize_data_augmentation</code> (code presented at the end of this section).</td>
</tr>
</tbody>
</table>
<p>Implementing elastic deformations basically consists of generating random displacement fields, convolving these with a Gaussian filter for smoothening, scaling the result by a predefined factor to control the intensity and then computing the new pixel values for each displacement vector (using interpolation within the old grid), see Best Practices for CNNs by <a href="https://www.researchgate.net/publication/220860992_Best_Practices_for_Convolutional_Neural_Networks_Applied_to_Visual_Document_Analysis">Simard et al. (2003)</a> for more details. <!-- Note that [Ronneberger et al. --> <!-- (2015)](https://arxiv.org/abs/1505.04597) use a slightly different --> <!-- method to generate elastic deformations by directly sampling from a Gaussian --> <!-- distribution instead of convolving uniform sampled random vectors --> <!-- with a Gaussian. --></p></li>
<li><p><strong>Color variations</strong> or in this case rather <strong>gray value variations</strong> in the input image should make the network invariant to small color changes. This can easily be implemented by adding Gaussian noise (other distributions may also be possible) to the input image, see image below.</p>
<table class="table">
<colgroup>
<col style="width: 100%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><img src="./img/gray_variation.png" title="Gray Value Variation Visualization" class="img-fluid" alt="Gray Value Variation Visualization"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Gray value variation visualization. Left side shows input image before noise is applied. Right side shows the corresponding data after transformation (segmentation mask does not change). Image created with <code>visualize_data_augmentation</code> (code presented at the end of this section).</td>
</tr>
</tbody>
</table></li>
</ul>
<p>The whole data augmentation process is put into a self written Pytorch <code>Dataset</code> class, see code below. Note that while this class includes all described transformations (<em>affine transformation</em>, <em>elastic deformation</em> and <em>gray value variation</em>), in the <code>__get_item__</code> method only <code>elastic_deform</code> is applied to speed up the training process<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. However, if you want to create a more sophisticated data augmentation process, you can easily add the other transformations in the <code>__get_item__</code> method.</p>
<div class="cell" data-execution_count="3">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.ndimage.interpolation <span class="im">import</span> map_coordinates</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.signal <span class="im">import</span> convolve2d</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms.functional <span class="im">as</span> TF</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EM_Dataset(Dataset):</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""EM Dataset (from ISBI 2012) to train U-Net on including data</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co">    augmentation as proposed by Ronneberger et al. (2015)</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co">        imgs (tensor): torch tensor containing input images [1, 512, 512]</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co">        labels (tensor): torch tensor containing segmented images [1, 512, 512]</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="co">        stride (int): stride that is used for overlap-tile strategy,</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co">            Note: stride must be chosen such that all labels are retrieved</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co">        transformation (bool): transform should be applied (True) or not (False)</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="co">    ------- transformation related -------</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co">        probability (float): probability that transformation is applied</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="co">        alpha (float): intensity of elastic deformation</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co">        sigma (float): std dev. of Gaussian kernel, i.e., smoothing parameter</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="co">        kernel dim (int): kernel size is [kernel_dim, kernel_dim]</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, imgs, labels, stride, transformation<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>                 probability<span class="op">=</span><span class="va">None</span>, alpha<span class="op">=</span><span class="va">None</span>, sigma<span class="op">=</span><span class="va">None</span>, kernel_dim<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> <span class="bu">isinstance</span>(stride, <span class="bu">int</span>) <span class="kw">and</span> stride <span class="op">&lt;=</span> <span class="dv">124</span> <span class="kw">and</span> <span class="op">\</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>          <span class="bu">round</span>((<span class="dv">512</span><span class="op">-</span><span class="dv">388</span>)<span class="op">/</span>stride) <span class="op">==</span> (<span class="dv">512</span><span class="op">-</span><span class="dv">388</span>)<span class="op">/</span>stride</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.orig_imgs <span class="op">=</span> imgs</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.imgs <span class="op">=</span> EM_Dataset._extrapolate_by_mirroring(imgs)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.labels <span class="op">=</span> labels</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.stride <span class="op">=</span> stride</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transformation <span class="op">=</span> transformation</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> transformation:</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>            <span class="cf">assert</span> <span class="dv">0</span> <span class="op">&lt;=</span> probability <span class="op">&lt;=</span> <span class="dv">1</span></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.probability <span class="op">=</span> probability</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.alpha <span class="op">=</span> alpha</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.kernel <span class="op">=</span> EM_Dataset._create_gaussian_kernel(kernel_dim, sigma)</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, index):</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""images and labels are divided into several overlaping parts using the</span></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a><span class="co">        overlap-tile strategy</span></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>        number_of_tiles_1D <span class="op">=</span> (<span class="dv">1</span> <span class="op">+</span> <span class="bu">int</span>((<span class="dv">512</span> <span class="op">-</span> <span class="dv">388</span>)<span class="op">/</span><span class="va">self</span>.stride))</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>        number_of_tiles_2D <span class="op">=</span> number_of_tiles_1D<span class="op">**</span><span class="dv">2</span></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>        img_index <span class="op">=</span> <span class="bu">int</span>(index<span class="op">/</span>number_of_tiles_2D)</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># tile indexes of image</span></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>        tile_index <span class="op">=</span> (index <span class="op">%</span> number_of_tiles_2D)</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>        tile_index_x <span class="op">=</span> (tile_index <span class="op">%</span> number_of_tiles_1D) <span class="op">*</span> <span class="va">self</span>.stride</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>        tile_index_y <span class="op">=</span> <span class="bu">int</span>(tile_index <span class="op">/</span> number_of_tiles_1D) <span class="op">*</span> <span class="va">self</span>.stride</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>        img <span class="op">=</span> <span class="va">self</span>.imgs[img_index, :,</span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>                        tile_index_y:tile_index_y <span class="op">+</span> <span class="dv">572</span>,</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>                        tile_index_x:tile_index_x <span class="op">+</span> <span class="dv">572</span>]</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>        label <span class="op">=</span> <span class="va">self</span>.labels[img_index, :,</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>                            tile_index_y: tile_index_y <span class="op">+</span> <span class="dv">388</span>,</span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>                            tile_index_x: tile_index_x <span class="op">+</span> <span class="dv">388</span>]</span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.transformation:</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> np.random.random() <span class="op">&gt;</span> <span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.probability:</span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>                img, label <span class="op">=</span> EM_Dataset.elastic_deform(img, label, <span class="va">self</span>.alpha,</span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>                                                       <span class="va">self</span>.kernel)</span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (img, label)</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a>        number_of_imgs <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.imgs)</span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a>        number_of_tiles <span class="op">=</span> (<span class="dv">1</span> <span class="op">+</span> <span class="bu">int</span>((<span class="dv">512</span> <span class="op">-</span> <span class="dv">388</span>)<span class="op">/</span><span class="va">self</span>.stride))<span class="op">**</span><span class="dv">2</span></span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> number_of_imgs <span class="op">*</span> number_of_tiles</span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> gray_value_variations(image, sigma):</span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""applies gray value variations by adding Gaussian noise</span></span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a><span class="co">            image (torch tensor): extrapolated image tensor [1, 572, 572]</span></span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a><span class="co">            sigma (float): std. dev. of Gaussian distribution</span></span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a><span class="co">            image (torch tensor): image tensor w. gray value var. [1, 572, 572]</span></span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a>        <span class="co"># see https://stats.stackexchange.com/a/383976</span></span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a>        noise <span class="op">=</span> torch.randn(image.shape, dtype<span class="op">=</span>torch.float32) <span class="op">*</span> sigma</span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image <span class="op">+</span> noise</span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> affine_transform(image, label, angle, translate):</span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""applies random affine translations and rotation on image and label</span></span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a><span class="co">            image (torch tensor): extrapolated image tensor [1, 572, 572]</span></span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a><span class="co">            label (torch tensor): label tensor [1, 388, 388]</span></span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a><span class="co">            angle (float): rotation angle</span></span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a><span class="co">            translate (list): entries correspond to horizontal and vertical shift</span></span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a><span class="co">            image (torch tensor): transformed image tensor [1, 572, 572]</span></span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a><span class="co">            label (torch tensor): transformed label tensor [1, 388, 388]</span></span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a>        <span class="co"># transform to PIL</span></span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> transforms.ToPILImage()(image[<span class="dv">0</span>])</span>
<span id="cb2-103"><a href="#cb2-103" aria-hidden="true" tabindex="-1"></a>        label <span class="op">=</span> transforms.ToPILImage()(label[<span class="dv">0</span>])</span>
<span id="cb2-104"><a href="#cb2-104" aria-hidden="true" tabindex="-1"></a>        <span class="co"># apply affine transformation</span></span>
<span id="cb2-105"><a href="#cb2-105" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> TF.affine(image, angle<span class="op">=</span>angle, translate<span class="op">=</span>translate,</span>
<span id="cb2-106"><a href="#cb2-106" aria-hidden="true" tabindex="-1"></a>                          scale<span class="op">=</span><span class="dv">1</span>, shear<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb2-107"><a href="#cb2-107" aria-hidden="true" tabindex="-1"></a>        label <span class="op">=</span> TF.affine(label, angle<span class="op">=</span>angle, translate<span class="op">=</span>translate,</span>
<span id="cb2-108"><a href="#cb2-108" aria-hidden="true" tabindex="-1"></a>                          scale<span class="op">=</span><span class="dv">1</span>, shear<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb2-109"><a href="#cb2-109" aria-hidden="true" tabindex="-1"></a>        <span class="co"># transform back to tensor</span></span>
<span id="cb2-110"><a href="#cb2-110" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> transforms.ToTensor()(np.array(image))</span>
<span id="cb2-111"><a href="#cb2-111" aria-hidden="true" tabindex="-1"></a>        label <span class="op">=</span> transforms.ToTensor()(np.array(label))</span>
<span id="cb2-112"><a href="#cb2-112" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image, label</span>
<span id="cb2-113"><a href="#cb2-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-114"><a href="#cb2-114" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb2-115"><a href="#cb2-115" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> elastic_deform(image, label, alpha, gaussian_kernel):</span>
<span id="cb2-116"><a href="#cb2-116" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""apply smooth elastic deformation on image and label data as</span></span>
<span id="cb2-117"><a href="#cb2-117" aria-hidden="true" tabindex="-1"></a><span class="co">        described in</span></span>
<span id="cb2-118"><a href="#cb2-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-119"><a href="#cb2-119" aria-hidden="true" tabindex="-1"></a><span class="co">        [Simard2003] "Best Practices for Convolutional Neural Networks applied</span></span>
<span id="cb2-120"><a href="#cb2-120" aria-hidden="true" tabindex="-1"></a><span class="co">        to Visual Document Analysis"</span></span>
<span id="cb2-121"><a href="#cb2-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-122"><a href="#cb2-122" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb2-123"><a href="#cb2-123" aria-hidden="true" tabindex="-1"></a><span class="co">            image (torch tensor): extrapolated image tensor [1, 572, 572]</span></span>
<span id="cb2-124"><a href="#cb2-124" aria-hidden="true" tabindex="-1"></a><span class="co">            label (torch tensor): label tensor [1, 388, 388]</span></span>
<span id="cb2-125"><a href="#cb2-125" aria-hidden="true" tabindex="-1"></a><span class="co">            alpha (float): intensity of transformation</span></span>
<span id="cb2-126"><a href="#cb2-126" aria-hidden="true" tabindex="-1"></a><span class="co">            gaussian_kernel (np array): gaussian kernel used for smoothing</span></span>
<span id="cb2-127"><a href="#cb2-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-128"><a href="#cb2-128" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb2-129"><a href="#cb2-129" aria-hidden="true" tabindex="-1"></a><span class="co">            deformed_img (torch tensor): deformed image tensor [1, 572, 572]</span></span>
<span id="cb2-130"><a href="#cb2-130" aria-hidden="true" tabindex="-1"></a><span class="co">            deformed_label (torch tensor): deformed label tensor [1, 388, 388]</span></span>
<span id="cb2-131"><a href="#cb2-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-132"><a href="#cb2-132" aria-hidden="true" tabindex="-1"></a><span class="co">        code is adapted from https://github.com/vsvinayak/mnist-helper</span></span>
<span id="cb2-133"><a href="#cb2-133" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb2-134"><a href="#cb2-134" aria-hidden="true" tabindex="-1"></a>        <span class="co"># generate standard coordinate grids</span></span>
<span id="cb2-135"><a href="#cb2-135" aria-hidden="true" tabindex="-1"></a>        x_i, y_i <span class="op">=</span> np.meshgrid(np.arange(<span class="dv">572</span>), np.arange(<span class="dv">572</span>))</span>
<span id="cb2-136"><a href="#cb2-136" aria-hidden="true" tabindex="-1"></a>        x_l, y_l <span class="op">=</span> np.meshgrid(np.arange(<span class="dv">388</span>), np.arange(<span class="dv">388</span>))</span>
<span id="cb2-137"><a href="#cb2-137" aria-hidden="true" tabindex="-1"></a>        <span class="co"># generate random displacement fields (uniform distribution [-1, 1])</span></span>
<span id="cb2-138"><a href="#cb2-138" aria-hidden="true" tabindex="-1"></a>        dx <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>np.random.rand(<span class="op">*</span>x_i.shape) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb2-139"><a href="#cb2-139" aria-hidden="true" tabindex="-1"></a>        dy <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>np.random.rand(<span class="op">*</span>y_i.shape) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb2-140"><a href="#cb2-140" aria-hidden="true" tabindex="-1"></a>        <span class="co"># smooth by convolving with gaussian kernel</span></span>
<span id="cb2-141"><a href="#cb2-141" aria-hidden="true" tabindex="-1"></a>        dx <span class="op">=</span> alpha <span class="op">*</span> convolve2d(dx, gaussian_kernel, mode<span class="op">=</span><span class="st">'same'</span>)</span>
<span id="cb2-142"><a href="#cb2-142" aria-hidden="true" tabindex="-1"></a>        dy <span class="op">=</span> alpha <span class="op">*</span> convolve2d(dy, gaussian_kernel, mode<span class="op">=</span><span class="st">'same'</span>)</span>
<span id="cb2-143"><a href="#cb2-143" aria-hidden="true" tabindex="-1"></a>        <span class="co"># one dimensional coordinates (neccessary for map_coordinates)</span></span>
<span id="cb2-144"><a href="#cb2-144" aria-hidden="true" tabindex="-1"></a>        x_img <span class="op">=</span> np.reshape(x_i <span class="op">+</span> dx, (<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb2-145"><a href="#cb2-145" aria-hidden="true" tabindex="-1"></a>        y_img <span class="op">=</span> np.reshape(y_i <span class="op">+</span> dy, (<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb2-146"><a href="#cb2-146" aria-hidden="true" tabindex="-1"></a>        x_label <span class="op">=</span> np.reshape(x_l <span class="op">+</span> dx[<span class="dv">92</span>:<span class="dv">480</span>, <span class="dv">92</span>:<span class="dv">480</span>], (<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb2-147"><a href="#cb2-147" aria-hidden="true" tabindex="-1"></a>        y_label <span class="op">=</span> np.reshape(y_l <span class="op">+</span> dy[<span class="dv">92</span>:<span class="dv">480</span>, <span class="dv">92</span>:<span class="dv">480</span>], (<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb2-148"><a href="#cb2-148" aria-hidden="true" tabindex="-1"></a>        <span class="co"># deformation using map_coordinates interpolation (spline not bicubic)</span></span>
<span id="cb2-149"><a href="#cb2-149" aria-hidden="true" tabindex="-1"></a>        deformed_img <span class="op">=</span> map_coordinates(image[<span class="dv">0</span>], [y_img, x_img], order<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb2-150"><a href="#cb2-150" aria-hidden="true" tabindex="-1"></a>                                       mode<span class="op">=</span><span class="st">'reflect'</span>)</span>
<span id="cb2-151"><a href="#cb2-151" aria-hidden="true" tabindex="-1"></a>        deformed_label <span class="op">=</span> map_coordinates(label[<span class="dv">0</span>], [y_label, x_label], order<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb2-152"><a href="#cb2-152" aria-hidden="true" tabindex="-1"></a>                                         mode<span class="op">=</span><span class="st">'reflect'</span>)</span>
<span id="cb2-153"><a href="#cb2-153" aria-hidden="true" tabindex="-1"></a>        <span class="co"># reshape into desired shape and cast to tensor</span></span>
<span id="cb2-154"><a href="#cb2-154" aria-hidden="true" tabindex="-1"></a>        deformed_img <span class="op">=</span> torch.from_numpy(deformed_img.reshape(image.shape))</span>
<span id="cb2-155"><a href="#cb2-155" aria-hidden="true" tabindex="-1"></a>        deformed_label <span class="op">=</span> torch.from_numpy(deformed_label.reshape(label.shape))</span>
<span id="cb2-156"><a href="#cb2-156" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> deformed_img, deformed_label</span>
<span id="cb2-157"><a href="#cb2-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-158"><a href="#cb2-158" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb2-159"><a href="#cb2-159" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _extrapolate_by_mirroring(data):</span>
<span id="cb2-160"><a href="#cb2-160" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""increase data by mirroring (needed for overlap-tile strategy)</span></span>
<span id="cb2-161"><a href="#cb2-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-162"><a href="#cb2-162" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb2-163"><a href="#cb2-163" aria-hidden="true" tabindex="-1"></a><span class="co">            data (torch tensor): shape [num_samples, 1, 512, 512]</span></span>
<span id="cb2-164"><a href="#cb2-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-165"><a href="#cb2-165" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb2-166"><a href="#cb2-166" aria-hidden="true" tabindex="-1"></a><span class="co">            extrapol_data (torch tensor): shape [num_samples, 1, 696, 696]</span></span>
<span id="cb2-167"><a href="#cb2-167" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb2-168"><a href="#cb2-168" aria-hidden="true" tabindex="-1"></a>        num_samples <span class="op">=</span> <span class="bu">len</span>(data)</span>
<span id="cb2-169"><a href="#cb2-169" aria-hidden="true" tabindex="-1"></a>        extrapol_data <span class="op">=</span> torch.zeros(num_samples, <span class="dv">1</span>, <span class="dv">696</span>, <span class="dv">696</span>)</span>
<span id="cb2-170"><a href="#cb2-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-171"><a href="#cb2-171" aria-hidden="true" tabindex="-1"></a>        <span class="co"># put data into center of extrapol data</span></span>
<span id="cb2-172"><a href="#cb2-172" aria-hidden="true" tabindex="-1"></a>        extrapol_data[:,:, <span class="dv">92</span>:<span class="dv">92</span><span class="op">+</span><span class="dv">512</span>, <span class="dv">92</span>:<span class="dv">92</span><span class="op">+</span><span class="dv">512</span>] <span class="op">=</span> data</span>
<span id="cb2-173"><a href="#cb2-173" aria-hidden="true" tabindex="-1"></a>        <span class="co"># mirror left</span></span>
<span id="cb2-174"><a href="#cb2-174" aria-hidden="true" tabindex="-1"></a>        extrapol_data[:,:, <span class="dv">92</span>:<span class="dv">92</span><span class="op">+</span><span class="dv">512</span>, <span class="dv">0</span>:<span class="dv">92</span>] <span class="op">=</span> data[:,:,:,<span class="dv">0</span>:<span class="dv">92</span>].flip(<span class="dv">3</span>)</span>
<span id="cb2-175"><a href="#cb2-175" aria-hidden="true" tabindex="-1"></a>        <span class="co"># mirror right</span></span>
<span id="cb2-176"><a href="#cb2-176" aria-hidden="true" tabindex="-1"></a>        extrapol_data[:,:, <span class="dv">92</span>:<span class="dv">92</span><span class="op">+</span><span class="dv">512</span>, <span class="dv">92</span><span class="op">+</span><span class="dv">512</span>::] <span class="op">=</span> data[:,:,:,<span class="op">-</span><span class="dv">92</span>::].flip(<span class="dv">3</span>)</span>
<span id="cb2-177"><a href="#cb2-177" aria-hidden="true" tabindex="-1"></a>        <span class="co"># mirror top</span></span>
<span id="cb2-178"><a href="#cb2-178" aria-hidden="true" tabindex="-1"></a>        extrapol_data[:,:, <span class="dv">0</span>:<span class="dv">92</span>,:] <span class="op">=</span> extrapol_data[:,:,<span class="dv">92</span>:<span class="dv">92</span><span class="op">+</span><span class="dv">92</span>,:].flip(<span class="dv">2</span>)</span>
<span id="cb2-179"><a href="#cb2-179" aria-hidden="true" tabindex="-1"></a>        <span class="co"># mirror buttom</span></span>
<span id="cb2-180"><a href="#cb2-180" aria-hidden="true" tabindex="-1"></a>        extrapol_data[:,:, <span class="dv">92</span><span class="op">+</span><span class="dv">512</span>::,:] <span class="op">=</span> extrapol_data[:,:, <span class="dv">512</span>:<span class="dv">512</span><span class="op">+</span><span class="dv">92</span>,:].flip(<span class="dv">2</span>)</span>
<span id="cb2-181"><a href="#cb2-181" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> extrapol_data</span>
<span id="cb2-182"><a href="#cb2-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-183"><a href="#cb2-183" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb2-184"><a href="#cb2-184" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _create_gaussian_kernel(kernel_dim, sigma):</span>
<span id="cb2-185"><a href="#cb2-185" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""returns a 2D Gaussian kernel with the standard deviation</span></span>
<span id="cb2-186"><a href="#cb2-186" aria-hidden="true" tabindex="-1"></a><span class="co">        denoted by sigma</span></span>
<span id="cb2-187"><a href="#cb2-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-188"><a href="#cb2-188" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb2-189"><a href="#cb2-189" aria-hidden="true" tabindex="-1"></a><span class="co">            kernel_dim (int): kernel size will be [kernel_dim, kernel_dim]</span></span>
<span id="cb2-190"><a href="#cb2-190" aria-hidden="true" tabindex="-1"></a><span class="co">            sigma (float): std dev of Gaussian (smoothing parameter)</span></span>
<span id="cb2-191"><a href="#cb2-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-192"><a href="#cb2-192" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb2-193"><a href="#cb2-193" aria-hidden="true" tabindex="-1"></a><span class="co">            gaussian_kernel (numpy array): centered gaussian kernel</span></span>
<span id="cb2-194"><a href="#cb2-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-195"><a href="#cb2-195" aria-hidden="true" tabindex="-1"></a><span class="co">        code is adapted from https://github.com/vsvinayak/mnist-helper</span></span>
<span id="cb2-196"><a href="#cb2-196" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb2-197"><a href="#cb2-197" aria-hidden="true" tabindex="-1"></a>        <span class="co"># check if the dimension is odd</span></span>
<span id="cb2-198"><a href="#cb2-198" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> kernel_dim <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb2-199"><a href="#cb2-199" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Kernel dimension should be odd"</span>)</span>
<span id="cb2-200"><a href="#cb2-200" aria-hidden="true" tabindex="-1"></a>        <span class="co"># initialize the kernel</span></span>
<span id="cb2-201"><a href="#cb2-201" aria-hidden="true" tabindex="-1"></a>        kernel <span class="op">=</span> np.zeros((kernel_dim, kernel_dim), dtype<span class="op">=</span>np.float16)</span>
<span id="cb2-202"><a href="#cb2-202" aria-hidden="true" tabindex="-1"></a>        <span class="co"># calculate the center point</span></span>
<span id="cb2-203"><a href="#cb2-203" aria-hidden="true" tabindex="-1"></a>        center <span class="op">=</span> kernel_dim<span class="op">/</span><span class="dv">2</span></span>
<span id="cb2-204"><a href="#cb2-204" aria-hidden="true" tabindex="-1"></a>        <span class="co"># calculate the variance</span></span>
<span id="cb2-205"><a href="#cb2-205" aria-hidden="true" tabindex="-1"></a>        variance <span class="op">=</span> sigma <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb2-206"><a href="#cb2-206" aria-hidden="true" tabindex="-1"></a>        <span class="co"># calculate the normalization coefficeint</span></span>
<span id="cb2-207"><a href="#cb2-207" aria-hidden="true" tabindex="-1"></a>        coeff <span class="op">=</span> <span class="fl">1.</span> <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> variance)</span>
<span id="cb2-208"><a href="#cb2-208" aria-hidden="true" tabindex="-1"></a>        <span class="co"># create the kernel</span></span>
<span id="cb2-209"><a href="#cb2-209" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, kernel_dim):</span>
<span id="cb2-210"><a href="#cb2-210" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> y <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, kernel_dim):</span>
<span id="cb2-211"><a href="#cb2-211" aria-hidden="true" tabindex="-1"></a>                x_val <span class="op">=</span> <span class="bu">abs</span>(x <span class="op">-</span> center)</span>
<span id="cb2-212"><a href="#cb2-212" aria-hidden="true" tabindex="-1"></a>                y_val <span class="op">=</span> <span class="bu">abs</span>(y <span class="op">-</span> center)</span>
<span id="cb2-213"><a href="#cb2-213" aria-hidden="true" tabindex="-1"></a>                numerator <span class="op">=</span> x_val<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> y_val<span class="op">**</span><span class="dv">2</span></span>
<span id="cb2-214"><a href="#cb2-214" aria-hidden="true" tabindex="-1"></a>                denom <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>variance</span>
<span id="cb2-215"><a href="#cb2-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-216"><a href="#cb2-216" aria-hidden="true" tabindex="-1"></a>                kernel[x,y] <span class="op">=</span> coeff <span class="op">*</span> np.exp(<span class="op">-</span><span class="fl">1.</span> <span class="op">*</span> numerator<span class="op">/</span>denom)</span>
<span id="cb2-217"><a href="#cb2-217" aria-hidden="true" tabindex="-1"></a>        <span class="co"># normalise it</span></span>
<span id="cb2-218"><a href="#cb2-218" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> kernel<span class="op">/</span><span class="bu">sum</span>(<span class="bu">sum</span>(kernel))</span>
<span id="cb2-219"><a href="#cb2-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-220"><a href="#cb2-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-221"><a href="#cb2-221" aria-hidden="true" tabindex="-1"></a><span class="co"># generate datasets</span></span>
<span id="cb2-222"><a href="#cb2-222" aria-hidden="true" tabindex="-1"></a>stride <span class="op">=</span> <span class="dv">124</span></span>
<span id="cb2-223"><a href="#cb2-223" aria-hidden="true" tabindex="-1"></a>whole_dataset <span class="op">=</span> EM_Dataset(imgs, labels, stride<span class="op">=</span>stride,</span>
<span id="cb2-224"><a href="#cb2-224" aria-hidden="true" tabindex="-1"></a>                           transformation<span class="op">=</span><span class="va">True</span>, probability<span class="op">=</span><span class="fl">0.5</span>, alpha<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb2-225"><a href="#cb2-225" aria-hidden="true" tabindex="-1"></a>                           sigma<span class="op">=</span><span class="dv">5</span>, kernel_dim<span class="op">=</span><span class="dv">25</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The visualization functions used to generate the images in this section are provided below:</p>
<div class="cell" data-execution_count="4">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> visualize_overlap_tile_strategy(dataset, img_index, tile_indexes):</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># compute tiling data</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    number_of_tiles_1D <span class="op">=</span> (<span class="dv">1</span> <span class="op">+</span> <span class="bu">int</span>((<span class="dv">512</span> <span class="op">-</span> <span class="dv">388</span>)<span class="op">/</span>dataset.stride))</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    number_of_tiles_2D <span class="op">=</span> number_of_tiles_1D<span class="op">**</span><span class="dv">2</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># original image [1, 512, 512]</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    orig_img <span class="op">=</span> dataset.orig_imgs[img_index]</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># extrapolated image [1, 696, 696]</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    extrapol_img <span class="op">=</span> dataset.imgs[img_index]</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># start plotting</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">7</span>))</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># original image</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">1</span>, <span class="bu">len</span>(tile_indexes) <span class="op">+</span> <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    plt.imshow(transforms.ToPILImage()(orig_img), cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Original Image'</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># extrapolated image with bounding boxes and mirror lines for tile_indexes</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> index, tile_index <span class="kw">in</span> <span class="bu">enumerate</span>(tile_indexes):</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        plt.subplot(<span class="dv">1</span>, <span class="bu">len</span>(tile_indexes) <span class="op">+</span> <span class="dv">1</span>, <span class="dv">2</span> <span class="op">+</span> index)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        plt.imshow(transforms.ToPILImage()(extrapol_img), cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># calculate tile index x and y</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>        tile_ix <span class="op">=</span> (tile_index <span class="op">%</span> number_of_tiles_1D) <span class="op">*</span> dataset.stride</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>        tile_iy <span class="op">=</span> <span class="bu">int</span>(tile_index <span class="op">/</span> number_of_tiles_1D) <span class="op">*</span> dataset.stride</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># add focus of current input tile</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>        plt.plot([tile_ix, tile_ix <span class="op">+</span> <span class="dv">572</span>, tile_ix <span class="op">+</span> <span class="dv">572</span>, tile_ix, tile_ix],</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>                 [tile_iy, tile_iy, tile_iy <span class="op">+</span> <span class="dv">572</span>, tile_iy <span class="op">+</span> <span class="dv">572</span>, tile_iy],</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>                 <span class="st">'blue'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># add focus of current segmentation mask</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>        tile_ix, tile_iy <span class="op">=</span> tile_ix <span class="op">+</span> <span class="dv">92</span>, tile_iy <span class="op">+</span> <span class="dv">92</span></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>        plt.plot([tile_ix, tile_ix <span class="op">+</span> <span class="dv">388</span>, tile_ix <span class="op">+</span> <span class="dv">388</span>, tile_ix, tile_ix],</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>                 [tile_iy, tile_iy, tile_iy <span class="op">+</span> <span class="dv">388</span>, tile_iy <span class="op">+</span> <span class="dv">388</span>, tile_iy],</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>                 <span class="st">'yellow'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># add mirror lines</span></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>        plt.vlines([<span class="dv">92</span>, <span class="dv">604</span>], <span class="dv">0</span>, <span class="dv">696</span>, <span class="st">'white'</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>        plt.hlines([<span class="dv">92</span>, <span class="dv">604</span>], <span class="dv">0</span>, <span class="dv">696</span>, <span class="st">'white'</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>        plt.title(<span class="st">'Extrapolated Image, Tile '</span><span class="op">+</span> <span class="bu">str</span>(tile_index <span class="op">+</span> <span class="dv">1</span>) <span class="op">+</span> <span class="st">'/'</span> <span class="op">+</span></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>                  <span class="bu">str</span>(number_of_tiles_2D))</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>        plt.xlim(<span class="dv">0</span>, <span class="dv">696</span>)</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>        plt.ylim(<span class="dv">696</span>, <span class="dv">0</span>)</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span></span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> visualize_data_augmentation(dataset, index, show_grid, kind):</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get untransformed img, label</span></span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>    dataset.transformation <span class="op">=</span> <span class="va">False</span></span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>    img, label <span class="op">=</span> dataset[index]</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># copy image (since it may be modified)</span></span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>    cur_img <span class="op">=</span> img.clone().numpy()</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>    cur_label <span class="op">=</span> label.clone().numpy()</span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> show_grid:</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># modify image to include outer grid (outside of label)</span></span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>        cur_img[<span class="dv">0</span>, <span class="dv">0</span>:<span class="dv">91</span>:<span class="dv">25</span>] <span class="op">=</span> <span class="fl">10.0</span></span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>        cur_img[<span class="dv">0</span>, <span class="dv">480</span>::<span class="dv">25</span>] <span class="op">=</span> <span class="fl">10.0</span></span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>        cur_img[<span class="dv">0</span>, :, <span class="dv">0</span>:<span class="dv">91</span>:<span class="dv">25</span>] <span class="op">=</span> <span class="fl">10.0</span></span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>        cur_img[<span class="dv">0</span>, :, <span class="dv">480</span>::<span class="dv">25</span>] <span class="op">=</span> <span class="fl">10.0</span></span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># modify image to include label grid</span></span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>        cur_img[<span class="dv">0</span>, <span class="dv">92</span>:<span class="dv">480</span>:<span class="dv">20</span>, <span class="dv">92</span>:<span class="dv">480</span>] <span class="op">=</span> <span class="op">-</span><span class="dv">5</span></span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>        cur_img[<span class="dv">0</span>,  <span class="dv">92</span>:<span class="dv">480</span>, <span class="dv">92</span>:<span class="dv">480</span>:<span class="dv">20</span>] <span class="op">=</span> <span class="op">-</span><span class="dv">5</span></span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a>        <span class="co"># modify label to include label grid</span></span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a>        cur_label[<span class="dv">0</span>, ::<span class="dv">20</span>] <span class="op">=</span> <span class="op">-</span><span class="dv">5</span></span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a>        cur_label[<span class="dv">0</span>, :, ::<span class="dv">20</span>] <span class="op">=</span> <span class="op">-</span><span class="dv">5</span></span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> kind <span class="op">==</span> <span class="st">'elastic deformation'</span>:</span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a>        <span class="co"># set transformation</span></span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a>        kernel <span class="op">=</span> dataset.kernel</span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a>        alpha <span class="op">=</span> dataset.alpha</span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a>        new_img, new_label <span class="op">=</span> EM_Dataset.elastic_deform(cur_img, cur_label,</span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a>                                                       alpha, kernel)</span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> kind <span class="op">==</span> <span class="st">'affine transformation'</span>:</span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a>        angle <span class="op">=</span> np.random.randint(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a>        translate <span class="op">=</span> <span class="bu">list</span>(np.random.randint(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, size<span class="op">=</span><span class="dv">2</span>))</span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a>        new_img, new_label <span class="op">=</span> EM_Dataset.affine_transform(cur_img, cur_label,</span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a>                                                         angle, translate)</span>
<span id="cb3-76"><a href="#cb3-76" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> kind <span class="op">==</span> <span class="st">'gray value variation'</span>:</span>
<span id="cb3-77"><a href="#cb3-77" aria-hidden="true" tabindex="-1"></a>        sigma <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb3-78"><a href="#cb3-78" aria-hidden="true" tabindex="-1"></a>        new_img <span class="op">=</span> EM_Dataset.gray_value_variations(img, sigma)</span>
<span id="cb3-79"><a href="#cb3-79" aria-hidden="true" tabindex="-1"></a>        new_label <span class="op">=</span> label</span>
<span id="cb3-80"><a href="#cb3-80" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb3-81"><a href="#cb3-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NameError</span>(<span class="st">'Unknown `kind`, can only be `elastic deformation`, '</span> <span class="op">+</span></span>
<span id="cb3-82"><a href="#cb3-82" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'`affine transformation` or `gray value variation`'</span>)</span>
<span id="cb3-83"><a href="#cb3-83" aria-hidden="true" tabindex="-1"></a>    <span class="co"># start plotting</span></span>
<span id="cb3-84"><a href="#cb3-84" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">10</span>))</span>
<span id="cb3-85"><a href="#cb3-85" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb3-86"><a href="#cb3-86" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Before '</span> <span class="op">+</span> kind)</span>
<span id="cb3-87"><a href="#cb3-87" aria-hidden="true" tabindex="-1"></a>    plt.imshow(cur_img[<span class="dv">0</span>], cmap<span class="op">=</span><span class="st">'gray'</span>, aspect<span class="op">=</span><span class="st">'equal'</span>,</span>
<span id="cb3-88"><a href="#cb3-88" aria-hidden="true" tabindex="-1"></a>               interpolation<span class="op">=</span><span class="st">'gaussian'</span>, vmax<span class="op">=</span><span class="dv">1</span>, vmin<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-89"><a href="#cb3-89" aria-hidden="true" tabindex="-1"></a>    <span class="co"># focus of current segmentation mask</span></span>
<span id="cb3-90"><a href="#cb3-90" aria-hidden="true" tabindex="-1"></a>    plt.plot([<span class="dv">92</span>, <span class="dv">480</span>, <span class="dv">480</span>, <span class="dv">92</span>, <span class="dv">92</span>], [<span class="dv">92</span>, <span class="dv">92</span>, <span class="dv">480</span>, <span class="dv">480</span>, <span class="dv">92</span>],</span>
<span id="cb3-91"><a href="#cb3-91" aria-hidden="true" tabindex="-1"></a>            <span class="st">'yellow'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb3-92"><a href="#cb3-92" aria-hidden="true" tabindex="-1"></a>    plt.subplots_adjust(hspace<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb3-93"><a href="#cb3-93" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">3</span>)</span>
<span id="cb3-94"><a href="#cb3-94" aria-hidden="true" tabindex="-1"></a>    plt.imshow(cur_label[<span class="dv">0</span>], cmap<span class="op">=</span><span class="st">'gray'</span>, aspect<span class="op">=</span><span class="st">'equal'</span>,</span>
<span id="cb3-95"><a href="#cb3-95" aria-hidden="true" tabindex="-1"></a>               interpolation<span class="op">=</span><span class="st">'gaussian'</span>, vmax<span class="op">=</span><span class="dv">1</span>, vmin<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-96"><a href="#cb3-96" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>)</span>
<span id="cb3-97"><a href="#cb3-97" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'After '</span> <span class="op">+</span> kind)</span>
<span id="cb3-98"><a href="#cb3-98" aria-hidden="true" tabindex="-1"></a>    plt.imshow(new_img[<span class="dv">0</span>], cmap<span class="op">=</span><span class="st">'gray'</span>, aspect<span class="op">=</span><span class="st">'equal'</span>,</span>
<span id="cb3-99"><a href="#cb3-99" aria-hidden="true" tabindex="-1"></a>               interpolation<span class="op">=</span><span class="st">'gaussian'</span>, vmax<span class="op">=</span><span class="dv">1</span>, vmin<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-100"><a href="#cb3-100" aria-hidden="true" tabindex="-1"></a>    <span class="co"># focus of current segmentation mask</span></span>
<span id="cb3-101"><a href="#cb3-101" aria-hidden="true" tabindex="-1"></a>    plt.plot([<span class="dv">92</span>, <span class="dv">480</span>, <span class="dv">480</span>, <span class="dv">92</span>, <span class="dv">92</span>], [<span class="dv">92</span>, <span class="dv">92</span>, <span class="dv">480</span>, <span class="dv">480</span>, <span class="dv">92</span>],</span>
<span id="cb3-102"><a href="#cb3-102" aria-hidden="true" tabindex="-1"></a>            <span class="st">'yellow'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb3-103"><a href="#cb3-103" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">4</span>)</span>
<span id="cb3-104"><a href="#cb3-104" aria-hidden="true" tabindex="-1"></a>    plt.imshow(new_label[<span class="dv">0</span>], cmap<span class="op">=</span><span class="st">'gray'</span>, aspect<span class="op">=</span><span class="st">'equal'</span>,</span>
<span id="cb3-105"><a href="#cb3-105" aria-hidden="true" tabindex="-1"></a>               interpolation<span class="op">=</span><span class="st">'gaussian'</span>, vmax<span class="op">=</span><span class="dv">1</span>, vmin<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-106"><a href="#cb3-106" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span></span>
<span id="cb3-107"><a href="#cb3-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-108"><a href="#cb3-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-109"><a href="#cb3-109" aria-hidden="true" tabindex="-1"></a><span class="co"># generate images in order of appearance</span></span>
<span id="cb3-110"><a href="#cb3-110" aria-hidden="true" tabindex="-1"></a>visualize_overlap_tile_strategy(whole_dataset, img_index<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb3-111"><a href="#cb3-111" aria-hidden="true" tabindex="-1"></a>                                tile_indexes<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb3-112"><a href="#cb3-112" aria-hidden="true" tabindex="-1"></a>visualize_data_augmentation(whole_dataset, index<span class="op">=</span><span class="dv">0</span>, show_grid<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb3-113"><a href="#cb3-113" aria-hidden="true" tabindex="-1"></a>                            kind<span class="op">=</span><span class="st">'affine transformation'</span>)</span>
<span id="cb3-114"><a href="#cb3-114" aria-hidden="true" tabindex="-1"></a>visualize_data_augmentation(whole_dataset, index<span class="op">=</span><span class="dv">0</span>, show_grid<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb3-115"><a href="#cb3-115" aria-hidden="true" tabindex="-1"></a>                            kind<span class="op">=</span><span class="st">'elastic deformation'</span>)</span>
<span id="cb3-116"><a href="#cb3-116" aria-hidden="true" tabindex="-1"></a>visualize_data_augmentation(whole_dataset, index<span class="op">=</span><span class="dv">0</span>, show_grid<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb3-117"><a href="#cb3-117" aria-hidden="true" tabindex="-1"></a>                            kind<span class="op">=</span><span class="st">'gray value variation'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="model-implementation" class="level3">
<h3 class="anchored" data-anchor-id="model-implementation">Model Implementation</h3>
<p>Model implementation can be divided into three tasks:</p>
<ul>
<li><p><strong>Network Architecture</strong>: The model architecture is given in the <a href="https://borea17.github.io/paper_summaries/u_net#model-description">model description</a> in which one can identify several blocks of two <span class="math inline">\(3 \times 3\)</span> convolutional layers each followed by a ReLU non-linearity (called <code>_block</code> in the implementation). Note that the output of the last prediction layer can be understood as the unnormalized prediction for each class, i.e., <span class="math inline">\(a_{i,j}^{(k)} \in ] -\infty, +\infty[\)</span> where <span class="math inline">\(a^{(k)}\)</span> denotes the activation in feature channel <span class="math inline">\(k \in \{1, 2\}\)</span> (one channel for each class) and the indices <span class="math inline">\({i,j}\)</span> describe the pixel position. In order to get normalized probabilities for each pixel <span class="math inline">\(\hat{p}_{i,j}^{(k)}\)</span>, a pixel-wise softmax is applied at the end (last operation in <code>forward</code>), i.e., after this operation the sum of the two output channels equals one for each pixel <span class="math inline">\(\hat{p}_{i,j}^{(1)} + \hat{p}_{i,j}^{(2)} = 1\)</span>.</p>
<div class="cell" data-execution_count="5">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Unet(nn.Module):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""original U-Net architecture proposed by Ronneberger et al. (2015)</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Attributes:</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co">        encoder_blocks (list):  four u_net blocks of encoder path</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">        bottleneck_bock: block that mediates between encoder and decoder</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co">        decoder_blocks (list):  four u_net blocks of decoder path</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co">        cropped_img_size (list): cropped images size in order of encoder blocks</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co">        up_convs (list): upsampling (transposed convolutional) layers (decoder)</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co">        max_pool: max pool operation used in encoder path</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder_blocks <span class="op">=</span> nn.ModuleList([</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>            Unet._block(<span class="dv">1</span>, <span class="dv">64</span>),</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>            Unet._block(<span class="dv">64</span>, <span class="dv">128</span>),</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>            Unet._block(<span class="dv">128</span>, <span class="dv">256</span>),</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>            Unet._block(<span class="dv">256</span>, <span class="dv">512</span>)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bottleneck_block <span class="op">=</span> Unet._block(<span class="dv">512</span>, <span class="dv">1024</span>)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder_blocks <span class="op">=</span> nn.ModuleList([</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>            Unet._block(<span class="dv">1024</span>, <span class="dv">512</span>),</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>            Unet._block(<span class="dv">512</span>, <span class="dv">256</span>),</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>            Unet._block(<span class="dv">256</span>, <span class="dv">128</span>),</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>            Unet._block(<span class="dv">128</span>, <span class="dv">64</span>)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cropped_img_sizes <span class="op">=</span> [<span class="dv">392</span>, <span class="dv">200</span>, <span class="dv">104</span>, <span class="dv">56</span>]</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.up_convs <span class="op">=</span> nn.ModuleList([</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>            nn.ConvTranspose2d(<span class="dv">1024</span>, <span class="dv">512</span>, kernel_size<span class="op">=</span>(<span class="dv">2</span>,<span class="dv">2</span>), stride<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>            nn.ConvTranspose2d(<span class="dv">512</span>, <span class="dv">256</span>, kernel_size<span class="op">=</span>(<span class="dv">2</span>,<span class="dv">2</span>), stride<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>            nn.ConvTranspose2d(<span class="dv">256</span>, <span class="dv">128</span>, kernel_size<span class="op">=</span>(<span class="dv">2</span>,<span class="dv">2</span>), stride<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>            nn.ConvTranspose2d(<span class="dv">128</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span>(<span class="dv">2</span>,<span class="dv">2</span>), stride<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_pool <span class="op">=</span> nn.MaxPool2d(kernel_size<span class="op">=</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.prediction <span class="op">=</span> nn.Conv2d(<span class="dv">64</span>, <span class="dv">2</span>, kernel_size<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">1</span>), stride<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># go through encoder path and store cropped images</span></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>        cropped_imgs <span class="op">=</span> []</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> index, encoder_block <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.encoder_blocks):</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> encoder_block(x)</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>            <span class="co"># center crop and add to cropped image list</span></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>            cropped_img <span class="op">=</span> Unet._center_crop(out, <span class="va">self</span>.cropped_img_sizes[index])</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>            cropped_imgs.append(cropped_img)</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>            <span class="co"># max pool output of encoder block</span></span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> <span class="va">self</span>.max_pool(out)</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># bottleneck block (no max pool)</span></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bottleneck_block(x)  <span class="co"># [batch_size, 1024, 28, 28]</span></span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># go through decoder path with stored cropped images</span></span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> index, decoder_block <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.decoder_blocks):</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> <span class="va">self</span>.up_convs[index](x)</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>            <span class="co"># concatenate x and cropped img along channel dimension</span></span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> torch.cat((cropped_imgs[<span class="op">-</span><span class="dv">1</span><span class="op">-</span>index], x), <span class="dv">1</span>)</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>            <span class="co"># feed through decoder_block</span></span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> decoder_block(x)</span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># feed through prediction layer [batch_size, 2, 388, 388]</span></span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>        x_pred_unnormalized <span class="op">=</span> <span class="va">self</span>.prediction(x)</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># normalize prediction for each pixel</span></span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>        x_pred <span class="op">=</span> torch.softmax(x_pred_unnormalized, <span class="dv">1</span>)</span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x_pred</span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _center_crop(x, new_size):</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""center croping of a square input tensor</span></span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a><span class="co">            x: input tensor shape [batch_size, channels, resolution, resolution]</span></span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a><span class="co">            new_size: the desired output resolution (taking center of input)</span></span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a><span class="co">            x_cropped: tensor shape [batch_size, channels, new_size, new_size]</span></span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a>        img_size <span class="op">=</span> x.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a>        i_start <span class="op">=</span> <span class="bu">int</span>((img_size <span class="op">-</span> new_size)<span class="op">/</span><span class="dv">2</span>)</span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a>        i_end <span class="op">=</span> <span class="bu">int</span>((img_size <span class="op">+</span> new_size)<span class="op">/</span><span class="dv">2</span>)</span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a>        x_cropped <span class="op">=</span> x[:, :, i_start:i_end, i_start:i_end]</span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x_cropped</span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _block(in_channels, out_channels):</span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""block for use in U-Net architecture,</span></span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a><span class="co">        consists of two conv 3x3, ReLU layers</span></span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a><span class="co">            in_channels: number of input channels for first convolution</span></span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a><span class="co">            out_channels: number of output channels for both convolutions</span></span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a><span class="co">            u_net_block: Sequential U net block</span></span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a>        conv1 <span class="op">=</span> nn.Conv2d(in_channels, out_channels, kernel_size<span class="op">=</span>(<span class="dv">3</span>,<span class="dv">3</span>), stride<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a>        conv2 <span class="op">=</span> nn.Conv2d(out_channels, out_channels, kernel_size<span class="op">=</span>(<span class="dv">3</span>,<span class="dv">3</span>), stride<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a>        N_1, N_2 <span class="op">=</span> <span class="dv">9</span><span class="op">*</span>in_channels, <span class="dv">9</span><span class="op">*</span>out_channels</span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a>        <span class="co"># initialize by drawing weights from Gaussian distribution</span></span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a>        conv1.weight.data.normal_(mean<span class="op">=</span><span class="dv">0</span>, std<span class="op">=</span>np.sqrt(<span class="dv">2</span><span class="op">/</span>N_1))</span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a>        conv2.weight.data.normal_(mean<span class="op">=</span><span class="dv">0</span>, std<span class="op">=</span>np.sqrt(<span class="dv">2</span><span class="op">/</span>N_2))</span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a>        <span class="co"># define u_net_block</span></span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a>        u_net_block <span class="op">=</span> nn.Sequential(</span>
<span id="cb4-105"><a href="#cb4-105" aria-hidden="true" tabindex="-1"></a>            conv1,</span>
<span id="cb4-106"><a href="#cb4-106" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb4-107"><a href="#cb4-107" aria-hidden="true" tabindex="-1"></a>            conv2,</span>
<span id="cb4-108"><a href="#cb4-108" aria-hidden="true" tabindex="-1"></a>            nn.ReLU()</span>
<span id="cb4-109"><a href="#cb4-109" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-110"><a href="#cb4-110" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> u_net_block</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div></li>
<li><p><strong>Loss Function</strong>: Since the segmentation labels are clearly imbalanced (much more white pixels than black pixels), <a href="https://arxiv.org/abs/1505.04597">Ronneberger et al. (2015)</a> use the weighted cross entropy as the loss function (which they term <em>energy function</em>)</p>
<p><span class="math display">\[
\begin{align}
   J (\textbf{x}, \textbf{m}) &amp;= -\sum_{i=1}^{388}\sum_{j=1}^{388}
   \sum_{k=1}^2 w_{i,j} (\textbf{m}) \cdot
   p_{i,j}^{(k)} \log \left( \widehat{p}_{i,j}^{(k)} \left( \textbf{x};
\boldsymbol{\theta} \right)  \right) \\
   &amp;\text{with} \quad p_{i,j}^{(1)} = \begin{cases} 1 &amp; \text{if }
m_{i,j}=1 \\ 0 &amp;\text{else} \end{cases} \quad \text{and} \quad p_{i,j}^{(2)} =
\begin{cases} 1 &amp; \text{if } m_{i, j} = 0 \\0 &amp; \text{else}, \end{cases}
\end{align}
\]</span></p>
<p>where <span class="math inline">\(\textbf{x}\in [0, 1]^{572\times 572}\)</span> denotes the input image, <span class="math inline">\(\textbf{m} \in \{0, 1\}^{388 \times 388}\)</span> the corresponding segmentation mask, <span class="math inline">\(\textbf{p}^{(k)}\in \{0, 1\}^{388 \times 388}\)</span> the groundtruth probability for each class <span class="math inline">\(k\)</span>, <span class="math inline">\(\widehat{\textbf{p}}^{(k)} \in [0, 1]^{388\times 388}\)</span> denotes the <span class="math inline">\(k\)</span>-th channel output of the network parameterized by <span class="math inline">\(\boldsymbol{\theta}\)</span> and <span class="math inline">\(\textbf{w} \left( \textbf{m} \right) \in \mathbb{R}^{388 \times 388}\)</span> is a introduced weight map (computed via the segmentation mask <span class="math inline">\(\textbf{m}\)</span>) to give some pixels more importance during training. Accordingly, the loss function can be interpreted as penalizing the deviation from 1 for each true class output pixel weighted by the corresponding entry of the weight map.</p>
<p><strong>Weight Map</strong>: To compensate for the imbalance between separation borders and segmented object<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>, <a href="https://arxiv.org/abs/1505.04597">Ronneberger et al. (2015)</a> introduce the following weight map</p>
<p><span class="math display">\[
  w(\textbf{m}) = {w_c (\textbf{m})} + {w_0 \cdot \exp \left( - \frac
  {\left(d_1 (\textbf{m}) - d_2 (\textbf{m})\right)^2}{2\sigma^2}\right)},
\]</span></p>
<p>where the first term reweights each pixel of the minority class (i.e., black pixels) to balance the class frequencies. In the second term <span class="math inline">\(d_1\)</span> and <span class="math inline">\(d_2\)</span> denote the distance to the border of the nearest and second nearest cell, respectively. <span class="math inline">\(w_0\)</span> and <span class="math inline">\(\sigma\)</span> are predefined hyperparameters. Thus, the second term can be understood as putting additional weight to smaller borders, see code and image below.</p>
<div class="cell" data-execution_count="6">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mpl_toolkits.axes_grid1 <span class="im">import</span> make_axes_locatable</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> skimage <span class="im">import</span> measure</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.ndimage.morphology <span class="im">import</span> distance_transform_edt</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> skimage.segmentation <span class="im">import</span> find_boundaries</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_weight_map(label_mask, w_0, sigma, plot<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""compute weight map for each ground truth segmentation to compensate</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co">    for the different class frequencies and to put additional</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co">    emphasis on small borders as proposed by Ronneberger et al.</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="co">        label mask (torch tensor): true segmentation masks [batch_size, 1, 388, 388]</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="co">        w_0 (float): hyperparameter in second term of weight map</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="co">        sigma (float): hyperparameter in second term of weight map</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="co">        weight_map (torch tensor): computed weight map [batch_size, 1, 388, 388]</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="co">    researchgate.net/post/creating_a_weight_map_from_a_binary_image_U-net_paper</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    batch_size <span class="op">=</span> label_mask.shape[<span class="dv">0</span>]</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    weight_map <span class="op">=</span> torch.zeros_like(label_mask)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute w_c to balance class frequencies</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        w_c <span class="op">=</span> label_mask[i][<span class="dv">0</span>].clone()</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        class_freq_0 <span class="op">=</span> (label_mask[i]<span class="op">==</span><span class="dv">0</span>).<span class="bu">sum</span>().item()</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>        class_freq_1 <span class="op">=</span> (label_mask[i]<span class="op">==</span><span class="dv">1</span>).<span class="bu">sum</span>().item()</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>        w_c[label_mask[i][<span class="dv">0</span>]<span class="op">==</span><span class="dv">0</span>] <span class="op">=</span> class_freq_1 <span class="op">/</span> class_freq_0</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute d_1, d_2, i.e., euclid. dist. to border of (1st/2nd) closest cell</span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>        d_1 <span class="op">=</span> np.zeros(label_mask[i][<span class="dv">0</span>].shape)</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>        d_2 <span class="op">=</span> np.zeros(label_mask[i][<span class="dv">0</span>].shape)</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># distinguish all cells (connected components of ones)</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>        all_cells <span class="op">=</span> measure.label(label_mask[i][<span class="dv">0</span>], background<span class="op">=</span><span class="dv">0</span>, connectivity<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>        num_cells <span class="op">=</span> np.<span class="bu">max</span>(all_cells)</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># initialize distances for all cells</span></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>        dists <span class="op">=</span> np.zeros([num_cells, d_2.shape[<span class="dv">0</span>], d_2.shape[<span class="dv">1</span>]])</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># iterate over all zero components</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> index, i_cell <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">range</span>(<span class="dv">1</span>, num_cells <span class="op">+</span> <span class="dv">1</span>)):</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>            <span class="co"># cell segmentation (segmented cell 1, rest 0)</span></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>            cell_segmentation <span class="op">=</span> all_cells<span class="op">==</span>i_cell</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>            <span class="co"># find boundary (boundary 1, rest 0)</span></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>            boundary <span class="op">=</span> find_boundaries(cell_segmentation, mode<span class="op">=</span><span class="st">'inner'</span>)</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>            <span class="co"># compute distance to boundary (set boundary 0, rest -1)</span></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>            bound_dists <span class="op">=</span> distance_transform_edt(<span class="dv">1</span> <span class="op">-</span> boundary)</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>            dists[index] <span class="op">=</span> bound_dists</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># sort dists along first axis (each pixel)</span></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>        dists.sort(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>        d_1 <span class="op">=</span> dists[<span class="dv">0</span>]</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>        d_2 <span class="op">=</span> dists[<span class="dv">1</span>]</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>        w <span class="op">=</span> w_c <span class="op">+</span> w_0 <span class="op">*</span> np.exp(<span class="op">-</span> (d_1 <span class="op">+</span> d_2)<span class="op">**</span><span class="dv">2</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>sigma<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># save w to weight map</span></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>        weight_map[i, <span class="dv">0</span>] <span class="op">=</span> w</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># visualize weight map</span></span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> plot <span class="kw">and</span> i<span class="op">==</span><span class="dv">0</span>:</span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a>            fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">18</span>, <span class="dv">14</span>))</span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a>            ax <span class="op">=</span> plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a>            plt.title(<span class="st">'Segmenation Mask'</span>)</span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a>            plt.imshow(label_mask[<span class="dv">0</span>, <span class="dv">0</span>], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a>            divider <span class="op">=</span> make_axes_locatable(ax)</span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a>            cax <span class="op">=</span> divider.append_axes(<span class="st">"right"</span>, size<span class="op">=</span><span class="st">"5%"</span>, pad<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a>            plt.colorbar(cax<span class="op">=</span>cax)</span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a>            ax <span class="op">=</span> plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a>            plt.title(<span class="st">'w_c'</span>)</span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a>            plt.imshow(w_c, cmap<span class="op">=</span><span class="st">'jet'</span>)</span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a>            divider <span class="op">=</span> make_axes_locatable(ax)</span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a>            cax <span class="op">=</span> divider.append_axes(<span class="st">"right"</span>, size<span class="op">=</span><span class="st">"5%"</span>, pad<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a>            plt.colorbar(cax<span class="op">=</span>cax)</span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a>            ax <span class="op">=</span> plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a>            plt.title(<span class="st">'w'</span>)</span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a>            plt.imshow(w, cmap<span class="op">=</span><span class="st">'jet'</span>)</span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a>            divider <span class="op">=</span> make_axes_locatable(ax)</span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a>            cax <span class="op">=</span> divider.append_axes(<span class="st">"right"</span>, size<span class="op">=</span><span class="st">"5%"</span>, pad<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a>            plt.colorbar(cax<span class="op">=</span>cax)</span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> weight_map</span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a>img, label_mask <span class="op">=</span> whole_dataset[<span class="dv">0</span>]</span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a>weight_map <span class="op">=</span> compute_weight_map(label_mask.unsqueeze(<span class="dv">0</span>), w_0<span class="op">=</span><span class="dv">10</span>, sigma<span class="op">=</span><span class="dv">5</span>, plot<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./img/weight_map.png" title="compute_weight_map" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">compute_weight_map</figcaption>
</figure>
</div></li>
</ul>
<ul>
<li><p><strong>Training Procedure</strong>: A simple <code>SGD</code> (<em>Stochastic Gradient Descent</em>) optimizer with a high momentum (0.99) and a <code>batch_size</code> of 1 are choosen for training as proposed by <a href="https://arxiv.org/abs/1505.04597">Ronneberger et al. (2015)</a>, see code below. Note that we take the mean instead of the sum in the loss function calculation to avoid overflow (i.e., nans). This will only change the strength of a gradient step (which can be adjusted by the learning rate), but not its direction.</p>
<div class="cell" data-execution_count="7">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> livelossplot <span class="im">import</span> PlotLosses</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(u_net, dataset, epochs):</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># hyperparameters weight map</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    w_0, sigma <span class="op">=</span> <span class="dv">10</span>, <span class="dv">5</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Device: </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(device))</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    data_loader <span class="op">=</span> DataLoader(dataset, batch_size<span class="op">=</span><span class="dv">1</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    u_net.to(device)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.SGD(u_net.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>, momentum<span class="op">=</span><span class="fl">0.99</span>)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    losses_plot <span class="op">=</span> PlotLosses()</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>        avg_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> counter, (imgs, label_masks) <span class="kw">in</span> <span class="bu">enumerate</span>(data_loader):</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>            u_net.zero_grad()</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>            <span class="co"># retrieve predictions of u_net [batch, 2, 388, 388]</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>            pred_masks <span class="op">=</span> u_net(imgs.to(device))</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>            <span class="co"># compute weight map</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>            weight_map <span class="op">=</span> compute_weight_map(label_masks, w_0, sigma).to(device)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>            <span class="co"># put label_masks to device</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>            label_masks <span class="op">=</span> label_masks.to(device)</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>            <span class="co"># compute weighted binary cross entropy loss</span></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> <span class="op">-</span>(weight_map<span class="op">*</span></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>                    (pred_masks[:, <span class="dv">0</span>:<span class="dv">1</span>].log() <span class="op">*</span> label_masks <span class="op">+</span></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>                      pred_masks[:, <span class="dv">1</span>:<span class="dv">2</span>].log() <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> label_masks))</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>                    ).mean()</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>            avg_loss <span class="op">+=</span> loss.item() <span class="op">/</span> <span class="bu">len</span>(dataset)</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>            losses_plot.update({<span class="st">'current weighted loss'</span>: loss.item()},</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>                              current_step<span class="op">=</span>epoch <span class="op">+</span> counter<span class="op">/</span><span class="bu">len</span>(data_loader))</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>            losses_plot.draw()</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>        losses_plot.update({<span class="st">'avg weighted loss'</span>: avg_loss},</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>                          current_step<span class="op">=</span>epoch <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>        losses_plot.draw()</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>    trained_u_net <span class="op">=</span> u_net</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> trained_u_net</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Beware</strong>: Training for 30 epochs (i.e., the code below) takes about 2 hours with a NVIDIA Tesla K80 as GPU. The loss plot (see below <code>avg weighted loss</code>) indicates that training for more epochs might improve the model even more<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>. For people who are interested in using the model without waiting for 2 hours, I stored a trained version on <a href="https://nextjournal.com/borea17/u-net">nextjournal</a>.</p>
<div class="cell" data-execution_count="8">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>u_net <span class="op">=</span> Unet()</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># all image indexes</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>idx <span class="op">=</span> np.arange(<span class="dv">30</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co"># random inplace shuffling of indexes</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">1</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>np.random.shuffle(idx)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co"># split data into training and test data</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>train_imgs, train_labels <span class="op">=</span> imgs[idx[<span class="dv">0</span>:<span class="dv">25</span>]], labels[idx[<span class="dv">0</span>:<span class="dv">25</span>]]</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>test_imgs, test_labels <span class="op">=</span> imgs[idx[<span class="dv">25</span>:]], labels[idx[<span class="dv">25</span>:]]</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co"># generate datasets</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>stride <span class="op">=</span> <span class="dv">124</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> EM_Dataset(train_imgs, train_labels, stride<span class="op">=</span>stride,</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>                          transformation<span class="op">=</span><span class="va">True</span>, probability<span class="op">=</span><span class="fl">0.7</span>, alpha<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>                          sigma<span class="op">=</span><span class="dv">5</span>, kernel_dim<span class="op">=</span><span class="dv">25</span>)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>test_dataset <span class="op">=</span> EM_Dataset(test_imgs, test_labels, stride<span class="op">=</span>stride,</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>                          transformation<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="co"># start training procedure</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>trained_u_net <span class="op">=</span> train(u_net, train_dataset, epochs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./img/loss_plot.png" title="train plot" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">train result</figcaption>
</figure>
</div></li>
</ul>
</section>
<section id="results" class="level3">
<h3 class="anchored" data-anchor-id="results">Results</h3>
<p>Letâ€™s look at some image segmentations generated by the trained model on the unseen test set:</p>
<div class="cell" data-execution_count="9">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> visualize_results(trained_u_net, test_dataset, num_test_images<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># take random tile from each test image</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    num_tiles <span class="op">=</span> (<span class="dv">1</span> <span class="op">+</span> <span class="bu">int</span>((<span class="dv">512</span> <span class="op">-</span> <span class="dv">388</span>)<span class="op">/</span>test_dataset.stride))<span class="op">**</span><span class="dv">2</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    num_images <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(test_dataset) <span class="op">/</span> num_tiles)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> num_test_images:</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># number of images &lt; number of images in test set</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>        num_images <span class="op">=</span> <span class="bu">min</span>(num_test_images, num_images)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    random_tile_idx <span class="op">=</span> np.random.choice(<span class="bu">range</span>(num_tiles), num_images,</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>                                       replace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(num_images<span class="op">*</span><span class="dv">6</span>, <span class="dv">10</span>))</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># annotation plots</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> plt.subplot(<span class="dv">3</span>, num_images <span class="op">+</span> <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    ax.annotate(<span class="st">'cell image</span><span class="ch">\n</span><span class="st">(input)'</span>, xy<span class="op">=</span>(<span class="dv">1</span>, <span class="fl">0.5</span>), xycoords<span class="op">=</span><span class="st">'axes fraction'</span>,</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>                 fontsize<span class="op">=</span><span class="dv">14</span>, va<span class="op">=</span><span class="st">'center'</span>, ha<span class="op">=</span><span class="st">'right'</span>)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    ax.set_aspect(<span class="st">'equal'</span>)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    ax.axis(<span class="st">'off'</span>)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> plt.subplot(<span class="dv">3</span>, num_images <span class="op">+</span> <span class="dv">1</span>, num_images <span class="op">+</span> <span class="dv">2</span>)</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>    ax.annotate(<span class="st">'true segmentation</span><span class="ch">\n</span><span class="st">(label)'</span>, xy<span class="op">=</span>(<span class="dv">1</span>, <span class="fl">0.5</span>),</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>                xycoords<span class="op">=</span><span class="st">'axes fraction'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, va<span class="op">=</span><span class="st">'center'</span>, ha<span class="op">=</span><span class="st">'right'</span>)</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    ax.set_aspect(<span class="st">'equal'</span>)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    ax.axis(<span class="st">'off'</span>)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> plt.subplot(<span class="dv">3</span>, num_images <span class="op">+</span> <span class="dv">1</span>, <span class="dv">2</span><span class="op">*</span>(num_images <span class="op">+</span> <span class="dv">1</span>) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>    ax.annotate(<span class="st">'U-net prediction'</span>, xy<span class="op">=</span>(<span class="dv">1</span>, <span class="fl">0.5</span>), xycoords<span class="op">=</span><span class="st">'axes fraction'</span>,</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>                 fontsize<span class="op">=</span><span class="dv">14</span>, va<span class="op">=</span><span class="st">'center'</span>, ha<span class="op">=</span><span class="st">'right'</span>)</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>    ax.set_aspect(<span class="st">'equal'</span>)</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>    ax.axis(<span class="st">'off'</span>)</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># image, label, predicted label plots</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> index <span class="kw">in</span> <span class="bu">range</span>(num_images):</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>        img, label <span class="op">=</span> test_dataset[index<span class="op">*</span>num_tiles <span class="op">+</span> random_tile_idx[index]]</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>        label_pred <span class="op">=</span> u_net(img.unsqueeze(<span class="dv">0</span>).to(device)).squeeze(<span class="dv">0</span>)[<span class="dv">0</span>] <span class="op">&gt;</span> <span class="fl">0.5</span></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># plot original image</span></span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>        plt.subplot(<span class="dv">3</span>, num_images <span class="op">+</span> <span class="dv">1</span>, index <span class="op">+</span> <span class="dv">2</span>)</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>        plt.imshow(transforms.ToPILImage()(img), cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>        plt.plot([<span class="dv">92</span>, <span class="dv">480</span>, <span class="dv">480</span>, <span class="dv">92</span>, <span class="dv">92</span>], [<span class="dv">92</span>, <span class="dv">92</span>, <span class="dv">480</span>, <span class="dv">480</span>, <span class="dv">92</span>],</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>                 <span class="st">'yellow'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>        plt.xticks([])</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>        plt.yticks([])</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># plot original segmentation mask</span></span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>        plt.subplot(<span class="dv">3</span>, num_images <span class="op">+</span> <span class="dv">1</span>, index <span class="op">+</span> num_images <span class="op">+</span> <span class="dv">3</span>)</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>        plt.imshow(transforms.ToPILImage()(label), cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>        plt.xticks([])</span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>        plt.yticks([])</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># plot prediction segmentation mask</span></span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>        plt.subplot(<span class="dv">3</span>, num_images <span class="op">+</span> <span class="dv">1</span>, index <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>(num_images <span class="op">+</span> <span class="dv">1</span>) <span class="op">+</span> <span class="dv">2</span>)</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>        plt.imshow(label_pred.detach().cpu().numpy(), cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>        plt.xticks([])</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>        plt.yticks([])</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span></span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>visualize_results(trained_u_net, test_dataset, num_test_images<span class="op">=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./img/u_net_prediction.png" title="visualize results" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">visualize results</figcaption>
</figure>
</div>
<p>The predictions are pretty decent, though far from perfect. Bear in mind, that our model had only 25 example images to learn from and that training for more epochs might have led to even better predictions.</p>
<hr>


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>A greater input image than output segmentation size makes sense since the network has no information about the surrounding of the input image.<a href="#fnref1" class="footnote-back" role="doc-backlink">â†©ï¸Ž</a></p></li>
<li id="fn2"><p>Actually, CNNs should put more emphasis on the <code>where</code> or rather the local relation between context information, see <a href="https://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/clyj4jv/">Geoffrey Hintonâ€™s comment about pooling</a>.<a href="#fnref2" class="footnote-back" role="doc-backlink">â†©ï¸Ž</a></p></li>
<li id="fn3"><p>The implementation intends to be easily understandable, while keeping the computational resources low. Thus, it is not aimed to generate the best training results or model performance.<a href="#fnref3" class="footnote-back" role="doc-backlink">â†©ï¸Ž</a></p></li>
<li id="fn4"><p>Since the separation borders are much smaller than the segmented objects, the network could be trapped into merging touching objects without being penalized enough.<a href="#fnref4" class="footnote-back" role="doc-backlink">â†©ï¸Ž</a></p></li>
<li id="fn5"><p>The implementation intends to be easily understandable, while keeping the computational resources low. Thus, it is not aimed to generate the best training results or model performance.<a href="#fnref5" class="footnote-back" role="doc-backlink">â†©ï¸Ž</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>