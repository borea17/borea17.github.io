[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Markus Norden",
    "section": "",
    "text": "Machine Learning Engineer at SST"
  },
  {
    "objectID": "ml101/engineering/mean_of_means/index.html",
    "href": "ml101/engineering/mean_of_means/index.html",
    "title": "How can the average loss be calculated with batch means?",
    "section": "",
    "text": "Assuming equal batch sizes \\(b\\), the overall mean equals the mean of batch means.\n\n\n\n\n\n\nDerivation\n\n\n\n\n\n\\[\\begin{align*}\n\\mu = \\frac {1}{N} \\sum_{i=1}^N x_i&= \\frac {1}{N} \\left(\\sum_{i=1}^b x_i + \\sum_{i=b+1}^{b+b} x_i + \\dots + \\sum_{i=(N-1) \\cdot b + 1}^{N} x_i \\right)\\\\&= \\frac {b}{N} \\left( \\frac {1}{b} \\left(\n\\sum_{i=1}^b x_i + \\sum_{i=b+1}^{b+b} x_i + \\dots + \\sum_{i=(N-1) \\cdot b + 1}^{N} x_i\n\\right)\\right)\\\\\n&= \\frac {1}{N/b} \\left( \\mu_{1,b} + \\mu_{b+1, b+b} + \\dots + \\mu_{N-b+1,N} \\right)\\\\\n&= \\frac {1}{k} \\sum_{i=1}^{k} \\mu_{ib+1, (i+1)b}\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n\nWhy is this helpful\n\n\n\n\n\nConsider the following case, where we have a dataset \\(\\textbf{X} =\n\\{ \\textbf{x}_i \\}_{i=1}^N\\) consisting of \\(N\\) samples \\(\\textbf{x}_i\\). Instead of iterating over each sample independently, we decide stream minibatches of size \\(b\\) (as this often leads to a stabilized training). Now the question arises, what is the average loss? E.g., can we simply take the average of the batch losses to compute the average of the epoch ?\nThe answer is YES (at least if the batch sizes are always equal). Even if the last batch size doesn’t match, we get a pretty good estimate."
  },
  {
    "objectID": "ml101/engineering/sigmoid_loss/index.html",
    "href": "ml101/engineering/sigmoid_loss/index.html",
    "title": "Why should a MSE loss be avoided after a sigmoid layer?",
    "section": "",
    "text": "MSE loss after a sigmoid layer leads to the vanishing gradients problem in cases where the outputs of the sigmoid layer are close to \\(0\\) or \\(1\\) irrespective of the true probability/label. I.e., in the extreme case where the network output is something close to zero while the true label is 1, gradients w.r.t. network parameters are close to zero.\n\n\n\n\n\n\n\nDerivation\n\n\n\n\n\nLet’s denote the output of the network by \\(\\widetilde{\\textbf{p}} \\in [0, 1]^{N}\\) (for estimated probabilities) and the input to the sigmoid \\(\\textbf{x}\\in \\mathbb{R}^{N}\\) (i.e., layer output before sigmoid is applied)\n\\[\n\\textbf{p} = \\text{Sigmoid} (\\textbf{x}) = \\frac {1}{1 + \\exp \\left({-\\textbf{x}}\\right)}\n\\]\nNow suppose that \\(\\textbf{p}\\in [0, 1]^{N}\\) denotes the true probabilities, then applying the MSE loss gives\n\\[\n\\text{MSE loss} \\left(\\widetilde{\\textbf{p}}, \\textbf{p}\\right)\n=J\\left(\\widetilde{\\textbf{p}}, \\textbf{p}\\right) = \\sum_{i=1}^N\n\\left(\\widetilde{p}_i - p_i \\right)^2,\n\\]\nThe gradient w.r.t. network parameters will be proportional to the gradient w.r.t. \\(\\textbf{x}\\) (backpropagation rule) which is as follows\n\\[\n\\frac {\\partial J \\left(\\widetilde{\\textbf{p}}, \\textbf{p}\\right)}{\\partial\nx_i} = \\sum_{j=1}^N \\frac {\\partial J}{\\partial p_j} \\cdot \\frac {\\partial p_j}{\\partial x_j} = 2\n\\left(\\widetilde{p}_i - p_i \\right) \\cdot \\frac {\\partial p_i}{\\partial x_i} = 2\n\\left(\\widetilde{p}_i - p_i \\right) \\cdot p_i (1 - p_i)\n\\]\nHere we can directly see that even if the absolute error approaches 1, i.e., \\(\\left(\\widetilde{p}_i - p_i \\right)\\rightarrow 1\\), the gradient vanishes for \\(p_i\\rightarrow 1\\) and \\(p_i\\rightarrow 0\\).\nLet’s derive the gradient of the sigmoid using substitution and the chain rule\n\\[\n\\begin{align}\n\\frac {\\partial p_i} {\\partial x_i} &= \\frac {\\partial u}{\\partial t} \\cdot \\frac\n{\\partial t}{\\partial x} \\quad \\text{with} \\quad u(t) = t^{-1}, \\quad t(x_i) = 1 +\n\\exp{(-x_i)}\\\\\n&= -t^{-2} \\cdot \\left(-\\exp{x_i}\\right) = \\frac {exp\\left(-x_i\\right)}{\\left(1 +\n\\exp\\left(-x_i\\right)\\right)^2}\\\\\n&= \\underbrace{\\frac {1}{1 + \\exp \\left( -x \\right)}}_{p_i} \\underbrace{\\frac {\\exp\n(-x_i)}{1+exp(-x_i)}}_{1-p_i} = p_i \\left(1 - p_i \\right)\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\nVisualization\n\n\n\n\n\nLet’s write a nice visualization using pytorch’s autograd.\n\n\nCode\nimport torch\nfrom torch import nn\nimport matplotlib.pyplot as plt\n\n\ndef make_visualization():\n    p0, p1 = 0, 1  # true probability\n\n    points = torch.arange(-10, 10, step=0.01)\n    # create variables to track gradients\n    grad_points = nn.Parameter(points, requires_grad=True)\n    p_tilde_0 = nn.Parameter(torch.sigmoid(points), requires_grad=True)\n    p_tilde_1 = nn.Parameter(torch.sigmoid(points), requires_grad=True)\n    grad_points_p0_x = nn.Parameter(points, requires_grad=True)\n    grad_points_p1_x = nn.Parameter(points, requires_grad=True)\n\n    # computations over which gradients are calculated\n    out_sigmoid = torch.sigmoid(grad_points)\n    MSE_loss_p0_p = (p_tilde_0 - p0)**2\n    MSE_loss_p1_p = (p_tilde_1 - p1)**2\n    MSE_loss_p0_x = (torch.sigmoid(grad_points_p0_x) - p0)**2\n    MSE_loss_p1_x = (torch.sigmoid(grad_points_p1_x) - p1)**2\n    # calculate gradients\n    out_sigmoid.sum().backward()\n    MSE_loss_p0_p.sum().backward()\n    MSE_loss_p1_p.sum().backward()\n    MSE_loss_p0_x.sum().backward()\n    MSE_loss_p1_x.sum().backward()\n\n    fig = plt.figure(figsize=(9,8))\n    # sigmoid\n    plt.subplot(3, 3, 1)\n    plt.plot(grad_points.detach().numpy(), out_sigmoid.detach().numpy())\n    plt.title(r'$\\widetilde{p}(x)$ = Sigmoid $(x)$')\n    # MSE loss w.r.t. \\widetilde{p}\n    plt.subplot(3, 3, 2)\n    plt.plot(p_tilde_0.detach().numpy(), MSE_loss_p0_p.detach().numpy(),\n             color='blue', label=r'$p=$' + f'{p0}')\n    plt.plot(p_tilde_1.detach().numpy(), MSE_loss_p1_p.detach().numpy(),\n             color='red', label=r'$p=$' + f'{p1}')\n    plt.legend()\n    plt.title(r'MSE Loss $(\\widetilde{p}, p) = (\\widetilde{p} - p)^2$')\n    plt.subplots_adjust(bottom=-0.2)\n    # MSE loss w.r.t. x\n    plt.subplot(3, 3, 3)\n    plt.plot(grad_points_p0_x.detach().numpy(), MSE_loss_p0_x.detach().numpy(),\n             color='blue', label=r'$p=$' + f'{p0}')\n    plt.plot(grad_points_p1_x.detach().numpy(), MSE_loss_p1_x.detach().numpy(),\n             color='red', label=r'$p=$' + f'{p1}')\n    plt.legend()\n    plt.title(r'MSE Loss $(x, p) = (\\widetilde{p}(x) - p)^2$')\n    # derivative of sigmoid\n    plt.subplot(3, 3, 4)\n    plt.plot(grad_points.detach().numpy(), grad_points.grad)\n    plt.title(r'Derivative Sigmoid w.r.t. x')\n    plt.xlabel('x')\n    # derivative of MSE loss w.r.t. \\widetilde{p}\n    plt.subplot(3, 3, 5)\n    plt.plot(p_tilde_0.detach().numpy(), p_tilde_0.grad, color='blue',\n             label=r'$p=$' + f'{p0}')\n    plt.plot(p_tilde_1.detach().numpy(), p_tilde_1.grad, color='red',\n             label=r'$p=$' + f'{p1}')\n    plt.xlabel(r'$\\widetilde{p}$')\n    plt.title(r'Derivative MSE loss w.r.t. $\\widetilde{p}$')\n    plt.legend()\n    # derivative of MSE Loss w.r.t x\n    plt.subplot(3, 3, 6)\n    plt.plot(grad_points_p0_x.detach().numpy(), grad_points_p0_x.grad,\n             color='blue', label=r'$p=$' + f'{p0}')\n    plt.plot(grad_points_p1_x.detach().numpy(), grad_points_p1_x.grad,\n             color='red', label=r'$p=$' + f'{p1}')\n    plt.xlabel(r'$x$')\n    plt.title(r'Derivative MSE loss w.r.t. x')\n    plt.legend()\n    return\n\nmake_visualization()"
  },
  {
    "objectID": "ml101/probability_theory/evidence_lower_bound/index.html",
    "href": "ml101/probability_theory/evidence_lower_bound/index.html",
    "title": "What is the ELBO?",
    "section": "",
    "text": "Consider the following case: Let \\(\\textbf{X} = \\{\\textbf{x}^{(i)}\\}_{i=1}^N\\) denote a dataset consisting of \\(N\\) i.i.d. samples where each observed datapoint \\(\\textbf{x}^{(i)}\\) is generated from a process in which firstly a latent (hidden) variable \\(\\textbf{z}^{(i)}\\) is sampled from a prior distribution \\(p_{\\boldsymbol{\\theta}} (\\textbf{z})\\) and then \\(\\textbf{x}^{(i)}\\) is sampled from a conditional distribution \\(p_{\\boldsymbol{\\theta}} \\left(\\textbf{x} | \\textbf{z}^{(i)}\\right)\\).\nThe evidence lower bound (ELBO) \\(\\mathcal{L} \\left(\n  \\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\textbf{x}^{(i)}\\right)\\) (or variational lower bound) which is\n\\[\n  \\mathcal{L} \\left( \\boldsymbol{\\theta}, \\boldsymbol{\\phi};\n  \\textbf{x}^{(i)} \\right) = - D_{KL} \\left( q_{\\boldsymbol{\\phi}}\n  \\left( \\textbf{z} | \\textbf{x}^{(i)} \\right) ||\n  p_{\\boldsymbol{\\theta}} (\\textbf{z})\\right) +\n  \\mathbb{E}_{q_{\\boldsymbol{\\phi}}\n  \\left(\\textbf{z}|\\textbf{x}^{(i)}\\right)} \\left[ \\log\n  p_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)} | \\textbf{z}\\right) \\right],\n\\]\ndefines a lower bound on the log-evidence \\(\\log\np_{\\boldsymbol{\\theta}}(\\textbf{x}^{(i)})\\) given a variational approximation \\(q_{\\boldsymbol{\\phi}} \\left(\\textbf{z} |\n\\textbf{x}^{(i)} \\right)\\) of the true posterior \\(p_{\\boldsymbol{\\theta}} \\left( \\textbf{z} |\n\\textbf{x}^{(i)}\\right)\\), i.e.,\n\\[\n\\begin{align}\n  0 &\\ge \\log p_{\\boldsymbol{\\theta}} (\\textbf{x}^{(i)}) = \\underbrace{D_{KL} \\left(\n  q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}^{(i)} \\right)\n  ||\n  p_{\\boldsymbol{\\theta}} \\left( \\textbf{z} |\n  \\textbf{x}^{(i)}\\right)\\right)}_{\\ge 0} + \\mathcal{L} \\left(\n  \\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\textbf{x}^{(i)}\\right),\\\\\n  &\\Rightarrow \\mathcal{L} \\left( \\boldsymbol{\\theta}, \\boldsymbol{\\phi} \\right) \\le \\log p_{\\boldsymbol{\\theta}} (\\textbf{x}^{(i)}),\n\\end{align}\n\\] where \\(p_{\\boldsymbol{\\theta}} (\\textbf{z})\\) is a prior on the latent distribution. Note that true posterior \\(p_{\\boldsymbol{\\theta}} \\left( \\textbf{z} |\n\\textbf{x}^{(i)}\\right)\\) (which is often unknown) does not appear in the ELBO!\nVariational Bayesian methods are a very popular framework in machine learning, since they allow to cast statistical inference problems into optimization problems. E.g., the inference problem of determining the true posterior distribution \\(p_{\\boldsymbol{\\theta}} \\left( \\textbf{z} |\n\\textbf{x}^{(i)}\\right)\\) can be cast into an optimization problem by maximizing the ELBO using/introducing a variational approximation \\(q_{\\boldsymbol{\\phi}} \\left(\\textbf{z} | \\textbf{x} \\right)\\) and a latent prior \\(p_{\\boldsymbol{\\theta}} (\\textbf{z})\\). This is the main idea of variational auto-encoders (VAEs) by Kingma and Welling (2013).\n\n\n\n\n\n\nDerivation - Version 1: Optimization Problem\n\n\n\n\n\nLet’s start with the optimization problem:\n\\[\n  \\min_{\\boldsymbol{\\phi}} D_{KL} \\left[ q_{\\boldsymbol{\\phi}} \\left(\\textbf{z} | \\textbf{x} \\right) || p_{\\boldsymbol{\\theta}} \\left( \\textbf{z} |\n\\textbf{x}\\right) \\right],\n\\]\ni.e., we are aiming to find the parameters \\(\\boldsymbol{\\phi}\\) such that the true probability distributions are as similiar as possible (have minimal KL divergence, just ignore the fact that the KL divergence is not symmetric). Actually, we cannot compute this quantity since we do not have access to the true posterior (if we had, we wouldn’t need to introduce a variational approximation).\nHowever, we can rewrite the KL divergence as follows\n\\[\n\\begin{align}\nD_{KL} \\left[ q_{\\boldsymbol{\\phi}} \\left(\\textbf{z} | \\textbf{x} \\right) || p_{\\boldsymbol{\\theta}} \\left( \\textbf{z} |\n\\textbf{x}\\right) \\right] &= \\int_\\textbf{z}\nq_{\\boldsymbol{\\phi}} \\left(\\textbf{z} | \\textbf{x} \\right) \\log \\frac\n{q_{\\boldsymbol{\\phi}} \\left(\\textbf{z} | \\textbf{x} \\right)} {p_{\\boldsymbol{\\theta}} \\left( \\textbf{z} |\n\\textbf{x}\\right)} d\\textbf{z}\\\\\n&= \\mathbb{E}_{\\textbf{z} \\sim q_{\\boldsymbol{\\phi}} \\left(\\textbf{z}\n| \\textbf{x} \\right)}\n\\left[\\log q_{\\boldsymbol{\\phi}} \\left(\\textbf{z} | \\textbf{x} \\right) - \\log p_{\\boldsymbol{\\theta}} \\left( \\textbf{z} |\n\\textbf{x}\\right) \\right]\n\\end{align}\n\\]\nRemember Bayes rule:\n\\[\np_{\\boldsymbol{\\theta}} \\left( \\textbf{z} |\n\\textbf{x}\\right) = \\frac {p_{\\boldsymbol{\\theta}} \\left( \\textbf{x} |\n\\textbf{z}\\right) p_{\\boldsymbol{\\theta}} \\left( \\textbf{z} \\right)  }\n{p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}\\right)}\n\\]\nLet’s put this into the above equation (and use the logarithm rules)\n\\[\n\\begin{align}\nD_{KL} \\left[ q_{\\boldsymbol{\\phi}} \\left(\\textbf{z} | \\textbf{x} \\right) || p_{\\boldsymbol{\\theta}} \\left( \\textbf{z} |\n\\textbf{x}\\right) \\right] & =  \\mathbb{E}_{\\textbf{z} \\sim q_{\\boldsymbol{\\phi}} \\left(\\textbf{z}\n| \\textbf{x} \\right)}\n\\left[ \\log q_{\\boldsymbol{\\phi}} \\left(\\textbf{z} | \\textbf{x}\n\\right) - \\left(\\log  p_{\\boldsymbol{\\theta}} \\left( \\textbf{x} |\n\\textbf{z} \\right) + \\log p_{\\boldsymbol{\\theta}} \\left( \\textbf{z} \\right)\n- \\log p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}\\right)\n\\right)\\right] \\\\\n&=\\mathbb{E}_{\\textbf{z} \\sim q_{\\boldsymbol{\\phi}} \\left(\\textbf{z}\n| \\textbf{x} \\right)}\n\\left[ \\log q_{\\boldsymbol{\\phi}} \\left(\\textbf{z} | \\textbf{x}\n\\right) - \\log  p_{\\boldsymbol{\\theta}} \\left( \\textbf{x} |\n\\textbf{z} \\right) - \\log p_{\\boldsymbol{\\theta}} \\left( \\textbf{z} \\right)\n\\right] + \\log p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}\\right)\n\\end{align}\n\\]\nThat already looks suspiciously close to what we acutally want to show. Let’s put the log evidence term on one side and the rest on the other side to better see what we have\n\\[\n\\begin{align}\n\\log p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}\\right) &=  D_{KL} \\left[ q_{\\boldsymbol{\\phi}} \\left(\\textbf{z} | \\textbf{x} \\right) || p_{\\boldsymbol{\\theta}} \\left( \\textbf{z} |\n\\textbf{x}\\right) \\right] \\\\\n&\\quad - \\underbrace{\\mathbb{E}_{\\textbf{z} \\sim q_{\\boldsymbol{\\phi}} \\left(\\textbf{z}\n| \\textbf{x} \\right)}\n\\left[ \\log q_{\\boldsymbol{\\phi}} \\left(\\textbf{z} | \\textbf{x}\n\\right) - \\log  p_{\\boldsymbol{\\theta}} \\left( \\textbf{x} |\n\\textbf{z} \\right) - \\log p_{\\boldsymbol{\\theta}} \\left( \\textbf{z} \\right)\n\\right] + \\log p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}\\right)}_{- \\mathcal{L}}\n\\end{align}\n\\]\nSome final rewritings\n\\[\n\\begin{align}\n\\mathcal{L} &= - \\mathbb{E}_{\\textbf{z} \\sim q_{\\boldsymbol{\\phi}} \\left(\\textbf{z}\n| \\textbf{x} \\right)}\n\\left[ \\log q_{\\boldsymbol{\\phi}} \\left(\\textbf{z} | \\textbf{x}\n\\right) - \\log  p_{\\boldsymbol{\\theta}} \\left( \\textbf{x} |\n\\textbf{z} \\right) - \\log p_{\\boldsymbol{\\theta}} \\left( \\textbf{z} \\right)\n\\right] \\\\\n&= -\\mathbb{E}_{\\textbf{z} \\sim q_{\\boldsymbol{\\phi}} \\left(\\textbf{z}\n| \\textbf{x} \\right)}\n\\left[ \\log q_{\\boldsymbol{\\phi}} \\left(\\textbf{z} | \\textbf{x}\n\\right) - \\log p_{\\boldsymbol{\\theta}} \\left( \\textbf{z} \\right)\n\\right] + \\mathbb{E}_{\\textbf{z} \\sim q_{\\boldsymbol{\\phi}} \\left(\\textbf{z}\n| \\textbf{x} \\right)} \\left[ \\log  p_{\\boldsymbol{\\theta}} \\left( \\textbf{x} |\n\\textbf{z} \\right)\\right]\\\\\n&= - D_{KL} \\left( q_{\\boldsymbol{\\phi}}\n  \\left( \\textbf{z} | \\textbf{x} \\right) ||\n  p_{\\boldsymbol{\\theta}} (\\textbf{z})\\right) +\n  \\mathbb{E}_{q_{\\boldsymbol{\\phi}}\n  \\left(\\textbf{z}|\\textbf{x}\\right)} \\left[ \\log\n  p_{\\boldsymbol{\\theta}} \\left( \\textbf{x} | \\textbf{z}\\right) \\right]\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\nDerivation - Version 2: Jensen’s Inequality\n\n\n\n\n\nTaking Jensen’s inequality is a different path to arrive at the variational lower bound on the log-likelihood.\n\nJensen’s Inequality: Let \\(X\\) be a random variable and \\(\\varphi\\) a concave function, then\n\\[\n\\varphi \\Big(\\mathbb{E} \\left[ X \\right] \\Big) \\ge \\mathbb{E} \\Big[ \\varphi\n\\left( X \\right) \\Big]\n\\]\n\nLet’s simply state the marginal likelihood of \\(\\textbf{x}\\) and include our variatonal approximation of the true posterior:\n\\[\np_{\\boldsymbol{\\theta}}(\\textbf{x})\n=  \\int p_{\\boldsymbol{\\theta}} (\\textbf{x}, \\textbf{z}) d\\textbf{z}\n= \\int q_{\\phi} \\left( \\textbf{z}| \\textbf{x} \\right)\n\\frac {p_{\\boldsymbol{\\theta}} (\\textbf{x}, \\textbf{z})}\n{q_{\\phi} \\left( \\textbf{z} | \\textbf{x} \\right)} d\\textbf{z} =\n\\mathbb{E}_{\\textbf{z} \\sim q_{\\phi} \\left( \\textbf{z}| \\textbf{x} \\right)}\n\\left[\n\\frac {p_{\\boldsymbol{\\theta}} (\\textbf{x}, \\textbf{z})}\n{q_{\\phi} \\left( \\textbf{z} | \\textbf{x} \\right)} \\right]\n\\]\nApplying logarithm (concave function) and Jensen’s inequality, we arrive at\n\\[\n\\log p_{\\boldsymbol{\\theta}}(\\textbf{x}) = \\log\n\\mathbb{E}_{\\textbf{z} \\sim q_{\\phi} \\left( \\textbf{z}| \\textbf{x} \\right)}\n\\left[\n\\frac {p_{\\boldsymbol{\\theta}} (\\textbf{x}, \\textbf{z})}\n{q_{\\phi} \\left( \\textbf{z} | \\textbf{x} \\right)} \\right] \\ge\n\\mathbb{E}_{\\textbf{z} \\sim q_{\\phi} \\left( \\textbf{z}| \\textbf{x} \\right)}\n\\left[ \\log\n\\frac {p_{\\boldsymbol{\\theta}} (\\textbf{x}, \\textbf{z})}\n{q_{\\phi} \\left( \\textbf{z} | \\textbf{x} \\right)}\n\\right] = \\mathcal{L}\n\\]\nSome final rewritings\n\\[\n\\begin{align*}\n\\mathcal{L} &=\n\\mathbb{E}_{\\textbf{z} \\sim q_{\\phi} \\left( \\textbf{z}| \\textbf{x} \\right)}\n\\left[ \\log\np_{\\boldsymbol{\\theta}} (\\textbf{z} )  + \\log p_{\\boldsymbol{\\theta}}\n(\\textbf{x} | \\textbf{z} )  - \\log q_{\\phi} \\left( \\textbf{z} | \\textbf{x}\n\\right) \\right] \\\\\n&=\n-\\mathbb{E}_{\\textbf{z} \\sim q_{\\phi} \\left( \\textbf{z}| \\textbf{x} \\right)}\n\\left[ \\log q_{\\phi} \\left( \\textbf{z} | \\textbf{x}\n\\right) - \\log\np_{\\boldsymbol{\\theta}} (\\textbf{z} )\\right]\n+\n\\mathbb{E}_{\\textbf{z} \\sim q_{\\phi} \\left( \\textbf{z}| \\textbf{x} \\right)}\n\\left[\n\\log\np_{\\boldsymbol{\\theta}} (\\textbf{x} | \\textbf{z} )\n\\right]\\\\\n&= - D_{KL} \\left( q_{\\boldsymbol{\\phi}}\n  \\left( \\textbf{z} | \\textbf{x} \\right) ||\n  p_{\\boldsymbol{\\theta}} (\\textbf{z})\\right) +\n  \\mathbb{E}_{q_{\\boldsymbol{\\phi}}\n  \\left(\\textbf{z}|\\textbf{x}\\right)} \\left[ \\log\n  p_{\\boldsymbol{\\theta}} \\left( \\textbf{x} | \\textbf{z}\\right) \\right]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "ml101/probability_theory/linear_regression_and_maximum_likelihood/index.html",
    "href": "ml101/probability_theory/linear_regression_and_maximum_likelihood/index.html",
    "title": "How is linear regression connected to maximum likelihood?",
    "section": "",
    "text": "Under the assumption that the data is generated from a linear function where each observation exhibts i.i.d. Gaussian noise (i.e., each noise variable \\(\\nu_i\\) is sampled independently from the same/identical Gaussian distribution), the negative log-likelihood is proportional to the squared sum of the residuals. Thus, minimizing the negative log-likelihood (maximizing the log-likelihood) leads to the same solution as the least squares method.\n\n\n\n\n\n\nDerivation\n\n\n\n\n\nGiven a dataset \\(\\mathcal{D} = \\{(\\textbf{x}_1, y_1), \\dots,\n(\\textbf{x}_N, y_N)\\}\\) and the model assumption:\n\\[\n    y_i = \\textbf{w}^{\\text{T}} \\textbf{x}_i + \\beta + \\nu_i \\quad \\quad \\nu \\sim \\mathcal{N}(0, \\sigma^2)\n\\]\nthe parameters of the model are \\(\\boldsymbol{\\theta} = (\\textbf{w}, \\beta,\n\\sigma^{2})\\). In terms of maximum likelihood, we want to maximize:\n\\[\n\\begin{align}\np(\\mathcal{D} | \\boldsymbol{\\theta}) &=\np (\\textbf{x}_1) \\cdot p(y_1| \\textbf{x}_1; \\boldsymbol{\\theta}) \\cdot ... \\cdot p(\\textbf{x}_N| \\textbf{x}_1, \\dots,\n\\textbf{x}_{N-1}) \\cdot p(y_N| \\textbf{x}_N; \\boldsymbol{\\theta})\\\\&=\np_X (\\textbf{x}_1, \\dots, \\textbf{x}_N) \\prod_{i=1}^N p(y_i | \\textbf{x}_i; \\boldsymbol{\\theta})\n\\end{align}\n\\]\nwhere the density \\(p_X\\) is unknown and independent of \\(\\boldsymbol{\\theta}\\). Given our model assumption, we can write\n\\[\np(y_i|\\textbf{x}_i, \\boldsymbol{\\theta})  \\sim\n\\mathcal{N} \\left(\\textbf{w}^{\\text{T}} \\textbf{x}_i + \\beta, \\sigma^2  \\right) = \\frac {1}{\\sqrt{2\\pi \\sigma^2}}\n\\exp\\left( - \\frac {(y_i - \\textbf{w}^{\\text{T}} \\textbf{x}_i - \\beta)^2} {\\sigma^2} \\right)\n\\]\nThus, we get for the negative log-likelihood\n\\[\n-\\ln p(\\mathcal{D}|\\theta) = \\sum_{i=1}^N \\frac {1}{2} \\log \\big(2 \\pi \\sigma^2 \\big) + \\frac {1}{2\\sigma^2}\n\\big(y_i - \\textbf{w}^{\\text{T}} \\textbf{x}_i - \\beta \\big)^2 \\propto\n\\sum_{i=1}^N \\big(y_i - \\textbf{w}^{\\text{T}} \\textbf{x}_i - \\beta \\big)^2\n\\]\nAs stated above, minimizing the negative log-likelihood (left side) leads to the same solution as minimizing the sum of the squares of the residuals (right side), i.e., the method of least squares. For a more detailed derivation, see d2l.ai. Note that in linear regression, we do not intent to estimate \\(\\sigma^2\\). However, it may be helpful since a large variance indicates that our linear regression model will produce highly biased estimates."
  },
  {
    "objectID": "ml101/probability_theory/better_score_function_estimator/index.html",
    "href": "ml101/probability_theory/better_score_function_estimator/index.html",
    "title": "How can the variance of the score function estimator be decreased?",
    "section": "",
    "text": "Remind that the score function estimator moves the gradient of an expectation inside the expecation to allow for Monte Carlo integration, i.e.,\n\\[\n\\begin{align}\n\\nabla_{\\boldsymbol{\\theta}} \\mathbb{E}_{\\textbf{z} \\sim\n  p_{\\boldsymbol{\\theta}}\\left(\\textbf{z} \\right)} \\big[ f\n  (\\textbf{z})\\big] &= \\mathbb{E}_{\\textbf{z} \\sim\n  p_{\\boldsymbol{\\theta}}\\left(\\textbf{z} \\right)} \\left[\n  f(\\textbf{z}) \\nabla_{\\boldsymbol{\\theta}} \\log\n  p_{\\boldsymbol{\\theta}} (\\textbf{z}) \\right] \\\\\n  &\\approx \\frac {1}{N}\n  \\sum_{i=1}^N f\\left(\\textbf{z}^{(i)}\\right) \\nabla_{\\boldsymbol{\\theta}} \\log\n  p_{\\boldsymbol{\\theta}} \\left( \\textbf{z}^{(i)}\\right) \\quad\n  \\text{with} \\quad \\textbf{z}^{(i)} \\sim p_{\\boldsymbol{\\theta}}(\\textbf{z}),\n\\end{align}\n\\]\nwhere the term \\(\\nabla_{\\boldsymbol{\\theta}}\\log p_{\\boldsymbol{\\theta}} (\\textbf{z})\\) is called the score function. A nice property of the score function is that its expectated value is zero, i.e.,\n\\[\n\\mathbb{E}_{\\textbf{z} \\sim\n  p_{\\boldsymbol{\\theta}}\\left(\\textbf{z} \\right)} \\left[\n  \\nabla_{\\boldsymbol{\\theta}}\\log p_{\\boldsymbol{\\theta}}\n  (\\textbf{z}) \\right] = \\textbf{0}\n\\]\nUsing this property and the linearity of the expectation, we can add an arbitrary term with zero expectation:\n\\[\n\\begin{align}\n\\nabla_{\\boldsymbol{\\theta}} \\mathbb{E}_{\\textbf{z} \\sim\n  p_{\\boldsymbol{\\theta}}\\left(\\textbf{z} \\right)} \\big[ f\n  (\\textbf{z})\\big] &= \\mathbb{E}_{\\textbf{z} \\sim\n  p_{\\boldsymbol{\\theta}}\\left(\\textbf{z} \\right)} \\left[\n  \\Big(f(\\textbf{z}) - \\lambda \\Big)\\nabla_{\\boldsymbol{\\theta}} \\log\n  p_{\\boldsymbol{\\theta}} (\\textbf{z}) \\right] \\\\\n  &\\approx \\frac {1}{N}\n  \\sum_{i=1}^N \\Big(f\\left(\\textbf{z}^{(i)}\\right) - \\lambda \\Big)\\nabla_{\\boldsymbol{\\theta}} \\log\n  p_{\\boldsymbol{\\theta}} \\left( \\textbf{z}^{(i)}\\right) \\quad\n  \\text{with} \\quad \\textbf{z}^{(i)} \\sim p_{\\boldsymbol{\\theta}}(\\textbf{z}),\n\\end{align}\n\\]\nwhere \\(\\lambda\\) is called control variate or baseline which allows us to decrease the variance. This can be done via\n\nminimzing the variance of the above term w.r.t. \\(\\lambda\\), or\nusing a data-dependent baseline or neural baseline in which we train the control variate such that\n\\[\n\\lambda_{\\boldsymbol{\\phi}} (\\textbf{z}) \\approx f(\\textbf{z})\n\\]\n\n\n\n\n\n\n\n\nExpectation of Score Function\n\n\n\n\n\nThe expectation of the score function is zero:\n\\[\n\\begin{align}\n\\mathbb{E}_{\\textbf{z} \\sim\n  p_{\\boldsymbol{\\theta}}\\left(\\textbf{z} \\right)} \\left[\n  \\nabla_{\\boldsymbol{\\theta}}\\log p_{\\boldsymbol{\\theta}}\n  (\\textbf{z}) \\right] &= \\int \\nabla_{\\boldsymbol{\\theta}}\\log p_{\\boldsymbol{\\theta}}\n  (\\textbf{z}) p_{\\boldsymbol{\\theta}} (\\textbf{z}) d\\textbf{z} \\\\\n  &=\\int \\frac {\\nabla_{\\boldsymbol{\\theta}} p_{\\boldsymbol{\\theta}}\n  (\\textbf{z})} {p_{\\boldsymbol{\\theta}}\n  (\\textbf{z})}  p_{\\boldsymbol{\\theta}}\n  (\\textbf{z}) d\\textbf{z} &&\\text{(log derivative trick)}\\\\\n  &= \\nabla_{\\boldsymbol{\\theta}} \\int p_{\\boldsymbol{\\theta}}\n  (\\textbf{z}) d\\textbf{z} &&\\text{(Leibniz integral rule)}\\\\\n  &= \\nabla_{\\boldsymbol{\\theta}} \\textbf{1} = \\textbf{0}\n\\end{align}\n\\]\nThe log derivative trick comes from the application of the chain rule, see this post.\n\n\n\n\n\n\n\n\n\nVariance of Score Function\n\n\n\n\n\nThe variance of the score function estimator without baseline \\(\\lambda\\) \\[\n\\begin{align}\n&\\text{Var} \\Big[ \\mathbb{E}_{\\textbf{z} \\sim\n  p_{\\boldsymbol{\\theta}}\\left(\\textbf{z} \\right)} \\left[\n  f(\\textbf{z})\\nabla_{\\boldsymbol{\\theta}} \\log\n  p_{\\boldsymbol{\\theta}} (\\textbf{z}) \\right] \\Big] \\\\\n  &=\n\\mathbb{E}_{\\textbf{z} \\sim\n  p_{\\boldsymbol{\\theta}}\\left(\\textbf{z} \\right)} \\Big[ \\Big(\n  f(\\textbf{z}) \\nabla_{\\boldsymbol{\\theta}} \\log\n  p_{\\boldsymbol{\\theta}} (\\textbf{z}) \\Big)^2\\Big]  \n  -\n\\Big(\\mathbb{E}_{\\textbf{z} \\sim\n  p_{\\boldsymbol{\\theta}}\\left(\\textbf{z} \\right)} \\Big[\n  f(\\textbf{z}) \\nabla_{\\boldsymbol{\\theta}} \\log\n  p_{\\boldsymbol{\\theta}} (\\textbf{z}) \\Big]  \n  \\Big)^2\\\\\n  &= \\text{Var}_{\\text{score}}\n\\end{align}\n\\]\nThe variance of the score function estimator including baseline \\(\\lambda\\)\n\\[\n\\begin{align}\n&\\text{Var} \\Big[ \\mathbb{E}_{\\textbf{z} \\sim\n  p_{\\boldsymbol{\\theta}}\\left(\\textbf{z} \\right)} \\left[\n  \\Big(f(\\textbf{z}) - \\lambda \\Big)\\nabla_{\\boldsymbol{\\theta}} \\log\n  p_{\\boldsymbol{\\theta}} (\\textbf{z}) \\right] \\Big] \\\\\n  &=\n\\mathbb{E}_{\\textbf{z} \\sim\n  p_{\\boldsymbol{\\theta}}\\left(\\textbf{z} \\right)} \\Big[ \\big(\n  \\Big(f(\\textbf{z}) - \\lambda \\Big)\\nabla_{\\boldsymbol{\\theta}} \\log\n  p_{\\boldsymbol{\\theta}} (\\textbf{z}) \\big)^2\\Big]  \n  -\n\\Big(\\mathbb{E}_{\\textbf{z} \\sim\n  p_{\\boldsymbol{\\theta}}\\left(\\textbf{z} \\right)} \\Big[\n  \\Big(f(\\textbf{z}) - \\lambda \\Big)\\nabla_{\\boldsymbol{\\theta}} \\log\n  p_{\\boldsymbol{\\theta}} (\\textbf{z}) \\Big]  \n  \\Big)^2\\\\\n  &=\\text{Var}_{\\text{score}} -\n\\mathbb{E}_{\\textbf{z} \\sim\n  p_{\\boldsymbol{\\theta}}\\left(\\textbf{z} \\right)}\n  \\Big[\n  \\big(  2 f(\\textbf{z}) \\lambda  - \\lambda^2  \n  \\big)\n\\nabla_{\\boldsymbol{\\theta}} \\log\n  p_{\\boldsymbol{\\theta}} (\\textbf{z})\n  \\Big]\n  +\n  \\Big( \\mathbb{E}_{\\textbf{z} \\sim\n  p_{\\boldsymbol{\\theta}}\\left(\\textbf{z} \\right)}\n  \\big[ \\lambda\n\\nabla_{\\boldsymbol{\\theta}} \\log\n  p_{\\boldsymbol{\\theta}} (\\textbf{z})\n  \\big]\n  \\Big)^2\n\\end{align}\n\\]"
  },
  {
    "objectID": "paper_summaries.html",
    "href": "paper_summaries.html",
    "title": "Paper Summaries",
    "section": "",
    "text": "MONet: Unsupervised Scene Decomposition and Representation\n\n\n\n\n\n\nreimplementation\n\n\nunsupervised\n\n\nVAE\n\n\n\n\n\n\nMar 27, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nThe Concrete Distribution: A Continuous Relaxation of Discrete Random Variables\n\n\n\n\n\n\nreimplementation\n\n\nVAE\n\n\nD-VAE\n\n\n\n\n\n\nFeb 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nImportance Weighted Autoencoders\n\n\n\n\n\n\nreimplementation\n\n\nVAE\n\n\nIWAE\n\n\ngenerative\n\n\n\n\n\n\nJan 10, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nAttend, Infer, Repeat: Fast Scene Understanding with Generative Models\n\n\n\n\n\n\nreimplementation\n\n\nunsupervised\n\n\ngenerative\n\n\n\n\n\n\nSep 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial Transformer Networks\n\n\n\n\n\n\nreimplementation\n\n\n\n\n\n\nAug 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nU-Net: Convolutional Networks for Biomedical Image Segmentation\n\n\n\n\n\n\nreimplementation\n\n\nCNN\n\n\nsegmentation\n\n\n\n\n\n\nAug 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial Broadcast Decoder: A Simple Architecture for Learning Disentangled Representations in VAEs\n\n\n\n\n\n\nreimplementation\n\n\nVAE\n\n\ndisentanglement\n\n\n\n\n\n\nAug 7, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nAuto-Encoding Variational Bayes\n\n\n\n\n\n\nreimplementation\n\n\nVAE\n\n\ngenerative\n\n\n\n\n\n\nJul 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nSchema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics\n\n\n\n\n\n\nRL\n\n\n\n\n\n\nJul 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nRelational Inductive Biases, Deep Learning, and Graph Networks\n\n\n\n\n\n\ninteraction network\n\n\ngraph networks\n\n\ngeneralization\n\n\n\n\n\n\nJul 10, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nInteraction Networks for Learning about Objects, Relations and Physics\n\n\n\n\n\n\ngraph networks\n\n\n\n\n\n\nJun 26, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "paper_summaries/u_net/index.html",
    "href": "paper_summaries/u_net/index.html",
    "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
    "section": "",
    "text": "Ronneberger et al. (2015) introduced a novel neural network architecture to generate better semantic segmentations (i.e., class label assigend to each pixel) in limited datasets which is a typical challenge in the area of biomedical image processing (see figure below for an example). In essence, their model consists of a U-shaped convolutional neural network (CNN) with skip connections between blocks to capture context information, while allowing for precise localizations. In addition to the network architecture, they describe some data augmentation methods to use available data more efficiently. By the time the paper was published, the proposed architecture won several segmentation challenges in the field of biomedical engineering, outperforming state-of-the-art models by a large margin. Due to its success and efficiency, U-Net has become a standard architecture when it comes to image segmentations tasks even in the non-biomedical area (e.g., image-to-image translation, neural style transfer, Multi-Objetct Network)."
  },
  {
    "objectID": "paper_summaries/u_net/index.html#model-description",
    "href": "paper_summaries/u_net/index.html#model-description",
    "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
    "section": "Model Description",
    "text": "Model Description\nU-Net builds upon the ideas of Fully Convolutional Networks (FCNs) for Semantic Segmentation by Long et al. (2015) who successfully trained FCNs (including convolutional prediction, upsampling layers and skip connections) end-to-end (pixels-to-pixels) on semantic segmentation tasks. U-Net is basically a modified version of the FCN by making the architecture more symmetric, i.e., adding a more powerful expansive path. Ronneberger et al. (2015) argue that this modification yields more precise segmentations due to its capacity to better propagate context information to higher resolution layers.\nFCN architecture: The main idea of the FCN architecture is to take a standard classification network (such as VGG-16), discard the final classifier layer, convert fully connected layers into convolutions (i.e., prediction layers) and add skip connections to (some) pooling layers, see figure below. The skip connections consist of a prediction (\\(1 \\times 1\\) convolutional layer with channel dimension equal to number of possible classes) and a deconvolutional (upsampling) layer.\n\n\n\n\n\n\n\n\nExample of FCN Architecture. VGG-16 net is used as feature learning part. Numbers under the cubes indicate the number of output channels. The prediction layer is itself a \\(1 \\times 1\\) convolutional layer (the final output consists only of 6 possible classes). A final softmax layer is added to output a normalized classification per pixel. Taken from Tai et al. (2017)\n\n\n\nU-Net architecture: The main idea of the U-Net architecture is to build an encoder-decoder FCN with skip connections between corresponding blocks, see figure below. The left side of U-Net, i.e., contractive path or encoder, is very similar to the left side of the FC architecture above. The right side of U-Net, i.e., expansive path or decoder, differs due to its number of feature channels and the convolutional + ReLu layers. Note that the input image size is greater than the output segmentation size, i.e., the network only segments the inner part of the image1.\n\n\n\n\n\n\n\n\nU-Net architecture as proposed by Ronneberger et al. (2015).\n\n\n\nMotivation: Semantic segmentation of images can be divided into two tasks\n\nContext Information Retrieval: Global information about the different parts of the image, e.g., in a CNN classification network after training there might be some feature representation for nose, eyes and mouth. Depending on the feature combination at hand, the network may classify the image as human or not human.\nLocalization of Context Information: In addition to what, localization ensures where. Semantic segmentation is only possible when content information can be localized. Note: In image classification, we are often not interested in where2.\n\nLong et al. (2015) argue that CNNs during classification tasks must learn useful feature representations, i.e., classification nets are capable to solve the context information retrieval task. Fully connected layers are inappropriate for semantic segmentation as they throw away the principle of localization. These two arguments motivate the use of FCNs that take the feature representation part of classification nets and convert fully connected layers into convolutions. During the contractive path, information gets compressed into coarse appearance/context information. However, in this process the dimensionality of the input is reduced massively. Skip connections are introduced to combine coarse, semantic information of deeper layers with finer, appearance information of early layers. Thereby, the localization task is addressed.\nRonneberger et al. (2015) extend these ideas by essentially increasing the capacity of the decoder path. The symmetric architecture allows to combine low level feature maps (left side, fine information) with high level feature maps (right side, coarse information) more effectively such that context information can be better propagated to higher resolution layers (top right). As a result, more precise segmentations can be retrieved even with few training examples, indicating that the optimization problem is better posed in U-Nets."
  },
  {
    "objectID": "paper_summaries/u_net/index.html#implementatation",
    "href": "paper_summaries/u_net/index.html#implementatation",
    "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
    "section": "Implementatation",
    "text": "Implementatation\nRonneberger et al. (2015) demonstrated U-Net application results for three different segmentation tasks and open-sourced their original U-Net implementation (or rather the ready trained network). The whole training process and data augmentation procedures are not provided (except for overlap-tile segmentation). The following reimplementation aims to give an understanding of the whole paper (data augmentation and training process included), while being as simple as possible. Note that there are lots of open-source U-Net reimplementations out there, however most of them are already modified versions.\n\nEM Dataset\nOnly the first task of the three different U-Net applications is reimplemented: The segmentation of neuronal structures in electron microscopic (EM) recordings. The training data consists of 30 images (\\(512 \\times 512\\) pixels with 8-bit grayscale) from the ventral nerve cord of some species of fruit flies together with the corresponding 30 binary segmentation masks (white pixels for segmented objects, black for the rest), see gif below. The dataset formed part of the 2D EM segmentation challenge at the ISBI 2012 conference. Although the workshop competition is done, the challenge remains open for new contributions. Further details about the data can be found at the ISBI Challenge website, where also the training and test data can be downloaded (after registration).\n\n\n\n\n\n\n\n\nEM training data. Taken from ISBI Challenge.\n\n\n\nThe following function can be used to load the training dataset.\n\n\nCode\nfrom PIL import Image\nimport torch\nimport torchvision.transforms as transforms\n\n\ndef load_dataset():\n    num_img, img_size = 30, 512\n    # initialize\n    imgs = torch.zeros(num_img, 1, img_size, img_size)\n    labels = torch.zeros(num_img, 1, img_size, img_size)\n    # fill tensors with data\n    for index in range(num_img):\n        cur_name = str(index) + '.png'\n\n        img_frame = Image.open('./Dataset/train/image/' + cur_name)\n        label_frame = Image.open('./Dataset/train/label/' + cur_name)\n\n        imgs[index] = transforms.ToTensor()(img_frame).type(torch.float32)\n        labels[index] = transforms.ToTensor()(label_frame).type(torch.float32)\n    return imgs, labels\n\n\nimgs, labels = load_dataset()\n\n\n\n\nData Augmentation\nTraining neural networks on image data typically requires large amounts of data to make the model robust (i.e., avoid overfitting) and accurate (i.e., avoid underfitting). However, data scarcity is a common problem in biomedical segmentation tasks, since labeling is expensive and time consuming. In such cases, data augmentation offers a solution by generating additional data (using plausible transformations) to expand the training dataset. In most image segmentation tasks the function to be learned has some transformation-invariance properties (e.g., translating the input should result in a translated output). The data augmentation applied by Ronneberger et al. (2015) can be divided into four parts:\n\nOverlap-tile strategy is used to divide an arbitrary large image into several overlaping parts (each forming an input and label to the training algorithm). Remind that the input to the neural network is greater than the output, in case of the EM dataset the input is even greater than the whole image. Therefore, Ronneberger et al. (2015) expand the images by mirroring at the sides. The overlap-tile strategy is shown below. Depending on the stride (i.e., how much the next rectangle is shifted to the right), the training dataset is enlarged by a factor greater than 4.\n\n\n\n\n\n\n\n\n\n\n\nOverlap-Tile Strategy for seamless segmentation of arbitrary large images. Blue area depicts input to neural network, yellow area corresponds to the prediction area. Missing input is extrapolated by mirroring (white lines). The number of tiles depends on the stride length (here: stride=124). Image created with visualize_overlap_tile_strategy (code presented at the end of this section).\n\n\n\nAffine transformations are mathematically defined as geometric transformations preserving lines and parallelisms, e.g., scaling, translation, rotation, reflection or any mix of them. Ronneberger et al. (2015) state that in case of microscopical image data mainly translation and rotation invariance (as affine transformation invariances) are desired properties of the resulting function. Note that the overlap-tile strategy itself leads to some translation invariance.\n\n\n\n\n\n\n\n\n\n\n\nAffine transformation visualization. Left side shows input and label data before transformation is applied. Right side shows the corresponding data after random affine transformation (random rotation and shifting). The grid is artificially added to emphasize that image and label are transformed in the same way. Image created with visualize_data_augmentation (code presented at the end of this section).\n\n\n\nElastic deformations are basically distinct affine transformations for each pixel. The term is probably derived from physics in which an elastic deformation describes a temporary change in shape of an elastic material (due to induced force). The transformation result looks similar to the physics phenomenon, see image below. Ronneberger et al. (2015) noted that elastic deformations seem to be a key concept for successfully training with few samples. A possible reason may be that the model’s generalization capabilities improve more by elastic deformations since the resulting images have more variability than with coherent affine transformations.\n\n\n\n\n\n\n\n\n\n\n\nElastic deformation visualization. Left side shows input and label data before deformation is applied. Right side shows the corresponding data after deformation. The grid is artificially added to emphasize that image and label are deformed in the same way. Image created with visualize_data_augmentation (code presented at the end of this section).\n\n\n\nImplementing elastic deformations basically consists of generating random displacement fields, convolving these with a Gaussian filter for smoothening, scaling the result by a predefined factor to control the intensity and then computing the new pixel values for each displacement vector (using interpolation within the old grid), see Best Practices for CNNs by Simard et al. (2003) for more details.     \nColor variations or in this case rather gray value variations in the input image should make the network invariant to small color changes. This can easily be implemented by adding Gaussian noise (other distributions may also be possible) to the input image, see image below.\n\n\n\n\n\n\n\n\n\n\n\nGray value variation visualization. Left side shows input image before noise is applied. Right side shows the corresponding data after transformation (segmentation mask does not change). Image created with visualize_data_augmentation (code presented at the end of this section).\n\n\n\n\nThe whole data augmentation process is put into a self written Pytorch Dataset class, see code below. Note that while this class includes all described transformations (affine transformation, elastic deformation and gray value variation), in the __get_item__ method only elastic_deform is applied to speed up the training process3. However, if you want to create a more sophisticated data augmentation process, you can easily add the other transformations in the __get_item__ method.\n\n\nCode\nfrom torch.utils.data import Dataset\nimport numpy as np\nfrom scipy.ndimage.interpolation import map_coordinates\nfrom scipy.signal import convolve2d\nimport torchvision.transforms.functional as TF\n\n\nclass EM_Dataset(Dataset):\n    \"\"\"EM Dataset (from ISBI 2012) to train U-Net on including data\n    augmentation as proposed by Ronneberger et al. (2015)\n\n    Args:\n        imgs (tensor): torch tensor containing input images [1, 512, 512]\n        labels (tensor): torch tensor containing segmented images [1, 512, 512]\n        stride (int): stride that is used for overlap-tile strategy,\n            Note: stride must be chosen such that all labels are retrieved\n        transformation (bool): transform should be applied (True) or not (False)\n    ------- transformation related -------\n        probability (float): probability that transformation is applied\n        alpha (float): intensity of elastic deformation\n        sigma (float): std dev. of Gaussian kernel, i.e., smoothing parameter\n        kernel dim (int): kernel size is [kernel_dim, kernel_dim]\n    \"\"\"\n\n    def __init__(self, imgs, labels, stride, transformation=False,\n                 probability=None, alpha=None, sigma=None, kernel_dim=None):\n        super().__init__()\n        assert isinstance(stride, int) and stride &lt;= 124 and \\\n          round((512-388)/stride) == (512-388)/stride\n        self.orig_imgs = imgs\n        self.imgs = EM_Dataset._extrapolate_by_mirroring(imgs)\n        self.labels = labels\n        self.stride = stride\n        self.transformation = transformation\n        if transformation:\n            assert 0 &lt;= probability &lt;= 1\n            self.probability = probability\n            self.alpha = alpha\n            self.kernel = EM_Dataset._create_gaussian_kernel(kernel_dim, sigma)\n        return\n\n    def __getitem__(self, index):\n        \"\"\"images and labels are divided into several overlaping parts using the\n        overlap-tile strategy\n        \"\"\"\n        number_of_tiles_1D = (1 + int((512 - 388)/self.stride))\n        number_of_tiles_2D = number_of_tiles_1D**2\n\n        img_index = int(index/number_of_tiles_2D)\n        # tile indexes of image\n        tile_index = (index % number_of_tiles_2D)\n        tile_index_x = (tile_index % number_of_tiles_1D) * self.stride\n        tile_index_y = int(tile_index / number_of_tiles_1D) * self.stride\n\n        img = self.imgs[img_index, :,\n                        tile_index_y:tile_index_y + 572,\n                        tile_index_x:tile_index_x + 572]\n        label = self.labels[img_index, :,\n                            tile_index_y: tile_index_y + 388,\n                            tile_index_x: tile_index_x + 388]\n        if self.transformation:\n            if np.random.random() &gt; 1 - self.probability:\n                img, label = EM_Dataset.elastic_deform(img, label, self.alpha,\n                                                       self.kernel)\n        return (img, label)\n\n    def __len__(self):\n        number_of_imgs = len(self.imgs)\n        number_of_tiles = (1 + int((512 - 388)/self.stride))**2\n        return number_of_imgs * number_of_tiles\n\n    @staticmethod\n    def gray_value_variations(image, sigma):\n        \"\"\"applies gray value variations by adding Gaussian noise\n\n        Args:\n            image (torch tensor): extrapolated image tensor [1, 572, 572]\n            sigma (float): std. dev. of Gaussian distribution\n\n        Returns:\n            image (torch tensor): image tensor w. gray value var. [1, 572, 572]\n        \"\"\"\n        # see https://stats.stackexchange.com/a/383976\n        noise = torch.randn(image.shape, dtype=torch.float32) * sigma\n        return image + noise\n\n    @staticmethod\n    def affine_transform(image, label, angle, translate):\n        \"\"\"applies random affine translations and rotation on image and label\n\n        Args:\n            image (torch tensor): extrapolated image tensor [1, 572, 572]\n            label (torch tensor): label tensor [1, 388, 388]\n            angle (float): rotation angle\n            translate (list): entries correspond to horizontal and vertical shift\n\n        Returns:\n            image (torch tensor): transformed image tensor [1, 572, 572]\n            label (torch tensor): transformed label tensor [1, 388, 388]\n        \"\"\"\n        # transform to PIL\n        image = transforms.ToPILImage()(image[0])\n        label = transforms.ToPILImage()(label[0])\n        # apply affine transformation\n        image = TF.affine(image, angle=angle, translate=translate,\n                          scale=1, shear=0)\n        label = TF.affine(label, angle=angle, translate=translate,\n                          scale=1, shear=0)\n        # transform back to tensor\n        image = transforms.ToTensor()(np.array(image))\n        label = transforms.ToTensor()(np.array(label))\n        return image, label\n\n    @staticmethod\n    def elastic_deform(image, label, alpha, gaussian_kernel):\n        \"\"\"apply smooth elastic deformation on image and label data as\n        described in\n\n        [Simard2003] \"Best Practices for Convolutional Neural Networks applied\n        to Visual Document Analysis\"\n\n        Args:\n            image (torch tensor): extrapolated image tensor [1, 572, 572]\n            label (torch tensor): label tensor [1, 388, 388]\n            alpha (float): intensity of transformation\n            gaussian_kernel (np array): gaussian kernel used for smoothing\n\n        Returns:\n            deformed_img (torch tensor): deformed image tensor [1, 572, 572]\n            deformed_label (torch tensor): deformed label tensor [1, 388, 388]\n\n        code is adapted from https://github.com/vsvinayak/mnist-helper\n        \"\"\"\n        # generate standard coordinate grids\n        x_i, y_i = np.meshgrid(np.arange(572), np.arange(572))\n        x_l, y_l = np.meshgrid(np.arange(388), np.arange(388))\n        # generate random displacement fields (uniform distribution [-1, 1])\n        dx = 2*np.random.rand(*x_i.shape) - 1\n        dy = 2*np.random.rand(*y_i.shape) - 1\n        # smooth by convolving with gaussian kernel\n        dx = alpha * convolve2d(dx, gaussian_kernel, mode='same')\n        dy = alpha * convolve2d(dy, gaussian_kernel, mode='same')\n        # one dimensional coordinates (neccessary for map_coordinates)\n        x_img = np.reshape(x_i + dx, (-1, 1))\n        y_img = np.reshape(y_i + dy, (-1, 1))\n        x_label = np.reshape(x_l + dx[92:480, 92:480], (-1, 1))\n        y_label = np.reshape(y_l + dy[92:480, 92:480], (-1, 1))\n        # deformation using map_coordinates interpolation (spline not bicubic)\n        deformed_img = map_coordinates(image[0], [y_img, x_img], order=1,\n                                       mode='reflect')\n        deformed_label = map_coordinates(label[0], [y_label, x_label], order=1,\n                                         mode='reflect')\n        # reshape into desired shape and cast to tensor\n        deformed_img = torch.from_numpy(deformed_img.reshape(image.shape))\n        deformed_label = torch.from_numpy(deformed_label.reshape(label.shape))\n        return deformed_img, deformed_label\n\n    @staticmethod\n    def _extrapolate_by_mirroring(data):\n        \"\"\"increase data by mirroring (needed for overlap-tile strategy)\n\n        Args:\n            data (torch tensor): shape [num_samples, 1, 512, 512]\n\n        Returns:\n            extrapol_data (torch tensor): shape [num_samples, 1, 696, 696]\n        \"\"\"\n        num_samples = len(data)\n        extrapol_data = torch.zeros(num_samples, 1, 696, 696)\n\n        # put data into center of extrapol data\n        extrapol_data[:,:, 92:92+512, 92:92+512] = data\n        # mirror left\n        extrapol_data[:,:, 92:92+512, 0:92] = data[:,:,:,0:92].flip(3)\n        # mirror right\n        extrapol_data[:,:, 92:92+512, 92+512::] = data[:,:,:,-92::].flip(3)\n        # mirror top\n        extrapol_data[:,:, 0:92,:] = extrapol_data[:,:,92:92+92,:].flip(2)\n        # mirror buttom\n        extrapol_data[:,:, 92+512::,:] = extrapol_data[:,:, 512:512+92,:].flip(2)\n        return extrapol_data\n\n    @staticmethod\n    def _create_gaussian_kernel(kernel_dim, sigma):\n        \"\"\"returns a 2D Gaussian kernel with the standard deviation\n        denoted by sigma\n\n        Args:\n            kernel_dim (int): kernel size will be [kernel_dim, kernel_dim]\n            sigma (float): std dev of Gaussian (smoothing parameter)\n\n        Returns:\n            gaussian_kernel (numpy array): centered gaussian kernel\n\n        code is adapted from https://github.com/vsvinayak/mnist-helper\n        \"\"\"\n        # check if the dimension is odd\n        if kernel_dim % 2 == 0:\n            raise ValueError(\"Kernel dimension should be odd\")\n        # initialize the kernel\n        kernel = np.zeros((kernel_dim, kernel_dim), dtype=np.float16)\n        # calculate the center point\n        center = kernel_dim/2\n        # calculate the variance\n        variance = sigma ** 2\n        # calculate the normalization coefficeint\n        coeff = 1. / (2 * variance)\n        # create the kernel\n        for x in range(0, kernel_dim):\n            for y in range(0, kernel_dim):\n                x_val = abs(x - center)\n                y_val = abs(y - center)\n                numerator = x_val**2 + y_val**2\n                denom = 2*variance\n\n                kernel[x,y] = coeff * np.exp(-1. * numerator/denom)\n        # normalise it\n        return kernel/sum(sum(kernel))\n\n\n# generate datasets\nstride = 124\nwhole_dataset = EM_Dataset(imgs, labels, stride=stride,\n                           transformation=True, probability=0.5, alpha=50,\n                           sigma=5, kernel_dim=25)\n\n\nThe visualization functions used to generate the images in this section are provided below:\n\n\nCode\nimport matplotlib.pyplot as plt\n\n\ndef visualize_overlap_tile_strategy(dataset, img_index, tile_indexes):\n    # compute tiling data\n    number_of_tiles_1D = (1 + int((512 - 388)/dataset.stride))\n    number_of_tiles_2D = number_of_tiles_1D**2\n    # original image [1, 512, 512]\n    orig_img = dataset.orig_imgs[img_index]\n    # extrapolated image [1, 696, 696]\n    extrapol_img = dataset.imgs[img_index]\n\n\n    # start plotting\n    fig = plt.figure(figsize=(14, 7))\n    # original image\n    plt.subplot(1, len(tile_indexes) + 1, 1)\n    plt.imshow(transforms.ToPILImage()(orig_img), cmap='gray')\n    plt.title('Original Image')\n    # extrapolated image with bounding boxes and mirror lines for tile_indexes\n    for index, tile_index in enumerate(tile_indexes):\n        plt.subplot(1, len(tile_indexes) + 1, 2 + index)\n        plt.imshow(transforms.ToPILImage()(extrapol_img), cmap='gray')\n        # calculate tile index x and y\n        tile_ix = (tile_index % number_of_tiles_1D) * dataset.stride\n        tile_iy = int(tile_index / number_of_tiles_1D) * dataset.stride\n        # add focus of current input tile\n        plt.plot([tile_ix, tile_ix + 572, tile_ix + 572, tile_ix, tile_ix],\n                 [tile_iy, tile_iy, tile_iy + 572, tile_iy + 572, tile_iy],\n                 'blue', linewidth=2)\n        # add focus of current segmentation mask\n        tile_ix, tile_iy = tile_ix + 92, tile_iy + 92\n        plt.plot([tile_ix, tile_ix + 388, tile_ix + 388, tile_ix, tile_ix],\n                 [tile_iy, tile_iy, tile_iy + 388, tile_iy + 388, tile_iy],\n                 'yellow', linewidth=2)\n        # add mirror lines\n        plt.vlines([92, 604], 0, 696, 'white', linewidth=1)\n        plt.hlines([92, 604], 0, 696, 'white', linewidth=1)\n        plt.title('Extrapolated Image, Tile '+ str(tile_index + 1) + '/' +\n                  str(number_of_tiles_2D))\n        plt.xlim(0, 696)\n        plt.ylim(696, 0)\n    return\n\n\ndef visualize_data_augmentation(dataset, index, show_grid, kind):\n    # get untransformed img, label\n    dataset.transformation = False\n    img, label = dataset[index]\n    # copy image (since it may be modified)\n    cur_img = img.clone().numpy()\n    cur_label = label.clone().numpy()\n    if show_grid:\n        # modify image to include outer grid (outside of label)\n        cur_img[0, 0:91:25] = 10.0\n        cur_img[0, 480::25] = 10.0\n        cur_img[0, :, 0:91:25] = 10.0\n        cur_img[0, :, 480::25] = 10.0\n        # modify image to include label grid\n        cur_img[0, 92:480:20, 92:480] = -5\n        cur_img[0,  92:480, 92:480:20] = -5\n        # modify label to include label grid\n        cur_label[0, ::20] = -5\n        cur_label[0, :, ::20] = -5\n    if kind == 'elastic deformation':\n        # set transformation\n        kernel = dataset.kernel\n        alpha = dataset.alpha\n        new_img, new_label = EM_Dataset.elastic_deform(cur_img, cur_label,\n                                                       alpha, kernel)\n    elif kind == 'affine transformation':\n        angle = np.random.randint(-3, 3)\n        translate = list(np.random.randint(-3, 3, size=2))\n        new_img, new_label = EM_Dataset.affine_transform(cur_img, cur_label,\n                                                         angle, translate)\n    elif kind == 'gray value variation':\n        sigma = 0.2\n        new_img = EM_Dataset.gray_value_variations(img, sigma)\n        new_label = label\n    else:\n        raise NameError('Unknown `kind`, can only be `elastic deformation`, ' +\n                        '`affine transformation` or `gray value variation`')\n    # start plotting\n    fig = plt.figure(figsize=(10,10))\n    plt.subplot(2, 2, 1)\n    plt.title('Before ' + kind)\n    plt.imshow(cur_img[0], cmap='gray', aspect='equal',\n               interpolation='gaussian', vmax=1, vmin=0)\n    # focus of current segmentation mask\n    plt.plot([92, 480, 480, 92, 92], [92, 92, 480, 480, 92],\n            'yellow', linewidth=2)\n    plt.subplots_adjust(hspace=0.01)\n    plt.subplot(2,2,3)\n    plt.imshow(cur_label[0], cmap='gray', aspect='equal',\n               interpolation='gaussian', vmax=1, vmin=0)\n    plt.subplot(2,2,2)\n    plt.title('After ' + kind)\n    plt.imshow(new_img[0], cmap='gray', aspect='equal',\n               interpolation='gaussian', vmax=1, vmin=0)\n    # focus of current segmentation mask\n    plt.plot([92, 480, 480, 92, 92], [92, 92, 480, 480, 92],\n            'yellow', linewidth=2)\n    plt.subplot(2,2,4)\n    plt.imshow(new_label[0], cmap='gray', aspect='equal',\n               interpolation='gaussian', vmax=1, vmin=0)\n    return\n\n\n# generate images in order of appearance\nvisualize_overlap_tile_strategy(whole_dataset, img_index=0,\n                                tile_indexes=[0, 1])\nvisualize_data_augmentation(whole_dataset, index=0, show_grid=True,\n                            kind='affine transformation')\nvisualize_data_augmentation(whole_dataset, index=0, show_grid=True,\n                            kind='elastic deformation')\nvisualize_data_augmentation(whole_dataset, index=0, show_grid=False,\n                            kind='gray value variation')\n\n\n\n\nModel Implementation\nModel implementation can be divided into three tasks:\n\nNetwork Architecture: The model architecture is given in the model description in which one can identify several blocks of two \\(3 \\times 3\\) convolutional layers each followed by a ReLU non-linearity (called _block in the implementation). Note that the output of the last prediction layer can be understood as the unnormalized prediction for each class, i.e., \\(a_{i,j}^{(k)} \\in ] -\\infty, +\\infty[\\) where \\(a^{(k)}\\) denotes the activation in feature channel \\(k \\in \\{1, 2\\}\\) (one channel for each class) and the indices \\({i,j}\\) describe the pixel position. In order to get normalized probabilities for each pixel \\(\\hat{p}_{i,j}^{(k)}\\), a pixel-wise softmax is applied at the end (last operation in forward), i.e., after this operation the sum of the two output channels equals one for each pixel \\(\\hat{p}_{i,j}^{(1)} + \\hat{p}_{i,j}^{(2)} = 1\\).\n\n\nCode\nfrom torch import nn\n\n\nclass Unet(nn.Module):\n    \"\"\"original U-Net architecture proposed by Ronneberger et al. (2015)\n\n    Attributes:\n        encoder_blocks (list):  four u_net blocks of encoder path\n        bottleneck_bock: block that mediates between encoder and decoder\n        decoder_blocks (list):  four u_net blocks of decoder path\n        cropped_img_size (list): cropped images size in order of encoder blocks\n        up_convs (list): upsampling (transposed convolutional) layers (decoder)\n        max_pool: max pool operation used in encoder path\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.encoder_blocks = nn.ModuleList([\n            Unet._block(1, 64),\n            Unet._block(64, 128),\n            Unet._block(128, 256),\n            Unet._block(256, 512)\n        ])\n        self.bottleneck_block = Unet._block(512, 1024)\n        self.decoder_blocks = nn.ModuleList([\n            Unet._block(1024, 512),\n            Unet._block(512, 256),\n            Unet._block(256, 128),\n            Unet._block(128, 64)\n        ])\n        self.cropped_img_sizes = [392, 200, 104, 56]\n        self.up_convs = nn.ModuleList([\n            nn.ConvTranspose2d(1024, 512, kernel_size=(2,2), stride=2),\n            nn.ConvTranspose2d(512, 256, kernel_size=(2,2), stride=2),\n            nn.ConvTranspose2d(256, 128, kernel_size=(2,2), stride=2),\n            nn.ConvTranspose2d(128, 64, kernel_size=(2,2), stride=2),\n        ])\n        self.max_pool = nn.MaxPool2d(kernel_size=(2,2))\n        self.prediction = nn.Conv2d(64, 2, kernel_size=(1,1), stride=1)\n        return\n\n    def forward(self, x):\n        # go through encoder path and store cropped images\n        cropped_imgs = []\n        for index, encoder_block in enumerate(self.encoder_blocks):\n            out = encoder_block(x)\n            # center crop and add to cropped image list\n            cropped_img = Unet._center_crop(out, self.cropped_img_sizes[index])\n            cropped_imgs.append(cropped_img)\n            # max pool output of encoder block\n            x = self.max_pool(out)\n        # bottleneck block (no max pool)\n        x = self.bottleneck_block(x)  # [batch_size, 1024, 28, 28]\n        # go through decoder path with stored cropped images\n        for index, decoder_block in enumerate(self.decoder_blocks):\n            x = self.up_convs[index](x)\n            # concatenate x and cropped img along channel dimension\n            x = torch.cat((cropped_imgs[-1-index], x), 1)\n            # feed through decoder_block\n            x = decoder_block(x)\n        # feed through prediction layer [batch_size, 2, 388, 388]\n        x_pred_unnormalized = self.prediction(x)\n        # normalize prediction for each pixel\n        x_pred = torch.softmax(x_pred_unnormalized, 1)\n        return x_pred\n\n    @staticmethod\n    def _center_crop(x, new_size):\n        \"\"\"center croping of a square input tensor\n\n        Args:\n            x: input tensor shape [batch_size, channels, resolution, resolution]\n            new_size: the desired output resolution (taking center of input)\n\n        Returns:\n            x_cropped: tensor shape [batch_size, channels, new_size, new_size]\n        \"\"\"\n        img_size = x.shape[-1]\n        i_start = int((img_size - new_size)/2)\n        i_end = int((img_size + new_size)/2)\n        x_cropped = x[:, :, i_start:i_end, i_start:i_end]\n        return x_cropped\n\n    @staticmethod\n    def _block(in_channels, out_channels):\n        \"\"\"block for use in U-Net architecture,\n        consists of two conv 3x3, ReLU layers\n\n        Args:\n            in_channels: number of input channels for first convolution\n            out_channels: number of output channels for both convolutions\n\n        Returns:\n            u_net_block: Sequential U net block\n        \"\"\"\n        conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(3,3), stride=1)\n        conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(3,3), stride=1)\n\n        N_1, N_2 = 9*in_channels, 9*out_channels\n        # initialize by drawing weights from Gaussian distribution\n        conv1.weight.data.normal_(mean=0, std=np.sqrt(2/N_1))\n        conv2.weight.data.normal_(mean=0, std=np.sqrt(2/N_2))\n        # define u_net_block\n        u_net_block = nn.Sequential(\n            conv1,\n            nn.ReLU(),\n            conv2,\n            nn.ReLU()\n        )\n        return u_net_block\n\n\nLoss Function: Since the segmentation labels are clearly imbalanced (much more white pixels than black pixels), Ronneberger et al. (2015) use the weighted cross entropy as the loss function (which they term energy function)\n\\[\n\\begin{align}\n   J (\\textbf{x}, \\textbf{m}) &= -\\sum_{i=1}^{388}\\sum_{j=1}^{388}\n   \\sum_{k=1}^2 w_{i,j} (\\textbf{m}) \\cdot\n   p_{i,j}^{(k)} \\log \\left( \\widehat{p}_{i,j}^{(k)} \\left( \\textbf{x};\n\\boldsymbol{\\theta} \\right)  \\right) \\\\\n   &\\text{with} \\quad p_{i,j}^{(1)} = \\begin{cases} 1 & \\text{if }\nm_{i,j}=1 \\\\ 0 &\\text{else} \\end{cases} \\quad \\text{and} \\quad p_{i,j}^{(2)} =\n\\begin{cases} 1 & \\text{if } m_{i, j} = 0 \\\\0 & \\text{else}, \\end{cases}\n\\end{align}\n\\]\nwhere \\(\\textbf{x}\\in [0, 1]^{572\\times 572}\\) denotes the input image, \\(\\textbf{m} \\in \\{0,\n1\\}^{388 \\times 388}\\) the corresponding segmentation mask, \\(\\textbf{p}^{(k)}\\in \\{0,\n1\\}^{388 \\times 388}\\) the groundtruth probability for each class \\(k\\), \\(\\widehat{\\textbf{p}}^{(k)} \\in\n[0, 1]^{388\\times 388}\\) denotes the \\(k\\)-th channel output of the network parameterized by \\(\\boldsymbol{\\theta}\\) and \\(\\textbf{w} \\left(\n\\textbf{m} \\right) \\in \\mathbb{R}^{388 \\times 388}\\) is a introduced weight map (computed via the segmentation mask \\(\\textbf{m}\\)) to give some pixels more importance during training. Accordingly, the loss function can be interpreted as penalizing the deviation from 1 for each true class output pixel weighted by the corresponding entry of the weight map.\nWeight Map: To compensate for the imbalance between separation borders and segmented object4, Ronneberger et al. (2015) introduce the following weight map\n\\[\n  w(\\textbf{m}) = {w_c (\\textbf{m})} + {w_0 \\cdot \\exp \\left( - \\frac\n  {\\left(d_1 (\\textbf{m}) - d_2 (\\textbf{m})\\right)^2}{2\\sigma^2}\\right)},\n\\]\nwhere the first term reweights each pixel of the minority class (i.e., black pixels) to balance the class frequencies. In the second term \\(d_1\\) and \\(d_2\\) denote the distance to the border of the nearest and second nearest cell, respectively. \\(w_0\\) and \\(\\sigma\\) are predefined hyperparameters. Thus, the second term can be understood as putting additional weight to smaller borders, see code and image below.\n\n\nCode\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nfrom skimage import measure\nfrom scipy.ndimage.morphology import distance_transform_edt\nfrom skimage.segmentation import find_boundaries\n\n\ndef compute_weight_map(label_mask, w_0, sigma, plot=False):\n    \"\"\"compute weight map for each ground truth segmentation to compensate\n    for the different class frequencies and to put additional\n    emphasis on small borders as proposed by Ronneberger et al.\n\n    Args:\n        label mask (torch tensor): true segmentation masks [batch_size, 1, 388, 388]\n        w_0 (float): hyperparameter in second term of weight map\n        sigma (float): hyperparameter in second term of weight map\n\n    Returns:\n        weight_map (torch tensor): computed weight map [batch_size, 1, 388, 388]\n\n    researchgate.net/post/creating_a_weight_map_from_a_binary_image_U-net_paper\n    \"\"\"\n    batch_size = label_mask.shape[0]\n    weight_map = torch.zeros_like(label_mask)\n    for i in range(batch_size):\n        # compute w_c to balance class frequencies\n        w_c = label_mask[i][0].clone()\n        class_freq_0 = (label_mask[i]==0).sum().item()\n        class_freq_1 = (label_mask[i]==1).sum().item()\n        w_c[label_mask[i][0]==0] = class_freq_1 / class_freq_0\n        # compute d_1, d_2, i.e., euclid. dist. to border of (1st/2nd) closest cell\n        d_1 = np.zeros(label_mask[i][0].shape)\n        d_2 = np.zeros(label_mask[i][0].shape)\n        # distinguish all cells (connected components of ones)\n        all_cells = measure.label(label_mask[i][0], background=0, connectivity=2)\n        num_cells = np.max(all_cells)\n        # initialize distances for all cells\n        dists = np.zeros([num_cells, d_2.shape[0], d_2.shape[1]])\n        # iterate over all zero components\n        for index, i_cell in enumerate(range(1, num_cells + 1)):\n            # cell segmentation (segmented cell 1, rest 0)\n            cell_segmentation = all_cells==i_cell\n            # find boundary (boundary 1, rest 0)\n            boundary = find_boundaries(cell_segmentation, mode='inner')\n            # compute distance to boundary (set boundary 0, rest -1)\n            bound_dists = distance_transform_edt(1 - boundary)\n            dists[index] = bound_dists\n        # sort dists along first axis (each pixel)\n        dists.sort(axis=0)\n        d_1 = dists[0]\n        d_2 = dists[1]\n        w = w_c + w_0 * np.exp(- (d_1 + d_2)**2/(2*sigma**2))\n        # save w to weight map\n        weight_map[i, 0] = w\n\n        # visualize weight map\n        if plot and i==0:\n            fig = plt.figure(figsize=(18, 14))\n\n            ax = plt.subplot(1, 3, 1)\n            plt.title('Segmenation Mask')\n            plt.imshow(label_mask[0, 0], cmap='gray')\n            divider = make_axes_locatable(ax)\n            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n            plt.colorbar(cax=cax)\n\n            ax = plt.subplot(1, 3, 2)\n            plt.title('w_c')\n            plt.imshow(w_c, cmap='jet')\n            divider = make_axes_locatable(ax)\n            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n            plt.colorbar(cax=cax)\n\n\n            ax = plt.subplot(1, 3, 3)\n            plt.title('w')\n            plt.imshow(w, cmap='jet')\n            divider = make_axes_locatable(ax)\n            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n            plt.colorbar(cax=cax)\n    return weight_map\n\n\nimg, label_mask = whole_dataset[0]\nweight_map = compute_weight_map(label_mask.unsqueeze(0), w_0=10, sigma=5, plot=True)\n\n\n\n\n\ncompute_weight_map\n\n\n\n\nTraining Procedure: A simple SGD (Stochastic Gradient Descent) optimizer with a high momentum (0.99) and a batch_size of 1 are choosen for training as proposed by Ronneberger et al. (2015), see code below. Note that we take the mean instead of the sum in the loss function calculation to avoid overflow (i.e., nans). This will only change the strength of a gradient step (which can be adjusted by the learning rate), but not its direction.\n\n\nCode\nfrom livelossplot import PlotLosses\nfrom torch.utils.data import DataLoader\n\n\ndef train(u_net, dataset, epochs):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    # hyperparameters weight map\n    w_0, sigma = 10, 5\n\n    print('Device: {}'.format(device))\n\n    data_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n\n    u_net.to(device)\n    optimizer = torch.optim.SGD(u_net.parameters(), lr=0.001, momentum=0.99)\n\n    losses_plot = PlotLosses()\n    for epoch in range(epochs):\n        avg_loss = 0\n        for counter, (imgs, label_masks) in enumerate(data_loader):\n            u_net.zero_grad()\n            # retrieve predictions of u_net [batch, 2, 388, 388]\n            pred_masks = u_net(imgs.to(device))\n            # compute weight map\n            weight_map = compute_weight_map(label_masks, w_0, sigma).to(device)\n            # put label_masks to device\n            label_masks = label_masks.to(device)\n            # compute weighted binary cross entropy loss\n            loss = -(weight_map*\n                    (pred_masks[:, 0:1].log() * label_masks +\n                      pred_masks[:, 1:2].log() * (1 - label_masks))\n                    ).mean()\n            loss.backward()\n            optimizer.step()\n\n            avg_loss += loss.item() / len(dataset)\n\n            losses_plot.update({'current weighted loss': loss.item()},\n                              current_step=epoch + counter/len(data_loader))\n            losses_plot.draw()\n        losses_plot.update({'avg weighted loss': avg_loss},\n                          current_step=epoch + 1)\n        losses_plot.draw()\n    trained_u_net = u_net\n    return trained_u_net\n\n\nBeware: Training for 30 epochs (i.e., the code below) takes about 2 hours with a NVIDIA Tesla K80 as GPU. The loss plot (see below avg weighted loss) indicates that training for more epochs might improve the model even more5. For people who are interested in using the model without waiting for 2 hours, I stored a trained version on nextjournal.\n\n\nCode\nu_net = Unet()\nepochs = 30\n# all image indexes\nidx = np.arange(30)\n# random inplace shuffling of indexes\nnp.random.seed(1)\nnp.random.shuffle(idx)\n# split data into training and test data\ntrain_imgs, train_labels = imgs[idx[0:25]], labels[idx[0:25]]\ntest_imgs, test_labels = imgs[idx[25:]], labels[idx[25:]]\n# generate datasets\nstride = 124\ntrain_dataset = EM_Dataset(train_imgs, train_labels, stride=stride,\n                          transformation=True, probability=0.7, alpha=50,\n                          sigma=5, kernel_dim=25)\ntest_dataset = EM_Dataset(test_imgs, test_labels, stride=stride,\n                          transformation=False)\n# start training procedure\ntrained_u_net = train(u_net, train_dataset, epochs)\n\n\n\n\n\ntrain result\n\n\n\n\n\nResults\nLet’s look at some image segmentations generated by the trained model on the unseen test set:\n\n\nCode\ndef visualize_results(trained_u_net, test_dataset, num_test_images=None):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    # take random tile from each test image\n    num_tiles = (1 + int((512 - 388)/test_dataset.stride))**2\n    num_images = int(len(test_dataset) / num_tiles)\n    if num_test_images:\n        # number of images &lt; number of images in test set\n        num_images = min(num_test_images, num_images)\n    random_tile_idx = np.random.choice(range(num_tiles), num_images,\n                                       replace=True)\n\n    fig = plt.figure(figsize=(num_images*6, 10))\n    # annotation plots\n    ax = plt.subplot(3, num_images + 1, 1)\n    ax.annotate('cell image\\n(input)', xy=(1, 0.5), xycoords='axes fraction',\n                 fontsize=14, va='center', ha='right')\n    ax.set_aspect('equal')\n    ax.axis('off')\n    ax = plt.subplot(3, num_images + 1, num_images + 2)\n    ax.annotate('true segmentation\\n(label)', xy=(1, 0.5),\n                xycoords='axes fraction', fontsize=14, va='center', ha='right')\n    ax.set_aspect('equal')\n    ax.axis('off')\n    ax = plt.subplot(3, num_images + 1, 2*(num_images + 1) + 1)\n    ax.annotate('U-net prediction', xy=(1, 0.5), xycoords='axes fraction',\n                 fontsize=14, va='center', ha='right')\n    ax.set_aspect('equal')\n    ax.axis('off')\n    # image, label, predicted label plots\n    for index in range(num_images):\n        img, label = test_dataset[index*num_tiles + random_tile_idx[index]]\n        label_pred = u_net(img.unsqueeze(0).to(device)).squeeze(0)[0] &gt; 0.5\n\n        # plot original image\n        plt.subplot(3, num_images + 1, index + 2)\n        plt.imshow(transforms.ToPILImage()(img), cmap='gray')\n        plt.plot([92, 480, 480, 92, 92], [92, 92, 480, 480, 92],\n                 'yellow', linewidth=2)\n        plt.xticks([])\n        plt.yticks([])\n        # plot original segmentation mask\n        plt.subplot(3, num_images + 1, index + num_images + 3)\n        plt.imshow(transforms.ToPILImage()(label), cmap='gray')\n        plt.xticks([])\n        plt.yticks([])\n        # plot prediction segmentation mask\n        plt.subplot(3, num_images + 1, index + 2*(num_images + 1) + 2)\n        plt.imshow(label_pred.detach().cpu().numpy(), cmap='gray')\n        plt.xticks([])\n        plt.yticks([])\n    return\n\n\nvisualize_results(trained_u_net, test_dataset, num_test_images=3)\n\n\n\n\n\nvisualize results\n\n\nThe predictions are pretty decent, though far from perfect. Bear in mind, that our model had only 25 example images to learn from and that training for more epochs might have led to even better predictions."
  },
  {
    "objectID": "paper_summaries/u_net/index.html#footnotes",
    "href": "paper_summaries/u_net/index.html#footnotes",
    "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA greater input image than output segmentation size makes sense since the network has no information about the surrounding of the input image.↩︎\nActually, CNNs should put more emphasis on the where or rather the local relation between context information, see Geoffrey Hinton’s comment about pooling.↩︎\nThe implementation intends to be easily understandable, while keeping the computational resources low. Thus, it is not aimed to generate the best training results or model performance.↩︎\nSince the separation borders are much smaller than the segmented objects, the network could be trapped into merging touching objects without being penalized enough.↩︎\nThe implementation intends to be easily understandable, while keeping the computational resources low. Thus, it is not aimed to generate the best training results or model performance.↩︎"
  },
  {
    "objectID": "paper_summaries/multi-object_network/index.html",
    "href": "paper_summaries/multi-object_network/index.html",
    "title": "MONet: Unsupervised Scene Decomposition and Representation",
    "section": "",
    "text": "Burgess et al. (2019) developed the Multi-Object Network (MONet) as an end-to-end trainable model to decompose images into meaningful entities such as objects. Similar to AIR, the whole training process is unsupervised, i.e., there are no labeled segmentations, handcrafted bounding boxes or whatsoever. In essence, their model combines a Variational Auto-Encoder (VAE) with a recurrent attention network (U-Net segmentation network) to spatially decompose scenes into attention masks (over which the VAE needs to reconstruct masked regions) and latent representations of each masked region. In contrast to AIR, MONet does not contain a fully generative model and its latent space is less structured. As a proof of concept, they show that their model could learn disentangled representations in a common latent code (i.e., representations of object features in latent space) and object segmentations (i.e., attention masks on the original image) on non-trivial 3D scenes."
  },
  {
    "objectID": "paper_summaries/multi-object_network/index.html#model-description",
    "href": "paper_summaries/multi-object_network/index.html#model-description",
    "title": "MONet: Unsupervised Scene Decomposition and Representation",
    "section": "Model Description",
    "text": "Model Description\nMONet builds upon the inductive bias that the world (or rather simple images of the world) can often be approximated as a composition of individual objects with the same underlying structure (i.e., different instantiations of the same class). To put this into practice, Burgess et al. (2019) developed a conditional generative sampling scheme in which scenes are spatially decomposed into parts that have to be individually modelled through a common representation code. The architecture incorporates two kind of neural networks that are trained in tandem:\n\nAttention Network: Its purpose is to deliver attention masks \\(\\textbf{m}_k\\) for the image such that the whole image is completely spatially decomposed into \\(K\\) parts, i.e., \\(\\sum_{k=1}^K\n\\textbf{m}_k = \\textbf{1}\\). Ideally, after training each mask focuses on a semantically meaningful element/segment of the image. Thus, it may also be understood as a segmentation network.\nTo allow for a variable number of attention masks, Burgess et al. (2019) use a recurrent neural network \\(\\alpha_{\\boldsymbol{\\psi}}\\) for the decomposition. Therein, an auto-regressive process is defined for the ongoing state. This state is called scope \\(\\textbf{s}_k \\in [0, 1]^{W\\times\nH}\\) (image width \\(W\\) and height \\(H\\)) as it is used to track the image parts that remain to be explained, i.e., the scope for the next state is given by\n\\[\n   \\textbf{s}_{k+1} = \\textbf{s}_k \\odot \\left(\\textbf{1} -\n\\underbrace{\\alpha_{\\boldsymbol{\\psi}} \\left( \\textbf{x};\n\\textbf{s}_{k} \\right)}_{[0,1]^{W \\times H}} \\right)\n\\]\nwith the first scope \\(\\textbf{s}_0 = \\textbf{1}\\) (\\(\\odot\\) denotes element-wise multiplication). The attention masks are given by\n\\[\n  \\textbf{m}_k  = \\begin{cases} \\textbf{s}_{k-1} \\odot\n  \\alpha_{\\boldsymbol{\\psi}} \\left( \\textbf{x}; \\textbf{s}_{k-1}\n  \\right) & \\forall k &lt; K \\\\\n  \\textbf{s}_{k-1} & k=K \\end{cases}\n\\]\nBy construction, we get that\n\\[\n\\begin{align}\n  &\\textbf{s}_{k} = \\textbf{s}_{k+1} + \\textbf{m}_{k + 1} =\n  \\textbf{s}_{k+2} + \\textbf{m}_{k+2} + \\textbf{m}_{k+1} \\\\\n  \\textbf{1}=&\\textbf{s}_0 =\n  \\textbf{s}_{K-1} + \\sum_{k=1}^{K-1} \\textbf{m}_{k} = \\sum_{k=1}^K \\textbf{m}_k,\n\\end{align}\n\\]\ni.e., at each recursion the remaining part to be explained \\(\\textbf{s}_{k}\\) is divided into a segmentation mask \\(\\textbf{m}_{k+1}\\) and a new scope \\(\\textbf{s}_{k+1}\\) such that with \\(\\textbf{s}_0=\\textbf{1}\\) the entire image is explained by the resulting segmentation masks, i.e., \\(\\sum_{k=1}^K \\textbf{m}_k = \\textbf{1}\\).\nComponent VAE: Its purpose is to represent each masked region in a common latent code, i.e., each segment is encoded by the same VAE1. The encoder distribution \\(q_{\\boldsymbol{\\phi}}\n\\left(\\textbf{z}_k | \\textbf{x}, \\textbf{m}_k\\right)\\) is conditioned both on the input image \\(\\textbf{x}\\) and the corresponding attention mask \\(\\textbf{m}_k\\). I.e., instead of feeding each masked region into the network, Burgess et al. (2019) use the whole image \\(\\textbf{x}\\) concatenated with the corresponding attention mask \\(\\textbf{m}_k\\). As a result, we get \\(K\\) different latent codes \\(\\textbf{z}_k\\) (termed “slots”) which represent the features of each object (masked region) in a common latent/feature space across all objects.\nThe decoder distribution \\(p_{\\boldsymbol{\\theta}}\\) is required to reconstruct the image component \\(\\widetilde{\\textbf{x}}_k \\sim p_{\\boldsymbol{\\theta}} \\left( \\textbf{x} | \\textbf{z}_k \\right)\\) and the attention masks2 \\(\\widetilde{\\textbf{m}}_k \\sim p_{\\boldsymbol{\\theta}}\n\\left(\\textbf{c} | \\textbf{z}_k \\right)\\) from these latent codes. Note that \\(p_{\\boldsymbol{\\theta}} \\left(\\textbf{c} | \\textbf{z}_k\n\\right)\\) defines the mask distribution of the Component VAE, whereas \\(q_{\\boldsymbol{\\psi}} \\left(\\textbf{c} | \\textbf{x}\\right)\\) denotes the mask distribution of the attention network3.\nImportantly, each of the \\(k\\) component reconstruction distributions is multiplied with the corresponding attention mask \\(\\textbf{m}_k\\), i.e.,\n\\[\n\\text{Reconstruction Distribution}_k = \\textbf{m}_k \\odot\n   p_{\\boldsymbol{\\theta}} \\left(\\textbf{x} | \\textbf{z}_k \\right).\n\\]\nThe negative (decoder) log likelihood NLL (can be interpreted as the reconstruction error, see my post on VAEs) of the whole image is given by\n\\[\n\\text{NLL} = - \\log \\left( \\sum_{k=1}^K \\textbf{m}_k \\odot p_{\\boldsymbol{\\theta}} \\left(\\textbf{x} | \\textbf{z}_k \\right)\\right),\n\\]\nwhere the sum can be understood as the reconstruction distribution of the whole image (mixture of components) conditioned on the latent codes \\(\\textbf{z}_k\\) and the attention masks \\(\\textbf{m}_k\\). Clearly, the reconstructions \\(\\widetilde{\\textbf{x}}_k \\sim p_{\\boldsymbol{\\theta}} \\left(\n\\textbf{x} | \\textbf{z}_k\\right)\\) are unconstrained outside of the masked regions (i.e., where \\(m_{k,i} = 0\\)).\nNote that they use a prior for the latent codes \\(\\textbf{z}_k\\), but not for the attentions masks \\(\\textbf{m}_k\\). Thus, the model is not fully generative, but rather a conditional generative model.\n\nThe figure below summarizes the whole architecture of the model by showing the individual components (attention network, component VAE) and their interaction.\n\n\n\n\n\n\n\n\n\n\n\nSchematic of MONet. A recurrent attention network is used to obtain the attention masks \\(\\textbf{m}^{(i)}\\). Afterwards, a group structured representation is retrieved by feeding each concatenation of \\(\\textbf{m}^{(i)}, \\textbf{x}\\) through the same VAE with encoder parameters \\(\\boldsymbol{\\phi}\\) and decoder parameters \\(\\boldsymbol{\\theta}\\). The outputs of the VAE are the unmasked image reconstructions \\(\\widetilde{\\textbf{x}}^{(i)}\\) and mask reconstructions \\(\\widetilde{\\textbf{m}}^{(i)}\\). Lastly, the reconstructed image is assembled using the deterministic attention masks \\(\\textbf{m}^{(i)}\\) and the sampled unmasked image reconstructions \\(\\widetilde{\\textbf{x}}^{(i)}\\).\n\n\n\nThe whole model is end-to-end trainable with the following loss function\n\\[\n\\begin{align}\n\\mathcal{L}\\left(\\boldsymbol{\\phi}; \\boldsymbol{\\theta};\n\\boldsymbol{\\psi}; \\textbf{x} \\right) &= \\underbrace{- \\log \\left( \\sum_{k=1}^K \\textbf{m}_k \\odot p_{\\boldsymbol{\\theta}} \\left(\\textbf{x} |\n\\textbf{z}_k \\right)\\right)}_{\\text{Reconstruction Error between }\n\\widetilde{\\textbf{x}} \\text{ and } \\textbf{x}} + \\beta\n\\underbrace{D_{KL} \\left( \\prod_{k=1}^K q_{\\boldsymbol{\\phi}} \\left(\\textbf{z}_k |\n\\textbf{x}, \\textbf{m}_k\\right) || p(\\textbf{z})\n\\right)}_{\\text{Regularization Term for Distribution of }\\textbf{z}_k}\\\\\n&+ \\gamma \\underbrace{D_{KL} \\left( q_{\\boldsymbol{\\psi}} \\left( \\textbf{c} |\n\\textbf{x} \\right) || p_{\\boldsymbol{\\theta}} \\left( \\textbf{c} | \\{\n\\textbf{z}_k \\} \\right) \\right)}_{\\text{Reconstruction Error between }\n\\widetilde{\\textbf{m}}_k \\text{ and } \\textbf{m}_k},\n\\end{align}\n\\]\nwhere the first term measures the reconstruction error of the fully reconstructed image (sum) as mentioned above. The second term is the KL divergence between the variational posterior approximation factorized across slots, i.e., \\(q_{\\boldsymbol{\\phi}}\n\\left( \\textbf{z} | \\textbf{x} \\right) = \\prod_{k=1}^K\nq_{\\boldsymbol{\\phi}} \\left(\\textbf{z}_k| \\textbf{x},\n\\textbf{m}_k\\right)\\), and the prior of the latent distribution \\(p(\\textbf{z})\\). As this term pushes the encoder distribution to be close to the prior distribution, it is commonly referred to as regularization term. It is weighted by the tuneable hyperparameter \\(\\beta\\) to encourage learning of disentanglement latent representions, see Higgins et al. (2017). Note that the first two terms are derived from the standard VAE loss. The third term is the KL divergence between the attention mask distribution generated by the attention network \\(q_{\\boldsymbol{\\psi}} \\left( \\textbf{c} | \\textbf{x} \\right)\\) and the component VAE \\(p_{\\boldsymbol{\\theta}}\n\\left(\\textbf{c} |\\{\\textbf{z}_k\\} \\right)\\), i.e., it forces these distributions to lie close to each other. It could be understood as the reconstructions error of the VAE’s attention masks \\(\\widetilde{\\textbf{m}}_k\\), as it forces them to lie close to the attention masks \\(\\textbf{m}_k\\) of the attention network. Note however that the attention network itself is trainable, thus the network could also react by pushing the attention mask distribution towards the reconstructed mask distribution of the VAE. \\(\\gamma\\) is a tuneable hypeterparameter to modulate the importance of this term, i.e., increasing \\(\\gamma\\) results in close distributions.\nMotivation: The model aims to produce semantically meaningful decompositions in terms of segmentation and latent space attributes. Previous work such as the Spatial Broadcast decoder has shown that VAEs are extensively capable of decomposing simple single-object scenes into disentangled latent space representations. However, even simple multi-object scenes are far more challenging to encode due to their complexity. Burgess et al. (2019) hypothesize that exploiting the compositional structure of scenes (inductive bias) may help to reduce this complexity. Instead of decomposing the entire multi-object scene in one sweep, MONet breaks the image in multiple (\\(K\\)) tasks which it decomposes with the same VAE4. Restricting the model complexity of the decoder (e.g., by using few layers), forces the model to produce segmentation with similar tasks, i.e., segmentations over structurally similar scene elements such that the VAE is capable of solving all tasks (note that this is a hypothesis). The authors argue that optimization should push towards a meaningful decomposition. Furthermore, they empirically validate their hypothesis by showing that for the Objects Room dataset the reconstruction error is much lower when the ground truth attention masks are given compared to an all-in-one (single sweep) or a wrong masks situation.\nAdding some more motivation: It might be helpful to think about the data-generating process: Commonly, artificial multi-object scenes are created by adding each object successively to the image. Assuming that each of these objects is generated from the same class with different instantiations (i.e., different color/shape/size/…), it seems most natural to recover this process by decomposing the image and then decoding each part."
  },
  {
    "objectID": "paper_summaries/multi-object_network/index.html#implementation",
    "href": "paper_summaries/multi-object_network/index.html#implementation",
    "title": "MONet: Unsupervised Scene Decomposition and Representation",
    "section": "Implementation",
    "text": "Implementation\nBurgess et al. (2019) tested MONet on three different multi-object scene datasets (Objects Room, CLEVR, Multi-dSprites) and showed that their model could successively learn to    \n\ndecompose scenes into semantically meaningful parts, i.e., produce meaningful segmentation masks,\nrepresent each segmented object in a common (nearly disentangled) latent code, and\ngeneralize to unseen scene configurations\n\nwithout any supervision. Notably, MONet can handle a variable number of objects by producing latent codes that map to an empty scene, see image below. Furthermore, it turned out that MONet is also able to deal with occlusions: In the CLEVR dataset the unmasked reconstructions could even recover occluded objects, see image below. Burgess et al. (2019) argue that this indicates how MONet is learning from and constrained by the structure of the data.\n\n\n\n\n\n\n\n\nMONet Paper Results: Decomposition on Multi-dSprties and CLEVR images. First row shows the input image, second and third row the corresponding reconstructions and segmentations by MONet (trained for 1,000,000 iterations). The last three rows show the unmasked component reconstructions from some chosen slots (indicated by \\(S\\)). Red arrows highlight occluded regions of shapes that are completed as full shapes. Taken from Burgess et al. (2019).\n\n\n\nThe following reimplementation aims to reproduce some of these results while providing an in-depth understanding of the model architecture. Therefore, a dataset that is similar to the Multi-dSprites dataset is created, then the whole model (as faithfully as possible close to the original architecture) is reimplemented and trained in Pytorch and lastly, some useful visualizations of the trained model are created.\n\nData Generation\nA dataset that is similar in spirit to the Multi-dSprites will be generated. Burgess et al. (2019) generated this dataset by sampling \\(1-4\\) images randomly from the binary dsprites dataset(consisting of \\(737,280\\) images), colorizing these by sampling from a uniform random RGB color and compositing those (with occlusion) onto a uniform random RGB background.\nTo reduce training time, we are going to generate a much simpler dataset of \\(x\\) images with two non-overlaping objects (square or circle) and a fixed color space (red, green or aqua) for these objects, see image below. The dataset is generated by sampling uniformly random from possible latent factors, i.e., random non-overlaping positions for the two objects, random object constellations and random colors from color space, see code below image.\n\n\n\n\n\n\n\n\nVisualization of self-written dataset.\n\n\n\n\n\nCode\nfrom PIL import Image, ImageDraw\nimport torchvision.transforms as transforms\nimport numpy as np\nfrom torch.utils.data import TensorDataset\nimport torch\n\n\ndef generate_dataset(n_samples, SEED=1):\n    ############### CONFIGURATION ###############\n    canvas_size=64\n    min_obj_size, max_obj_size = 12, 20\n    min_num_obj, max_num_obj = 0, 2\n    shapes = [\"circle\", \"square\"]\n    colors = [\"red\", \"green\", \"aqua\"]\n    #############################################\n    data = torch.empty([n_samples, 3, canvas_size, canvas_size])\n    labels = torch.empty([n_samples, 1, canvas_size, canvas_size])\n\n    pos_positions = np.arange(canvas_size - max_obj_size - 1)\n\n    np.random.seed(SEED)\n    rnd_positions = np.random.choice(pos_positions, size=(n_samples, 2), replace=True)\n    rnd_num_objs = np.random.randint(min_num_obj, max_num_obj + 1, size=(n_samples))\n    rnd_obj_sizes = np.random.randint(min_obj_size, max_obj_size + 1, \n                                      size=(n_samples, max_num_obj))\n    rnd_shapes = np.random.choice(shapes, size=(n_samples, max_num_obj), replace=True)\n    rnd_colors = np.random.choice(colors, size=(n_samples, max_num_obj), replace=True)\n    for i_data in range(n_samples):\n        x_0, y_0 = rnd_positions[i_data]\n        num_objs = rnd_num_objs[i_data]\n        if num_objs &gt; 1:\n            # make sure that there is no overlap\n            max_obj_size = max(rnd_obj_sizes[i_data])\n            impos_x_pos = np.arange(x_0 - max_obj_size, x_0 + max_obj_size + 1)\n            impos_y_pos = np.arange(y_0 - max_obj_size, y_0 + max_obj_size + 1)\n            x_1 = np.random.choice(np.setdiff1d(pos_positions, impos_x_pos), size=1)\n            y_1 = np.random.choice(np.setdiff1d(pos_positions, impos_y_pos), size=1)\n        else:\n            x_1 = 0\n            y_1 = 0\n\n        # current latent factors\n        num_objs = rnd_num_objs[i_data]\n        x_positions, y_positions = [x_0, x_1], [y_0, y_1]\n        obj_sizes = rnd_obj_sizes[i_data]\n        shapes = rnd_shapes[i_data]\n        colors = rnd_colors[i_data]\n\n        # create img and label tensors\n        img, label = generate_img_and_label(\n            x_pos=x_positions[:num_objs],\n            y_pos=y_positions[:num_objs],\n            shapes=shapes[:num_objs],\n            colors=colors[:num_objs],\n            sizes=obj_sizes[:num_objs],\n            img_size=canvas_size\n        )\n        data[i_data] = img\n        labels[i_data] = label\n    dataset = TensorDataset(data, labels)\n    return dataset\n\n\ndef generate_img_and_label(x_pos, y_pos, shapes, colors, sizes, img_size):\n    \"\"\"generates a img and corresponding segmentation label mask\n    from the provided latent factors\n\n    Args:\n        x_pos (list): x positions of objects\n        y_post (list): y positions of objects\n        shapes (list): shape can only be `circle` or `square`\n        colors (list): colors of object\n        sizes (list): object sizes\n\n    Returns:\n        img (torch tensor): generated image represented as tensor\n        label (torch tensor): corresponding semantic segmentation mask\n    \"\"\"\n    out_img = Image.new(\"RGB\", (img_size, img_size), color=\"black\")\n    labels = []\n    # add objects\n    for x, y, shape, color, size in zip(x_pos, y_pos, shapes, colors, sizes):\n        img = Image.new(\"RGB\", (img_size, img_size), color=\"black\")\n        # define end coordinates\n        x_1, y_1 = x + size, y + size\n        # draw new image onto black image\n        img1 = ImageDraw.Draw(img)\n        img2 = ImageDraw.Draw(out_img)\n        if shape == \"square\":\n            img1.rectangle([(x, y), (x_1, y_1)], fill=color)\n            img2.rectangle([(x, y), (x_1, y_1)], fill=color)\n        elif shape == \"circle\":\n            img1.ellipse([(x, y), (x_1, y_1)], fill=color)\n            img2.ellipse([(x, y), (x_1, y_1)], fill=color)\n        labels.append((transforms.ToTensor()(img).sum(0) &gt; 0).unsqueeze(0))\n    out_image = transforms.ToTensor()(out_img).type(torch.float32)\n    out_label = torch.zeros(1, img_size, img_size)\n    for i_object in range(len(labels)):\n        out_label[labels[i_object]] = i_object + 1\n    return out_image, out_label\n\n\n\n\nModel Implementation\nMONet is a rather sophisticated model composing two powerful neural network architectures in a reasonable way. One major downside of such complex models is that they comprise lots of hyperparamters from which much remains unknown such as sensitivity to small pertubations (e.g., changing layers within network architectures or parameters \\(\\beta\\), \\(\\gamma\\)). Therefore, the model implementation aims to be as close as possible to the original model. Note that Burgess et al. (2019) did not publish their implementation.\nFor the sake of simplicity, this section is divided into four parts:\n\nAttention Network: The architecture of the recurrent neural network \\(\\boldsymbol{\\alpha}_{\\psi}\\) is described in appendix B.2 of Burgess et al. (2019). Basically, it consists of a slightly modified U-Net architecture that (at the \\(k\\)th step) takes as input the concatenation of the image \\(\\textbf{x}\\) and the current scope mask in log units \\(\\log \\textbf{s}_k\\). The output of the modified U-Net is a one channel image \\(\\textbf{o} \\in ]-\\infty, + \\infty[^{W\\times\nH}\\) in which each entry can be interpreted as the logits probability \\(\\text{logits }\\boldsymbol{\\alpha}_k\\). A sigmoid layer can be used to transform these logits into probabilities, i.e.,\n\\[\n\\begin{align}\n  \\boldsymbol{\\alpha}_k &= \\text{Sigmoid} \\left(\\text{logits }\n  \\boldsymbol{\\alpha}_k \\right) = \\frac {1} {1 + \\exp\\left(- \\text{logits }\n  \\boldsymbol{\\alpha}_k \\right)}\\\\\n  1 - \\boldsymbol{\\alpha}_k &= 1 - \\text{Sigmoid} \\left(\\text{logits }\n  \\boldsymbol{\\alpha}_k \\right) = \\frac {\\exp\\left(- \\text{logits }\n  \\boldsymbol{\\alpha}_k \\right) } { 1 + \\exp\\left(- \\text{logits }\n  \\boldsymbol{\\alpha}_k \\right)}\n\\end{align}\n\\]\nAdditionally, Burgess et al. (2019) transform these probabilties into logaritmic units, i.e.,\n\\[\n\\begin{align}\n   \\log \\boldsymbol{\\alpha}_k &= - \\log \\left( 1 + \\exp\\left(- \\text{logits }\n  \\boldsymbol{\\alpha}_k \\right)\\right)=\\text{LogSigmoid }\\left(\n\\text{logits } \\boldsymbol{\\alpha}_k \\right)\\\\\n   \\log \\left(1 - \\boldsymbol{\\alpha}_k\\right) &= - \\text{logits }\n\\boldsymbol{\\alpha}_k + \\log \\boldsymbol{\\alpha}_k,\n\\end{align}\n\\]\ni.e., a LogSigmoid layer can be used (instead of a sigmoid layer with applying logarithm to both outputs) to speed up the computations. From the model description above, it follows\n\\[\n\\begin{align}\n  \\textbf{s}_{k+1} &= \\textbf{s}_k \\odot \\left( 1 -\n  \\boldsymbol{\\alpha}_k \\right) \\quad &&\\Leftrightarrow  \\quad \\log\n  \\textbf{s}_{k+1} = \\log \\textbf{s}_k + \\log \\left(1 - \\boldsymbol{\\alpha}_k \\right)\\\\\n  \\textbf{m}_{k+1} &= \\textbf{s}_{k} \\odot \\boldsymbol{\\alpha}_k \\quad\n&&\\Leftrightarrow \\quad \\log \\textbf{m}_{k+1} = \\log \\textbf{s}_{k} + \\log \\boldsymbol{\\alpha}_k,\n\\end{align}\n\\]\ni.e., the output of the \\(k\\)th step can be computed by simply adding the log current scope \\(\\log \\textbf{s}_k\\) to each log probability. As a result, the next log attention mask \\(\\log \\textbf{m}_{k+1}\\) and next log scope \\(\\log \\textbf{s}_{k+1}\\) can be retrieved. Note that using log units instead of standard units is beneficial as it ensures numerical stability while simplifying the optimization due to an increased learning signal. \nThe code below summarizes the network architecture, Burgess et al. (2019) did not state the channel dimensionality within the U-Net blocks explicitely. However, as they mentioned to use a U-Net blueprint, it is assumed that they use the same dimensionality as in the original U-Net paper. To reduce training time and memory capacity, the following implementation caps the channel dimensionality in the encoder to 64 output channels.\n\n\n\nCode\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass UNet(nn.Module):\n    \"\"\"U-Net architecture with blocks proposed by Burgess et al. (2019)\n\n    Attributes:\n        encoder_blocks (list): u_net blocks of encoder path\n        decoder_blocks (list): u_net blocks of decoder path\n        bottleneck_MLP (list): bottleneck is a 3-layer MLP with ReLUs\n        out_conv (nn.Conv2d): convolutional classification layer\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.encoder_blocks = nn.ModuleList(\n            [\n                UNet._block(4, 16),              # [batch_size, 16, 64, 64]\n                UNet._block(16, 32),             # [batch_size, 32, 32, 32]\n                UNet._block(32, 64),             # [batch_size, 64, 16, 16]\n                UNet._block(64, 64),             # [batch_size, 64, 8, 8]\n                UNet._block(64, 64),             # [batch_size, 75, 4, 4]\n            ]\n        )\n        self.bottleneck_MLP = nn.Sequential(\n            nn.Flatten(),                        # [batch_size, 512*4*4]\n            nn.Linear(64*4*4, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),                 # [batch_size, 512*4*4]\n            nn.ReLU(),\n            nn.Linear(128, 64*4*4),              # [batch_size, 512*4*4]\n            nn.ReLU(),             # reshaped into [batch_size, 512, 4, 4]\n        )\n        self.decoder_blocks = nn.ModuleList(\n            [\n                UNet._block(128, 64),             # [batch_size, 64, 4, 4]\n                UNet._block(128, 64),             # [batch_size, 64, 8, 8]\n                UNet._block(128, 32),             # [batch_size, 32, 16, 16]\n                UNet._block(64, 16),              # [batch_size, 32, 32, 32]\n                UNet._block(32, 16),              # [batch_size, 64, 64, 64]\n            ]\n        )\n\n        self.out_conv = nn.Conv2d(16, 1, kernel_size=(1,1), stride=1)\n        return\n\n    def forward(self, x):\n        # go through encoder path and store intermediate results\n        skip_tensors = []\n        for index, encoder_block in enumerate(self.encoder_blocks):\n            out = encoder_block(x)\n            skip_tensors.append(out)\n            # no resizing in the last block\n            if index &lt; len(self.encoder_blocks) - 1:  # downsample\n                x = F.interpolate(\n                    out, scale_factor=0.5, mode=\"nearest\", \n                    recompute_scale_factor=False\n                )\n        last_skip = out\n        # feed last skip tensor through bottleneck\n        out_MLP = self.bottleneck_MLP(last_skip)\n        # reshape output to match last skip tensor\n        out = out_MLP.view(last_skip.shape)\n        # go through decoder path and use skip tensors\n        for index, decoder_block in enumerate(self.decoder_blocks):\n            inp = torch.cat((skip_tensors[-1 - index], out), 1)\n            out = decoder_block(inp)\n            # no resizing in the last block\n            if index &lt; len(self.decoder_blocks) - 1:  # upsample\n                out = F.interpolate(out, scale_factor=2, mode=\"nearest\")\n        prediction = self.out_conv(out)\n        return prediction\n\n    @staticmethod\n    def _block(in_channels, out_channels):\n        \"\"\"U-Net block as described by Burgess et al. (2019)\"\"\"\n        u_net_block = nn.Sequential(\n            nn.Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size=(3, 3),\n                stride=1,\n                padding=1,\n                bias=False,\n            ),\n            nn.InstanceNorm2d(num_features=out_channels, affine=True),\n            nn.ReLU(),\n        )\n        return u_net_block\n\n\nclass AttentionNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.unet = UNet()\n        return\n\n    def forward(self, x, num_slots):\n        log_s_k = torch.zeros_like(x[:, 0:1, :, :])\n        # initialize list to store intermediate results\n        log_m = []\n        for slot in range(num_slots - 1):\n            inp = torch.cat((x, log_s_k), 1)\n            alpha_logits = self.unet(inp)  # [batch_size, 1, image_dim, image_dim]\n            # transform into log probabilties log (alpha_k) and log (1 - alpha_k)\n            log_alpha_k = F.logsigmoid(alpha_logits)\n            log_1_m_alpha_k = -alpha_logits + log_alpha_k\n            # compute log_new_mask, log_new_scope\n            log_new_mask = log_s_k + log_alpha_k\n            log_new_scope = log_s_k + log_1_m_alpha_k\n            # store intermediate results in list\n            log_m.append(log_new_mask)\n            # update log scope\n            log_s_k = log_new_scope\n        log_m.append(log_s_k)\n        # convert list to tensor [batch_size, num_slots, 1, image_dim, image_dim]\n        log_m = torch.cat(log_m, dim=1).unsqueeze(2)\n        return log_m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComponent VAE: The architectures for the encoder \\(q_{\\boldsymbol{\\phi}}\\) and decoder \\(p_{\\boldsymbol{\\theta}}\\) neural networks are described in appendix B.1 of Burgess et al. (2019). Basically, the encoder \\(q_{\\boldsymbol{\\phi}}(\\textbf{z}_k | \\textbf{x}, \\textbf{m}_k)\\) is a typical CNN that takes the concatentation of an image \\(\\textbf{x}\\) and a segmentation mask in logaritmic units \\(\\log \\textbf{m}_k\\) as input to compute the mean \\(\\boldsymbol{\\mu}_{E, k}\\) and logarithmed variance \\(\\boldsymbol{\\sigma}^2_{E,k}\\) of the Gaussian latent space distribution \\(\\mathcal{N} \\left(\n\\boldsymbol{\\mu}_{E, k}, \\text{diag}\\left(\\boldsymbol{\\sigma}^2_{E,k} \\right)\n\\right)\\). Sampling from this distribution is avoided by using the reparametrization trick, i.e., the latent variable \\(\\textbf{z}_k\\) is expressed as a deterministic variable5\n\\[\n  \\textbf{z}_k = \\boldsymbol{\\mu}_{E, k} +\n  \\boldsymbol{\\sigma}^2_{E,k} \\odot \\boldsymbol{\\epsilon} \\quad\n  \\text{where} \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}\\left(\n  \\textbf{0}, \\textbf{I}\n  \\right).\n\\]\nThe component VAE uses a Spatial Broadcast decoder \\(p_{\\boldsymbol{\\theta}}\\) to transform the latent vector \\(\\textbf{z}_k\\) into the reconstructed image component \\(\\widetilde{\\textbf{x}}_k \\sim p_{\\boldsymbol{\\theta}}\n\\left(\\textbf{x} | \\textbf{z}_k \\right)\\) and mask \\(\\widetilde{\\textbf{m}}_k \\sim p_{\\boldsymbol{\\theta}} \\left(\n\\textbf{c}|\\textbf{z}_k \\right)\\). Burgess et al. (2019) chose independent Gaussian distributions with fixed variances for each pixel as the reconstructed image component distributions \\(p_{\\boldsymbol{\\theta}} \\left( x_i | \\textbf{z}_k \\right) \\sim\n\\mathcal{N} \\left(\\mu_{k,i} (\\boldsymbol{\\theta}), \\sigma_k^2\n\\right)\\) and independent Bernoulli distributions for each pixel as the reconstructed mask distributions \\(p\\left(c_{k, i}| \\textbf{z}_k\n\\right) \\sim \\text{Bern} \\left( p_{k,i}\n(\\boldsymbol{\\theta})\\right)\\). I.e., the decoder output is a 4 channel image from which the first three channels correspond to the 3 RGB channels for the means of the image components \\(\\boldsymbol{\\mu}_k\\) and the last channel corresponds to the logits probabilities of the Bernoulli distribution \\(\\text{logits }\\textbf{p}_k\\).\n\n\n\nCode\nclass CNN_VAE(nn.Module):\n    \"\"\"simple CNN-VAE class with a Gaussian encoder (mean and diagonal variance\n    structure) and a Gaussian decoder with fixed variance \n    (decoder is implemented as a Spatial Broadcast decoder) \n\n    Attributes\n        latent_dim (int): dimension of latent space\n        encoder (nn.Sequential): encoder network for mean and log_var\n        decoder (nn.Sequential): spatial broadcast decoder  for mean (fixed var)\n        x_grid (torch tensor): appended x coordinates for spatial broadcast decoder\n        y_grid (torch tensor): appended x coordinates for spatial broadcast decoder\n    \"\"\"\n\n    def __init__(self):\n        super(CNN_VAE, self).__init__()\n        self.latent_dim = 8\n        self.encoder = nn.Sequential(\n            # shape: [batch_size, 4, 64, 64]\n            nn.Conv2d(4, 32, kernel_size=(3,3), stride=2, padding=1),\n            nn.ReLU(),\n            # shape: [batch_size, 32, 32, 32]\n            nn.Conv2d(32, 32, kernel_size=(3,3), stride=2, padding=1),\n            nn.ReLU(),\n            # shape: [batch_size, 32, 16, 16]\n            nn.Conv2d(32, 64, kernel_size=(3,3), stride=2, padding=1),\n            nn.ReLU(),\n            # shape: [batch_size, 64, 8, 8]\n            nn.Conv2d(64, 64, kernel_size=(3,3), stride=2, padding=1),\n            nn.ReLU(),\n            # shape: [batch_size, 64, 4, 4],\n            nn.Flatten(),\n            # shape: [batch_size, 1024]\n        )\n        self.MLP = nn.Sequential(\n            nn.Linear(64*4*4, 256),\n            nn.ReLU(),\n            nn.Linear(256, 2*self.latent_dim),\n        )\n        # spatial broadcast decoder configuration\n        img_size = 64\n        # \"input width and height of CNN both 8 larger than target output\"\n        x = torch.linspace(-1, 1, img_size + 8)\n        y = torch.linspace(-1, 1, img_size + 8)\n        x_grid, y_grid = torch.meshgrid(x, y)\n        # reshape into [1, 1, img_size, img_size] and save in state_dict\n        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape).clone())\n        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape).clone())\n        self.decoder = nn.Sequential(\n             # shape [batch_size, latent_dim + 2, 72, 72]\n            nn.Conv2d(in_channels=self.latent_dim+2, out_channels=16,\n                      stride=(1, 1), kernel_size=(3,3)),\n            nn.ReLU(),\n            # shape [batch_size, 16, 70, 70]\n            nn.Conv2d(in_channels=16, out_channels=16, stride=(1,1),\n                      kernel_size=(3, 3)),\n            nn.ReLU(),\n            # shape [batch_size, 16, 68, 68]\n            nn.Conv2d(in_channels=16, out_channels=16, stride=(1,1),\n                      kernel_size=(3, 3)),\n            nn.ReLU(),\n            # shape [batch_size, 16, 66, 66]\n            nn.Conv2d(in_channels=16, out_channels=16, stride=(1,1),\n                      kernel_size=(3, 3)),\n            # shape [batch_size, 4, 64, 64]\n            nn.ReLU(),\n            nn.Conv2d(in_channels=16, out_channels=4, stride=(1,1),\n                      kernel_size=(1, 1)),\n        )\n        return\n\n    def forward(self, x):\n        [z, mu_E, log_var_E] = self.encode(x)\n        x_rec = self.decode(z)\n        return x_rec, z, mu_E, log_var_E\n\n    def encode(self, x):\n        out_encoder = self.MLP(self.encoder(x))\n        mu_E, log_var_E = torch.chunk(out_encoder, 2, dim=1)\n        # sample noise variable for each batch\n        epsilon = torch.randn_like(log_var_E)\n        # get latent variable by reparametrization trick\n        z = mu_E + torch.exp(0.5 * log_var_E) * epsilon\n        return [z, mu_E, log_var_E]\n\n    def decode(self, z):\n        batch_size = z.shape[0]\n        # reshape z into [batch_size, latent_dim, 1, 1]\n        z = z.view(z.shape + (1, 1))\n        # tile across image [batch_size, latent_im, 64+8, 64+8]\n        z_b = z.repeat(1, 1, 64 + 8, 64 + 8)\n        # upsample x_grid and y_grid to [batch_size, 1, 64+8, 64+8]\n        x_b = self.x_grid.repeat(batch_size, 1, 1, 1)\n        y_b = self.y_grid.repeat(batch_size, 1, 1, 1)\n        # concatenate vectors [batch_size, latent_dim+2, 64+8, 64+8]\n        z_sb = torch.cat((z_b, x_b, y_b), dim=1)\n        # apply convolutional layers mu_D\n        mu_D = self.decoder(z_sb)\n        return mu_D\n\n    \nclass ComponentVAE(CNN_VAE):\n    \"\"\"Component VAE class for use in MONet as proposed by Burgess et al. (2019)\n\n    Attributes:\n        #################### CNN_VAE ########################\n        encoder (nn.Sequential): encoder network for mean and log_var\n        decoder (nn.Sequential): decoder network for mean (fixed var)\n        img_dim (int): image dimension along one axis\n        expand_dim (int): expansion of latent image to accomodate for lack of padding\n        x_grid (torch tensor): appended x coordinates for spatial broadcast decoder\n        y_grid (torch tensor): appended x coordinates for spatial broadcast decoder\n        #####################################################\n        img_channels (int): number of channels in image\n    \"\"\"\n\n    def __init__(self,):\n        super().__init__()\n        self.img_channels = 3\n        return\n\n    def forward(self, image, log_mask, deterministic=False):\n        \"\"\"\n        parellize computation of reconstructions\n\n        Args:\n            image (torch.tensor): input image [batch, img_channels, img_dim, img_dim]\n            log_mask (torch.tensor): all seg masks [batch, slots, 1, img_dim, img_dim]\n\n        Returns:\n            mu_z_k (torch.tensor): latent mean [batch, slot, latent_dim]\n            log_var_z_k (torch.tensor): latent log_var [batch, slot, latent_dim]\n            z_k (torch.tensor): latent log_var [batch, slot, latent_dim]\n            x_r_k (torch.tensor): img reconstruction \n                [batch, slot, img_chan, img_dim, img_dim]\n            logits_m_r_k (torch.tensor): mask recons. [batch, slot, 1, img_dim, img_dim]\n        \"\"\"\n        num_slots = log_mask.shape[1]\n        # create input [batch_size*num_slots, image_channels+1, img_dim, img_dim]\n        x = ComponentVAE._prepare_input(image, log_mask, num_slots)\n        # get encoder distribution parameters [batch*slots, latent_dim]\n        [z_k, mu_z_k, log_var_z_k] = self.encode(x)\n        if deterministic:\n            z_k = mu_z_k\n        # get decoder dist. parameters [batch*slots, image_channels, img_dim, img_dim]\n        [x_r_k, logits_m_r_k] = self.decode(z_k)\n        # convert outputs into easier understandable shapes\n        [mu_z_k, log_var_z_k, z_k, x_r_k, logits_m_r_k] = ComponentVAE._prepare_output(\n            mu_z_k, log_var_z_k, z_k, x_r_k, logits_m_r_k, num_slots\n        )\n        return [mu_z_k, log_var_z_k, z_k, x_r_k, logits_m_r_k]\n\n    def decode(self, z):\n        \"\"\"\n        Args:\n            z (torch.tensor): [batch_size*num_slots, latent_dim]\n\n        Returns:\n            mu_x (torch.tensor): [batch*slots, img_channels, img_dim, img_dim]\n            logits_m (torch.tensor): [batch*slots, 1, img_dim, img_dim]\n\n        \"\"\"\n        mu_D = super().decode(z)\n        # split into means of x and logits of m\n        mu_x, logits_m = torch.split(mu_D, self.img_channels, dim=1)\n        # enforce positivity of mu_x\n        mu_x = mu_x.abs()\n        return [mu_x, logits_m]\n\n    @staticmethod\n    def _prepare_input(image, log_mask, num_slots):\n        \"\"\"\n        Args:\n            image (torch.tensor): input image [batch, img_channels, img_dim, img_dim]\n            log_mask (torch.tensor): all seg masks [batch, slots, 1, img_dim, img_dim]\n            num_slots (int): number of slots (log_mask.shape[1])\n\n        Returns:\n            x (torch.tensor): input image [batch*slots, img_channels+1, img_dim, img_dim]\n        \"\"\"\n        # prepare image [batch_size*num_slots, image_channels, img_dim, img_dim]\n        image = image.repeat(num_slots, 1, 1, 1)\n        # prepare log_mask [batch_size*num_slots, 1, img_dim, img_dim]\n        log_mask = torch.cat(log_mask.squeeze(2).chunk(num_slots, 1), 0)\n        # concatenate along color channel\n        x = torch.cat((image, log_mask), dim=1)\n        return x\n\n    @staticmethod\n    def _prepare_output(mu_z_k, log_var_z_k, z_k, x_r_k, logits_m_r_k, num_slots):\n        \"\"\"\n        convert output into an easier understandable format\n\n        Args:\n            mu_z_k (torch.tensor): [batch_size*num_slots, latent_dim]\n            log_var_z_k (torch.tensor): [batch_size*num_slots, latent_dim]\n            z_k (torch.tensor): [batch_size*num_slots, latent_dim]\n            x_r_k (torch.tensor): [batch_size*num_slots, img_channels, img_dim, img_dim]\n            logits_m_r_k (torch.tensor): [batch_size*num_slots, 1, img_dim, img_dim]\n            num_slots (int): number of slots (log_mask.shape[1])\n\n        Returns:\n            mu_z_k (torch.tensor): [batch, slot, latent_dim]\n            log_var_z_k (torch.tensor): [batch, slot, latent_dim]\n            z_k (torch.tensor): [batch, slot, latent_dim]\n            x_r_k (torch.tensor): [batch, slots, img_channels, img_dim, img_dim]\n            logits_m_r_k (torch.tensor): [batch, slots, 1, img_dim, img_dim]\n        \"\"\"\n        mu_z_k = torch.stack(mu_z_k.chunk(num_slots, dim=0), 1)\n        log_var_z_k = torch.stack(log_var_z_k.chunk(num_slots, dim=0), 1)\n        z_k = torch.stack(z_k.chunk(num_slots, dim=0), 1)\n        x_r_k = torch.stack(x_r_k.chunk(num_slots, dim=0), 1)\n        logits_m_r_k = torch.stack(logits_m_r_k.chunk(num_slots, dim=0), 1)\n        return [mu_z_k, log_var_z_k, z_k, x_r_k, logits_m_r_k]\n\n\n\nMONet Implementation: The compositional structure is achieved by looping for \\(K\\) steps over the image and combining the attention network with the component VAE. While attention masks and latent codes can be generated easily (during test time), computing the loss \\(\\mathcal{L}\\) is more complicated. Remind that the loss function is given by\n\\[\n\\begin{align}\n\\mathcal{L}\\left(\\boldsymbol{\\phi}; \\boldsymbol{\\theta};\n\\boldsymbol{\\psi}; \\textbf{x} \\right) &= \\underbrace{- \\log \\left( \\sum_{k=1}^K \\textbf{m}_k \\odot p_{\\boldsymbol{\\theta}} \\left(\\textbf{x} |\n\\textbf{z}_k \\right)\\right)}_{\\text{Reconstruction Error between }\n\\widetilde{\\textbf{x}} \\text{ and } \\textbf{x}} + \\beta\n\\underbrace{D_{KL} \\left( \\prod_{k=1}^K q_{\\boldsymbol{\\phi}} \\left(\\textbf{z}_k |\n\\textbf{x}, \\textbf{m}_k\\right) || p(\\textbf{z})\n\\right)}_{\\text{Regularization Term for Distribution of }\\textbf{z}_k}\\\\\n&+ \\gamma \\underbrace{D_{KL} \\left( q_{\\boldsymbol{\\psi}} \\left( \\textbf{c} |\n\\textbf{x} \\right) || p_{\\boldsymbol{\\theta}} \\left( \\textbf{c} | \\{\n\\textbf{z}_k \\} \\right) \\right)}_{\\text{Reconstruction Error between }\n\\widetilde{\\textbf{m}}_k \\text{ and } \\textbf{m}_k}.\n\\end{align}\n\\]\nEach of these three terms can be written in a more explicit form such that the implementation becomes trivial:\n\nReconstruction Error between \\(\\widetilde{\\textbf{x}}\\) and \\(\\textbf{x}\\): This term is also known as the negative log likelihood (NLL) of the whole reconstructed image. Burgess et al. (2019) chose independent Gaussian distributions with fixed variance for each pixel as the decoder distribution \\(p_{\\boldsymbol{\\theta}} \\left(\nx_{i} | \\textbf{z}_k \\right) \\sim \\mathcal{N} \\left(\\mu_{k,\ni}(\\boldsymbol{\\theta}), \\sigma_k^2 \\right)\\).\nRegularization Term for Distribution of \\(\\textbf{z}_k\\): The coding space is regularized using the KL divergence between the latent (posterior) distribution \\(q_{\\boldsymbol{\\phi}} \\left( \\textbf{z}_k \\right) \\sim\n\\mathcal{N} \\left( \\boldsymbol{\\mu}_k, \\left(\\boldsymbol{\\sigma}_k^2\\right)^{\\text{T}}\n\\textbf{I} \\right)\\) factorized across slots and the latent prior distribution weighted with the hyperparameter \\(\\beta\\). The product of multiple Gaussians is itself a Gaussian, however it is rather complicated to compute the new mean and covariance matrix of this Gaussian. Fortunately, each \\(\\textbf{z}_k\\) is sampled independently from the corresponding latent distribution \\(q_{\\boldsymbol{\\phi}}(\\textbf{z}_k)\\), thus we can generate the new mean and covariance by concatenation (see this post), i.e.,\n\\[\n  q(\\textbf{z}_1, \\dots, \\textbf{z}_K) = \\prod_{k=1}^K q_{\\boldsymbol{\\phi}} \\left(\\textbf{z}_k\n\\right)  = q\\left( \\begin{bmatrix} \\textbf{z}_1 \\\\ \\vdots\n  \\\\ \\textbf{z}_K  \\end{bmatrix}\\right) = \\mathcal{N} \\left(\n  \\underbrace{\n  \\begin{bmatrix} \\boldsymbol{\\mu}_1 \\\\ \\vdots\n   \\\\ \\boldsymbol{\\mu}_K \\end{bmatrix}}_{\n   \\widehat{\\boldsymbol{\\mu}}}, \\underbrace{\\text{diag}\\left(\n  \\begin{bmatrix} \\boldsymbol{\\sigma}_1^2 \\\\  \\vdots\\\\\n  \\boldsymbol{\\sigma}_K^2  \\end{bmatrix}\n  \\right)}_{ \\left(\\widehat{\\boldsymbol{\\sigma}}^2\\right)^{\\text{T}} \\textbf{I}}\\right)\n\\]\nBurgess et al. (2019) chose a unit Gaussian distribution as the latent prior \\(p(\\textbf{z})\n\\sim \\mathcal{N} \\left(\\textbf{0}, \\textbf{I} \\right)\\) with \\(\\text{dim}(\\textbf{0}) = \\text{dim}(\\hat{\\boldsymbol{\\mu}})\\). The KL divergence between those two Gaussian distributions can be calculated in closed form (see Appendix B of Kingma and Welling (2013))\n\\[\n\\begin{align}\n   D_{KL} \\left( \\prod_{k=1}^K q_{\\boldsymbol{\\phi}}\n\\left(\\textbf{z}_k \\right) || p(\\textbf{z}) \\right) &= -\\frac\n{1}{2} \\sum_{j=1}^{K \\cdot L} \\left(1 + \\log \\left(\n\\widehat{\\sigma}_j^2 \\right) - \\widehat{\\mu}_j^2 - \\widehat{\\sigma}_j^2 \\right),\n\\end{align}\n\\]\nwhere \\(L\\) denotes the dimensionality of the latent space.\nReconstruction Error between \\(\\widetilde{\\textbf{m}}_k\\) and \\(\\textbf{m}_k\\): Remind that the attention network \\(\\boldsymbol{\\alpha}_{\\boldsymbol{\\psi}}\\) produces \\(K\\) segmentation masks in logaritmic units, i.e., \\(\\log\n\\textbf{m}_k\\). By construction \\(\\sum_{k=1}^K \\textbf{m}_k =\n\\textbf{1}\\), i.e., concatentation of the attention masks \\(\\textbf{m} = \\begin{bmatrix} \\textbf{m}_1 & \\dots &\n\\textbf{m}_K \\end{bmatrix}^{\\text{T}}\\) can be interpreted as a pixel-wise categorical distribution6. Similarly, concatenating the logits probabilties of the component VAE and applying a pixel-wise softmax, i.e.,\n\\[\n\\widetilde{\\textbf{m}} = \\begin{bmatrix} \\widetilde{\\textbf{m}}_1 \\\\ \\vdots \\\\\n\\widetilde{\\textbf{m}}_K \\end{bmatrix} = \\text{Softmax}\\left(\\begin{bmatrix} \\text{logits }\\textbf{p}_1 \\\\ \\vdots \\\\\n\\text{logits }\\textbf{p}_K \\end{bmatrix}\\right),\n\\]\ntransforms the logits outputs of the component VAE into a pixel-wise categorical distribution. Thus, the KL-divergence can be calculated as follows\n\\[\n\\begin{align}\n  D_{KL} \\left( q_{\\boldsymbol{\\psi}} \\left( \\textbf{c} |\n\\textbf{x} \\right) || p_{\\boldsymbol{\\theta}} \\left( \\textbf{c} | \\{\n\\textbf{z}_k \\} \\right) \\right) &=\n  \\sum_{i=1}^{H\\cdot W} D_{KL} \\left( {\\textbf{m}}_i || \\widetilde{\\textbf{m}}_i \\right) \\\\\n  &= \\sum_{i=1}^{H\\cdot W} \\textbf{m}_i \\odot \\left(\\log \\textbf{m}_i - \\log \\widetilde{\\textbf{m}}_i \\right),\n\\end{align}\n\\]\nwhere \\(i\\) denotes the pixel space, i.e., \\(\\textbf{m}_i \\in [0,\n1]^{K}\\). To make the computation more efficient, we directly compute the reconstructed segmentations in logaritmic units using pixel-wise logsoftmax, i.e.,\n\\[\n\\log \\widetilde{\\textbf{m}} = \\text{LogSoftmax}\\left(\\begin{bmatrix} \\text{logits }\\textbf{p}_1 \\\\ \\vdots \\\\\n\\text{logits }\\textbf{p}_K \\end{bmatrix}\\right).\n\\]\n\n\n\n\nCode\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\n\n\nclass MONet(pl.LightningModule):\n    \"\"\"Multi-Object Network class as described by Burgess et al. (2019)\n    \n    Atributes:\n        n_samples (int): number of samples in training dataset\n        attention_network (AttentionNetwork)\n        component_VAE (ComponentVAE)\n        ############## loss specific ##############\n        bg_var (float): background variance\n        fg_var (float): foreground variance\n        beta (float): hyperparamater for loss\n        gamma (float): hyperparameter for loss\n        ###########################################\n        ############ training specific ############\n        num_slots_train (int): number of slots used during training time\n        lr (float): learning rate\n        batch_size (int): batch size used during training\n        log_every_k_epochs (int): how often current result img should be logged\n        ###########################################\n    \"\"\"\n\n    def __init__(self, n_samples):\n        super(MONet, self).__init__()\n        self.n_samples = n_samples\n        self.attention_network = AttentionNetwork()\n        self.component_VAE = ComponentVAE()\n        # initialize all biases to zero\n        self.attention_network.apply(MONet.weight_init)\n        self.component_VAE.apply(MONet.weight_init)\n        ############## loss specific ##############\n        self.num_slots_train = 3\n        self.bg_var, self.fg_var = 0.09**2, 0.11**2\n        self.beta = 0.5\n        self.gamma = 0.5\n        ###########################################\n        ############ training specific ############\n        self.lr, self.batch_size = 0.0001, 64\n        self.log_every_k_epochs = 1\n        # Initialise pixel output standard deviations (NLL calculation)\n        var = self.fg_var * torch.ones(1, self.num_slots_train, 1, 1, 1)\n        var[0, 0, 0, 0, 0] = self.bg_var  # first step\n        self.register_buffer(\"var\", var)\n        self.save_hyperparameters()\n        return\n\n    def forward(self, x, num_slots):\n        \"\"\"\n        defines the inference procedure of MONet, i.e., computes the latent\n        space and keeps track of useful metrics\n\n        Args:\n            x (torch.tensor): image [batch_size, img_channels, img_dim, img_dim]\n            num_slots (int): number of slots\n\n        Returns:\n            out (dict): output dictionary containing\n                log_m_k (torch.tensor) [batch, slots, 1, img_dim, img_dim]\n                    (logarithmized attention masks of attention_network)\n                mu_k (torch.tensor) [batch, slots, latent_dim]\n                    (means of component VAE latent space)\n                log_var_k (torch.tensor) [batch, slots, latent_dim]\n                    (logarithmized variances of component VAE latent space)\n                x_r_k (torch.tensor) [batch, slots, img_channels, img_dim, img_dim]\n                    (slot-wise VAE image reconstructions)\n                logits_m_r_k (torch.tensor) [batch, slots, 1, img_dim, img_dim]\n                    (slot-wise VAE mask reconstructions in logits)\n                x_tilde (torch.tensor) [batch, img_channels, img_dim, img_dim]\n                    (reconstructed image using x_r_k and log_m_k)\n        \"\"\"\n        # compute all logarithmized masks (iteratively)\n        log_m_k = self.attention_network(x, num_slots)\n        # compute all VAE reconstructions (parallel)\n        [mu_z_k, log_var_z_k, z_k, x_r_k, logits_m_r_k] = self.component_VAE(x, \n                                                                             log_m_k.exp())\n        # store output in dict\n        output = dict()\n        output[\"log_m_k\"] = log_m_k\n        output[\"mu_z_k\"] = mu_z_k\n        output[\"log_var_z_k\"] = log_var_z_k\n        output[\"z_k\"] = z_k\n        output[\"x_r_k\"] = x_r_k\n        output[\"logits_m_r_k\"] = logits_m_r_k\n        output[\"x_tilde\"] = (log_m_k.exp() * x_r_k).sum(axis=1)\n        return output\n    \n    \n    ########################################\n    #########  TRAINING FUNCTIONS  #########\n    ########################################\n\n    def training_step(self, batch, batch_idx):\n        x, labels = batch  # labels are not used here (unsupervised)\n        output = self.forward(x, self.num_slots_train)        \n        ############ NLL \\sum_k m_k log p(x_k) #############################\n        NLL = (\n            output[\"log_m_k\"].exp() * \n            (((x.unsqueeze(1) - output[\"x_r_k\"]) ** 2 / (2 * self.var)))\n        ).sum(axis=(1, 2, 3, 4))\n        # compute KL divergence of latent space (component VAE) per batch\n        KL_div_VAE = -0.5 * (\n            1 + output[\"log_var_z_k\"] - output[\"mu_z_k\"] ** 2 \n            - output[\"log_var_z_k\"].exp()\n        ).sum(axis=(1, 2))\n        # compute KL divergence between masks\n        log_m_r_k = output[\"logits_m_r_k\"].log_softmax(dim=1)\n        KL_div_masks = (output[\"log_m_k\"].exp() * (output[\"log_m_k\"] - log_m_r_k)).sum(\n            axis=(1, 2, 3, 4)\n        )\n        # compute loss\n        loss = (NLL.mean() + self.beta * KL_div_VAE.mean() \n                + self.gamma * KL_div_masks.mean())\n        # log results in TensorBoard\n        step = self.global_step\n        self.logger.experiment.add_scalar(\"loss/NLL\", NLL.mean(), step)\n        self.logger.experiment.add_scalar(\"loss/KL VAE\", KL_div_VAE.mean(), step)\n        self.logger.experiment.add_scalar(\"loss/KL masks\", KL_div_masks.mean(), step)\n        self.logger.experiment.add_scalar(\"loss/loss\", loss, step)\n        return {\"loss\":loss, \"x\": x}\n\n    def training_epoch_end(self, outputs):\n        \"\"\"this function is called after each epoch\"\"\"\n        step = int(self.current_epoch)\n        if (step + 1) % self.log_every_k_epochs == 0:\n            # log some images, their segmentations and reconstructions\n            n_samples = 7\n            \n            last_x = outputs[-1][\"x\"]\n            i_samples = np.random.choice(range(len(last_x)), n_samples, False)\n            images = last_x[i_samples]\n            \n            fig_rec = self.plot_reconstructions_and_decompositions(images, \n                                                                   self.num_slots_train)\n            self.logger.experiment.add_figure(\"image and reconstructions\", \n                                              fig_rec, global_step=step)\n        return\n    \n    ########################################\n    ######### TRAINING SETUP HOOKS #########\n    ########################################\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.RMSprop(self.parameters(), lr=self.lr)\n        return optimizer\n    \n    @staticmethod\n    def weight_init(m):\n        \"\"\"initialize all bias to zero\"\"\"\n        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n            if m.bias is not None:\n                torch.nn.init.zeros_(m.bias)\n        return\n    \n    ########################################\n    ####### PLOT AND HELPER FUNCTIONS ######\n    ########################################\n\n    @staticmethod\n    def convert_masks_indices_to_mask_rgb(masks_ind, slots):\n        colors = plt.cm.get_cmap(\"hsv\", slots + 1)\n        cmap_rgb = colors(np.linspace(0, 1, slots + 1))[:, 0:3]\n        masks_RGB = cmap_rgb[masks_ind].squeeze(1)\n        masks_RGB_tensor = torch.from_numpy(masks_RGB)\n        return masks_RGB_tensor\n\n    def plot_reconstructions_and_decompositions(self, images, num_slots):\n        monet_output = self.forward(images, num_slots)\n        batch_size, img_channels = images.shape[0:2]\n        \n        colors = plt.cm.get_cmap(\"hsv\", num_slots + 1)\n        cmap = colors(np.linspace(0, 1, num_slots + 1))\n        \n        # get mask indices using argmax [batch_size, 1, 64, 64]\n        masks_ind = monet_output[\"log_m_k\"].exp().argmax(1).detach().cpu()\n        # convert into RGB values  [batch_size, 64, 64, 3]\n        masks_RGB = MONet.convert_masks_indices_to_mask_rgb(masks_ind, num_slots)              \n        fig = plt.figure(figsize=(14, 10))\n        for counter in range(batch_size):\n            orig_img = images[counter]\n            # data\n            plt.subplot(3 + num_slots, batch_size + 1, counter + 2)\n            plt.imshow(transforms.ToPILImage()(orig_img))\n            plt.axis('off')\n            # reconstruction mixture\n            x_tilde = monet_output[\"x_tilde\"][counter].clamp(0, 1)\n            plt.subplot(3 + num_slots, batch_size + 1, counter + 2 + (batch_size + 1))\n            plt.imshow(transforms.ToPILImage()(x_tilde))\n            plt.axis('off')\n            # segmentation (binary) from attention network\n            plt.subplot(3 + num_slots, batch_size + 1, counter + 2 + (batch_size + 1)*2)\n            plt.imshow(masks_RGB[counter])\n            plt.axis('off')\n            # unmasked component reconstructions\n            x_r_k = monet_output[\"x_r_k\"][counter].clamp(0, 1)\n            for slot in range(num_slots):\n                x_rec = x_r_k[slot]\n                plot_idx =  counter + 2 + (batch_size + 1)*(slot+3)\n                plt.subplot(3 + num_slots, batch_size + 1, plot_idx)\n                plt.imshow(transforms.ToPILImage()(x_rec))\n                plt.axis('off')\n        # annotation plots\n        ax = plt.subplot(3 + num_slots, batch_size + 1, 1)\n        ax.annotate('Data', xy=(1, 0.5), xycoords='axes fraction',\n                    fontsize=14, va='center', ha='right')\n        ax.set_aspect('equal')\n        ax.axis('off')\n        ax = plt.subplot(3 + num_slots, batch_size + 1, batch_size + 2)\n        ax.annotate('Reconstruction\\nmixture', xy=(1, 0.5), xycoords='axes fraction',\n                    fontsize=14, va='center', ha='right')\n        ax.set_aspect('equal')\n        ax.axis('off')\n        ax = plt.subplot(3 + num_slots, batch_size + 1, 2*batch_size + 3)\n        ax.annotate('Segmentation', xy=(1, 0.5), xycoords='axes fraction',\n                    fontsize=14, va='center', ha='right')\n        ax.set_aspect('equal')\n        ax.axis('off')\n        for slot in range(num_slots):\n            ax = plt.subplot(3 + num_slots, batch_size + 1, \n                             1 + (batch_size + 1)*(slot+3))\n            ax.annotate(f'S{slot+1}', xy=(1, 0.5), xycoords='axes fraction',\n                        fontsize=14, va='center', ha='right', weight='bold',\n                        color=cmap[slot])\n            ax.set_aspect('equal')\n            ax.axis('off')\n        return fig\n    \n    def plot_ComponentVAE_results(self, images, num_slots):\n        monet_output = self.forward(images, num_slots)\n        x_r_k = monet_output[\"x_r_k\"]\n        masks = monet_output[\"log_m_k\"].exp()\n        # get mask indices using argmax [batch_size, 1, 64, 64]\n        masks_ind = masks.argmax(1).detach().cpu()\n        # convert into RGB values  [batch_size, 64, 64, 3]\n        masks_RGB = MONet.convert_masks_indices_to_mask_rgb(masks_ind, num_slots) \n        \n        colors = plt.cm.get_cmap('hsv', num_slots + 1)\n        cmap = colors(np.linspace(0, 1, num_slots + 1))\n        n_samples, img_channels = images.shape[0:2]\n        fig = plt.figure(constrained_layout=False, figsize=(14, 14))\n        grid_spec = fig.add_gridspec(2, n_samples, hspace=0.1)\n        \n        for counter in range(n_samples):\n            orig_img = images[counter]\n            x_tilde = monet_output[\"x_tilde\"][counter].clamp(0, 1)\n            segmentation_mask = masks_RGB[counter]\n            # upper plot: Data, Reconstruction Mixture, Segmentation\n            upper_grid = grid_spec[0, counter].subgridspec(3, 1)\n            for upper_plot_index in range(3):\n                ax = fig.add_subplot(upper_grid[upper_plot_index])\n                if upper_plot_index == 0:\n                    plt.imshow(transforms.ToPILImage()(orig_img))\n                elif upper_plot_index == 1:\n                    plt.imshow(transforms.ToPILImage()(x_tilde))   \n                else:\n                    plt.imshow(segmentation_mask)\n                plt.axis('off')\n                if counter == 0:  # annotations\n                    if upper_plot_index == 0:  # Data\n                        ax.annotate('Data', xy=(-0.1, 0.5), \n                                    xycoords='axes fraction', ha='right',\n                                    fontsize=14, va='center',)\n                    elif upper_plot_index == 1:  # Reconstruction mixture\n                        ax.annotate('Reconstruction\\nmixture', xy=(-0.1, 0.5), \n                                     va='center',\n                                     xycoords='axes fraction', fontsize=14, ha='right')\n                    else:  # Segmentation\n                        ax.annotate('Segmentation', xy=(-0.1, 0.5), va='center',\n                                     xycoords='axes fraction', fontsize=14, ha='right')\n            # lower plot: Component VAE reconstructions\n            lower_grid = grid_spec[1, counter].subgridspec(num_slots, 2, \n                                                           wspace=0.1, hspace=0.1)\n            for row_index in range(num_slots):\n                x_slot_r = x_r_k[counter][row_index]\n                m_slot_r = masks[counter][row_index]\n                for col_index in range(2):\n                    ax = fig.add_subplot(lower_grid[row_index, col_index])\n                    if col_index == 0:  # unmasked\n                        plt.imshow(transforms.ToPILImage()(x_slot_r.clamp(0, 1)))\n                        if row_index == 0:\n                            plt.title('Unmasked', fontsize=14)\n                        plt.axis('off')\n                    else:  # masked\n                        masked = ((1 - m_slot_r)*torch.ones_like(x_slot_r) \n                                  + m_slot_r*x_slot_r)\n                        #masked = m_slot_r*x_slot_r\n                        plt.imshow(transforms.ToPILImage()(masked.clamp(0, 1)))\n                        if row_index == 0:\n                            plt.title('Masked', fontsize=14)\n                        plt.axis('off')\n                    ax.set_aspect('equal')\n                    if counter == 0 and col_index == 0:  # annotations\n                        ax.annotate(f'S{row_index+1}', xy=(-0.1, 0.5), \n                                    xycoords='axes fraction', ha='right',\n                                    fontsize=14, va='center', weight='bold',\n                                    color=cmap[row_index])\n        return\n                                   \n    ########################################\n    ########## DATA RELATED HOOKS ##########\n    ########################################\n\n    def prepare_data(self) -&gt; None:\n        n_samples = self.n_samples\n        self.dataset = generate_dataset(n_samples=n_samples)\n        return\n\n    def train_dataloader(self):\n        return DataLoader(self.dataset, batch_size=self.batch_size, \n                          num_workers=12, shuffle=True)\n\n\n\nTraining Procedure: Burgess et al. (2019) chose RMSProp for the optimization with a learning rate of 0.0001 and a batch size of 64, see Appendix B.3. Thanks to the PyTorch-Lightning framework, these paramters are already defined in the model and we can easily integrate tensorboard into our training procedure:\n\n\n\nCode\nfrom pytorch_lightning import seed_everything\nfrom pytorch_lightning.loggers import TensorBoardLogger\n\n\ndef train(n_samples, num_epochs, SEED=1):\n    seed_everything(SEED)\n\n    monet = MONet(n_samples)\n    logger = TensorBoardLogger('./results', name=\"SimplifiedMultiSprites\")\n    # initialize pytorch lightning trainer\n    num_gpus = 1 if torch.cuda.is_available() else 0\n    trainer = pl.Trainer(\n        deterministic=True,\n        gpus=num_gpus,\n        track_grad_norm=2,\n        gradient_clip_val=2,  # don't clip\n        max_epochs=num_epochs,\n        progress_bar_refresh_rate=20,\n        logger=logger,\n    )\n     # train model\n    trainer.fit(monet)\n    trained_monet = monet\n    return trained_monet\n\ntrained_monet = train(n_samples=50000, num_epochs=10)\n\n\n\n\n\nTraining\n\n\n\n\nResults\nThe following visualization are inspired by Figure 3 and 7 of Burgess et al. (2019) and mainly serve to evaluate the representation quality of the trained model.\n\nMONet Reconstructions and Decompositions: The most intuitive visualization is to show some (arbitrarly chosen) fully reconstructed images (i.e, Reconstruction mixture \\(\\widetilde{\\textbf{x}} = \\sum_{k=1}^K\n\\textbf{m}_k \\odot \\widetilde{\\textbf{x}}_k\\)) compared to the original input \\(\\textbf{x}\\) (Data) together with the learned segmentation masks (i.e., Segmentation \\(\\{ \\textbf{m}_k \\}\\)) of the attention network. Note that in order to visualize the segmentations in one plot, we cast the attenion masks into binary attention masks by applying arg max pixel-wise over all \\(K\\) attention masks. In addition, all umasked component VAE reconstructions (i.e., S(k) \\(\\widetilde{\\textbf{x}}_k\\)) are shown, see figure below.\n\n\n\n\n\n\n\n\nFigure 7 of Burgess et al. (2019): Each example shows the image fed as input data to the model, with corresponding outputs from the model. Reconstruction mixtures show sum of components from all slots, weighted by the learned masks from the attention network. Colour-coded segmentation maps summarize the attention masks \\(\\{\\textbf{m}_k \\}\\). Rows labeld S1-5 show the reconstruction components of each slot.\n\n\n\n\n\n\nCode\ndataloader = trained_monet.train_dataloader()\nrandom_batch = next(iter(dataloader))[0]\nfig = trained_monet.plot_reconstructions_and_decompositions(batch[0: 4], 3)\n\n\n\n\n\nMONet Reconstructions and Decompositions after Train\n\n\n\nComponent VAE Results: In order to evaluate the perfomance of the component VAE, we are interested in the unmasked slot-wise reconstructions (i.e., unmasked refers to \\(\\widetilde{\\textbf{x}}_k\\) for each slot \\(k\\)) and the slot-wise reconstructions masked by the VAE’s reconstructed masks (i.e., masked refers to \\(\\widetilde{\\textbf{m}}_k \\odot\n\\widetilde{\\textbf{x}}_k\\)). Ideally, masked versions capture either a single object, the background or nothing at all (representing no object), see figure below. In addition, we are going to plot the ground truth masked reconstructions (i.e., gt masked refers to \\(\\textbf{m}_k \\odot \\widetilde{\\textbf{x}}_k\\)) such that the difference between gt masked and masked indicates the reconstruction error of the attention masks.\n\n\n\n\n\n\n\n\nFigure 3 of Burgess et al. (2019): Each example shows the image fet as input data to the model, with corresponding outputs from the model. Reconstruction mixtures show sum of components from all slots, weighted by the learned masks from the attention network. Color-coded segmentation maps summarise the attention masks \\(\\{\\textbf{m}_k\\}\\). Rows labeled S1-7 show the reconstruction components of each slot. Unmasked version are shown side-by-side with corresponding versions that are masked with the VAE’s reconstructed masks \\(\\widetilde{\\textbf{m}}_k\\).\n\n\n\n\n\n\nCode\ndataloader = trained_monet.train_dataloader()\nrandom_batch = next(iter(dataloader))[0]\nfig = trained_monet.plot_ComponentVAE_results(batch[0: 4], 3)\n\n\n\n\n\nMONet Component VAE"
  },
  {
    "objectID": "paper_summaries/multi-object_network/index.html#drawbacks-of-paper",
    "href": "paper_summaries/multi-object_network/index.html#drawbacks-of-paper",
    "title": "MONet: Unsupervised Scene Decomposition and Representation",
    "section": "Drawbacks of Paper",
    "text": "Drawbacks of Paper\n\ndeterministic attention mechanism implying that objective function is not a valid lower bound on the marginal likelihood (as mentioned by Engelcke et al. (2020))\nimage generation suffers from discrepancy between inferred and reconstructed masks  \nlots of hyperparameters (network architectures, \\(\\beta\\), \\(\\gamma\\), optimization)"
  },
  {
    "objectID": "paper_summaries/multi-object_network/index.html#acknowledgment",
    "href": "paper_summaries/multi-object_network/index.html#acknowledgment",
    "title": "MONet: Unsupervised Scene Decomposition and Representation",
    "section": "Acknowledgment",
    "text": "Acknowledgment\nThere are a lot of implementations out there that helped me very much in understanding the paper:\n\nDarwin Bautista’s implementation includes derivation of the NLL (which in the end, I did not use for simplicity).\nKarl Stelzner’s implementation is kept more simplisitic and is therefore easier to understand.\nMartin Engelcke, Claas Voelcker and Max Morrison included an implementation of MONet in the Genesis repository."
  },
  {
    "objectID": "paper_summaries/multi-object_network/index.html#footnotes",
    "href": "paper_summaries/multi-object_network/index.html#footnotes",
    "title": "MONet: Unsupervised Scene Decomposition and Representation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEncoding each segment through the same VAE can be understood as an architectural prior on common structure within individual objects.↩︎\nBurgess et al. (2019) do not explain why the Component VAE should also model the attention masks. Note however that this allows for better generalization, e.g., shape/class variation depends on attention mask.↩︎\nFor completeness \\(\\textbf{c} \\in \\{1, \\dots, K\\}\\) denotes a categorical variable to indicate the probability that pixels belong to a particular component \\(k\\), i.e., \\(\\textbf{m}_k =\np(\\textbf{c} = k)\\).↩︎\nPhilosophical note: Humans also tend to work better when focusing on one task at a time.↩︎\nThis is explained in more detail in my VAE post. For simplicity, we are setting the number of (noise variable) samples \\(L\\) per datapoint to 1 (see equation \\(\\displaystyle \\widetilde{\\mathcal{L}}\\) in Reparametrization Trick paragraph). Note that Kingma and Welling (2013) stated that in their experiments setting \\(L=1\\) sufficed as long as the minibatch size was large enough.↩︎\nNote that concatenation of masks leads to a three dimensional tensor.↩︎"
  },
  {
    "objectID": "paper_summaries/schema_networks/index.html",
    "href": "paper_summaries/schema_networks/index.html",
    "title": "Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics",
    "section": "",
    "text": "Kansky et al. (2017) showed remarkable results of zero-shot transfer in several variations of Breakout by introducing Schema Networks as a generative model for object-oriented reinforcement learning and planning. This model incorporates objects as entities, represents local cause-effect relationships including one or more entities and is based on Probabilistic Graphical Models (PGMs). Due to its foundation in PGMs, Schema Networks support flexible inference and search strategies for planning."
  },
  {
    "objectID": "paper_summaries/schema_networks/index.html#model-description",
    "href": "paper_summaries/schema_networks/index.html#model-description",
    "title": "Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics",
    "section": "Model Description",
    "text": "Model Description\nBuilding upon the ideas of object-oriented Markov decision processes (OO-MDPs), states are represented as a list of entities where each entity can be understood as a different instantiation from the same class (i.e., all entities share the same attributes). Additionally, the attributes, actions and rewards are binarized using discretization and one-hot encoding, see image below. This representation comes from a handcrafted image parser with the handwavy argument that in practice a vision module could be responsible for this task.\n\n\n\n\n\n\n\n\n\n\n\nExemplary State Representation in a Schema Network. A handcrafted image parser converts an image (left) into the state representation (right), where filled green circles indicate that the binary variable is set to True.\n\n\n\nKansky et al. (2017) define 53 attributes for each entity in the Breakout domain (21 for bricks, 30 for the paddle, 1 for walls, 1 for the ball). However, they do not elaborate on what these attributes describe exactly. Furthermore, each pixel is identified as a part of an object and assigned the corresponding attributes. Accordingly, their representation could rather be understood as a 53-channel image where each entry can either be 0 or 1, e.g., one layer showing the walls. In this form, the entity-based state representation can also be provided to other algorithms such as A3C.\nSimiliar to OO-MDPs, state transitions are determined by a change of entity-attributes. However, due to the specific representation in Schema Networks, entity-attributes can only be active or inactive (with an associated probability). An attribute becomes activated if a grounded schema1 is active. Grounded schemas can include a variable size of entity attributes from a variable number of entities and may include one or more actions. Thus, these schemas can be interpreted as local cause-effect relationships. Formally, a grounded schema \\(\\phi^{k}\\) is a binary variable that becomes activated via a probabilistic AND over the binary variables \\(v_1, \\dots,\nv_n\\) that are included in it:\n\\[\n\\begin{align}\n  \\phi^{k} = \\text{AND} (v_1, \\dots, v_n) = \\prod_{i=1}^n P(v_i = 1).\n\\end{align}\n\\]\nThe binary variables \\(v_1, \\dots, v_n\\) may be entity-attributes2 or actions, see image below.\nMultiple grounded schemas can predict the same attribute which is formalized through an OR factor, e.g., let \\(\\alpha_{i, j}^{(t+1)}\\) denote the \\(j^{th}\\) attribute of the \\(i^{th}\\) entity at time \\(t+1\\) and assume there are \\(n\\) grounded schemas that predict this entity attribute. Then, this formalizes into\n\\[\n\\begin{align}\n  \\alpha_{i,j}^{(t+1)} = \\text{OR} (\\phi_{i,j}^{1}, \\dots, \\phi_{i, j}^{n}) = 1 - \\prod_{k=1}^n  \\big(1 - P(\\phi_{i,j}^k)\\big).\n\\end{align}\n\\]\nKansky et al. (2017) divide entity attributes into two classes: * Positional Attributes: These attributes correspond to discrete positions. * Non-Positional Attributes: The semantic meaning of those attributes is unknown to the model such that they may encode completely different things, e.g., color and shape.\nA self-transition variable is introduced for positional attributes which represents the probability that a position attribute will remain active in the next time step when no schema predicts a change from that position. Note that through this mechanism, they include the bias that an object cannot be at multiple positions at the same time.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransition dynamics in Schema Networks are governed by changes in entity-attributes due to activated grounded schemas. In this example all relevant gates are shown to illustrate the state transition dynamics via this mechanics. Note that there are two schemas that predict the same behavior, i.e., only one input in OR is necessary to activate \\(y=1\\).\n\n\n\nFormally, a self transition is a NOR factor over the grounded schemas that predict a change from a position attribute (i.e., the grounded schemas that predict towards a different position) combined with an AND factor over the NOR factor and the position attribute at the current time step. E.g., let \\(\\alpha_{i,j}^{t}\\) denote the \\(j^{th}\\) position attribute of the the \\(i^{th}\\) entity at time \\(t\\) and assume that the set \\(\\{\\phi^1, \\dots, \\phi^{n} \\}\\) includes all schemas predicting towards a different position of that entity. Then, the self-transition is formalized as follows\n\\[\n\\begin{align}\n  \\Lambda_{i,j}^{t+1}\n  = \\text{AND} \\big(\\lnot \\phi^{1}, \\dots, \\lnot \\phi^{n}, \\alpha_{i,j}^{t} \\big).\n\\end{align}\n\\]\nFinally, the transition function in this model can be factorized as\n\\[\n\\begin{align}\n  T\\left(s^{(t+1)} | s^{(t)}, a^{(t)}\\right) = \\prod_{i=1}^N \\prod_{j=1}^M T_{i, j} \\left(s_{i, j}^{(t+1)}|s^{(t)}, a^{(t)}\\right),\n\\end{align}\n\\]\nwhere \\(T_{i,j}\\) denotes the transition probability of the \\(j^{th}\\) attribute of the \\(i^{th}\\) entity towards its value defined in \\(s_{i,j}^{(t+1)}\\). The entity attribute \\(s_{i,j}^{(t+1)}\\) is by definition activated if one of its grounded schema is active or if a self-transition occured, thus the entity-attribute transition probability is defined as\n\\[\n\\begin{align}\n  T_{i,j} \\left( s_{i,j}^{(t+1)} | s^{(t)}, a^{(t)}\\right) = \\text{OR}\\left( \\phi^{k_1}, \\dots, \\phi^{k_Q}, \\Lambda_{i,j} \\right),\n\\end{align}\n\\]\nwhere \\(\\Lambda_{i,j}\\) denotes the self-transition variable of \\(s_{i,j}\\) and \\(k_1, \\dots, k_Q\\) are the indices of all grounded schemas that predict \\(s_{i,j}\\). Note that although all variables are defined as binary variables, this model could still be used for non-deterministic environments.\nTo increase the generality of their model such that the attribute change of two entities is described by the same schema, Kansky et al. (2017) introduce the term ungrounded schema or template. An ungrounded schema can be understood as template for specific grounded schemas, i.e., it describes a grounded schema where the included entity-attributes are assigned relative to the position of the entity-attribute that should be predicted."
  },
  {
    "objectID": "paper_summaries/schema_networks/index.html#learning-the-model",
    "href": "paper_summaries/schema_networks/index.html#learning-the-model",
    "title": "Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics",
    "section": "Learning the Model",
    "text": "Learning the Model\nThe Schema Network is essentially a factor graph that is aimed to be a probabilistic simulator of the game environment using the aforementioned representation. Assuming that the environment dynamics can be represented by some Schema Network, learning the model comes down to structure learning in graphical models. Kansky et al. (2017) preprocess a sequence of observations (dataset of state-action-reward-new state tuples over time) into a convenient representation, define a NP-hard optimization problem based on this representation as the optimal solution and retrieve the schemas (and Schema Network) through solving the optimization problem approximately using linear programming (LP) relaxations.\n\nNotation\nLet \\(\\alpha\\_{i,j}^{(t)}\\) denote the \\(j^{th}\\) attribute of the \\(i^{th}\\) entity at time \\(t\\) and let \\(\\textbf{e}\\_i^{(t)} \\in \\\\{0, 1\\\\}^{M}\\) be an\n\\(M\\)-dimensional binary vector representing all entity-attributes values of the \\(i^{th}\\) entity at time \\(t\\), i.e., \\(\\textbf{e}\\_i^{(t)} =\n\\begin{bmatrix}\\alpha\\_{i,1}^{(t)} & \\dots & \\alpha\\_{i,M}^{(t)}\n\\end{bmatrix}^{\\text{T}}\\) where \\(M\\) denotes the number of entity-attributes.\nLet \\(\\boldsymbol{\\beta}\\_{i}^{(t)}\\in \\\\{0,1\\\\}^E\\) be a row vector representing the attribute values of the \\(i^{th}\\) entity and the entity-attributes of the \\(R-1\\) (fixed radius) spatial neighbors, i.e., \\(\\boldsymbol{\\beta}_{i}^{(t)} = \\begin{bmatrix} \\textbf{e}\\_{i}^{(t)} &\n\\textbf{e}\\_{i+1}^{(t)}\n&\n\\dots\n&\n\\textbf{e}\\_{R-1}^{(t)}\n\\end{bmatrix}\\) has length \\(E=M(R-1) + M = MR\\).\nSuppose there are \\(N\\) entities observed for \\(\\tau\\) timesteps. Then, let \\(\\textbf{X}\\in\\\\{0,1\\\\}^{D\\times E}\\) be a binary matrix where each row consists of a \\(\\boldsymbol{\\beta}\\_{i}^{(t)}\\) and there are \\(D=N\\cdot \\tau\\) rows (all entities and time steps). Similarly, let \\(\\textbf{y}\\in\\\\{0, 1\\\\}^D\\) be a binary vector where each entry refers to the future attribute value \\(\\alpha\\_{i,j}^{(t+1)}\\) corresonding to the a row of \\(\\textbf{X}\\) with entity \\(i\\) and time \\(t\\), i.e.,\n\\[\n\\begin{align*}\n  \\textbf{X} &=\n  \\begin{bmatrix}\n    \\begin{bmatrix} \\boldsymbol{\\beta}_{1}^{(1)} & \\dots & \\boldsymbol{\\beta}_{N}^{(1)} \\end{bmatrix}^{\\text{T}} \\\\\n    \\vdots\\\\\n    \\begin{bmatrix} \\boldsymbol{\\beta}_{1}^{(\\tau)} & \\dots & \\boldsymbol{\\beta}_{N}^{(\\tau)} \\end{bmatrix}^{\\text{T}}\n  \\end{bmatrix} , \\quad \\textbf{y} =\n   \\begin{bmatrix}\n\\begin{bmatrix} \\alpha_{1,j}^{(2)} & \\dots & \\alpha_{N,j}^{(2)} \\end{bmatrix}^{\\text{T}}\n\\\\ \\vdots \\\\\n\\begin{bmatrix} \\alpha_{1,j}^{(\\tau+1)} & \\dots & \\alpha_{N,j}^{(\\tau+1)} \\end{bmatrix}^{\\text{T}}\n    \\end{bmatrix}\n\\end{align*}\n\\]\n\n\nLearning Problem\nThe goal is to predict \\(\\alpha\\_{i,j}^{(t+1)}\\) based on the entity-attributes of itself and its spatial neighbors. Using the introduced notation, the learning problem can be defined as follows\n\\[\n  \\textbf{y} = f_{\\textbf{W}} (\\textbf{X}) = \\overline{\\overline{\\textbf{X}} \\textbf{W}} \\textbf{1},\n\\]\nwhere \\(f\\_{\\textbf{W}}\\) denotes the desired function of ungrounded schemas which is applied row-wise to the argument \\(\\textbf{X}\\) to produce either active or inactive grounded schemas. \\(f\\_{\\textbf{W}}\\) is parametrized by a binary matrix \\(\\textbf{W} \\in \\\\{0, 1\\\\}^{E \\times\nL}\\) with each column representing one ungrounded schema for a maximum of \\(L\\) schemas. Each element that is set to 1 in a column of \\(\\textbf{W}\\) indicates that for this schema the corresponding input attribute (from \\(\\textbf{X}\\)) is necessary for an activated grounded schema. On the right-hand side of the equation above all variables and operations follow Boolean logic: addition corresponds to ORing and overlining to negation.\n\nE.g., let \\(\\textbf{w}\\_i = \\begin{bmatrix} w\\_{i_1} & \\dots & w\\_{i_ E}\\end{bmatrix}^{\\text{T}}\\) denote the \\(i^{th}\\) column of \\(\\hspace{0.1cm}\\textbf{W}\\) and \\(\\textbf{x}_r = \\begin{bmatrix} x\\_{r_1} & \\dots & x\\_{r_E} \\end{bmatrix}\\) be the \\(r^{th}\\) row of \\(\\hspace{0.1cm}\\textbf{X}\\). Then, the (dot) product of the two vectors leads to\n\\[\n\\begin{align}\n\\textbf{x}_r \\textbf{w}_i = \\sum_{k=1}^{E} x_{r_k} \\cdot w_{i_k} = \\text{OR} \\left( x_{r_1} w_{i_1}, \\dots, x_{r_E} w_{i_E} \\right),\n\\end{align}\n\\]\nin this form the corresponding grounded schema would be activated as soon as one precondition is satisfied, i.e., as soon as for one \\(w\\_{i_j} = 1\\) the corresponding attribute variable is also \\(x\\_{i_j}=1\\).\n\nA grounded schema \\(\\phi\\) is defined through a logical AND over the necessary attributes (i.e., all preconditions)3 \n\\[\n\\begin{align}\n  \\phi = \\text{AND}\\left( \\{x_{r_j} \\mid \\forall j: w_{i_j} = 1 \\}  \\right) = \\text{Not} \\left(\n  \\text{OR} \\left(\\text{Not } \\{x_{r_j} \\mid \\forall j: w_{i_j} = 1 \\} \\right) \\right)\n  = \\overline{\\overline{\\textbf{x}}_r \\textbf{w}_i}.\n\\end{align}\n\\]\nThis equation states how one individual schema (\\(\\textbf{w}_{i}\\)) is applied to one attribute vector \\(\\textbf{x}_R\\). The first equation of this section summarizes this result into a matrix-matrix multiplication.\nAt the end all outputs of each individual schema are ORed to produce the final prediction for each attribute (corresponding to the provided attribute vector). This is done through multiplication with the identity tensor \\(\\textbf{1} \\in \\\\{1\\\\}^{L \\times D}\\). Remind that this is in alignment with the entity-attribute transition probability definition for non-positional attributes\n\\[\n\\begin{align}\n  T_{i,j} \\left( s_{i,j}^{(t+1)} | s^{(t)}, a^{(t)}\\right) = \\text{OR}\\left( \\phi^{k_1}, \\dots, \\phi^{k_Q}\\right)\n\\end{align}\n\\]\nAs stated above, for positional attributes a self-transition \\(\\Lambda_{i,j}\\) is added to allow these attributes to remain active when no change is predicted. Unfortunately, Kansky et al. (2017) did not elaborate on self-transitions in the learning problem. Thus, we can only guess how they are included. My idea would be to preprocess the data such that only positional attributes that changed (either from 0 to 1 or vice versa) are included in the learning problem. In the prediction phase, we then simply group all positional grounded schemas and apply the self-transition as a post-processing step.\n\n\nObjective Function\nAs there might be multiple Schemas that explain certain behaviors, the objective function is aimed to minimize the prediction error while keeping ungrounded schemas as simple as possible4:\n\\[\n\\begin{align}\n  \\min_{\\textbf{W}} J(\\textbf{W}) = \\min_{\\textbf{W}} \\underbrace{\\frac {1}{D} \\cdot \\Bigg| \\textbf{y} -\n  f_{\\textbf{W}} (\\textbf{X}) \\Bigg|_1 }_{\\text{Prediction Error}}+\n  \\underbrace{C \\cdot \\Bigg| \\textbf{W} \\Bigg|_1}_{\\text{Model Complexity}},\n\\end{align}\n\\]\nwhere \\(C\\) is a hyperparameter that can be used to control the trade-off between the complexity of the model and the accuracy of the predictions. This is a NP-hard optimization problem, since \\(\\textbf{W}\\in \\\\{0,1\\\\}^{E\\times L}\\) is a binary matrix. Furthermore, the search space is combinatorially large, i.e., there are \\(2^{E \\cdot L}\\) possible realizations of \\(\\textbf{W}\\). Hence, finding the optimal solution \\(\\textbf{W}^{*}\\) is infeasible (for larger environments such as Breakout).\n\n\nSchema Learning\nKansky et al. (2017) search for an approximate solution with the desired features (low prediction error and low model complexity) using a greedy algorithm of linear programming (LP) relaxations. This algorithm works as follows\n\nStart with an empty set of schemas, i.e., \\(\\textbf{W} = \\textbf{0}\\).\nGreedily select a schema \\(\\textbf{w}\\) that perfectly predicts a cluster of input samples:\n\nRandomly select an input sample \\(\\textbf{x}\\_n\\) for which \\(y_n\n= 1\\) and \\(f\\_{\\textbf{W}} (\\textbf{x}\\_n) = 0\\).\nPut sample in the set solved, then solve the following LP\n\\[\n\\begin{align}\n   \\begin{split}\n   &\\max_{\\textbf{w}\\in \\{0,1\\}^D} \\sum_{n: y_n = 1} \\overline{ \\overline{\\textbf{x}}_n \\textbf{w}} =\n   \\min_{\\textbf{w} \\in \\{0, 1\\}^{D}} \\sum (1 - {\\textbf{x}}_n \\textbf{w}) \\\\\n   &\\quad \\quad \\text{s.t. } \\forall_{n:y_n=0} \\quad (1 - \\textbf{x}_n) \\textbf{w} &gt; 1 \\qquad \\text{(no false alarms)}\\\\\n   &\\qquad \\quad \\hspace{0.3cm} \\forall_{n\\in \\text{solved}} \\hspace{0.2cm} (1-\\textbf{x}_n) \\textbf{w} = 0  \\qquad \\text{(active grounded schema)}\n   \\end{split}\n\\end{align}\n\\]\nUpdate the solved set, i.e., put all samples in for which \\((1-\\textbf{x}_n) \\textbf{w} = 0\\) in the solved set.\n\nSimplify resulting schema by making \\(\\textbf{w}\\) as sparse as possible while keeping the predictions correct without introducing false alarms:\n\\[\\begin{align}\n   \\begin{split}\n     &\\min_{\\textbf{w} \\in \\{0, 1\\}^D} \\textbf{w}^{\\text{T}} \\textbf{1} \\\\\n     &\\quad \\quad \\text{s.t. } \\forall_{n:y_n=0} \\quad (1 - \\textbf{x}_n) \\textbf{w} &gt; 1\\\\\n     &\\qquad \\quad \\hspace{0.3cm} \\forall_{n\\in \\text{solved}} \\hspace{0.2cm} (1-\\textbf{x}_n) \\textbf{w} = 0\n   \\end{split}\n\\end{align}\\]\nBinarize \\(\\textbf{w}\\)"
  },
  {
    "objectID": "paper_summaries/schema_networks/index.html#planning",
    "href": "paper_summaries/schema_networks/index.html#planning",
    "title": "Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics",
    "section": "Planning",
    "text": "Planning\nTODO: Summarize planning"
  },
  {
    "objectID": "paper_summaries/schema_networks/index.html#drawbacks",
    "href": "paper_summaries/schema_networks/index.html#drawbacks",
    "title": "Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics",
    "section": "Drawbacks",
    "text": "Drawbacks\n\nbased on specific and exact representation \\(\\Rightarrow\\) not end-to-end\nblow up of entity attributes, since:\n\nbinarized attributes using discretization and one-hot encoding\nall entities share the same set of attributes\n\nimage parser needs to know the whole attribute space beforehand\nNote: also the reward space needs to be defined beforehand.\nlearning algorithm is only capable of learning deterministic environments"
  },
  {
    "objectID": "paper_summaries/schema_networks/index.html#footnotes",
    "href": "paper_summaries/schema_networks/index.html#footnotes",
    "title": "Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA grounded schema in Schema Networks is similar to a rule in OO-MDP terms. Each attribute may have several grounded schemas. When one of those schemas is active (active effect condition in OO-MDP terms), the corresponding attribute is actived (set to True) in the next step.↩︎\nKansky et al. (2017) refer to the entity attributes in the binary variables \\(v_1, \\dots, v_n\\) of a grounded schema as entity-attribute preconditions. This terminology relates attributes to preconditions, since the actual condition (i.e., grounded schema) is only active iff all preconditions are active.↩︎\nIn Boolean logic, De Morgan’s law states that \\(\\text{AND} \\Big(A, B\\Big) =\\text{NOT} \\Big( \\text{OR} \\big(\\text{NOT } A, \\text{NOT } B \\big)\\Big)\\).↩︎\nOccam’s razor (law of parsimony) states that simplest solution is most likely the right one.↩︎"
  },
  {
    "objectID": "paper_summaries/concrete_distribution/index.html",
    "href": "paper_summaries/concrete_distribution/index.html",
    "title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables",
    "section": "",
    "text": "Maddison et al. (2016) introduce CONtinuous relaxations of disCRETE (concrete) random variables as an approximation to discrete variables. The Concrete distribution is motivated by the fact that backpropagation through discrete random variables is not directly possible. While for continuous random variables, the reparametrization trick is applicable to allow gradients to flow through a sampling operation, this does not work for discrete variables due to the discontinuous operations associated to their sampling. The concrete distribution allows for a simple reparametrization through which gradients can propagate such that a low-variance biased gradient estimator of the discrete path can be obtained."
  },
  {
    "objectID": "paper_summaries/concrete_distribution/index.html#model-description",
    "href": "paper_summaries/concrete_distribution/index.html#model-description",
    "title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables",
    "section": "Model Description",
    "text": "Model Description\nThe Concrete distribution builds upon the (very old) Gumbel-Max trick that allows for a reparametrization of a categorical distribution into a deterministic function over the distribution parameters and an auxiliary noise distribution. The problem within this reparameterization is that it relies on an \\(\\text{argmax}\\)-operation such that backpropagation remains out of reach. Therefore, Maddison et al. (2016) propose to use the \\(\\text{softmax}\\)-operation as a continuous relaxation of the \\(\\text{argmax}\\). This idea has been concurrently developed at the same time by Jang et al. (2016) who called it the Gumbel-Softmax trick.\n\nGumbel-Max Trick\nThe Gumbel-Max trick basically refactors sampling of a deterministic random variable into a component-wise addition of the discrete distribution parameters and an auxiliary noise followed by \\(\\text{argmax}\\), i.e.,\n\\[\n\\begin{align}\n\\text{Sampling }&z \\sim \\text{Cat} \\left(\\alpha_1, \\dots, \\alpha_N \\right)\n\\text{ can equally expressed as } z = \\arg\\max_{k} \\Big(\\log\\alpha_k + G_k\\Big)\\\\\n&\\text{with } G_k \\sim \\text{Gumbel Distribution}\\left(\\mu=0, \\beta=1 \\right)\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\nComputational Graph Gumbel-Max Trick. Taken from Maddison et al. (2016).\n\n\n\nDerivation: Let’s take a closer look on how and why that works. Firstly, we show that samples from \\(\\text{Cat} \\left(\\alpha_1, \\dots, \\alpha_N \\right)\\) are equally distributed to\n\\[\nz = \\arg \\min_{k} \\frac {\\epsilon_k}{\\alpha_k} \\quad \\text{with} \\quad\n\\epsilon_k \\sim \\text{Exp}\\left( 1 \\right)\n\\]\nTherefore, we observe that each term inside the \\(\\text{argmin}\\) is independent exponentially distributed with (easy proof)\n\\[\n\\frac {\\epsilon_k}{\\alpha_k} \\sim  \\text{Exp} \\Big( \\alpha_k \\Big)\n\\]\nThe next step is to show that the index of the variable which achieves the minimum is distributed according to the categorical distribution (easy proof)\n\\[\n\\arg \\min_{k} \\frac {\\epsilon_k}{\\alpha_k} = P \\left( k | z_k = \\min \\{ z_1, \\dots, z_N \\} \\right) = \\frac\n{\\alpha_k}{\\sum_{i=1}^N \\alpha_i}\n\\]\nA nice feature of this formulation is that the categorical distribution parameters \\(\\{\\alpha_i\\}_{i=1}^N\\) do not need to be normalized before reparameterization as normalization is ensured by the factorization itself. Lastly, we simply reformulate this mapping by applying the log and multiplying by minus 1\n\\[\nz = \\arg \\min_{k} \\frac {\\epsilon_k}{\\alpha_k} =\\arg \\min_k \\Big(\\log \\epsilon_k -\n\\log \\alpha_k \\Big) = \\arg \\max_k \\Big(\\log \\alpha_k  - \\log \\epsilon_k\\Big)\n\\]\nThis looks already very close to the Gumbel-Max trick defined above. Remind that to generate exponential distributed random variables, we can simply transform uniformly distributed samples of the unit interval as follows\n\\[\n\\epsilon_k = -\\log u_k \\quad \\text{with} \\quad u_k \\sim\n\\text{Uniform Distribution} \\Big(0, 1\\Big)\n\\]\nThus, we get that\n\\[\n- \\log \\epsilon_k = - \\log \\Big( - \\log u_k \\Big) = G_k \\sim\n\\text{Gumbel Distribution} \\Big( \\mu=0, \\beta=1 \\Big)\n\\]\n\n\nGumbel-Softmax Trick\nThe problem in the Gumbel-Max trick is the \\(\\text{argmax}\\)-operation as the derivative of \\(\\text{argmax}\\) is 0 everywhere except at the boundary of state changes, where it is undefined. Thus, Maddison et al. (2016) use the temperature-valued \\(\\text{Softmax}\\) as a continuous relaxation of the \\(\\text{argmax}\\) computation such that\n\\[\n\\begin{align}\n\\text{Sampling }&z \\sim \\text{Cat} \\left(\\alpha_1, \\dots, \\alpha_N \\right)\n\\text{ is relaxed cont. to } z_k = \\frac {\\exp \\left( \\frac {\\log \\alpha_k + G_k}{\\lambda} \\right)}\n{\\sum_{i=1}^N \\exp \\left( \\frac {\\log \\alpha_i + G_i}{\\lambda} \\right)}\\\\\n&\\text{with } G_k \\sim \\text{Gumbel Distribution}\\left(\\mu=0, \\beta=1 \\right)\n\\end{align}\n\\]\nwhere \\(\\lambda \\in [0, \\infty[\\) is the temperature and \\(\\alpha_k \\in [0,\n\\infty[\\) are the categorical distribution parameters. The temperature can be understood as a hyperparameter that controls the sharpness of the \\(\\text{softmax}\\), i.e., how much the winner-takes-all dynamics of the softmax is taken:\n\n\\(\\lambda \\rightarrow 0\\): \\(\\text{softmax}\\) smoothly approaches discrete \\(\\text{argmax}\\) computation\n\\(\\lambda \\rightarrow \\infty\\): \\(\\text{softmax}\\) leads to uniform distribution.\n\nNote that the samples \\(z_k\\) obtained by this reparameterization follow a new family of distributions, the Concrete distribution. Thus, \\(z_k\\) are called Concrete random variables.\n\n\n\n\n\n\n\n\nComputational Graph Gumbel-Softmax Trick. Taken from Maddison et al. (2016).\n\n\n\nIntuition: To better understand the relationship between the Concrete distribution and the discrete categorical distribution, let’s look on an exemplary result. Remind that the \\(\\text{argmax}\\) operation for a \\(n\\)-dimensional categorical distribution returns states on the vertices of the simplex\n\\[\n\\boldsymbol{\\Delta}^{n-1} = \\left\\{ \\textbf{x}\\in \\{0, 1\\}^n \\mid \\sum_{k=1}^n x_k = 1 \\right\\}\n\\]\nConcrete random variables are relaxed to return states in the interior of the simplex\n\\[\n\\widetilde{\\boldsymbol{\\Delta}}^{n-1} = \\left\\{ \\textbf{x}\\in [0, 1]^n \\mid \\sum_{k=1}^n x_k = 1 \\right\\}\n\\]\nThe image below shows how the distribution of concrete random variables changes for an exemplary discrete categorical distribution \\((\\alpha_1, \\alpha_2,\n\\alpha_3) = (2, 0.5, 1)\\) and different temperatures \\(\\lambda\\).\n\n\n\n\n\n\n\n\nRelationship between Concrete and Discrete Variables: A discrete distribution with unnormalized probabilities \\((\\alpha_1, \\alpha_2, \\alpha_3) = (2, 0.5, 1)\\) and three corresponding Concrete densities at increasing temperatures \\(\\lambda\\). Taken from Maddison et al. (2016).\n\n\n\n\n\nConcrete Distribution\nWhile the Gumbel-Softmax trick defines how to obtain samples from a Concrete distribution, Maddison et al. (2016) provide a definition of its density and prove some nice properties:\nDefinition: The Concrete distribution \\(\\text{X} \\sim\n\\text{Concrete}(\\boldsymbol{\\alpha}, \\lambda)\\) with temperature \\(\\lambda \\in\n[0, \\infty[\\) and location \\(\\boldsymbol{\\alpha} =\n\\begin{bmatrix} \\alpha_1 & \\dots & \\alpha_n \\end{bmatrix} \\in [0, \\infty]^{n}\\) has a density\n\\[\n  p_{\\boldsymbol{\\alpha}, \\lambda} (\\textbf{x}) = (n-1)! \\lambda^{n-1} \\prod_{k=1}^n\n  \\left( \\frac {\\alpha_k x_k^{-\\lambda - 1}} {\\sum_{i=1}^n \\alpha_i x_i^{-\\lambda}} \\right)\n\\]\nNice Properties and their Implications:\n\nReparametrization: Instead of sampling directly from the Concrete distribution, one can obtain samples by the following deterministic (\\(d\\)) reparametrization\n\\[\nX_k \\stackrel{d}{=} \\frac {\\exp \\left( \\frac {\\log \\alpha_k + G_k}{\\lambda} \\right)}\n{\\sum_{i=1}^N \\exp \\left( \\frac {\\log \\alpha_i + G_i}{\\lambda} \\right)} \\quad\n\\text{with} \\quad G_k \\sim \\text{Gumbel}(0, 1)\n\\]\nThis property ensures that we can easily compute unbiased low-variance gradients w.r.t. the location parameters \\(\\boldsymbol{\\alpha}\\) of the Concrete distribution.\nRounding: Rounding a Concrete random variable results in the discrete random variable whose distribution is described by the logits \\(\\log \\alpha_k\\)\n\\[\nP (\\text{X}_k &gt; \\text{X}_i \\text{ for } i\\neq k) = \\frac {\\alpha_k}{\\sum_{i=1}^n \\alpha_i}\n\\]\nThis property again indicates the close relationship between concrete and discrete distributions.\nConvex eventually:\n\\[\n\\text{If } \\lambda \\le \\frac {1}{n-1}, \\text{ then } p_{\\boldsymbol{\\alpha},\n\\lambda} \\text{ is log-convex in } x\n\\]\nThis property basically tells us if \\(\\lambda\\) is small enough, there are no modes in the interior of the probability simplex.\n\n\n\nDiscrete-Latent VAE\nOne use-case of the Concrete distribution and its reparameterization is the training of an variational autoencoder (VAE) with a discrete latent space. The main idea is to use the Concrete distribution during training and use discrete sampled latent variables at test-time. An obvious limitation of this approach is that during training non-discrete samples are returned such that our model needs to be able to handle continuous variables1. Let’s dive into the discrete-latent VAE described by Maddison et al. (2016).\nWe assume that we have a dataset \\(\\textbf{X} =\n\\{\\textbf{x}^{(i)}\\}_{i=1}^N\\) of \\(N\\) i.i.d. samples \\(\\textbf{x}^{(i)}\\) which were generated by the following process:\n\nWe sample a one-hot latent vector \\(\\textbf{d}\\in\\{0, 1\\}^{K}\\) from a categorical prior distribution \\(P_{\\boldsymbol{a}} (\\textbf{d})\\).\nWe use our sample \\(\\textbf{d}^{(i)}\\) and put it into the scene model \\(p_{\\boldsymbol{\\theta}}(\\textbf{x}|\\textbf{d})\\) from which we sample to generate the observed image \\(\\textbf{x}^{(i)}\\).\n\nAs a result, the marginal likelihood of an image can be stated as follows\n\\[\np_{\\boldsymbol{\\theta}, \\boldsymbol{a}} (\\textbf{x}) = \\mathbb{E}_{\\textbf{d}\n\\sim P_{\\boldsymbol{a}}(\\textbf{d})} \\Big[ p_{\\boldsymbol{\\theta}} (\\textbf{x} |\n\\textbf{d}) \\Big] = \\sum P_{\\boldsymbol{a}} \\left(\\textbf{d}^{(i)} \\right) p_{\\boldsymbol{\\theta}} \\left( \\textbf{x} |\n\\textbf{d}^{(i)} \\right),\n\\]\nwhere the sum is over all possible \\(K\\) dimensional one-hot vectors. In order to recover this generative process, we introduce a variational approximation \\(Q_{\\boldsymbol{\\phi}} (\\textbf{d}|\\textbf{x})\\) of the true, but unknown posterior. Now we exchange the sampling distribution towards this approximation\n\\[\np_{\\boldsymbol{\\theta}, \\boldsymbol{a}} (\\textbf{x}) = \\sum\n\\frac {p_{\\boldsymbol{\\theta}, \\boldsymbol{a}} \\left(\\textbf{x}, \\textbf{d}^{(i)}\n\\right)}{Q_{\\boldsymbol{\\phi}} \\left(\\textbf{d}^{(i)} | \\textbf{x}\\right)}\nQ_{\\boldsymbol{\\phi}} \\left(\\textbf{d}^{(i)} | \\textbf{x}\\right) =\n\\mathbb{E}_{\\textbf{d} \\sim  Q_{\\boldsymbol{\\phi}} \\left(\\textbf{d} |\n\\textbf{x}\\right)} \\left[ \\frac {p_{\\boldsymbol{\\theta}, \\boldsymbol{a}} \\left(\\textbf{x}, \\textbf{d}\n\\right)}{Q_{\\boldsymbol{\\phi}} \\left(\\textbf{d} | \\textbf{x}\\right)} \\right]\n\\]\nLastly, applying Jensen’s inequality on the log-likelihood leads to the evidence lower bound (ELBO) objective of VAEs\n\\[\n\\log p_{\\boldsymbol{\\theta}, \\boldsymbol{a}} (\\textbf{x}) =\n\\log \\left(\n\\mathbb{E}_{\\textbf{d} \\sim  Q_{\\boldsymbol{\\phi}} \\left(\\textbf{d} |\n\\textbf{x}\\right)} \\left[ \\frac {p_{\\boldsymbol{\\theta}, \\boldsymbol{a}} \\left(\\textbf{x}, \\textbf{d}\n\\right)}{Q_{\\boldsymbol{\\phi}} \\left(\\textbf{d} | \\textbf{x}\\right)} \\right]\\right)\n\\ge\n\\mathbb{E}_{\\textbf{d} \\sim  Q_{\\boldsymbol{\\phi}} \\left(\\textbf{d} |\n\\textbf{x}\\right)} \\left[ \\log  \\frac {p_{\\boldsymbol{\\theta}, \\boldsymbol{a}} \\left(\\textbf{x}, \\textbf{d}\n\\right)}{Q_{\\boldsymbol{\\phi}} \\left(\\textbf{d} | \\textbf{x}\\right)} \\right]\n= \\mathcal{L}^{\\text{ELBO}}\n\\]\nWhile we are able to compute this objective, we cannot simply optimize it using standard automatic differentiation (AD) due to the discrete sampling operations. The concrete distribution comes to rescue: Maddison et al. (2016) propose to relax the terms \\(P_{\\boldsymbol{a}}(\\textbf{d})\\) and \\(Q_{\\boldsymbol{\\phi}}(\\textbf{d}|\\textbf{x})\\) using concrete distributions instead, leading to the relaxed objective\n\\[\n\\mathcal{L}^{\\text{ELBO}}=\n\\mathbb{E}_{\\textbf{d} \\sim  Q_{\\boldsymbol{\\phi}} \\left(\\textbf{d} |\n\\textbf{x}\\right)} \\left[ \\log  \\frac {p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}| \\textbf{d}\n\\right) P_{\\boldsymbol{a}} (\\textbf{d}) }{Q_{\\boldsymbol{\\phi}} \\left(\\textbf{d} | \\textbf{x}\\right)} \\right]\n\\stackrel{\\text{relax}}{\\rightarrow}\n\\mathbb{E}_{\\textbf{z} \\sim  q_{\\boldsymbol{\\phi}, \\lambda_1} \\left(\\textbf{z} |\n\\textbf{x}\\right)} \\left[ \\log  \\frac {p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}| \\textbf{z}\n\\right) p_{\\boldsymbol{a}, \\lambda_2} (\\textbf{z}) }{q_{\\boldsymbol{\\phi}, \\lambda_1} \\left(\\textbf{z} | \\textbf{x}\\right)} \\right]\n\\]\nThen, during training we optimize the relaxed objective while during test time we evaluate the original objective including discrete sampling operations. The really neat thing here is that switching between the two modes works out of the box: we only need to switch between the \\(\\text{softmax}\\) and \\(\\text{argmax}\\) operations.\n\n\n\n\n\n\n\n\n\n\n\nDiscrete-Latent VAE Architecture\n\n\n\nThings to beware of: Maddison et al. (2016) noted that naively implementing [the relaxed objective] will result in numerical issues. Therefore, they give some implementation hints in Appendix C:\n\nLog-Probabilties of Concrete Variables can suffer from underflow: Let’s investigate why this might happen. The log-likelihood of a concrete variable \\(\\textbf{z}\\) is given by\n\\[\n\\begin{align}\n  \\log p_{\\boldsymbol{\\alpha}, \\lambda} (\\textbf{z}) =& \\log \\Big((K-1)! \\Big) + (K-1) \\log \\lambda\n  + \\left(\\sum_{i=1}^K  \\log \\alpha_i + (-\\lambda - 1) \\log  z_i \\right) \\\\\n   &- K \\log \\left(\\sum_{i=1}^K \\exp\\left( \\log \\alpha_i - \\lambda \\log z_i\\right)\\right)\n\\end{align}\n\\]\nNow let’s remind that concrete variables are pushing towards one-hot vectors (when \\(\\lambda\\) is set accordingly), i.e., due to rounding/underflow we might get some \\(z_i=0\\). This is problematic, since the \\(\\log\\) is not defined in this case.\nTo circumvent this, Maddison et al. (2016) propose to work with Concrete random variables in log-space, i.e., to use the following reparameterization\n\\[\ny_i = \\frac {\\log \\alpha_i + G_i}{\\lambda} - \\log \\left( \\sum_{i=1}^K \\exp\n\\left( \\frac {\\log \\alpha_i + G_i}{\\lambda} \\right) \\right)\n\\quad G_i \\sim \\text{Gumbel}(0, 1)\n\\]\nThe resulting random variable \\(\\textbf{y}\\in\\mathbb{R}^K\\) has the property that \\(\\exp(Y) \\sim \\text{Concrete}\\left(\\boldsymbol{\\alpha}, \\lambda \\right)\\), therefore they denote \\(Y\\) as an \\(\\text{ExpConcrete}\\left(\\boldsymbol{\\alpha},\n\\lambda\\right)\\). Accordingly, the log-likelihood \\(\\log\n\\kappa_{\\boldsymbol{\\alpha}, \\lambda}\\) of a variable ExpConcrete variable \\(\\textbf{y}\\) is given by\n\\[\n\\begin{align}\n  \\log p_{\\boldsymbol{\\alpha}, \\lambda} (\\textbf{y}) =& \\log \\Big((K-1)! \\Big) + (K-1) \\log \\lambda\n  + \\left(\\sum_{i=1}^K  \\log \\alpha_i + (\\lambda - 1) y_i \\right) \\\\\n   &- n \\log \\left(\\sum_{i=1}^n \\exp\\left( \\log \\alpha_i - \\lambda y_i\\right)\\right)\n\\end{align}\n\\]\nThis reparameterization does not change our approach due to the fact that the KL terms of a variational loss are invariant under invertible transformations, i.e., since \\(\\exp\\) is invertible, the KL divergence between two \\(\\text{ExpConcrete}\\) is the same the KL divergence between two \\(\\text{Concrete}\\) distributions.\nWorking with \\(\\text{ExpConcrete}\\) random variables: Remind the relaxed objective\n\\[\n\\mathcal{L}^{\\text{ELBO}}_{rel} =\n\\mathbb{E}_{\\textbf{z} \\sim  q_{\\boldsymbol{\\phi}, \\lambda_1} \\left(\\textbf{z} |\n\\textbf{x}\\right)} \\left[\n\\log p_{\\boldsymbol{\\theta}} \\left( \\textbf{x} | \\textbf{z}\\right) + \\log\n\\frac{\np_{\\boldsymbol{a}, \\lambda_2} (\\textbf{z})\n} {q_{\\boldsymbol{\\phi},\n\\lambda_1} \\left(\\textbf{z} | \\textbf{x}\\right)}\n\\right]\n\\]\nNow let’s exchange the \\(\\text{Concrete}\\) by \\(\\text{ExpConcrete}\\) distributions\n\\[\n\\mathcal{L}^{\\text{ELBO}}_{rel} =\n\\mathbb{E}_{\\textbf{y} \\sim  \\kappa_{\\boldsymbol{\\phi}, \\lambda_1} \\left(\\textbf{z} |\n\\textbf{x}\\right)} \\left[\n\\log p_{\\boldsymbol{\\theta}} \\left( \\textbf{x} | \\exp(\\textbf{y})\\right) + \\log\n\\frac{\n\\rho_{\\boldsymbol{a}, \\lambda_2} (\\textbf{y})\n} {\\kappa_{\\boldsymbol{\\phi},\n\\lambda_1} \\left(\\textbf{z} | \\textbf{y}\\right)}\n\\right],\n\\]\nwhere \\(\\rho_{\\boldsymbol{a}, \\lambda_2} (\\textbf{y})\\) is the density of an \\(\\text{ExpConcrete}\\) corresponding to the \\(\\text{Concrete}\\) distribution \\(p_{\\boldsymbol{a}, \\lambda_2} (\\textbf{z})\\). Thus, during the implementation we will simply use \\(\\text{ExpConcrete}\\) random variables \\(\\textbf{y}\\) as random variables and then perform an \\(\\exp\\) computation before putting them through the decoder.\nChoosing the temperature \\(\\lambda\\): Maddison et al. (2016) note that the success of the training heavily depends on the choice of temperature. It is rather intuitive that the relaxed nodes should not be able to represent precise real valued mode in the interior of the probability simplex, since otherwise the model is designed to fail. In other words, the only modes of the concrete distributions should be at the vertices of the probability simplex. Fortunately, Maddison et al. (2016) proved that\n\\[\n\\text{If } \\lambda \\le \\frac {1}{n-1}, \\text{ then } p_{\\boldsymbol{\\alpha},\n\\lambda} \\text{ is log-convex in } x\n\\]\nIn other words, if we keep \\(\\lambda \\le \\frac {1}{n-1}\\), there are no modes in the interior. However, Maddison et al. (2016) note that in practice, this upper-bound on \\(\\lambda\\) might be too tight, e.g., they found for \\(n=4\\) that \\(\\lambda=1\\) was the best temperature and in \\(n=8\\), \\(\\lambda=\\frac {2}{3}\\). As a result, they recommend to rather explore \\(\\lambda\\) as tuneable hyperparameters.\nLast note about the temperature \\(\\lambda\\): They found that choosing different temperatures \\(\\lambda_1\\) and \\(\\lambda_2\\) for the posterior \\(\\kappa_{\\boldsymbol{\\alpha}, \\lambda_1}\\) and prior \\(\\rho_{\\boldsymbol{a},\n\\lambda_2}\\) could dramatically improve the results."
  },
  {
    "objectID": "paper_summaries/concrete_distribution/index.html#implementation",
    "href": "paper_summaries/concrete_distribution/index.html#implementation",
    "title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables",
    "section": "Implementation",
    "text": "Implementation\nLet’s showcase how the discrete-latent VAE performs in comparison to the standard VAE (with Gaussian latents). For the sake of simplicity, I am going to create a very (VERY) simple dataset that should mimick the generative process we assume in the discrete-latent VAE, i.e., there are \\(K\\) one-hot vectors \\(\\textbf{d}\\) and a Gaussian distribution \\(p_{\\boldsymbol{\\theta}}\n(\\textbf{x} | \\textbf{d})\\).\n\nData Generation\nThe dataset is made of three (\\(K=3\\)) distinct shapes each is assigned a distinct color such that in fact there are only three images in the dataset \\(\\textbf{X}=\\{\\textbf{x}_i\\}_{i=1}^3\\). Therefore, the Gaussian distribution \\(p_{\\boldsymbol{\\theta}} (\\textbf{x} | \\textbf{d})\\) has an infinitely small variance. To allow for minibatches during training and to make the epochs larger than one iteration, we upsample the three images by repeating each image \\(1000\\) times in the dataset:\n\n\nCode\nimport numpy as np\nimport torch\nimport torchvision.transforms as transforms\nfrom PIL import Image, ImageDraw\nfrom torch.utils.data import TensorDataset\nfrom sklearn.preprocessing import OneHotEncoder\n\n\ndef generate_img(shape, color, img_size):\n    \"\"\"Generate an RGB image from the provided latent factors\n\n    Args:\n        shape (string): can only be 'circle', 'square', 'triangle'\n        color (string): color name or rgb string\n        img_size (int): describing the image size (img_size, img_size)\n        size (int): size of shape\n\n    Returns:\n        torch tensor [3, img_size, img_size]\n    \"\"\"\n    # blank image\n    img = Image.new('RGB', (img_size, img_size), color='black')\n    # center coordinates\n    center = img_size//2\n    # define coordinates\n    x_0, y_0 = center - size//2, center - size//2\n    x_1, y_1 = center + size//2, center + size//2\n    # draw shapes\n    img1 = ImageDraw.Draw(img)\n    if shape == 'square':\n        img1.rectangle([(x_0, y_0), (x_1, y_1)], fill=color)\n    elif shape == 'circle':\n        img1.ellipse([(x_0, y_0), (x_1, y_1)], fill=color)\n    elif shape == 'triangle':\n        y_0, y_1 = center + size//3,  center - size//3\n        img1.polygon([(x_0, y_0), (x_1, y_0), (center, y_1)], fill=color)\n    return transforms.ToTensor()(img)\n\n\ndef generate_dataset(n_samples_per_class, colors, shapes, sizes, img_size):\n    data, labels = [], []\n    for (n_samples, color, shape, size) in zip(n_samples_per_class,colors,shapes,sizes):\n        img = generate_img(shape, color, img_size, size)\n\n        data.append(img.unsqueeze(0).repeat(n_samples, 1, 1, 1))\n        labels.extend(n_samples*[shape])\n    # cast data to tensor [sum(n_samples_per_class), 3, img_size, img_size]\n    data = torch.vstack(data).type(torch.float32)\n    # create one-hot encoded labels\n    labels = OneHotEncoder().fit_transform(np.array(labels).reshape(-1, 1)).toarray()\n    # make tensor dataset\n    dataset = TensorDataset(data, torch.from_numpy(labels))\n    return dataset\n\nIMG_SIZE = 32\nN_SAMPLES_PER_CLASS = [1000, 1000, 1000]\nSHAPES = ['square', 'circle', 'triangle']\nCOLORS = ['red', 'green', 'blue']\nSIZES = [12, 14, 20]\ndataset = generate_dataset(N_SAMPLES_PER_CLASS,COLORS, SHAPES, SIZES, IMG_SIZE)\n\n\n\n\n\n\n\n\n\n\nDataset\n\n\n\n\n\nModel Implementation\n\nStandard VAE\n\n\n\nCode\nimport torch.nn as nn\nimport torch.distributions as dists\n\nHIDDEN_DIM = 200\nLATENT_DIM = 3\nFIXED_VAR = 0.1**2\n\n\nclass VAE(nn.Module):\n\n    def __init__(self):\n        super(VAE, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear((IMG_SIZE**2)*3, HIDDEN_DIM),\n            nn.ReLU(),\n            nn.Linear(HIDDEN_DIM, 2*LATENT_DIM)\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(LATENT_DIM, HIDDEN_DIM),\n            nn.ReLU(),\n            nn.Linear(HIDDEN_DIM, (IMG_SIZE**2)*3),\n        )\n        return\n\n    def compute_loss(self, x):\n        [x_tilde, z, mu_z, log_var_z] = self.forward(x)\n        # compute negative log-likelihood\n        NLL = -dists.Normal(x_tilde, FIXED_VAR).log_prob(x).sum(axis=(1, 2, 3)).mean()\n        # copmute kl divergence\n        KL_Div = -0.5*(1 + log_var_z - mu_z.pow(2) - log_var_z.exp()).sum(1).mean()\n        # compute loss\n        loss = NLL + KL_Div\n        return loss, NLL, KL_Div\n\n    def forward(self, x):\n        \"\"\"feed image (x) through VAE\n\n        Args:\n            x (torch tensor): input [batch, img_channels, img_dim, img_dim]\n\n        Returns:\n            x_tilde (torch tensor): [batch, img_channels, img_dim, img_dim]\n            z (torch tensor): latent space samples [batch, LATENT_DIM]\n            mu_z (torch tensor): mean latent space [batch, LATENT_DIM]\n            log_var_z (torch tensor): log var latent space [batch, LATENT_DIM]\n        \"\"\"\n        z, mu_z, log_var_z = self.encode(x)\n        x_tilde = self.decode(z)\n        return [x_tilde, z, mu_z, log_var_z]\n\n    def encode(self, x):\n        \"\"\"computes the approximated posterior distribution parameters and\n        samples from this distribution\n\n        Args:\n            x (torch tensor): input [batch, img_channels, img_dim, img_dim]\n\n        Returns:\n            z (torch tensor): latent space samples [batch, LATENT_DIM]\n            mu_E (torch tensor): mean latent space [batch, LATENT_DIM]\n            log_var_E (torch tensor): log var latent space [batch, LATENT_DIM]\n        \"\"\"\n        # get encoder distribution parameters\n        out_encoder = self.encoder(x)\n        mu_E, log_var_E = torch.chunk(out_encoder, 2, dim=1)\n        # sample noise variable for each batch and sample\n        epsilon = torch.randn_like(log_var_E)\n        # get latent variable by reparametrization trick\n        z = mu_E + torch.exp(0.5*log_var_E) * epsilon\n        return z, mu_E, log_var_E\n\n    def decode(self, z):\n        \"\"\"computes the Gaussian mean of p(x|z)\n\n        Args:\n            z (torch tensor): latent space samples [batch, LATENT_DIM]\n\n        Returns:\n            x_tilde (torch tensor): [batch, img_channels, img_dim, img_dim]\n        \"\"\"\n        # get decoder distribution parameters\n        x_tilde = self.decoder(z).view(-1, 3, IMG_SIZE, IMG_SIZE)\n        return x_tilde\n\n    def create_latent_traversal(self, image_batch, n_pert, pert_min_max=2, n_latents=3):\n        device = image_batch.device\n        # initialize images of latent traversal\n        images = torch.zeros(n_latents, n_pert, *image_batch.shape[1::])\n        # select the latent_dims with lowest variance (most informative)\n        [x_tilde, z, mu_z, log_var_z] = self.forward(image_batch)\n        i_lats = log_var_z.mean(axis=0).sort()[1][:n_latents]\n        # sweep for latent traversal\n        sweep = np.linspace(-pert_min_max, pert_min_max, n_pert)\n        # take first image and encode\n        [z, mu_E, log_var_E] = self.encode(image_batch[0:1])\n        for latent_dim, i_lat in enumerate(i_lats):\n            for pertubation_dim, z_replaced in enumerate(sweep):\n                # copy z and pertubate latent__dim i_lat\n                z_new = z.detach().clone()\n                z_new[0][i_lat] = z_replaced\n\n                img_rec = self.decode(z_new.to(device)).squeeze(0).cpu()\n\n                images[latent_dim][pertubation_dim] = img_rec\n        return images\n\n\n\nDiscrete-Latent VAE:\nLuckily, Pytorch distributions have already implemented the concrete distribution which even takes care of using the \\(\\text{ExpConcrete}\\) for the computation of the log probability, see source code.\nAs suggested by Maddison et al. (2016), we set \\(\\lambda_1 = \\frac{2}{3}\\). Setting \\(\\lambda_2 = 2\\) seemed to improve stability, however I did not take time to really tune these hyperparameters (which is just not necessary due to the simplicity of the task).\n\n\n\nCode\nLAMBDA_1 = torch.tensor([2/3])\nLAMBDA_2 = torch.tensor([2.])\nPRIOR_PROBS = 1/LATENT_DIM*torch.ones(LATENT_DIM)\n\n\nclass DiscreteVAE(nn.Module):\n\n    def __init__(self):\n        super(DiscreteVAE, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear((IMG_SIZE**2)*3, HIDDEN_DIM),\n            nn.ReLU(),\n            nn.Linear(HIDDEN_DIM, LATENT_DIM)\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(LATENT_DIM, HIDDEN_DIM),\n            nn.ReLU(),\n            nn.Linear(HIDDEN_DIM, (IMG_SIZE**2)*3),\n        )\n        self.register_buffer(\"LAMBDA_1\", LAMBDA_1)\n        self.register_buffer(\"LAMBDA_2\", LAMBDA_2)\n        self.register_buffer(\"PRIOR_PROBS\", PRIOR_PROBS)\n        return\n\n    def compute_loss(self, x):\n        [x_tilde, z, latent_dist] = self.forward(x, \"Train\")\n        # compute negative log-likelihood\n        NLL = -dists.Normal(x_tilde, FIXED_VAR).log_prob(x).sum(axis=(1, 2, 3)).mean()\n        # copmute kl divergence\n        PRIOR_DIST = dists.RelaxedOneHotCategorical(self.LAMBDA_2, self.PRIOR_PROBS)\n        KL_Div =  (latent_dist.log_prob(z) - PRIOR_DIST.log_prob(z)).mean()\n        # compute loss\n        loss = NLL + KL_Div\n        return loss, NLL, KL_Div\n\n    def forward(self, x, mode=\"Train\"):\n        latent_dist, z = self.encode(x, mode)\n        x_tilde = self.decode(z)\n        return [x_tilde, z, latent_dist]\n\n    def encode(self, x, mode=\"Train\"):\n        \"\"\"computes the approximated posterior distribution parameters and\n        returns the distribution (torch distribution) and a sample from that\n        distribution\n\n        Args:\n            x (torch tensor): input [batch, img_channels, img_dim, img_dim]\n\n        Returns:\n            dist (torch distribution): latent distribution\n        \"\"\"\n        # get encoder distribution parameters\n        log_alpha = self.encoder(x)\n        probs = log_alpha.exp()\n        if mode == \"Train\":\n            # concrete distribution\n            latent_dist = dists.RelaxedOneHotCategorical(self.LAMBDA_1, probs)\n            z = latent_dist.rsample()\n            return [latent_dist, z]\n        elif mode == \"Test\":\n            # discrete distribution\n            latent_dist = dists.OneHotCategorical(probs)\n            d = latent_dist.sample()\n            return [latent_dist, d]\n\n    def create_latent_traversal(self):\n        \"\"\"in the discrete case there are only LATENT_DIM possible latent states\"\"\"\n        # initialize images of latent traversal\n        images = torch.zeros(LATENT_DIM, 3, IMG_SIZE, IMG_SIZE)\n        latent_samples = torch.zeros(LATENT_DIM, LATENT_DIM)\n        for i_lat in range(LATENT_DIM):\n            d = torch.zeros(1, LATENT_DIM).to(self.LAMBDA_1.device)\n            d[0][i_lat] = 1\n            images[i_lat] = self.decode(d).squeeze(0)\n            latent_samples[i_lat] = d\n        return images, latent_samples\n\n    def decode(self, z):\n        \"\"\"computes the Gaussian mean of p(x|z)\n\n        Args:\n            z (torch tensor): latent space samples [batch, LATENT_DIM]\n\n        Returns:\n            x_tilde (torch tensor): [batch, img_channels, img_dim, img_dim]\n        \"\"\"\n        # get decoder distribution parameters\n        x_tilde = self.decoder(z).view(-1, 3, IMG_SIZE, IMG_SIZE)\n        return x_tilde\n\n\n\nTraining Procedure\n\n\n\nCode\nfrom torch.utils.data import DataLoader\nfrom livelossplot import PlotLosses\n\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-6\n\ndef train(dataset, std_vae, discrete_vae, num_epochs):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    print('Device: {}'.format(device))\n\n    data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True,\n                             num_workers=12)\n    std_vae.to(device)\n    discrete_vae.to(device)\n\n    optimizer_std_vae = torch.optim.Adam(std_vae.parameters(), lr=LEARNING_RATE,\n                                         weight_decay=WEIGHT_DECAY)\n    optimizer_dis_vae = torch.optim.Adam(discrete_vae.parameters(), lr=LEARNING_RATE,\n                                         weight_decay=WEIGHT_DECAY)\n    losses_plot = PlotLosses(groups={'KL Div': ['STD-VAE KL', 'Discrete-VAE KL'],\n                                     'NLL': ['STD-VAE NLL', 'Discrete-VAE NLL']})\n    for epoch in range(1, num_epochs + 1):\n        avg_KL_STD_VAE, avg_NLL_STD_VAE = 0, 0\n        avg_KL_DIS_VAE, avg_NLL_DIS_VAE = 0, 0\n        for (x, label) in data_loader:\n            x = x.to(device)\n            # standard vae update\n            optimizer_std_vae.zero_grad()\n            loss, NLL, KL_Div  = std_vae.compute_loss(x)\n            loss.backward()\n            optimizer_std_vae.step()\n            avg_KL_STD_VAE += KL_Div.item() / len(data_loader)\n            avg_NLL_STD_VAE += NLL.item() / len(data_loader)\n            # discrete vae update\n            optimizer_dis_vae.zero_grad()\n            loss, NLL, KL_Div  = discrete_vae.compute_loss(x)\n            loss.backward()\n            optimizer_dis_vae.step()\n            avg_KL_DIS_VAE += KL_Div.item() / len(data_loader)\n            avg_NLL_DIS_VAE += NLL.item() / len(data_loader)\n\n        # plot current losses\n        losses_plot.update({'STD-VAE KL': avg_KL_STD_VAE, 'STD-VAE NLL': avg_NLL_STD_VAE,\n                            'Discrete-VAE KL': avg_KL_DIS_VAE,\n                            'Discrete-VAE NLL': avg_NLL_DIS_VAE}, current_step=epoch)\n        losses_plot.send()\n    trained_std_vae, trained_discrete_vae = std_vae, discrete_vae\n    return trained_std_vae, trained_discrete_vae\n\n\n\n\nResults\nLet’s train both models for some seconds:\n\n\nCode\nnum_epochs = 15\nstd_vae = VAE()\ndiscrete_vae = DiscreteVAE()\n\ntrained_std_vae, trained_discrete_vae = train(dataset, std_vae, discrete_vae, num_epochs)\n\n\n\n\n\nTraining\n\n\nBoth models seem to be able to create descent reconstructions (really low NLL). From here on out, we will only run the discrete-latent VAE in test-mode, i.e., with a categorical latent distribution.\n\n\nVisualizations\n\nReconstructions: Let’s verify that both models are able to create good reconstructions.\n\n\n\nCode\ndef plot_reconstructions(std_vae, discrete_vae, dataset, SEED=1):\n    np.random.seed(SEED)\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n    std_vae.to(device)\n    discrete_vae.to(device)\n\n    n_samples = 7\n    i_samples = np.random.choice(range(len(dataset)), n_samples, replace=False)\n\n    fig = plt.figure(figsize=(10, 4))\n    plt.suptitle(\"Reconstructions\", fontsize=16, y=1, fontweight='bold')\n    for counter, i_sample in enumerate(i_samples):\n        orig_img = dataset[i_sample][0]\n        # plot original img\n        ax = plt.subplot(3, n_samples, 1 + counter)\n        plt.imshow(transforms.ToPILImage()(orig_img))\n        plt.axis('off')\n        if counter == 0:\n            ax.annotate(\"input\", xy=(-0.1, 0.5), xycoords=\"axes fraction\",\n                        va=\"center\", ha=\"right\", fontsize=12)\n        # plot img reconstruction STD VAE\n        [x_tilde, z, mu_z, log_var_z] = std_vae(orig_img.unsqueeze(0).to(device))\n\n        ax = plt.subplot(3, n_samples, 1 + counter + n_samples)\n        x_tilde = x_tilde[0].detach().cpu()\n        plt.imshow(transforms.ToPILImage()(x_tilde))\n        plt.axis('off')\n        if counter == 0:\n            ax.annotate(\"STD VAE recons\", xy=(-0.1, 0.5), xycoords=\"axes fraction\",\n                        va=\"center\", ha=\"right\", fontsize=12)\n        # plot img reconstruction IWAE\n        [x_tilde, z, dist] = discrete_vae(orig_img.unsqueeze(0).to(device), \"Test\")\n        ax = plt.subplot(3, n_samples, 1 + counter + 2*n_samples)\n        x_tilde = x_tilde[0].detach().cpu()\n        plt.imshow(transforms.ToPILImage()(x_tilde))\n        plt.axis('off')\n        if counter == 0:\n            ax.annotate(\"Discrete VAE recons\", xy=(-0.1, 0.5), xycoords=\"axes fraction\",\n                        va=\"center\", ha=\"right\", fontsize=12)\n    return\n\n\nplot_reconstructions(trained_std_vae, trained_discrete_vae, dataset)\n\n\n\n\n\nReconstructions\n\n\nInterestingly, the standard VAE does not always create valid reconstructions. This is due to the sampling from a Gaussian in the latent space, i.e., the decoder might see some \\(\\textbf{z}\\) it has not yet seen and then creates some weird reconstruction.\n\nLatent Traversal: Let’s traverse the latent dimension to see what the model has learnt. Note that for the standard VAE the latent space is continuous and therefore infinitely many latent sample exist. As usual, we will only show an limited amount by pertubating each latent dimension between -1 and +1 (while holding the other dimensions constant).\nFor the discrete-latent VAE, there are only \\(K\\) possible latent states.\n\n\n\nCode\ndef plot_latent_traversal(std_vae, discrete_vae, dataset, SEED=1):    \n    np.random.seed(SEED)\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    std_vae.to(device)\n    discrete_vae.to(device)\n    \n    n_samples = 1\n    i_samples = np.random.choice(range(len(dataset)), n_samples, replace=False)\n    img_batch = torch.cat([dataset[i][0].unsqueeze(0) for i in i_samples], 0)\n    img_batch = img_batch.to(device)\n    # generate latent traversals\n    n_pert, pert_min_max, n_lats = 5, 1, 3\n    img_trav_vae = std_vae.create_latent_traversal(img_batch, n_pert, pert_min_max, n_lats)\n    img_discrete_vae, latent_samples = discrete_vae.create_latent_traversal()\n    \n    fig = plt.figure(figsize=(12, 5))\n    n_rows, n_cols = n_lats + 1, 2*n_pert + 1\n    gs = GridSpec(n_rows, n_cols)\n    plt.suptitle(\"Latent Traversals\", fontsize=16, y=1, fontweight='bold')\n    for row_index in range(n_lats):\n        for col_index in range(n_pert):\n            img_rec_VAE = img_trav_vae[row_index][col_index]\n            \n            ax = plt.subplot(gs[row_index, col_index])\n            plt.imshow(transforms.ToPILImage()(img_rec_VAE))\n            plt.axis('off')\n            \n            if row_index == 0 and col_index == int(n_pert//2):\n                plt.title('STD VAE', fontsize=14, y=1.1)\n            \n            ax = plt.subplot(gs[row_index, col_index + n_pert + 1])\n            if col_index == 2:\n                plt.imshow(transforms.ToPILImage()(img_discrete_vae[row_index]))\n            if col_index == 3:\n                d = latent_samples[row_index].type(torch.uint8).tolist()\n                ax.annotate(f\"d = {d}\", xy=(0.5, 0.5))\n                \n            \n            plt.axis('off')\n            if row_index == 0 and col_index == int(n_pert//2):\n                plt.title('Discrete VAE', fontsize=14, y=1.1)\n            \n            \n    # add pertubation magnitude\n    for ax in [plt.subplot(gs[n_lats, 0:5])]:\n        ax.annotate(\"pertubation magnitude\", xy=(0.5, 0.6), xycoords=\"axes fraction\",\n                    va=\"center\", ha=\"center\", fontsize=10)\n        ax.set_frame_on(False)\n        ax.axes.set_xlim([-1.15 * pert_min_max, 1.15 * pert_min_max])\n        ax.xaxis.set_ticks([-pert_min_max, 0, pert_min_max])\n        ax.xaxis.set_ticks_position(\"top\")\n        ax.xaxis.set_tick_params(direction=\"inout\", pad=-16)\n        ax.get_yaxis().set_ticks([])\n    return\n\nplot_latent_traversal(trained_std_vae, trained_discrete_vae, dataset)\n\n\n\n\n\nLatent Traversal\n\n\nWell, this looks nice for the discrete VAE and really confusing for the Standard VAE."
  },
  {
    "objectID": "paper_summaries/concrete_distribution/index.html#acknowledgements",
    "href": "paper_summaries/concrete_distribution/index.html#acknowledgements",
    "title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe lecture on discrete latent variables by Artem Sobolev as well as the NIPS presentation by Eric Jang were really helpful resources."
  },
  {
    "objectID": "paper_summaries/concrete_distribution/index.html#footnotes",
    "href": "paper_summaries/concrete_distribution/index.html#footnotes",
    "title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhile continuous variables do not pose a problem for standard VAEs with neural networks as approximations, it should be noted that there are numerous cases in which we cannot operate with continuous variables, e.g., when the (discrete) variable is used as a decision variable.↩︎"
  },
  {
    "objectID": "paper_summaries/graph_networks/index.html",
    "href": "paper_summaries/graph_networks/index.html",
    "title": "Relational Inductive Biases, Deep Learning, and Graph Networks",
    "section": "",
    "text": "Few years after the IN paper, Battaglia et al. (2018) showed that the IN can be cast into a special case of a broader framework, termed Graph Networks (GNs). They hypothesize that despite the recent successes in deep learning with minimal representational biases, key ingredients of human-like intelligence such as combinatorial generalization1 remain out of reach. Arguing that solving this challenge should be a top priority of current research, they recommend2 the use of integrative approaches that build strong relational inductive biases3 into deep learning architectures. By presenting and formalizing a general GN framework for entity- and relation-based reasoning, they conclude that this framework could be a stepping stone towards combinatorial generalization."
  },
  {
    "objectID": "paper_summaries/graph_networks/index.html#model-description",
    "href": "paper_summaries/graph_networks/index.html#model-description",
    "title": "Relational Inductive Biases, Deep Learning, and Graph Networks",
    "section": "Model Description",
    "text": "Model Description\nSimiliar to the IN model, the GN block can be understood as a graph-to-graph module using a message passing scheme. In contrast to the IN, the GN block includes global attributes instead of external effects and uses these global attributes to update the edge attributes and node attributes instead of only updating the node attributes. Accordingly, the message passing scheme is slightly more complex.\nDefinition: Let \\(G=\\langle \\textbf{u}, V, E \\rangle\\) be an attributed, directed multigraph in which the global attribute (vector) \\(\\textbf{u}\\) represents system-level properties (e.g., gravitational field), the set of nodes \\(V=\\\\{\\textbf{v}_j\\\\}\\_{j=1 \\dots N_V}\\) represents entities4 and the set of edges \\(E = \\\\{ \\langle \\textbf{e}_k, r_k, s_k \\rangle \\\\}\\_{k=1\\dots N_E}\\) represents the attributed relations, i.e., the triplet \\(\\langle \\textbf{e}_k, r_k, s_k \\rangle\\) defines the \\(k^{\\text{th}}\\) relation from sender \\(o\\_{s_k}\\) to receiver \\(o\\_{r_k}\\) with relation attribute(s) \\(\\textbf{e}_k\\).\nFormally, the (full) GN block is defined as follows\n\\[\n\\begin{align}\n  \\begin{split}\n    \\text{GN}(G) &= \\text{GN} (\\langle \\textbf{u}, V, E \\rangle) = G^\\prime\\\\\n               &= \\left\\langle\n                 \\underbrace{\\phi^{u} \\Big(\\textbf{u}, \\rho^{v\\rightarrow u} \\big(V^\\prime\\big), \\rho^{e\\rightarrow u}\\big(E^{\\prime} \\big) \\Big)}_{\\textbf{u}^\\prime},\n   \\underbrace{\\phi^{v} \\Big(\\textbf{u}, V, \\rho^{e\\rightarrow v}\\big(E^\\prime \\big)\\Big)}_{V^\\prime},\n   \\underbrace{\\phi^{e} \\Big(\\textbf{u}, V, E\\Big)}_{E^\\prime} \\right\\rangle,\n   \\end{split}\n\\end{align}\n\\]\nwhere the updates within the graph triple \\(G=\\langle \\textbf{u}, V, E \\rangle\\) occur from right to left. More specifically, \\(\\phi^e\\) updates the edge attributes of all edges to compute the updated edge set \\(E^\\prime\\) as follows\n\\[\n\\begin{align}\n  E^\\prime  = \\phi^{e} (\\textbf{u}, \\textbf{V}, E)\n  = \\left\\{ f^e \\big(\\textbf{e}_1, \\textbf{v}_{r_1}, \\textbf{v}_{s_1}, \\textbf{u}\\big), \\dots,\n    f^{e} \\big(\\textbf{e}_{N_E}, \\textbf{v}_{r_{N_E}}, \\textbf{v}_{s_{N_E}}, \\textbf{u}\\big)\\right\\}.\n\\end{align}\n\\]\nThe updated edge set \\(E^\\prime\\) is used to compute the aggregated updated edge attributes per node \\(\\overline{\\textbf{e}}_i\\) using the aggregation function \\(\\rho^{e\\rightarrow v}\\), i.e.,\n\\[\n\\begin{align}\n  \\forall i \\in \\{1, \\dots, N_V\\}: \\overline{\\textbf{e}}_i = \\rho^{e\\rightarrow v} (E^\\prime)\n  = \\rho^{e\\rightarrow v} \\Big( \\left\\{  \\big(\\textbf{e}_k^\\prime, r_k, s_k\\big)  \\right\\}_{r_k=i, k=1:N_E}\\Big).\n\\end{align}\n\\]\nThe results are used to compute the updated node set \\(V^\\prime\\) using \\(\\phi^v\\) as follows\n\\[\n\\begin{align}\n  V^\\prime = \\phi^v \\Big(\\textbf{u}, V, \\rho^{e\\rightarrow v} \\big( E^\\prime \\big) \\Big)\n  = \\{ f^v \\big(\\overline{\\textbf{e}}_1, \\textbf{v}_1, \\textbf{u}\\big), \\dots,\n  f^v\\big(\\overline{\\textbf{e}}_{N_V}, \\textbf{v}_{N_V}, \\textbf{u}\\big)\\}.\n\\end{align}\n\\]\nLastly, the global attribute is updated towards \\(\\textbf{u}^\\prime\\) by aggregating the edge and node attributes globally, and then applying \\(\\phi^u\\). The figure below summarizes the internal structure within a (full) GN block and shows how different variants such as the relation network (Raposo et al., 2017) can be identified within the GN framework.\n\n\n\n\n\n\n\n\n(a) The internal GN block structure in its broadest formulation is shown including three update and three aggregation functions. (b) The relation network by Raposo et al. (2017) can be identified as a special case of the broader GN framework which only uses the edge predictions to predict global attributes. Taken from Battaglia et al. (2018)\n\n\n\nThe GN block can be understood as a building block to compose complex multi-block architectures, e.g., by stacking GN blocks similar to stacking layers in MLPs or reusing a GN block in a recurrent fashion. Additionally, the features inside the GN such as node attributes can be input to a standard MLP to infer abstract properties such as the potential energy (which was done in the IN paper (Battaglia et al., 2016))."
  },
  {
    "objectID": "paper_summaries/graph_networks/index.html#footnotes",
    "href": "paper_summaries/graph_networks/index.html#footnotes",
    "title": "Relational Inductive Biases, Deep Learning, and Graph Networks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBattaglia et al. (2018) define the principle of combinatorial generalization as the ability of constructing new inferences, predictions, and behaviors from known building blocks.↩︎\nThe paper was written by a large group of 27 researchers from DeepMind, GoogleBrain, MIT and University of Edinburgh. As directly stated in the abstract, it is part position paper, part review, and part unification.↩︎\nBattaglia et al. (2018) use the term relational inductive bias to refer generally to inductive biases which impose constraints on relationships and interactions among entities in a learning process. They motivate the use of relational inductive biases by human cognition which also uses (yet-to-understand) mechanisms for representing structure (e.g., world is understood as composition of objects) and relations (e.g., distance between objects).↩︎\nBattaglia et al. (2018) define an entity as an element with attributes. Thus, the term entity is more general than object capturing objects, parts of objects or any other attributed structure.↩︎"
  },
  {
    "objectID": "paper_summaries/iwae/index.html",
    "href": "paper_summaries/iwae/index.html",
    "title": "Importance Weighted Autoencoders",
    "section": "",
    "text": "Burda et al. (2016) introduce the Importance Weighted Autoencoder (IWAE) as a simple modification in the training of variational autoencoders (VAEs). Notably, they proved that this modification leads to a strictly tighter lower bound on the data log-likelihood. Furthermore, the standard VAE formulation is contained within the IWAE framework as a special case. In essence, the modification consists of using multiple samples from the recognition network / encoder and adapting the loss function with importance-weighted sample losses. In their experiments, they could emprically validate that employing IWAEs leads to improved test log-likelihoods and richer latent space representations compared to VAEs."
  },
  {
    "objectID": "paper_summaries/iwae/index.html#model-description",
    "href": "paper_summaries/iwae/index.html#model-description",
    "title": "Importance Weighted Autoencoders",
    "section": "Model Description",
    "text": "Model Description\nAn IWAE can be understood as a standard VAE in which multiple samples are drawn from the encoder distribution \\(q_{\\boldsymbol{\\phi}} \\Big( \\textbf{z} |\n\\textbf{x} \\Big)\\) and then fed through the decoder \\(p_{\\boldsymbol{\\theta}}\n\\Big( \\textbf{z} | \\textbf{x} \\Big)\\). In principle, this modification has been already proposed in the original VAE paper by Kingma and Welling (2013). However, Burda et al. (2016) additionally proposed to use a different objective function. The empirical objective function can be understood as the data log-likelihood \\(\\log p_{\\boldsymbol{\\theta}} (\\textbf{x})\\) where the sampling distribution is exchanged to \\(q_{\\boldsymbol{\\phi}} \\Big( \\textbf{z} |\n\\textbf{x} \\Big)\\) via the method of importance sampling.\n\nHigh-Level Overview\nThe IWAE framework builds upon a standard VAE architecture. There are two neural networks as approximations for the encoder \\(q_{\\boldsymbol{\\phi}} \\left(\\textbf{z} | \\textbf{x} \\right)\\) and the decoder distribution \\(p_{\\boldsymbol{\\theta}} \\left( \\textbf{x} | \\textbf{z} \\right)\\). More precisely, the networks estimate the parameters that parametrize these distributions. Typically, the latent distribution is assumed to be a Gaussian \\(q_{\\boldsymbol{\\phi}} \\left(\\textbf{z} | \\textbf{x} \\right) \\sim\n\\mathcal{N}\\left( \\boldsymbol{\\mu}_{\\text{E}}, \\text{diag} \\left(\n\\boldsymbol{\\sigma}^2_{\\text{E}}\\right) \\right)\\) such that the encoder network estimates its the mean \\(\\boldsymbol{\\mu}\\_{\\text{E}}\\) and variance1 \\(\\boldsymbol{\\Sigma} = \\text{diag}\n\\left(\\boldsymbol{\\sigma}^2_{\\text{E}}\\right)\\). To allow for backpropagation, we apply the reparametrization trick to the latent distribution which essentially consist of transforming samples from some (fixed) random distribution, e.g. \\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N} \\left(\\textbf{0},\n\\textbf{I} \\right)\\), into the desired distribution using a deterministic mapping.\nThe main difference between a VAE and an IWAE lies in the objective function which is explained in more detail in the next section.\n\n\n\n\n\n\n\n\n\n\n\nVAE/IWAE Architecture\n\n\n\n\n\nDerivation\nLet \\(\\textbf{X} = \\{\\textbf{x}^{(i)}\\}_{i=1}^N\\) denote a dataset of \\(N\\) i.i.d. samples where each observed datapoint \\(\\textbf{x}^{(i)}\\) is obtained by first sampling a latent vector \\(\\textbf{z}\\) from the prior \\(p_{\\boldsymbol{\\theta}}(\\textbf{z})\\) and then sampling \\(\\textbf{x}^{(i)}\\) itself from the scene model \\(p_{\\boldsymbol{\\theta}} \\Big( \\textbf{x} |\n\\textbf{z} \\Big)\\). Now we introduce an auxiliary distribution \\(q_{\\boldsymbol{\\phi}} \\Big( \\textbf{z} | \\textbf{x} \\Big)\\) (with its own paramaters) as an approximation to the true, but unknown posterior \\(p_{\\boldsymbol{\\theta}} \\left( \\textbf{z} | \\textbf{x} \\right)\\). Accordingly, the data likelihood of a one sample \\(\\textbf{x}^{(i)}\\) can be stated as follows\n\\[\np_{\\boldsymbol{\\theta}} (\\textbf{x}^{(i)}) = \\mathbb{E}_{\\textbf{z} \\sim\np_{\\boldsymbol{\\theta}} \\Big(\\textbf{z} \\Big)} \\Big[\np_{\\boldsymbol{\\theta}} \\left( \\textbf{z} | \\textbf{x}^{(i)} \\right) \\Big] =\n\\int p_{\\boldsymbol{\\theta}} (\\textbf{z}) p_{\\boldsymbol{\\theta}} \\left(\n\\textbf{z} | \\textbf{x}^{(i)} \\right) d\\textbf{z} =\n\\int p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}^{(i)} , \\textbf{z} \\right) d\\textbf{z}\n\\]\nNow, we use the simple trick of importance sampling to change the sampling distribution into the approximated posterior, i.e.,\n\\[\np_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)}\\right)\n= \\int \\frac {p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}^{(i)} , \\textbf{z}\n\\right)} {q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}^{(i)} \\right)}\nq_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}^{(i)} \\right) d\\textbf{z}\n= \\mathbb{E}_{\\textbf{z} \\sim q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} |\n\\textbf{x}^{(i)} \\right)}\n\\left[ \\frac {p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}^{(i)} , \\textbf{z}\n\\right)} {q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}^{(i)} \\right)}\n\\right]\n\\]\n\nVAE Formulation\nIn the standard VAE approach, we use the evidence lower bound (ELBO) on \\(\\log p_{\\boldsymbol{\\theta}} \\Big( \\textbf{x}\\Big)\\) as the objective function. This can be derived by applying Jensen’s Inequality on the data log-likelihood:\n\\[\n\\log p_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)} \\right)\n= \\log \\mathbb{E}_{\\textbf{z} \\sim q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} |\n\\textbf{x}^{(i)} \\right)}\n\\left[ \\frac {p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}^{(i)} , \\textbf{z}\n\\right)} {q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}^{(i)} \\right)}\n\\right] \\ge \\mathbb{E}_{\\textbf{z} \\sim q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} |\n\\textbf{x}^{(i)} \\right)}\n\\left[ \\log \\frac {p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}^{(i)} , \\textbf{z}\n\\right)} {q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}^{(i)} \\right)}\n\\right] = \\mathcal{L}^{\\text{ELBO}}\n\\]\nUsing simple algebra, this can be rearranged into\n\\[\n\\mathcal{L}^{\\text{ELBO}}\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\textbf{x}^{(i)} \\right) =\n\\underbrace{\n\\mathbb{E}_{\\textbf{z} \\sim q_{\\phi} \\left( \\textbf{z}| \\textbf{x}^{(i)} \\right)}\n\\left[ \\log p_{\\boldsymbol{\\theta}} \\Big(\\textbf{x}^{(i)} | \\textbf{z} \\Big)\n\\right]}_{\n\\text{Reconstruction Accuracy}}\n-\n\\underbrace{\nD_{KL} \\left( q_{\\phi} \\Big( \\textbf{z} |\n\\textbf{x}^{(i)} \\Big) || p_{\\boldsymbol{\\theta}} \\Big( \\textbf{z} \\Big) \\right)\n}_{\\text{Regularization}}\n\\]\nWhile the regularization term can usually be solved analytically, the reconstruction accuracy in its current formulation poses a problem for backpropagation: Gradients cannot backpropagate through a sampling operation. To circumvent this problem, the standard VAE formulation includes the reparametrization trick:\n\nSubstitute sampling \\(\\textbf{z} \\sim q_{\\boldsymbol{\\phi}}\\) by using a deterministic mapping \\(\\textbf{z} = g_{\\boldsymbol{\\phi}}\n(\\boldsymbol{\\epsilon},\n\\textbf{x})\\) with the differential transformation \\(g_{\\boldsymbol{\\phi}}\\) of an auxiliary noise variable \\(\\boldsymbol{\\epsilon}\\) with \\(\\boldsymbol{\\epsilon}\\sim p(\\boldsymbol{\\epsilon})\\).\n\n As a result, we can rewrite the EBLO as follows\n\\[\n\\mathcal{L}^{\\text{ELBO}}\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\phi};\n\\textbf{x}^{(i)} \\right) =\n\\mathbb{E}_{\\boldsymbol{\\epsilon} \\sim p \\left( \\boldsymbol{\\epsilon} \\right)}\n\\left[ \\log p_{\\boldsymbol{\\theta}} \\Big(\\textbf{x}^{(i)} |\ng_{\\boldsymbol{\\phi}} \\left( \\boldsymbol{\\epsilon}, \\textbf{x}^{(i)} \\right) \\Big)\n\\right] -\nD_{KL} \\left( q_{\\phi} \\Big( \\textbf{z} |\n\\textbf{x}^{(i)} \\Big) || p_{\\boldsymbol{\\theta}} \\Big( \\textbf{z} \\Big) \\right)\n\\]\nLastly, the expectation is approximated using Monte-Carlo integration, leading to the standard VAE objective\n\\[\n\\begin{align}\n  \\widetilde{\\mathcal{L}}^{\\text{VAE}}_k \\left(\\boldsymbol{\\theta},\n  \\boldsymbol{\\phi};\n  \\textbf{x}^{(i)}\\right) &=\n  \\frac {1}{k} \\sum_{l=1}^{k} \\log p_{\\boldsymbol{\\theta}}\\left(\n  \\textbf{x}^{(i)}| g_{\\boldsymbol{\\phi}}\n\\left( \\boldsymbol{\\epsilon}^{(l)}, \\textbf{x}^{(i)} \\right)\\right)\n  -D_{KL} \\left(  q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}^{(i)} \\right),\n  p_{\\boldsymbol{\\theta}} (\\textbf{z}) \\right)\\\\\n  &\\text{with} \\quad \\boldsymbol{\\epsilon}^{(l)} \\sim p(\\boldsymbol{\\epsilon})\n\\end{align}\n\\]\nNote that commonly \\(k=1\\) in VAEs as long as the minibatch size is large enough. As stated by Kingma and Welling (2013):\n\n\n\n\n\n\nQuote\n\n\n\nWe found that the number of samples per datapoint can be set to 1 as long as the minibatch size was large enough.\n\n\n\n\nIWAE Formulation\nBefore we introduce the IWAE estimator, remind that the Monte-Carlo estimator of the data likelihood (when the sampling distribution is changed via importance sampling, see Derivation) is given by\n\\[\np_{\\boldsymbol{\\theta}} (\\textbf{x} ) =\n\\mathbb{E}_{\\textbf{z} \\sim q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} |\n\\textbf{x}^{(i)} \\right)}\n\\left[ \\frac {p_{\\boldsymbol{\\theta}} \\left(\\textbf{x} , \\textbf{z}\n\\right)} {q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x} \\right)}\n\\right] \\approx \\frac {1}{k} \\sum_{l=1}^{k}\n\\frac {p_{\\boldsymbol{\\theta}} \\left(\\textbf{x} , \\textbf{z}^{(l)}\n\\right)} {q_{\\boldsymbol{\\phi}} \\left( \\textbf{z}^{(l)} | \\textbf{x}\n\\right)} \\quad \\text{with} \\quad \\textbf{z}^{(l)} \\sim q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} |\n\\textbf{x}^{(i)} \\right)\n\\]\nAs a result, the data log-likelihood estimator for one sample \\(\\textbf{x}^{(i)}\\) can be stated as follows\n\\[\n\\begin{align}\n\\log p_{\\boldsymbol{\\theta}} (\\textbf{x}^{(i)} ) &\\approx \\log \\left[ \\frac {1}{k} \\sum_{l=1}^{k}\n\\frac {p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}^{(i)} , \\textbf{z}^{(i, l)}\n\\right)} {q_{\\boldsymbol{\\phi}} \\left( \\textbf{z}^{(i, l)} | \\textbf{x}^{(i)}\n\\right)}\\right] = \\widetilde{\\mathcal{L}}^{\\text{IWAE}}_k \\left( \\boldsymbol{\\theta},\n\\boldsymbol{\\phi}; \\textbf{x}^{(i)} \\right) \\\\\n&\\text{with} \\quad \\textbf{z}^{(i, l)} \\sim q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} |\n\\textbf{x}^{(i)} \\right)\n\\end{align}\n\\]\nwhich leads to an empirical estimate of the IWAE objective. However, Burda et al. (2016) do not use the data log-likelihood in its plain form as the true IWAE objective. Instead they introduce the IWAE objective as follows\n\\[\n\\mathcal{L}^{\\text{IWAE}}_k \\left(\\boldsymbol{\\theta}, \\boldsymbol{\\phi};\n\\textbf{x}^{(i)}\\right)\n=  \\mathbb{E}_{\\textbf{z}^{(1)}, \\dots,  \\textbf{z}^{(k)} \\sim q_{\\phi} \\left( \\textbf{z}|\n\\textbf{x}^{(i)} \\right)}\n\\left[\n\\log \\frac {1}{k}\n\\sum_{l=1}^k\n\\frac {p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}^{(i)}, \\textbf{z}^{(l)}\\right)}\n{q_{\\phi} \\left( \\textbf{z}^{(l)} | \\textbf{x}^{(i)} \\right)}\n\\right]\n\\]\nFor notation purposes, they denote\n\\[\n\\text{(unnormalized) importance weights:} \\quad\n{w}^{(i, l)} = \\frac {p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}^{(i)}, \\textbf{z}^{(l)}\\right)}\n{q_{\\phi} \\left( \\textbf{z}^{(l)} | \\textbf{x}^{(i)} \\right)}\n\\]\nBy applying Jensen’s Inequality, we can see that in fact the (true) IWAE estimator is merely a lower-bound on the plain data log-likelihood\n\\[\n\\mathcal{L}^{\\text{IWAE}}_k \\left( \\boldsymbol{\\theta}, \\boldsymbol{\\phi};\n\\textbf{x}^{(i)} \\right)\n= \\mathbb{E} \\left[ \\log \\frac {1}{k} \\sum_{l=1}^{k} {w}^{(i,\nl)}\\right] \\le \\log \\mathbb{E} \\left[ \\frac {1}{k} \\sum_{l=1}^{k}\n{w}^{(i,l)} \\right] = \\log p_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)} \\right)\n\\]\nThey could prove that with increasing \\(k\\) the lower bound gets strictly tighter and approaches the true data log-likelihood in the limit of \\(k \\rightarrow\n\\infty\\). Note that since the empirical IWAE estimator \\(\\widetilde{\\mathcal{L}}_k^{\\text{IWAE}}\\) can be understood as a Monte-Carlo estimator on the true data log-likelihood, in the empirical case this property can simply be deduced from the properties of Monte-Carlo integration.\n\n\n\n\n\n\nWhat is motivation of the true IWAE objective?\n\n\n\n\n\nA very well explanation is given by Domke and Sheldon (2018). Starting from the property\n\\[\np(\\textbf{x}) = \\mathbb{E} \\Big[ w \\Big] = \\mathbb{E}_{\\textbf{z} \\sim q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} |\n\\textbf{x}^{(i)} \\right)}\n\\left[ \\frac {p_{\\boldsymbol{\\theta}} \\left(\\textbf{x} , \\textbf{z}\n\\right)} {q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x} \\right)}\n\\right]\n\\]\nWe derived the ELBO using Jensen’s inequality\n\\[\n\\log p(\\textbf{x}) \\ge \\mathbb{E} \\Big[ \\log w \\Big] = \\text{ELBO} \\Big[ q ||\np \\Big]\n\\]\nSuppose that we could make \\(w\\) more concentrated about its mean \\(p(\\textbf{x})\\). Clearly, this would yield a tighter lower bound when applying Jensen’s Inequality.\n(rhetorical break)\nCan we make \\(w\\) more concentrated about its mean? YES, WE CAN.\nFor example using the sample average \\(w_k = \\frac {1}{k}\n\\sum_{i=1}^k w^{(i)}\\). This leads directly to the true IWAE objective\n\\[\n\\log p(\\textbf{x})  \\ge \\mathbb{E} \\Big[ \\log w_k \\Big] = \\mathbb{E} \\left[\n\\log \\frac {1}{k} \\sum_{i=1}^{k} w^{(i)} \\right] = \\mathcal{L}^{\\text{IWAE}}_k\n\\]\n\n\n\n\n\n\n\n\n\nIWAE objective and plain log-likelihood lead to the same empirical estimate. How?\n\n\n\n\n\nHere it gets interesting. A closer analysis on the IWAE bound by Nowozin (2018) revealed the following property\n\\[\n\\begin{align}\n&\\quad \\mathcal{L}_k^{\\text{IWAE}} = \\log p(\\textbf{x}) - \\frac {1}{k} \\frac\n{\\mu_2}{2\\mu^2} + \\frac {1}{k^2} \\left( \\frac {\\mu_3}{3\\mu^3} - \\frac\n{3\\mu_2^2}{4\\mu^4} \\right) + \\mathcal{O}(k^{-3})\\\\\n&\\text{with} \\quad\n\\mu = \\mathbb{E}_{\\textbf{z} \\sim q_{\\boldsymbol{\\phi}}} \\left[ \\frac\n{p_{\\boldsymbol{\\theta}}\\left( \\textbf{x}, \\textbf{z}\n\\right)}{q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x} \\right)} \\right]\n\\quad\n\\mu_i = \\mathbb{E}_{\\textbf{z} \\sim q_{\\boldsymbol{\\phi}}} \\left[\n\\left( \\frac\n{p_{\\boldsymbol{\\theta}}\\left( \\textbf{x}, \\textbf{z}\n\\right)}{q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x} \\right)}\n- \\mathbb{E}_{\\textbf{z} \\sim q_{\\boldsymbol{\\phi}}} \\left[ \\frac\n{p_{\\boldsymbol{\\theta}}\\left( \\textbf{x}, \\textbf{z}\n\\right)}{q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x} \\right)} \\right]\n\\right)^2 \\right]\n\\end{align}\n\\]\nThus, the true objective is a biased - in the order of \\(\\mathcal{O}\\left(k^{-1}\\right)\\) - and consistent estimator of the marginal log likelihood \\(\\log p(\\textbf{x})\\). The empirical estimator of the true IWAE objective is basically a special Monte-Carlo estimator (only one sample per \\(k\\)) on the true IWAE objective. It is more or less luck that we can formulate the same empirical objective and interpret it differently as the Monte-Carlo estimator (with \\(k\\) samples) on the data log-likelihood.\n\n\n\n\nLet us take a closer look on how to compute gradients (fast) for the empirical estimate of the IWAE objective:\n\\[\n\\begin{align}\n\\nabla_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}}\n\\widetilde{\\mathcal{L}}_k^{\\text{IWAE}} \\left( \\boldsymbol{\\theta}, \\boldsymbol{\\phi};\n\\textbf{x}^{(i)} \\right) &= \\nabla_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}}\n\\log \\frac {1}{k} \\sum_{l=1}^k w^{(i,l)} \\left( \\textbf{x}^{(i)},\n\\textbf{z}^{(i, l)}_{\\boldsymbol{\\phi}}, \\boldsymbol{\\theta} \\right) \\quad\n\\text{with} \\quad\n\\textbf{z}^{(i, l)} \\sim q_{\\boldsymbol{\\phi}} \\left(\\textbf{z} |\n\\textbf{x}^{(i)} \\right)\\\\\n&\\stackrel{\\text{(*)}}{=}\n\\sum_{l=1}^{k} \\frac {w^{(i, l)}}{\\sum_{m=1}^{k} w^{(i,\nm)}} \\nabla_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}} \\log w^{(i,l)} =\n\\sum_{l=1}^{k} \\widetilde{w}^{(i, l)} \\nabla_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}} \\log w^{(i,l)},\n\\end{align}\n\\]\nwhere we introduced the following notation\n\\[\n\\text{(normalized) importance weights:} \\quad\n\\widetilde{w}^{(i, l)} = \\frac {w^{(i,l)}}{\\sum_{m=1}^k w^{(i, m)}}\n\\]\n\n\n\n\n\n\n\\((*)\\): Gradient Derivation:\n\n\n\n\n\n\\[\n\\begin{align}\n\\frac {\\partial \\left[ \\log \\frac {1}{k} \\sum_i^{k} w_i \\left( \\boldsymbol{\\theta}\n\\right) \\right]}{\\partial \\boldsymbol{\\theta}} &\\stackrel{\\text{chain rule}}{=}  \\frac {\\partial\n\\log a}{\\partial a} \\sum_{i}^{k} \\frac {\\partial a}{\\partial w_i} \\frac\n{\\partial w_i}{\\partial \\boldsymbol{\\theta}} \\quad \\text{with}\n\\quad a = \\frac {1}{k} \\sum_{i}^k w_i (\\boldsymbol{\\theta})\\\\\n&= \\frac {k}{\\sum_l^k w_l} \\sum_{i}^{k}\\frac {1}{k} \\frac {\\partial\nw_i (\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} = \\frac {1}{\\sum_l^k\nw_l} \\sum_{i}^{k} \\frac {\\partial\nw_i (\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}\n\\end{align}\n\\]\nLastly, we use the following identity\n\\[\n\\frac {\\partial w_i (\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} = w_i\n(\\boldsymbol{\\theta}) \\cdot\n\\frac {\\partial \\log w_i (\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}\n\\stackrel{\\text{chain rule}}{=} w_i (\\boldsymbol{\\theta}) \\cdot \\frac {1}{w_i\n(\\boldsymbol{\\theta})} \\cdot\n\\frac {\\partial w_i (\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} =\n\\frac {\\partial w_i (\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}\n\\]\n\n\n\nSimilar to VAEs, this formulation poses a problem for backpropagation due to the sampling operation. We use the same reparametrization trick to circumvent this problem and obtain a low variance update rule:\n\\[\n\\begin{align}\n\\nabla_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}}\n\\widetilde{\\mathcal{L}}_k^{\\text{IWAE}} &=\n\\sum_{l=1}^{k} \\widetilde{w}^{(i, l)} \\nabla_{\\boldsymbol{\\phi},\n\\boldsymbol{\\theta}} \\log w^{(i,l)} \\left( \\textbf{x}^{(i)},\n\\textbf{z}_{\\boldsymbol{\\phi}}^{(i,l)}, \\boldsymbol{\\theta} \\right)\n\\quad \\text{with} \\quad\n\\textbf{z}^{(i,l)} \\sim q_{\\boldsymbol{\\phi}} \\left(\\textbf{z} | \\textbf{x}^{(i)} \\right)\\\\\n&= \\sum_{l=1}^k \\widetilde{w}^{(i,l)} \\nabla_{\\boldsymbol{\\phi},\n\\boldsymbol{\\theta}} \\log w^{(i,l)} \\left(\\textbf{x}^{(i)},\ng_{\\boldsymbol{\\phi}} \\left( \\textbf{x}^{(i)},\n\\boldsymbol{\\epsilon}^{(l)}\\right), \\textbf{x}^{(i)} \\right), \\quad \\quad\n\\boldsymbol{\\epsilon} \\sim p(\\boldsymbol{\\epsilon})\n\\end{align}\n\\]\nTo make things clearer for the implementation, let us unpack the log\n\\[\n\\log w^{(i,l)} = \\log \\frac {p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}^{(i)}, \\textbf{z}^{(l)}\\right)}\n{q_{\\boldsymbol{\\phi}} \\left( \\textbf{z}^{(l)} | \\textbf{x}^{(i)} \\right)} = \\underbrace{\\log\np_{\\boldsymbol{\\theta}} \\left (\\textbf{x}^{(i)} | \\textbf{z}^{(l)}\n\\right)}_{\\text{NLL}} + \\log p_{\\boldsymbol{\\theta}} \\left( \\textbf{z}^{(l)}\n\\right) - \\log q_{\\boldsymbol{\\phi}} \\left( \\textbf{z}^{(l)} | \\textbf{x}^{(i)} \\right)\n\\]\nBefore, we are going to implement this formulation, let us look whether we can separate out the KL divergence for the true IWAE objective of Burda et al. (2016). Therefore, we state the update for the true objective:\n\\[\n\\begin{align}\n\\nabla_{\\boldsymbol{\\phi},\n\\boldsymbol{\\theta}}\n\\mathcal{L}_k^{\\text{IWAE}} &=\n\\nabla_{\\boldsymbol{\\phi},\n\\boldsymbol{\\theta}}\n\\mathbb{E}_{\\textbf{z}^{(1)}, \\dots, \\textbf{z}^{(l)}} \\left[ \\log \\frac {1}{k}\n\\sum_{l=1}^{k} w^{(l)} \\left( \\textbf{x},\n\\textbf{z}^{(l)}_{\\boldsymbol{\\phi}}, \\boldsymbol{\\theta} \\right) \\right]\\\\\n&=\n\\mathbb{E}_{\\textbf{z}^{(1)}, \\dots, \\textbf{z}^{(l)}} \\left[\n\\sum_{l=1}^{k} \\widetilde{w}_i\n\\nabla_{\\boldsymbol{\\phi},\n\\boldsymbol{\\theta}}\n\\log w^{(l)} \\left( \\textbf{x}, \\textbf{z}_{\\boldsymbol{\\phi}}^{(l)}, \\boldsymbol{\\theta} \\right) \\right]\\\\\n&=\\sum_{l=1}^{k} \\widetilde{w}_i \\mathbb{E}_{\\textbf{z}^{(l)}} \\left[\n\\nabla_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}} \\log w^{(l)} \\left( \\textbf{x},\n\\textbf{z}_{\\boldsymbol{\\phi}}^{(l)}, \\boldsymbol{\\theta} \\right)\n\\right]\\\\\n&\\neq \\sum_{l=1}^{k} \\widetilde{w}_i\n\\nabla_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}}\n\\mathbb{E}_{\\textbf{z}^{(l)}} \\left[\n\\log w^{(l)} \\left( \\textbf{x},\n\\textbf{z}_{\\boldsymbol{\\phi}}^{(l)}, \\boldsymbol{\\theta} \\right)\n\\right]\n\\end{align}\n\\]\nUnfortunately, we cannot simply move the gradient outside the expectation. If we could, we could simply rearrange the terms inside the expectation as in the standard VAE case.\n\nLet us look, what would happen, if we were to describe the true IWAE estimator as the data log-likelihood \\(\\log p \\left( \\textbf{x} \\right)\\) in which the sampling distribution is exchanged via importance sampling:\n\\[\n\\begin{align}\n\\nabla_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}} \\log p \\left( \\textbf{x}^{(i)} \\right) &=\n\\nabla_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}} \\log \\mathbb{E}_{\\textbf{z} \\sim\nq_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}^{(i)}\\right)} \\left[ w\n(\\textbf{x}^{(i)}, \\textbf{z}, \\boldsymbol{\\theta})\\right]\\\\\n&\\neq\n\\nabla_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}}  \\mathbb{E}_{\\textbf{z} \\sim\nq_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}^{(i)}\\right)} \\left[ \\log w\n(\\textbf{x}^{(i)}, \\textbf{z}, \\boldsymbol{\\theta})\\right]\n\\end{align}\n\\]\nHere, we also cannot separate the KL divergence out, since we cannot simply move the log inside the expectation."
  },
  {
    "objectID": "paper_summaries/iwae/index.html#implementation",
    "href": "paper_summaries/iwae/index.html#implementation",
    "title": "Importance Weighted Autoencoders",
    "section": "Implementation",
    "text": "Implementation\nLet’s put this into practice and compare the standard VAE with an IWAE. We are going to perform a very similar experiment to the density estimation experiment by Burda et al. (2016), i.e., we are going to train both a VAE and IWAE with different number of samples \\(k\\in \\{1,\n10\\}\\) on the binarized MNIST dataset.\n\nDataset\nLet’s first build a binarized version of the MNIST dataset. As noted by Burda et al. (2016), the generative modeling literature is inconsistent about the method of binarization. We employ the same procedure as Burda et al. (2016): binary-valued observations are sampled with expectations equal to the real values in the training set:\n\n\nCode\nimport torch.distributions as dists\nimport torch\nfrom torchvision import datasets, transforms\n\n\nclass Binarized_MNIST(datasets.MNIST):\n    def __init__(self, root, train, transform=None, target_transform=None, download=False):\n        super(Binarized_MNIST, self).__init__(root, train, transform, target_transform, download)\n\n    def __getitem__(self, idx):\n        img, target = super().__getitem__(idx)\n        return dists.Bernoulli(img).sample().type(torch.float32)\n\n\n\n\n\n\n\n\n\n\n\n\n\nBinarized MNIST Dataset\n\n\n\n\n\nModel Implementation\n\nVAE Implementation\nThe VAE implementation is straightforward. For later evaluation, I added create_latent_traversal and compute_marginal_log_likelihood. The ladder computes the marginal log-likelihood \\(\\log p(\\textbf{x})\\) in which the sampling distribution is exchanged to the approximated posterior \\(q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}\\right )\\) using the standard Monte-Carlo estimator, i.e.,\n\\[\n  \\log p(\\textbf{x}) = \\mathbb{E}_{z\\sim q_{\\boldsymbol{\\phi}}} \\left[ \\frac\n  {p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}, \\textbf{z}\\right)}\n  {q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x} \\right)} \\right]\n  \\approx \\log \\left[ \\frac {1}{k} \\sum_{l=1}^k w^{(l)} \\right] =\n  \\mathcal{L}^{\\text{IWAE}}_k (\\textbf{x})\n\\]\nRemind that this formulation equals the empirical IWAE estimator. However, we can only compute the (unnormalized) logarithmic importance weights\n\\[\n\\log w^{(i,l)} = \\log \\frac {p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}^{(i)}, \\textbf{z}^{(l)}\\right)}\n{q_{\\boldsymbol{\\phi}} \\left( \\textbf{z}^{(l)} | \\textbf{x}^{(i)} \\right)} = \\log\np_{\\boldsymbol{\\theta}} \\left (\\textbf{x}^{(i)} | \\textbf{z}^{(l)}\n\\right) + \\log p_{\\boldsymbol{\\theta}} \\left( \\textbf{z}^{(l)}\n\\right) - \\log q_{\\boldsymbol{\\phi}} \\left( \\textbf{z}^{(l)} | \\textbf{x}^{(i)} \\right)\n\\]\nAccordingly, we compute the marginal log-likelihood as follows\n\\[\n\\begin{align}\n\\widetilde{\\mathcal{L}}^{\\text{IWAE}}_k \\left( \\boldsymbol{\\theta},\n\\boldsymbol{\\phi}; \\textbf{x}^{(i)} \\right) &= \\underbrace{\\log 1}_{=0} - \\log\nk + \\log \\left( \\sum_{i=1}^k w^{(i,l)} \\right) \\\\\n&= -\\log k + \\underbrace{\\log \\left( \\sum_{i=1}^k \\exp \\big[ \\log w^{(i, l)} \\big] \\right)}_{=\\text{torch.logsumexp}}\n\\end{align}\n\\]\n\n\n\nCode\nimport torch.nn as nn\nimport numpy as np\n\n\nMNIST_SIZE = 28\nHIDDEN_DIM = 400\nLATENT_DIM = 50\n\n\nclass VAE(nn.Module):\n\n    def __init__(self, k):\n        super(VAE, self).__init__()\n        self.k = k\n        self.encoder = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(MNIST_SIZE**2, HIDDEN_DIM),\n            nn.ReLU(),\n            nn.Linear(HIDDEN_DIM, HIDDEN_DIM),\n            nn.ReLU(),\n            nn.Linear(HIDDEN_DIM, 2*LATENT_DIM)\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(LATENT_DIM, HIDDEN_DIM),\n            nn.ReLU(),\n            nn.Linear(HIDDEN_DIM, HIDDEN_DIM),\n            nn.ReLU(),\n            nn.Linear(HIDDEN_DIM, MNIST_SIZE**2),\n            nn.Sigmoid()\n        )\n        return\n\n    def compute_loss(self, x, k=None):\n        if not k:\n            k = self.k\n        [x_tilde, z, mu_z, log_var_z] = self.forward(x, k)\n        # upsample x\n        x_s = x.unsqueeze(1).repeat(1, k, 1, 1, 1)\n        # compute negative log-likelihood\n        NLL = -dists.Bernoulli(x_tilde).log_prob(x_s).sum(axis=(2, 3, 4)).mean()\n        # copmute kl divergence\n        KL_Div = -0.5*(1 + log_var_z - mu_z.pow(2) - log_var_z.exp()).sum(1).mean()\n        # compute loss\n        loss = NLL + KL_Div\n        return loss\n\n    def forward(self, x, k=None):\n        \"\"\"feed image (x) through VAE\n\n        Args:\n            x (torch tensor): input [batch, img_channels, img_dim, img_dim]\n\n        Returns:\n            x_tilde (torch tensor): [batch, k, img_channels, img_dim, img_dim]\n            z (torch tensor): latent space samples [batch, k, LATENT_DIM]\n            mu_z (torch tensor): mean latent space [batch, LATENT_DIM]\n            log_var_z (torch tensor): log var latent space [batch, LATENT_DIM]\n        \"\"\"\n        if not k:\n            k = self.k\n        z, mu_z, log_var_z = self.encode(x, k)\n        x_tilde = self.decode(z, k)\n        return [x_tilde, z, mu_z, log_var_z]\n\n    def encode(self, x, k):\n        \"\"\"computes the approximated posterior distribution parameters and\n        samples from this distribution\n\n        Args:\n            x (torch tensor): input [batch, img_channels, img_dim, img_dim]\n\n        Returns:\n            z (torch tensor): latent space samples [batch, k, LATENT_DIM]\n            mu_E (torch tensor): mean latent space [batch, LATENT_DIM]\n            log_var_E (torch tensor): log var latent space [batch, LATENT_DIM]\n        \"\"\"\n        # get encoder distribution parameters\n        out_encoder = self.encoder(x)\n        mu_E, log_var_E = torch.chunk(out_encoder, 2, dim=1)\n        # increase shape for sampling [batch, samples, latent_dim]\n        mu_E_ups = mu_E.unsqueeze(1).repeat(1, k, 1)\n        log_var_E_ups = log_var_E.unsqueeze(1).repeat(1, k, 1)\n        # sample noise variable for each batch and sample\n        epsilon = torch.randn_like(log_var_E_ups)\n        # get latent variable by reparametrization trick\n        z = mu_E_ups + torch.exp(0.5*log_var_E_ups) * epsilon\n        return z, mu_E, log_var_E\n\n    def decode(self, z, k):\n        \"\"\"computes the Bernoulli mean of p(x|z)\n        note that linear automatically parallelizes computation\n\n        Args:\n            z (torch tensor): latent space samples [batch, k, LATENT_DIM]\n\n        Returns:\n            x_tilde (torch tensor): [batch, k, img_channels, img_dim, img_dim]\n        \"\"\"\n        # get decoder distribution parameters\n        x_tilde = self.decoder(z)  # [batch*samples, MNIST_SIZE**2]\n        # reshape into [batch, samples, 1, MNIST_SIZE, MNIST_SIZE] (input shape)\n        x_tilde = x_tilde.view(-1, k, 1, MNIST_SIZE, MNIST_SIZE)\n        return x_tilde\n\n    def create_latent_traversal(self, image_batch, n_pert, pert_min_max=2, n_latents=5):\n        device = image_batch.device\n        # initialize images of latent traversal\n        images = torch.zeros(n_latents, n_pert, *image_batch.shape[1::])\n        # select the latent_dims with lowest variance (most informative)\n        [x_tilde, z, mu_z, log_var_z] = self.forward(image_batch)\n        i_lats = log_var_z.mean(axis=0).sort()[1][:n_latents]\n        # sweep for latent traversal\n        sweep = np.linspace(-pert_min_max, pert_min_max, n_pert)\n        # take first image and encode\n        [z, mu_E, log_var_E] = self.encode(image_batch[0:1], k=1)\n        for latent_dim, i_lat in enumerate(i_lats):\n            for pertubation_dim, z_replaced in enumerate(sweep):\n                z_new = z.detach().clone()\n                z_new[0][0][i_lat] = z_replaced\n\n                img_rec = self.decode(z_new.to(device), k=1).squeeze(0)\n                img_rec = img_rec[0].clamp(0, 1).cpu()\n\n                images[latent_dim][pertubation_dim] = img_rec\n        return images\n\n    def compute_marginal_log_likelihood(self, x, k=None):\n        \"\"\"computes the marginal log-likelihood in which the sampling\n        distribution is exchanged to q_{\\phi} (z|x),\n        this function can also be used for the IWAE loss computation\n\n        Args:\n            x (torch tensor): images [batch, img_channels, img_dim, img_dim]\n\n        Returns:\n            log_marginal_likelihood (torch tensor): scalar\n            log_w (torch tensor): unnormalized log importance weights [batch, k]\n        \"\"\"\n        if not k:\n            k = self.k\n        [x_tilde, z, mu_z, log_var_z] = self.forward(x, k)\n        # upsample mu_z, std_z, x_s\n        mu_z_s = mu_z.unsqueeze(1).repeat(1, k, 1)\n        std_z_s = (0.5 * log_var_z).exp().unsqueeze(1).repeat(1, k, 1)\n        x_s = x.unsqueeze(1).repeat(1, k, 1, 1, 1)\n        # compute logarithmic unnormalized importance weights [batch, k]\n        log_p_x_g_z = dists.Bernoulli(x_tilde).log_prob(x_s).sum(axis=(2, 3, 4))\n        log_prior_z = dists.Normal(0, 1).log_prob(z).sum(2)\n        log_q_z_g_x = dists.Normal(mu_z_s, std_z_s).log_prob(z).sum(2)\n        log_w = log_p_x_g_z + log_prior_z - log_q_z_g_x\n        # compute marginal log-likelihood\n        log_marginal_likelihood = (torch.logsumexp(log_w, 1) -  np.log(k)).mean()\n        return log_marginal_likelihood, log_w\n\n\n\nIWAE Implementation\nFor the IWAE class implementation, we only need to adapt the loss computation. Everything else can be inherited from the VAE class. In fact, we can simply use compute_marginal_log_likelihood as the loss function computation.\nFor the interested reader, it might be interesting to understand the original implementation. Therefore, I added to other modes of loss function calculation which are based on the idea of importance-weighted sample losses.\nAs shown in the derivation, we can derive the gradient to be a linear combination of importance-weighted sample losses, i.e.,\n\\[\n\\begin{align}\n\\nabla_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}}\n\\widetilde{\\mathcal{L}}_k^{\\text{IWAE}} &=\n\\sum_{l=1}^{k} \\widetilde{w}^{(i, l)} \\nabla_{\\boldsymbol{\\phi},\n\\boldsymbol{\\theta}} \\log w^{(i,l)} \\left( \\textbf{x}^{(i)},\n\\textbf{z}_{\\boldsymbol{\\phi}}^{(i,l)}, \\boldsymbol{\\theta} \\right)\n\\end{align}\n\\]\nHowever, computing the normalized importance weights \\(\\widetilde{w}^{(i,l)}\\) from the unnormalized logarithmic importance weights \\(\\log w^{(i,l)}\\) turns out to be problematic. To understand why, let’s look how the normalized importance weights are defined\n\\[\n\\widetilde{w}^{(i,l)} = \\frac {w^{(i, l)} } {\\sum_{l=1}^k w^{(i, l)}}\n\\]\nNote that \\(\\log w^{(i, l)} \\in [-\\infty, 0]\\) may be some big negative number. Simply taken the logs into the exp function and summing them up, is a bad idea for two reasons. Firstly, we might expect some rounding errors. Secondly, dividing by some really small number will likely produce nans. To circumvent this problem, there are two possible strategies:\n\nOriginal Implementation: While looking through the original implementation, I found that they simply shift the unnormalized logarithmic importance weights, i.e.,\n\\[\n\\log s^{(i, l)} = \\log w^{(i,l)} - \\underbrace{\\max_{l \\in [1, k]} \\log w^{(i,l)}}_{=a}\n\\]\nThen, the normalized importance weights can simply be calculated as follows\n\\[\n\\widetilde{w}^{(i,l)} = \\frac {\\exp \\left( \\log s^{(i, l)} \\right)} {\n\\sum_{l=1}^k \\exp \\left( \\log s^{(i,l)} \\right)} = \\frac { \\frac {\\exp \\left( \\log\nw^{(i, l)} \\right)}{\\exp a} } {\\sum_{l=1}^k \\frac {\\exp \\left( \\log\nw^{(i, l)} \\right)}{\\exp a} }\n\\]\nThe idea behind this approach is to increase numerical stability by shifting the logarithmic unnormalized importance weights into a range where less numerical issues occur (effectively simply increasing them).\nUse LogSumExp: Another common trick is to firstly calculate the normalized importance weights in log units. Then, we get\n\\[\n   \\log \\widetilde{w}^{(i, l)} = \\log \\frac {w^{(i,l)}}{\\sum_{l=1}^k\n   w^{(i,l)}} = \\log w^{(i, l)} - \\underbrace{\\log \\sum_{l=1}^k \\exp \\left( w^{(i,l)} \\right)}_{=\\text{torch.logsumexp}}\n\\]\n\n\n\n\nCode\nclass IWAE(VAE):\n\n    def __init__(self, k):\n        super(IWAE, self).__init__(k)\n        return\n\n    def compute_loss(self, x, k=None, mode='fast'):\n        if not k:\n            k = self.k\n        # compute unnormalized importance weights in log_units\n        log_likelihood, log_w = self.compute_marginal_log_likelihood(x, k)\n        # loss computation (several ways possible)\n        if mode == 'original':\n            ####################### ORIGINAL IMPLEMENTAION #######################\n            # numerical stability (found in original implementation)\n            log_w_minus_max = log_w - log_w.max(1, keepdim=True)[0]\n            # compute normalized importance weights (no gradient)\n            w = log_w_minus_max.exp()\n            w_tilde = (w / w.sum(axis=1, keepdim=True)).detach()\n            # compute loss (negative IWAE objective)\n            loss = -(w_tilde * log_w).sum(1).mean()\n        elif mode == 'normalized weights':\n            ######################## LOG-NORMALIZED TRICK ########################\n            # copmute normalized importance weights (no gradient)\n            log_w_tilde = log_w - torch.logsumexp(log_w, dim=1, keepdim=True)\n            w_tilde = log_w_tilde.exp().detach()\n            # compute loss (negative IWAE objective)\n            loss = -(w_tilde * log_w).sum(1).mean()\n        elif mode == 'fast':\n            ########################## SIMPLE AND FAST ###########################\n            loss = -log_likelihood\n        return loss\n\n\n\nTraining Procedure\n\n\n\nCode\nfrom torch.utils.data import DataLoader\nfrom livelossplot import PlotLosses\n\n\nBATCH_SIZE = 1000\nLEARNING_RATE = 1e-4\nWEIGHT_DECAY = 1e-6\n\n\ndef train(dataset, vae_model, iwae_model, num_epochs):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    print('Device: {}'.format(device))\n\n    data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True,\n                             num_workers=12)\n    vae_model.to(device)\n    iwae_model.to(device)\n\n    optimizer_vae = torch.optim.Adam(vae_model.parameters(), lr=LEARNING_RATE,\n                                     weight_decay=WEIGHT_DECAY)\n    optimizer_iwae = torch.optim.Adam(iwae_model.parameters(), lr=LEARNING_RATE,\n                                     weight_decay=WEIGHT_DECAY)\n    losses_plot = PlotLosses(groups={'Loss': ['VAE (ELBO)', 'IWAE (NLL)']})\n    for epoch in range(1, num_epochs + 1):\n        avg_NLL_VAE, avg_NLL_IWAE = 0, 0\n        for x in data_loader:\n            x = x.to(device)\n            # IWAE update\n            optimizer_iwae.zero_grad()\n            loss = iwae_model.compute_loss(x)\n            loss.backward()\n            optimizer_iwae.step()\n            avg_NLL_IWAE += loss.item() / len(data_loader)\n\n            # VAE update\n            optimizer_vae.zero_grad()\n            loss= vae_model.compute_loss(x)\n            loss.backward()\n            optimizer_vae.step()\n\n            avg_NLL_VAE += loss.item() / len(data_loader)\n        # plot current losses\n        losses_plot.update({'VAE (ELBO)': avg_NLL_VAE, 'IWAE (NLL)': avg_NLL_IWAE},\n                           current_step=epoch)\n        losses_plot.send()\n    trained_vae, trained_iwae = vae_model, iwae_model\n    return trained_vae, trained_iwae\n\n\n\n\nResults\nLet’s train both models for \\(k\\in \\{ 1, 10 \\}\\):\n\n\nCode\ntrain_ds = datasets.MNIST('./data', train=True,\n                          download=True, transform=transforms.ToTensor())\nnum_epochs = 50\nlist_of_ks = [1, 10]\nfor k in list_of_ks:\n    vae_model = VAE(k)\n    iwae_model = IWAE(k)\n    trained_vae, trained_iwae = train(train_ds, vae_model, iwae_model, num_epochs)\n    torch.save(trained_vae, f'./results/trained_vae_{k}.pth')\n    torch.save(trained_iwae, f'./results/trained_iwae_{k}.pth')\n\n\n\\(\\textbf{k=1}\\)  \\(\\textbf{k=10}\\) \nNote that during training, we compared the loss of the VAE (ELBO) with the loss of the IWAE (empirical estimate of marginal log-likelihood). Clearly, for \\(k=1\\) these losses are nearly equal (as expected). For \\(k=10\\), the difference is much greater (also expected). Now let’s compare the marginal log-likelihood on the test samples. Since the marginal log-likelihood estimator gets more accurate with increasing \\(k\\), we set \\(k=200\\) for the evaluation on the test set:\n\n\nCode\nfrom prettytable import PrettyTable\n\n\ndef compute_test_log_likelihood(test_dataset, trained_vae, trained_iwae, k=200):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n    data_loader = DataLoader(test_dataset, batch_size=20,\n                             shuffle=True, num_workers=12)\n    trained_vae.to(device)\n    trained_iwae.to(device)\n\n    avg_marginal_ll_VAE = 0\n    avg_marginal_ll_IWAE = 0\n    for x in data_loader:\n        marginal_ll, _ = trained_vae.compute_marginal_log_likelihood(x.to(device), k)\n        avg_marginal_ll_VAE += marginal_ll.item() / len(data_loader)\n\n        marginal_ll, _ = trained_iwae.compute_marginal_log_likelihood(x.to(device), k)\n        avg_marginal_ll_IWAE += marginal_ll.item() / len(data_loader)\n    return avg_marginal_ll_VAE, avg_marginal_ll_IWAE\n\n\nout_table = PrettyTable([\"k\", \"VAE\", \"IWAE\"])\ntest_ds = Binarized_MNIST('./data', train=False, download=True,\n                                  transform=transforms.ToTensor())\nfor k in list_of_ks:\n    # load models\n    trained_vae = torch.load(f'./results/trained_vae_{k}.pth')\n    trained_iwae = torch.load(f'./results/trained_iwae_{k}.pth')\n    # compute average marginal log-likelihood on test dataset\n    ll_VAE, ll_IWAE = compute_test_log_likelihood(test_ds, trained_vae, trained_iwae)\n    out_table.add_row([k, np.round(ll_VAE, 2), np.round(ll_IWAE, 2)])\nprint(out_table)\n\n\n\n\n\nResults NLL\n\n\nSimilar to the paper, the IWAE benefits from an increased \\(k\\) whereas the VAE performs nearly equal.\n\n\nVisualizations\nLastly, let’s make some nice plots. Note that the differences are very subtle and it’s not very helpful to make an argument based on the following visualization. They mainly serve as a verification that both models do something useful.\n\nReconstructions\n\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\n\n\ndef plot_reconstructions(vae_model, iwae_model, dataset, SEED=1):\n    np.random.seed(SEED)\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n    vae_model.to(device)\n    iwae_model.to(device)\n\n    n_samples = 7\n    i_samples = np.random.choice(range(len(dataset)), n_samples, replace=False)\n\n    fig = plt.figure(figsize=(10, 4))\n    plt.suptitle(\"Reconstructions\", fontsize=16, y=1, fontweight='bold')\n    for counter, i_sample in enumerate(i_samples):\n        orig_img = dataset[i_sample]\n        # plot original img\n        ax = plt.subplot(3, n_samples, 1 + counter)\n        plt.imshow(orig_img[0], vmin=0, vmax=1, cmap='gray')\n        plt.axis('off')\n        if counter == 0:\n            ax.annotate(\"input\", xy=(-0.1, 0.5), xycoords=\"axes fraction\",\n                        va=\"center\", ha=\"right\", fontsize=12)\n        # plot img reconstruction VAE\n        [x_tilde, z, mu_z, log_var_z] = vae_model(orig_img.unsqueeze(0).to(device))\n        ax = plt.subplot(3, n_samples, 1 + counter + n_samples)\n        x_tilde = x_tilde.squeeze(0)[0].detach().cpu().numpy()\n        plt.imshow(x_tilde[0], vmin=0, vmax=1, cmap='gray')\n        plt.axis('off')\n        if counter == 0:\n            ax.annotate(\"VAE recons\", xy=(-0.1, 0.5), xycoords=\"axes fraction\",\n                        va=\"center\", ha=\"right\", fontsize=12)\n        # plot img reconstruction IWAE\n        [x_tilde, z, mu_z, log_var_z] = iwae_model(orig_img.unsqueeze(0).to(device))\n        ax = plt.subplot(3, n_samples, 1 + counter + 2*n_samples)\n        x_tilde = x_tilde.squeeze(0)[0].detach().cpu().numpy()\n        plt.imshow(x_tilde[0], vmin=0, vmax=1, cmap='gray')\n        plt.axis('off')\n        if counter == 0:\n            ax.annotate(\"IWAE recons\", xy=(-0.1, 0.5), xycoords=\"axes fraction\",\n                        va=\"center\", ha=\"right\", fontsize=12)\n    return\n\n\nk = 10\ntrained_vae = torch.load(f'./results/trained_vae_{k}.pth')\ntrained_iwae = torch.load(f'./results/trained_iwae_{k}.pth')\nplot_reconstructions(trained_vae, trained_iwae , test_ds)\n\n\n\n\n\nReconstructions k=10\n\n\n\nLatent Traversals\n\n\n\nCode\ndef plot_latent_traversal(vae_model, iwae_model, dataset, SEED=1):\n    np.random.seed(SEED)\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n    vae_model.to(device)\n    iwae_model.to(device)\n\n    n_samples = 128\n    i_samples = np.random.choice(range(len(dataset)), n_samples, replace=False)\n    img_batch = torch.cat([dataset[i].unsqueeze(0) for i in i_samples], 0)\n    img_batch = img_batch.to(device)\n    # generate latent traversals\n    n_pert, pert_min_max, n_lats = 5, 2, 5\n    img_trav_vae = vae_model.create_latent_traversal(img_batch, n_pert, pert_min_max, n_lats)\n    img_trav_iwae = iwae_model.create_latent_traversal(img_batch, n_pert, pert_min_max, n_lats)\n\n    fig = plt.figure(figsize=(12, 7))\n    n_rows, n_cols = n_lats + 1, 2*n_pert + 1\n    gs = GridSpec(n_rows, n_cols + 1)\n    plt.suptitle(\"Latent Traversals\", fontsize=16, y=1, fontweight='bold')\n    for row_index in range(n_lats):\n        for col_index in range(n_pert):\n            img_rec_VAE = img_trav_vae[row_index][col_index]\n            img_rec_IWAE = img_trav_iwae[row_index][col_index]\n\n            ax = plt.subplot(gs[row_index, col_index])\n            plt.imshow(img_rec_VAE[0].detach(), cmap='gray', vmin=0, vmax=1)\n            plt.axis('off')\n\n            if row_index == 0 and col_index == int(n_pert//2):\n                plt.title('VAE', fontsize=14, y=1.1)\n\n            ax = plt.subplot(gs[row_index, col_index + n_pert + 1])\n            plt.imshow(img_rec_IWAE[0].detach(), cmap='gray', vmin=0, vmax=1)\n            plt.axis('off')\n            if row_index == 0 and col_index == int(n_pert//2):\n                plt.title('IWAE', fontsize=14, y=1.1)\n    # add pertubation magnitude\n    for ax in [plt.subplot(gs[n_lats, 0:5]), plt.subplot(gs[n_lats, 6:11])]:\n        ax.annotate(\"pertubation magnitude\", xy=(0.5, 0.6), xycoords=\"axes fraction\",\n                    va=\"center\", ha=\"center\", fontsize=10)\n        ax.set_frame_on(False)\n        ax.axes.set_xlim([-1.15 * pert_min_max, 1.15 * pert_min_max])\n        ax.xaxis.set_ticks([-pert_min_max, 0, pert_min_max])\n        ax.xaxis.set_ticks_position(\"top\")\n        ax.xaxis.set_tick_params(direction=\"inout\", pad=-16)\n        ax.get_yaxis().set_ticks([])\n    # add latent coordinate traversed annotation\n    ax = plt.subplot(gs[0:n_rows-1, n_cols])\n    ax.annotate(\"latent coordinate traversed\", xy=(0.4, 0.5), xycoords=\"axes fraction\",\n                    va=\"center\", ha=\"center\", fontsize=10, rotation=90)\n    plt.axis('off')\n    return\n\n\nk = 10\ntrained_vae = torch.load(f'./results/trained_vae_{k}.pth')\ntrained_iwae = torch.load(f'./results/trained_iwae_{k}.pth')\nplot_latent_traversal(trained_vae, trained_iwae , test_ds)\n\n\n\n\n\nLatent Traversal k=10"
  },
  {
    "objectID": "paper_summaries/iwae/index.html#footnotes",
    "href": "paper_summaries/iwae/index.html#footnotes",
    "title": "Importance Weighted Autoencoders",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSince the variance \\(\\text{diag}\n\\left(\\boldsymbol{\\sigma}^2_{\\text{E}}\\right)\\) needs to be greater than 0, we typically set the output to the variance in logarithmic units.↩︎"
  },
  {
    "objectID": "paper_summaries/spatial_broadcast_decoder/index.html",
    "href": "paper_summaries/spatial_broadcast_decoder/index.html",
    "title": "Spatial Broadcast Decoder: A Simple Architecture for Learning Disentangled Representations in VAEs",
    "section": "",
    "text": "Watters et al. (2019) introduce the Spatial Broadcast Decoder (SBD) as an architecture for the decoder in Variational Auto-Encoders (VAEs) to improve disentanglement in the latent space1, reconstruction accuracy and generalization in limited datasets (i.e., held-out regions in data space). Motivated by the limitations of deconvolutional layers in traditional decoders, these upsampling layers are replaced by a tiling operation in the Spatial Broadcast decoder. Furthermore, explicit spatial information (inductive bias) is appended in the form of coordinate channels leading to a simplified optimization problem and improved positional generalization. As a proof of concept, they tested the model on the colored sprites dataset (known factors of variation such as position, size, shape), Chairs and 3D Object-in-Room datasets (no positional variation), a dataset with small objects and a dataset with dependent factors. They could show that the Spatial Broadcast decoder can be used complementary or as an improvement to state-of-the-art disentangling techniques."
  },
  {
    "objectID": "paper_summaries/spatial_broadcast_decoder/index.html#model-description",
    "href": "paper_summaries/spatial_broadcast_decoder/index.html#model-description",
    "title": "Spatial Broadcast Decoder: A Simple Architecture for Learning Disentangled Representations in VAEs",
    "section": "Model Description",
    "text": "Model Description\nAs stated in the title, the model architecture of the Spatial Broadcast decoder is very simple: Take a standard VAE decoder and replace all upsampling deconvolutional layers by tiling the latent code \\(\\textbf{z}\\) across the original image space, appending fixed coordinate channels and applying an convolutional network with \\(1 \\times 1\\) stride, see the figure below.\n\n\n\n\n\n\n\n\nSchematic of the Spatial Broadcast VAE. In the decoder, the latent code \\(\\textbf{z}\\in\\mathbb{R}^{k}\\) is broadcasted (tiled) to the image width \\(w\\) and height \\(h\\). Additionally, two “coordinate” channels are appended. The result is fed to an unstrided convolutional decoder. (right) Pseudo-code of the spatial operation. Taken from Watters et al. (2019).\n\n\n\n\nMotivation: The presented architecture is mainly motivated by two reasons:\n\nDeconvolution layers cause optimization difficulties: Watters et al. (2019) argue that upsampling deconvolutional layers should be avoided, since these are prone to produce checkerboard artifacts, i.e., a checkerboard pattern can be identified on the resulting images (when looking closer), see figure below. These artifacts constrain the reconstruction accuracy and Watters et al. (2019) hypothesize that the resulting effects may raise problems for learning a disentangled representation in the latent space.\n\n\n\n\n\n\n\n\nA checkerboard pattern can often be identified in artifically generated images that use deconvolutional layers. Taken from Odena et al. (2016) (very worth reading).\n\n\n\nAppended coordinate channels improve positional generalization and optimization: Previous work by Liu et al. (2018) showed that standard convolution/deconvolution networks (CNNs) perform badly when trying to learn trivial coordinate transformations (e.g., learning a mapping from Cartesian space into one-hot pixel space or vice versa). This behavior may seem counterintuitive (easy task, small dataset), however the feature of translational equivariance (i.e., shifting an object in the input equally shifts its representation in the output) in CNNs2 hinders learning this task: The filters have by design no information about their position. Thus, coordinate transformations result in complicated functions which makes optimization difficult. E.g., changing the input coordinate slighlty might push the resulting function in a completelty different direction.\nCoordConv Solution: To overcome this problem, Liu et al. (2018) propose to append coordinate channels before convolution and term the resulting layer CoordConv, see figure below. In principle, this layer can learn to use or discard translational equivariance and keeps the other advantages of convolutional layers (fast computations, few parameters). Under this modification learning coordinate transformation problems works out of the box with perfect generalization in less time (150 times faster) and less memory (10-100 times fewer parameters). As coordinate transformations are implicitely needed in a variaty of tasks (such as producing bounding boxes in object detection) using CoordConv instead of standard convolutions might increase the performance of several other models.\n\n\n\n\n\n\n\n\nComparison of 2D convolutional and CoordConv layers. Taken from Liu et al. (2018).\n\n\n\nPositional Generalization: Appending fixed coordinate channels is mainly beneficial in datasets in which same objects may appear at distinct positions (i.e., there is positional variation). The main idea is that rendering an object at a specific position without spatial information (i.e., standard convolution/deconvolution) results in a very complicated function. In contrast,the Spatial Broadcast decoder architecture can leverage the spatial information to reveal objects easily: E.g., by convolving the positions in the latent space with the fixed coordinate channels and applying a threshold operation. Thus, Watters et al. (2019) argue that the Spatial Broadcast decoder architecture puts a prior on dissociating positional from non-positional features in the latent distribution. Datasets without positional variation in turn seem unlikely to benefit from this architecture. However, Watters et al. (2019) showed that the Spatial Broadcast decoder could still help in these datasets and attribute this to the replacement of deconvolutional layers."
  },
  {
    "objectID": "paper_summaries/spatial_broadcast_decoder/index.html#implementation",
    "href": "paper_summaries/spatial_broadcast_decoder/index.html#implementation",
    "title": "Spatial Broadcast Decoder: A Simple Architecture for Learning Disentangled Representations in VAEs",
    "section": "Implementation",
    "text": "Implementation\nWatters et al. (2019) conducted experiments with several datasets and could show that incorporating the Spatial Broadcast decoder into state-of-the-art VAE architectures consistently increased their perfomance. While this is impressive, it is always frustrating to not being able to reproduce results due to missing implementation details, less computing resources or simply not having enough time to work on a reimplementation.\nThe following reimplementation intends to eliminate that frustration by reproducing some of their experiments on much smaller datasets with similar characteristics such that training will take less time (less than 30 minutes with a NVIDIA Tesla K80 GPU).\n\nData Generation\nA dataset that is similar in spirit to the colored sprites dataset will be generated, i.e., procedurally generated objects from known factors of variation. Watters et al. (2019) use a binary dsprites dataset consisting of 737,280 images and transform these during training into colored images by uniformly sampling from a predefined HSV space (see Appendix A.3). As a result, the dataset has 8 factors of variation (\\(x\\)-position, \\(y\\)-position, size, shape, angle, 3D-color) with infinite samples (due to sampling of color). They used \\(1.5 \\cdot 10^6\\) training steps.\nTo reduce training time, we are going to generate a much simpler dataset consisting of \\(3675\\) images with a circle (fixed size) inside generated from a predefined set of possible colors and positions such that there are only 3 factors of variation (\\(x\\)-position, \\(y\\)-position, discretized color). In this case \\(3.4 \\cdot 10^2\\) training steps suffice for approximate convergence.\n\n\n\n\n\n\n\n\nVisualization of self-written Dataset\n\n\n\nThe code below creates the dataset. Note that it is kept more generic than necessary to allow the creation of several variations of this dataset, i.e., more dedicated experiments can be conducted.\n\n\n\n\n\n\n\n\n\nCode\nfrom PIL import Image, ImageDraw\nimport torchvision.transforms as transforms\nimport numpy as np\nimport torch\nfrom torch.utils.data import TensorDataset\n\n\ndef generate_img(x_position, y_position, shape, color, img_size, size=20):\n    \"\"\"Generate an RGB image from the provided latent factors\n\n    Args:\n        x_position (float): normalized x position\n        y_position (float): normalized y position\n        shape (string): can only be 'circle' or 'square'\n        color (string): color name or rgb string\n        img_size (int): describing the image size (img_size, img_size)\n        size (int): size of shape\n\n    Returns:\n        torch tensor [3, img_size, img_size] (dtype=torch.float32)\n    \"\"\"\n    # creation of image\n    img = Image.new('RGB', (img_size, img_size), color='black')\n    # map (x, y) position to pixel coordinates\n    x_position = (img_size - 2 - size) * x_position\n    y_position = (img_size - 2 - size) * y_position\n    # define coordinates\n    x_0, y_0 = x_position, y_position\n    x_1, y_1 = x_position + size, y_position + size\n    # draw shapes\n    img1 = ImageDraw.Draw(img)\n    if shape == 'square':\n        img1.rectangle([(x_0, y_0), (x_1, y_1)], fill=color)\n    elif shape == 'circle':\n        img1.ellipse([(x_0, y_0), (x_1, y_1)], fill=color)\n    return transforms.ToTensor()(img).type(torch.float32)\n\n\ndef generate_dataset(img_size, shape_sizes, num_pos, shapes, colors):\n    \"\"\"procedurally generated from 4 ground truth independent latent factors,\n       these factors are/can be\n           Position X: num_pos values in [0, 1]\n           Poistion Y: num_pos values in [0, 1]\n           Shape: square, circle\n           Color: standard HTML color name or 'rgb(x, y, z)'\n\n    Args:\n           img_size (int): describing the image size (img_size, img_size)\n           shape_sizes (list): sizes of shapes\n           num_pos (int): discretized positions\n           shapes (list): shapes (can only be 'circle', 'square')\n           colors (list): colors\n\n    Returns:\n           data: torch tensor [n_samples, 3, img_size, img_size]\n           latents: each entry describes the latents of corresp. data entry\n    \"\"\"\n    num_shapes, num_colors, sizes = len(shapes), len(colors), len(shape_sizes)\n\n    n_samples = num_pos*num_pos*num_shapes*num_colors*sizes\n    data = torch.empty([n_samples, 3, img_size, img_size])\n    latents = np.empty([n_samples], dtype=object)\n\n    index = 0\n    for x_pos in np.linspace(0, 1, num_pos):\n        for y_pos in np.linspace(0, 1, num_pos):\n            for shape in shapes:\n                for size in shape_sizes:\n                    for color in colors:\n                        img = generate_img(x_pos, y_pos, shape, color,\n                                           img_size, size)\n                        data[index] = img\n                        latents[index] = [x_pos, y_pos, shape, color]\n\n                        index += 1\n    return data, latents\n\n\ncircles_data, latents = generate_dataset(img_size=64, shape_sizes=[16],\n                                         num_pos=35,\n                                         shapes=['circle'],\n                                         colors=['red', 'green', 'blue'])\nsprites_dataset = TensorDataset(circles_data)\n\n\n\n\nModel Implementation\nAlthough in principle implementing a VAE is fairly simple (see my post for details), in practice one must choose lots of hyperparmeters. These can be divided into three broader categories:\n\nEncoder/Decoder and Prior Distribution: As suggested by Watters et al. (2019) in Appendix A, we use a Gaussian decoder distribution with fixed diagonal covariance structure \\(p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}^\\prime | \\textbf{z}^{(i)}\\right) = \\mathcal{N}\\left( \\textbf{x}^\\prime |\n  \\boldsymbol{\\mu}_D^{(i)}, \\sigma^2 \\textbf{I} \\right)\\), hence the reconstruction accuracy can be calculated as follows3\n\\[\n  \\text{Reconstruction Acc.} = \\log p_{\\boldsymbol{\\theta}} \\left(\n  \\textbf{x}^{(i)} | \\textbf{z}^{(i)} \\right) = - \\frac {1}{2 \\sigma^2}\n  \\sum_{k=1}^{D} \\left(x_k^{(i)} - \\mu_{D_k}^{(i)} \\right)^2 + \\text{const}.\n  \\]\nFor the encoder distribution a Gaussian with diagonal covariance \\(q_{\\boldsymbol{\\phi}} \\sim\n  \\mathcal{N} \\left( \\textbf{z} | \\boldsymbol{\\mu}_E,\n  \\boldsymbol{\\sigma}_D^2 \\textbf{I} \\right)\\) and as prior a centered multivariate Gaussian \\(p_{\\boldsymbol{\\theta}}\n  (\\textbf{z}) = \\mathcal{N}\\left( \\textbf{z} | \\textbf{0}, \\textbf{I} \\right)\\) are chosen (both typical choices).\nNetwork Architecture for Encoder/Decoder: The network architectures for the standard encoder and decoder consist of convolutional and deconvolutional layers (since these perform typically much better on image data). The Spatial Broadcast decoder defines a different kind of architecture, see Model Description. The exact architectures are taken from Appendix A.1 of Watters et al., see code below4:\n\n\nCode\nfrom torch import nn\n\n\nclass Encoder(nn.Module):\n    \"\"\"\"Encoder class for use in convolutional VAE\n\n    Args:\n        latent_dim: dimensionality of latent distribution\n\n    Attributes:\n        encoder_conv: convolution layers of encoder\n        fc_mu: fully connected layer for mean in latent space\n        fc_log_var: fully connceted layers for log variance in latent space\n    \"\"\"\n\n    def __init__(self, latent_dim=6):\n        super().__init__()\n        self.latent_dim = latent_dim\n\n        self.encoder_conv = nn.Sequential(\n            # shape: [batch_size, 3, 64, 64]\n            nn.Conv2d(3,  64, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(),\n            # shape: [batch_size, 64, 32, 32]\n            nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(),\n            # shape: [batch_size, 64, 4, 4],\n            nn.Flatten(),\n            # shape: [batch_size, 1024]\n            nn.Linear(1024, 256),\n            nn.ReLU(),\n            # shape: [batch_size, 256]\n        )\n        self.fc_mu = nn.Sequential(\n            nn.Linear(in_features=256, out_features=self.latent_dim),\n        )\n        self.fc_log_var = nn.Sequential(\n            nn.Linear(in_features=256, out_features=self.latent_dim),\n        )\n        return\n\n    def forward(self, inp):\n        out = self.encoder_conv(inp)\n        mu = self.fc_mu(out)\n        log_var = self.fc_log_var(out)\n        return [mu, log_var]\n\n\nclass Decoder(nn.Module):\n    \"\"\"(standard) Decoder class for use in convolutional VAE,\n    a Gaussian distribution with fixed variance (identity times fixed variance\n    as covariance matrix) used as the decoder distribution\n\n    Args:\n        latent_dim: dimensionality of latent distribution\n        fixed_variance: variance of distribution\n\n    Attributes:\n        decoder_upsampling: linear upsampling layer(s)\n        decoder_deconv: deconvolution layers of decoder (also upsampling)\n    \"\"\"\n\n    def __init__(self, latent_dim, fixed_variance):\n        super().__init__()\n        self.latent_dim = latent_dim\n        self.coder_type = 'Gaussian with fixed variance'\n        self.fixed_variance = fixed_variance\n\n        self.decoder_upsampling = nn.Sequential(\n            nn.Linear(self.latent_dim, 256),\n            nn.ReLU(),\n            # reshaped into [batch_size, 64, 2, 2]\n        )\n        self.decoder_deconv = nn.Sequential(\n            # shape: [batch_size, 64, 2, 2]\n            nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(),\n            # shape: [batch_size, 64, 4, 4]\n            nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(64,  3, kernel_size=4, stride=2, padding=1),\n            # shape: [batch_size, 3, 64, 64]\n        )\n        return\n\n    def forward(self, inp):\n        ups_inp = self.decoder_upsampling(inp)\n        ups_inp = ups_inp.view(-1, 64, 2, 2)\n        mu = self.decoder_deconv(ups_inp)\n        return mu\n\n\nclass SpatialBroadcastDecoder(nn.Module):\n    \"\"\"SBD class for use in convolutional VAE,\n      a Gaussian distribution with fixed variance (identity times fixed\n      variance as covariance matrix) used as the decoder distribution\n\n    Args:\n        latent_dim: dimensionality of latent distribution\n        fixed_variance: variance of distribution\n\n    Attributes:\n        img_size: image size (necessary for tiling)\n        decoder_convs: convolution layers of decoder (also upsampling)\n    \"\"\"\n\n    def __init__(self, latent_dim, fixed_variance):\n        super().__init__()\n        self.img_size = 64\n        self.coder_type = 'Gaussian with fixed variance'\n        self.latent_dim = latent_dim\n        self.fixed_variance = fixed_variance\n\n        x = torch.linspace(-1, 1, self.img_size)\n        y = torch.linspace(-1, 1, self.img_size)\n        x_grid, y_grid = torch.meshgrid(x, y, indexing=\"ij\")\n        # reshape into [1, 1, img_size, img_size] and save in state_dict\n        self.register_buffer('x_grid', x_grid.view((1, 1) + x_grid.shape))\n        self.register_buffer('y_grid', y_grid.view((1, 1) + y_grid.shape))\n\n        self.decoder_convs = nn.Sequential(\n            # shape [batch_size, latent_dim + 2, 64, 64]\n            nn.Conv2d(in_channels=self.latent_dim+2, out_channels=64,\n                      stride=(1, 1), kernel_size=(3,3), padding=1),\n            nn.ReLU(),\n            # shape [batch_size, 64, 64, 64]\n            nn.Conv2d(in_channels=64, out_channels=64, stride=(1,1),\n                      kernel_size=(3, 3), padding=1),\n            nn.ReLU(),\n            # shape [batch_size, 64, 64, 64]\n            nn.Conv2d(in_channels=64, out_channels=3, stride=(1,1),\n                      kernel_size=(3, 3), padding=1),\n            # shape [batch_size, 3, 64, 64]\n        )\n        return\n\n    def forward(self, z):\n        batch_size = z.shape[0]\n        # reshape z into [batch_size, latent_dim, 1, 1]\n        z = z.view(z.shape + (1, 1))\n        # tile across image [batch_size, latent_im, img_size, img_size]\n        z_b = z.expand(-1, -1, self.img_size, self.img_size)\n        # upsample x_grid and y_grid to [batch_size, 1, img_size, img_size]\n        x_b = self.x_grid.expand(batch_size, -1, -1, -1)\n        y_b = self.y_grid.expand(batch_size, -1, -1, -1)\n        # concatenate vectors [batch_size, latent_dim+2, img_size, img_size]\n        z_sb = torch.cat((z_b, x_b, y_b), dim=1)\n        # apply convolutional layers\n        mu_D = self.decoder_convs(z_sb)\n        return mu_D\n\n\nThe VAE implementation below combines the encoder and decoder architectures (slightly modified version of my last VAE implementation).\n\n\nCode\nfrom torch.distributions.multivariate_normal import MultivariateNormal\n\n\nclass VAE(nn.Module):\n    \"\"\"A simple VAE class\n\n    Args:\n        vae_tpe: type of VAE either 'Standard' or 'SBD'\n        latent_dim: dimensionality of latent distribution\n        fixed_var: fixed variance of decoder distribution\n    \"\"\"\n\n    def __init__(self, vae_type, latent_dim, fixed_var):\n        super().__init__()\n        self.vae_type = vae_type\n\n        if self.vae_type == 'Standard':\n            self.decoder = Decoder(latent_dim=latent_dim,\n                                  fixed_variance=fixed_var)\n        else:\n            self.decoder = SpatialBroadcastDecoder(latent_dim=latent_dim,\n                                                   fixed_variance=fixed_var)\n\n        self.encoder = Encoder(latent_dim=latent_dim)\n        self.normal_dist = MultivariateNormal(torch.zeros(latent_dim),\n                                              torch.eye(latent_dim))\n        return\n\n    def forward(self, x):\n        z, mu_E, log_var_E = self.encode(x)\n        # regularization term per batch, i.e., size: (batch_size)\n        regularization_term = 0.5 * (1 + log_var_E - mu_E**2\n                                      - torch.exp(log_var_E)).sum(axis=1)\n\n        batch_size = x.shape[0]\n        if self.decoder.coder_type == 'Gaussian with fixed variance':\n            # x_rec has shape (batch_size, 3, 64, 64)\n            x_rec = self.decode(z)\n            # reconstruction accuracy per batch, i.e., size: (batch_size)\n            factor = 0.5 * (1/self.decoder.fixed_variance)\n            recons_acc = - factor * ((x.view(batch_size, -1) -\n                                    x_rec.view(batch_size, -1))**2\n                                  ).sum(axis=1)\n        return -regularization_term.mean(), -recons_acc.mean()\n\n    def reconstruct(self, x):\n        mu_E, log_var_E = self.encoder(x)\n        x_rec = self.decoder(mu_E)\n        return x_rec\n\n    def encode(self, x):\n        # get encoder distribution parameters\n        mu_E, log_var_E = self.encoder(x)\n        # sample noise variable for each batch\n        batch_size = x.shape[0]\n        epsilon = self.normal_dist.sample(sample_shape=(batch_size, )\n                                          ).to(x.device)\n        # get latent variable by reparametrization trick\n        z = mu_E + torch.exp(0.5*log_var_E) * epsilon\n        return z, mu_E, log_var_E\n\n    def decode(self, z):\n        # get decoder distribution parameters\n        mu_D = self.decoder(z)\n        return mu_D\n\n\nTraining Parameters: Lastly, training neural networks itself consists of several hyperparmeters. Again, we are using the same setup as defined in Appendix A.1 of Watters et al. (2019), see code below.\n\n\nCode\nfrom livelossplot import PlotLosses\nfrom torch.utils.data import DataLoader\n\n\ndef train(dataset, epochs, VAE):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n    print('Device: {}'.format(device))\n\n    data_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n\n    VAE.to(device)\n    optimizer = torch.optim.Adam(VAE.parameters(), lr=3e-4)\n\n    losses_plot = PlotLosses(groups={'avg log loss':\n                                    ['kl loss', 'reconstruction loss']})\n    print('Start training with {} decoder\\n'.format(VAE.vae_type))\n    for epoch in range(1, epochs +1):\n        avg_kl = 0\n        avg_recons_err = 0\n        for counter, mini_batch_data in enumerate(data_loader):\n            VAE.zero_grad()\n\n            kl_div, recons_err = VAE(mini_batch_data[0].to(device))\n            loss = kl_div + recons_err\n            loss.backward()\n            optimizer.step()\n\n            avg_kl += kl_div.item() / len(dataset)\n            avg_recons_err += recons_err.item() / len(dataset)\n\n        losses_plot.update({'kl loss': np.log(avg_kl),\n                            'reconstruction loss': np.log(avg_recons_err)})\n        losses_plot.send()\n    trained_VAE = VAE\n    return trained_VAE\n\n\n\n\n\nVisualization Functions\nEvaluating the representation quality of trained models is a difficult task, since we are not only interested in the reconstruction accuracy but also in the latent space and its properties. Ideally the latent space offers a disentangled representation such that each latent variable represents a factor of variation with perfect reconstruction accuracy (i.e., for evaluation it is very helpful to know in advance how many and what factors of variation exist). Although there are some metrics to quantify disentanglement, many of them have serious shortcomings and there is yet no consensus in the literature which to use (Watters et al., 2019). Instead of focusing on some metric, we are going to visualize the results by using two approaches:\n\nReconstructions and Latent Traversals: A very popular and helpful plot is to show some (arbitrarly chosen) reconstructions compared to the original input together with a series of latent space traversals. I.e., taking some encoded input and looking at the reconstructions when sweeping each coordinate in the latent space in a predefined interval (here from -2 to +2) while keeping all other coordinates constant. Ideally, each sweep can be associated with a factor of variation. The code below will be used to generate these plots. Note that the reconstructions are clamped into \\([0, 1]\\) as this is the allowed image range.\n\n\nCode\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\ndef reconstructions_and_latent_traversals(STD_VAE, SBD_VAE, dataset, SEED=1):\n    np.random.seed(SEED)\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    latent_dims = STD_VAE.encoder.latent_dim\n\n    n_samples = 7\n    i_samples = np.random.choice(range(len(dataset)), n_samples, replace=False)\n\n    # preperation for latent traversal\n    i_latent = i_samples[n_samples//2]\n    lat_image = dataset[i_latent][0]\n    sweep = np.linspace(-2, 2, n_samples)\n\n    fig = plt.figure(constrained_layout=False, figsize=(2*n_samples, 2+latent_dims))\n    grid = plt.GridSpec(latent_dims + 5, n_samples*2 + 3,\n                        hspace=0.2, wspace=0.02, figure=fig)\n    # standard VAE\n    for counter, i_sample in enumerate(i_samples):\n        orig_image = dataset[i_sample][0]\n        # original\n        main_ax = fig.add_subplot(grid[1, counter + 1])\n        main_ax.imshow(transforms.ToPILImage()(orig_image))\n        main_ax.axis('off')\n        main_ax.set_aspect('equal')\n\n        # reconstruction\n        x_rec = STD_VAE.reconstruct(orig_image.unsqueeze(0).to(device))\n        # clamp output into [0, 1] and prepare for plotting\n        recons_image =  torch.clamp(x_rec, 0, 1).squeeze(0).cpu()\n\n        main_ax = fig.add_subplot(grid[2, counter + 1])\n        main_ax.imshow(transforms.ToPILImage()(recons_image))\n        main_ax.axis('off')\n        main_ax.set_aspect('equal')\n    # latent dimension traversal\n    z, mu_E, log_var_E = STD_VAE.encode(lat_image.unsqueeze(0).to(device))\n    for latent_dim in range(latent_dims):\n        for counter, z_replaced in enumerate(sweep):\n            z_new = z.detach().clone()\n            z_new[0][latent_dim] = z_replaced\n\n            # clamp output into [0, 1] and prepare for plotting\n            img_rec = torch.clamp(STD_VAE.decode(z_new), 0, 1).squeeze(0).cpu()\n\n            main_ax = fig.add_subplot(grid[4 + latent_dim, counter + 1])\n            main_ax.imshow(transforms.ToPILImage()(img_rec))\n            main_ax.axis('off')\n    # SBD VAE\n    for counter, i_sample in enumerate(i_samples):\n        orig_image = dataset[i_sample][0]\n        # original\n        main_ax = fig.add_subplot(grid[1, counter + n_samples + 2])\n        main_ax.imshow(transforms.ToPILImage()(orig_image))\n        main_ax.axis('off')\n        main_ax.set_aspect('equal')\n        # reconstruction\n        x_rec = SBD_VAE.reconstruct(orig_image.unsqueeze(0).to(device))\n        # clamp output into [0, 1] and prepare for plotting\n        recons_image = torch.clamp(x_rec, 0, 1).squeeze(0).cpu()\n\n        main_ax = fig.add_subplot(grid[2, counter + n_samples + 2])\n        main_ax.imshow(transforms.ToPILImage()(recons_image))\n        main_ax.axis('off')\n        main_ax.set_aspect('equal')\n    # latent dimension traversal\n    z, mu_E, log_var_E = SBD_VAE.encode(lat_image.unsqueeze(0).to(device))\n    for latent_dim in range(latent_dims):\n        for counter, z_replaced in enumerate(sweep):\n            z_new = z.detach().clone()\n            z_new[0][latent_dim] = z_replaced\n            # clamp output into [0, 1] and prepare for plotting\n            img_rec = torch.clamp(SBD_VAE.decode(z_new), 0, 1).squeeze(0).cpu()\n\n            main_ax = fig.add_subplot(grid[4+latent_dim, counter+n_samples+2])\n            main_ax.imshow(transforms.ToPILImage()(img_rec))\n            main_ax.axis('off')\n    # prettify by adding annotation texts\n    fig = prettify_with_annotation_texts(fig, grid, n_samples, latent_dims)\n    return fig\n\ndef prettify_with_annotation_texts(fig, grid, n_samples, latent_dims):\n    # figure titles\n    titles = ['Deconv Reconstructions', 'Spatial Broadcast Reconstructions',\n              'Deconv Traversals', 'Spatial Broadcast Traversals']\n    idx_title_pos = [[0, 1, n_samples+1], [0, n_samples+2, n_samples*2+2],\n                    [3, 1, n_samples+1], [3, n_samples+2, n_samples*2+2]]\n    for title, idx_pos in zip(titles, idx_title_pos):\n        fig_ax = fig.add_subplot(grid[idx_pos[0], idx_pos[1]:idx_pos[2]])\n        fig_ax.annotate(title, xy=(0.5, 0), xycoords='axes fraction',\n                        fontsize=14, va='bottom', ha='center')\n        fig_ax.axis('off')\n    # left annotations\n    fig_ax = fig.add_subplot(grid[1, 0])\n    fig_ax.annotate('input', xy=(1, 0.5), xycoords='axes fraction',\n                    fontsize=12,  va='center', ha='right')\n    fig_ax.axis('off')\n    fig_ax = fig.add_subplot(grid[2, 0])\n    fig_ax.annotate('recons', xy=(1, 0.5), xycoords='axes fraction',\n                    fontsize=12, va='center', ha='right')\n    fig_ax.axis('off')\n    fig_ax = fig.add_subplot(grid[4:latent_dims + 4, 0])\n    fig_ax.annotate('latent coordinate traversed', xy=(0.9, 0.5),\n                    xycoords='axes fraction', fontsize=12,\n                    va='center', ha='center', rotation=90)\n    fig_ax.axis('off')\n    # pertubation magnitude\n    for i_y_grid in [[1, n_samples+1], [n_samples+2, n_samples*2+2]]:\n        fig_ax = fig.add_subplot(grid[latent_dims + 4, i_y_grid[0]:i_y_grid[1]])\n        fig_ax.annotate('pertubation magnitude', xy=(0.5, 0),\n                        xycoords='axes fraction', fontsize=12,\n                        va='bottom', ha='center')\n        fig_ax.set_frame_on(False)\n        fig_ax.axes.set_xlim([-2.5, 2.5])\n        fig_ax.xaxis.set_ticks([-2, 0, 2])\n        fig_ax.xaxis.set_ticks_position('top')\n        fig_ax.xaxis.set_tick_params(direction='inout', pad=-16)\n        fig_ax.get_yaxis().set_ticks([])\n    # latent dim\n    for latent_dim in range(latent_dims):\n        fig_ax = fig.add_subplot(grid[4 + latent_dim, n_samples*2 + 2])\n        fig_ax.annotate('lat dim ' + str(latent_dim + 1), xy=(0, 0.5),\n                        xycoords='axes fraction',\n                        fontsize=12, va='center', ha='left')\n        fig_ax.axis('off')\n    return\n\n\nLatent Space Geometry: While latent traversals may be helpful, Watters et al. (2019) note that this techniques suffers from two shortcommings:\n\nLatent space entanglement might be difficult to perceive by eye.\nTraversals are only taken at some point in space. It could be that traversals at some points are more disentangled than at other positions. Thus, judging disentanglement by the aforementioned method might be ultimately dependent to randomness.\n\nTo overcome these limitations, they propose a new method which they term latent space geometry. The main idea is to visualize a transformation from a 2-dimensional generative factor space (subspace of all generative factors) into the 2-dimensional latent subspace (choosing the two latent components that correspond to the factors of variation). Latent space geometry that preserves the chosen geometry of the generative factor space (while scaling and rotation might be allowed depending on the chosen generative factor space) indicates disentanglement.\nTo put this into practice, the code below creates circle images by varying \\(x\\) and \\(y\\) positions uniformly and keeping the other generative factors (here only color) constant. Accordingly, the geometry of the generative factor space is a uniform grid (which will be plotted). These images will be encoded into mean and variance of the latent distribution. In order to find the latent components that correspond to the \\(x\\) and \\(y\\) position, we choose the components with smallest mean variance across all reconstructions, i.e., the most informative components5. Then, we can plot the latent space geometry by using the latent components of the mean (encoder distribution), see code below.\n\n\nCode\ndef latent_space_geometry(STD_VAE, SBD_VAE):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    plt.figure(figsize=(18, 6))\n    # x,y position grid in [0.2, 0.8] (generative factors)\n    equi = np.linspace(0.2, 0.8, 31)\n    equi_without_vert = np.setdiff1d(equi, np.linspace(0.2, 0.8, 6))\n\n    x_pos = np.append(np.repeat(np.linspace(0.2, 0.8, 6), len(equi)),\n                      np.tile(equi_without_vert, 6))\n    y_pos = np.append(np.tile(equi, 6),\n                      np.repeat(np.linspace(0.8, 0.2, 6), len(equi_without_vert)))\n    labels = np.append(np.repeat(np.arange(6), 31),\n                      np.repeat(np.arange(6)+10, 25))\n    # plot generative factor geometry\n    plt.subplot(1, 3, 1)\n    plt.scatter(x_pos, y_pos, c=labels, cmap=plt.cm.get_cmap('rainbow', 10))\n    plt.gca().set_title('Ground Truth Factors', fontsize=16)\n    plt.xlabel('X-Position')\n    plt.ylabel('Y-Position')\n\n    # generate images\n    img_size = 64\n    shape_size = 16\n    images = torch.empty([len(x_pos), 3, img_size, img_size]).to(device)\n    for counter, (x, y) in enumerate(zip(x_pos, y_pos)):\n        images[counter] = generate_img(x, y, 'circle', 'red',\n                                      img_size, shape_size)\n\n    # STD VAE\n    [all_mu, all_log_var] = STD_VAE.encoder(images)\n    # most informative latent variable\n    lat_1, lat_2 = all_log_var.mean(axis=0).sort()[1][:2]\n    # latent coordinates\n    x_lat = all_mu[:, lat_1].detach().cpu().numpy()\n    y_lat = all_mu[:, lat_2].detach().cpu().numpy()\n    # plot latent space geometry\n    plt.subplot(1, 3, 2)\n    plt.scatter(x_lat, y_lat, c=labels, cmap=plt.cm.get_cmap('rainbow', 10))\n    plt.gca().set_title('DeConv', fontsize=16)\n    plt.xlabel('latent 1 value')\n    plt.ylabel('latent 2 value')\n\n    # SBD VAE\n    [all_mu, all_log_var] = SBD_VAE.encoder(images)\n    # most informative latent variable\n    lat_1, lat_2 = all_log_var.mean(axis=0).sort()[1][:2]\n    # latent coordinates\n    x_lat = all_mu[:, lat_1].detach().cpu().numpy()\n    y_lat = all_mu[:, lat_2].detach().cpu().numpy()\n    # plot latent space geometry\n    plt.subplot(1, 3, 3)\n    plt.scatter(x_lat, y_lat, c=labels, cmap=plt.cm.get_cmap('rainbow', 10))\n    plt.gca().set_title('Spatial Broadcast', fontsize=16)\n    plt.xlabel('latent 1 value')\n    plt.ylabel('latent 2 value')\n    return\n\n\n\n\n\nResults\nLastly, let’s train our models and look at the results:\n\n\nCode\nepochs = 150\nlatent_dims = 5 # x position, y position, color, extra slots\nfixed_variance = 0.3\n\nstandard_VAE = VAE(vae_type='Standard', latent_dim=latent_dims,\n                   fixed_var=fixed_variance)\nSBD_VAE = VAE(vae_type='SBD', latent_dim=latent_dims,\n              fixed_var=fixed_variance)\n\n\n\n\nCode\ntrained_standard_VAE  = train(sprites_dataset, epochs, standard_VAE)\n\n\n\n\nCode\ntrained_SBD_VAE = train(sprites_dataset, epochs, SBD_VAE)\n\n\nAt the log-losses plots, we can already see that using the Spatial Broadcast decoder results in an improved reconstruction accuracy and regularization term. Now let’s compare both models visually by their\n\nReconstructions and Latent Traversals:\n\n\nCode\nreconstructions_and_latent_traversals(trained_standard_VAE,\n                                      trained_SBD_VAE, sprites_dataset)\n\n\nWhile the reconstructions within both models look pretty good, the latent space traversal shows an entangled representation in the standard (DeConv) VAE whereas the Spatial Broadcast model seems quite disentangled.\nLatent Space Geometry:\n\n\nCode\nlatent_space_geometry(trained_standard_VAE, trained_SBD_VAE)\n\n\nThe latent space geometry verifies our previous findings: The DeConv decoder has an entangled latent space (transformation is highly non linear) whereas in the Spatial Broadcast decoder the latent space geometry highly resembles the generating factors geometry (affine transformation). The transformation of the Spatial Broadcast decoder indicates very similar behavior in the \\(X-Y\\) position subspace (of generative factors) as in the corresponding latent subspace."
  },
  {
    "objectID": "paper_summaries/spatial_broadcast_decoder/index.html#drawbacks-of-paper",
    "href": "paper_summaries/spatial_broadcast_decoder/index.html#drawbacks-of-paper",
    "title": "Spatial Broadcast Decoder: A Simple Architecture for Learning Disentangled Representations in VAEs",
    "section": "Drawbacks of Paper",
    "text": "Drawbacks of Paper\n\nalthough there are fewer parameters in the Spatial Broadcast decoder, it does require more memory (in the implementation about 50% more)\nlonger training times compared to standard DeConv VAE\nappended coordinate channels do not help when there is no positional variation"
  },
  {
    "objectID": "paper_summaries/spatial_broadcast_decoder/index.html#acknowledgement",
    "href": "paper_summaries/spatial_broadcast_decoder/index.html#acknowledgement",
    "title": "Spatial Broadcast Decoder: A Simple Architecture for Learning Disentangled Representations in VAEs",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nDaniel Daza’s blog was really helpful and the presented code is highly inspired by his VAE-SBD implementation."
  },
  {
    "objectID": "paper_summaries/spatial_broadcast_decoder/index.html#footnotes",
    "href": "paper_summaries/spatial_broadcast_decoder/index.html#footnotes",
    "title": "Spatial Broadcast Decoder: A Simple Architecture for Learning Disentangled Representations in VAEs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs outlined by Watters et al. (2019), there is “yet no consensus on the definition of a disentangled representation”. However, in their paper they focus on feature compositionality (i.e., composing a scene in terms of independent features such as color and object) and refer to it as disentangled representation.↩︎\nIn typical image classification problems, translational equivariance is highly valued since it ensures that if a filter detects an object (e.g., edges), it will detect it irrespective of its position.↩︎\nFor simplicity, we are setting the number of (noise variable) samples \\(L\\) per datapoint to 1 (see equation for \\(\\displaystyle \\widetilde{\\mathcal{L}}\\) in Reparametrization Trick paragraph). Note that Kingma and Welling (2013) stated that in their experiments setting \\(L=1\\) sufficed as long as the minibatch size was large enough.↩︎\nThe Spatial Broadcast decoder architecture is slightly modified: Kernel size of 3 instead of 4 to get the desired output shapes.↩︎\nAn intuitve way to understand why latent compontents with smaller variance within the encoder distribution are more informative than others is to think about the sampled noise and the loss function: If the variance is high, the latent code \\(\\textbf{z}\\) will vary a lot which in turn makes the task for the decoder more difficult. However, the regularization term (KL-divergence) pushes the variances towards 1. Thus, the network will only reduce the variance of its components if it helps to increase the reconstruction accuracy.↩︎"
  },
  {
    "objectID": "paper_summaries/spatial_transformer/index.html",
    "href": "paper_summaries/spatial_transformer/index.html",
    "title": "Spatial Transformer Networks",
    "section": "",
    "text": "Jaderberg et al. (2015) introduced the learnable Spatial Transformer (ST) module that can be used to empower standard neural networks to actively spatially transform feature maps or input data. In essence, the ST can be understood as a black box that applies some spatial transformation (e.g., crop, scale, rotate) to a given input (or part of it) conditioned on the particular input during a single forward path. In general, STs can also be seen as a learnable attention mechanism (including spatial transformation on the region of interest). Notably, STs can be easily integrated in existing neural network architectures without any supervision or modification to the optimization, i.e., STs are differentiable plug-in modules. The authors could show that STs help the models to learn invariances to translation, scale, rotation and more generic warping which resulted in state-of-the-art performance on several benchmarks, see image below."
  },
  {
    "objectID": "paper_summaries/spatial_transformer/index.html#model-description",
    "href": "paper_summaries/spatial_transformer/index.html#model-description",
    "title": "Spatial Transformer Networks",
    "section": "Model Description",
    "text": "Model Description\nThe aim of STs is to provide neural networks with spatial transformation and attention capabilities in a reasonable and efficient way. Note that standard neural network architectures (e.g., CNNs) are limited in this regard1. Therefore, the ST constitutes parametrized transformations \\(\\mathcal{T}_{\\boldsymbol{\\theta}}\\) that transform the regular input grid to a new sampling grid, see image below. Then, some form of interpolation is used to compute the pixel values in the new sampling grid (i.e., interpolation between values of the old grid).\n\n\n\n\n\n\n\n\nTwo examples of applying the parametrised sampling grid to an image \\(\\textbf{U}\\) producing the output \\(\\textbf{V}\\). The green dots represent the new sampling grid which is obtained by transforming the regular grid \\(\\textbf{G}\\) (defined on \\(\\textbf{V}\\)) using the transformation \\(\\mathcal{T}\\).  (a) The sampling grid is the regular grid \\(\\textbf{G} = \\mathcal{T}_{\\textbf{I}} (\\textbf{G})\\), where \\(\\textbf{I}\\) is the identity transformation matrix.  (b) The sampling grid is the result of warping the regular grid with an affine transformation \\(\\mathcal{T}_{\\boldsymbol{\\theta}} (\\textbf{G})\\).  Taken from Jaderberg et al. (2015).\n\n\n\nTo this end, the ST is divided into three consecutive parts:\n\nLocalisation Network: Its purpose is to retrieve the parameters \\(\\boldsymbol{\\theta}\\) of the spatial transformation \\(\\mathcal{T}_{\\boldsymbol{\\theta}}\\) taking the current feature map \\(\\textbf{U}\\) as input, i.e., \\(\\boldsymbol{\\theta} = f_{\\text{loc}}\n\\left(\\textbf{U} \\right)\\). Thereby, the spatial transformation is conditioned on the input. Note that dimensionality of \\(\\boldsymbol{\\theta}\\) depends on the transformation type which needs to be defined beforehand, see some examples below. Furthermore, the localisation network can take any differentiable form, e.g., a CNN or FCN.\n\nExamples of Spatial Transformations\n\nThe following examples highlight how a regular grid\n\\[\n\\textbf{G} = \\left\\{ \\begin{bmatrix} x_i^t \\\\ y_i^t \\end{bmatrix}\n\\right\\}_{i=1}^{H^t \\cdot W^t}\n\\]\ndefined on the output/target map \\(\\textbf{V}\\) (i.e., \\(H^t\\) and \\(W^t\\) denote height and width of \\(\\textbf{V}\\)) can be transformed into a new sampling grid\n\\[\n\\widetilde{\\textbf{G}} = \\left\\{ \\begin{bmatrix} x_i^s \\\\ y_i^s \\end{bmatrix}\n\\right\\}_{i=1}^{H^s \\cdot W^s}\n\\]\ndefined on the input/source feature map \\(\\textbf{U}\\) using a parametrized transformation \\(\\mathcal{T}_{\\boldsymbol{\\theta}}\\), i.e., \\(\\widetilde{G} =\nT_{\\boldsymbol{\\theta}} (G)\\). Visualizations have bee created by me, interactive versions can be found here.\n\n\n\n\n\n\n\n\n\n\n\n\nThis transformation allows cropping, translation, rotation, scale and skew to be applied to the input feature map. It has 6 degrees of freedom (DoF).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis transformation is more constrained with only 3-DoF. Therefore it only allows cropping, translation and isotropic scaling to be applied to the input feature map.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis transformation has 8-DoF and can be seen as an extension to the affine transformation. The main difference is that affine transformations are constrained to preserve parallelism.\n\n\n\n\nGrid Generator: Its purpose to create the new sampling grid \\(\\widetilde{\\textbf{G}}\\) on the input feature map \\(\\textbf{U}\\) by applying the predefined parametrized transformation using the parameters \\(\\boldsymbol{\\theta}\\) obtained from the localisation network, see examples above.   \nSampler: Its purpose is to compute the warped version of the input feature map \\(\\textbf{U}\\) by computing the pixel values in the new sampling grid \\(\\widetilde{\\textbf{G}}\\) obtained from the grid generator. Note that the new sampling grid does not necessarily align with the input feature map grid, therefore some kind of interpolation is needed. Jaderberg et al. (2015) formulate this interpolation as the application of a sampling kernel centered at a particular location in the input feature map, i.e.,\n\\[\n  V_i^c = \\sum_{n=1}^{H^s} \\sum_{m=1}^{W^s} U_{n,m}^c \\cdot \\underbrace{k(x_i^s - x_m^t;\n  \\boldsymbol{\\Phi}_x)}_{k_{\\boldsymbol{\\Phi}_x}} \\cdot \\underbrace{k(y_i^s - y_n^t; \\boldsymbol{\\Phi}_y)}_{k_{\\boldsymbol{\\Phi}_y}},\n\\]\nwhere \\(V_i^c \\in \\mathbb{R}^{W^t \\times H^t}\\) denotes the new pixel value of the \\(c\\)-th channel at the \\(i\\)-th position of the new sampling grid coordinates2 \\(\\begin{bmatrix} x_i^s &\ny_i^s\\end{bmatrix}^{T}\\) and \\(\\boldsymbol{\\Phi}_x,\n\\boldsymbol{\\Phi}_y\\) are the parameters of a generic sampling kernel \\(k()\\) which defines the image interpolation. As the sampling grid coordinates are not channel-dependent, each channel is transformed in the same way resulting in spatial consistency between channels. Note that although in theory we need to sum over all input locations, in practice we can ignore this sum by just looking at the kernel support region for each \\(V_i^c\\) (similar to CNNs).\nThe sampling kernel can be chosen freely as long as (sub-)gradients can be defined with respect to \\(x_i^s\\) and \\(y_i^s\\). Some possible choices are shown below.\n\\[\n  \\begin{array}{lcc}\n  \\hline\n    \\textbf{Interpolation Method} & k_{\\boldsymbol{\\Phi}_x} &\n  k_{\\boldsymbol{\\Phi}_x} \\\\ \\hline\n    \\text{Nearest Neightbor} &  \\delta( \\lfloor x_i^s + 0.5\\rfloor -\n  x_m^t) &  \\delta( \\lfloor y_i^s + 0.5\\rfloor - y_n^t) \\\\\n    \\text{Bilinear} &  \\max \\left(0, 1 -  \\mid x_i^s - x_m^t \\mid\n  \\right) &  \\max (0, 1 - \\mid y_i^s - y_m^t\\mid ) \\\\ \\hline\n  \\end{array}\n\\]\n\nThe figure below summarizes the ST architecture and shows how the individual parts interact with each other.\n\n\n\n\n\n\n\n\nArchitecture of ST Module. Taken from Jaderberg et al. (2015).\n\n\n\nMotivation: With the introduction of GPUs, convolutional layers enabled computationally efficient training of feature detectors on patches due to their weight sharing and local connectivity concepts. Since then, CNNs have proven to be the most powerful framework when it comes to computer vision tasks such as image classification or segmentation.\nDespite their success, Jaderberg et al. (2015) note that CNNs are still lacking mechanisms to be spatially invariant to the input data in a computationally and parameter efficient manner. While convolutional layers are translation-equivariant to the input data and the use of max-pooling layers has helped to allow the network to be somewhat spatially invariant to the position of features, this invariance is limited to the (typically) small spatial support of max-pooling (e.g., \\(2\\times 2\\)). As a result, CNNs are typically not invariant to larger transformations, thus need to learn complicated functions to approximate these invariances.\n\n\nWhat if we could enable the network to learn transformations of the input data? This is the main idea of STs! Learning spatial invariances is much easier when you have spatial transformation capabilities. The second aim of STs is to be computationally and parameter efficient. This is done by using structured, parameterized transformations which can be seen as a weight sharing scheme."
  },
  {
    "objectID": "paper_summaries/spatial_transformer/index.html#implementation",
    "href": "paper_summaries/spatial_transformer/index.html#implementation",
    "title": "Spatial Transformer Networks",
    "section": "Implementation",
    "text": "Implementation\nJaderberg et al. (2015) performed several supervised learning tasks (distorted MNIST, Street View House Numbers, fine-grained bird classification) to test the performance of a standard architecture (FCN or CNN) against an architecture that includes one or several ST modules. They could emperically validate that including STs results in performance gains, i.e., higher accuracies across multiple tasks.\nThe following reimplementation aims to reproduce a subset of the distored MNIST experiment (RTS distorted MNIST) comparing a standard CNN with a ST-CNN architecture. A starting point for the implementation was this pytorch tutorial by Ghassen Hamrouni.\n\nRTS Distorted MNIST\nWhile Jaderberg et al. (2015) explored multiple distortions on the MNIST handwriting dataset, this reimplementation focuses on the rotation-translation-scale (RTS) distorted MNIST, see image below. As described in appendix A.4 of Jaderberg et al. (2015) this dataset can easily be generated by augmenting the standard MNIST dataset as follows: * randomly rotate by sampling the angle uniformly in \\([+45^{\\circ}, 45^{\\circ}]\\), * randomly scale by sampling the factor uniformly in \\([0.7, 1.2]\\), * translate by picking a random location on a \\(42\\times 42\\) image (MNIST digits are \\(28 \\times 28\\)).\n\n\n\n\n\n\n\n\n\n\n\nRTS Distorted MNIST Examples\n\n\n\nNote that this transformation could also be used as a data augmentation technique, as the resulting images remain (mostly) valid digit representations (humans could still assign correct labels).\nThe code below can be used to create this dataset:\n\n\nCode\nimport torch\nfrom torchvision import datasets, transforms\n\n\ndef load_data():\n    \"\"\"loads MNIST datasets with 'RTS' (rotation, translation, scale)\n    transformation\n\n    Returns:\n        train_dataset (torch dataset): training dataset\n        test_dataset (torch dataset): test dataset\n    \"\"\"\n    def place_digit_randomly(img):\n        new_img = torch.zeros([42, 42])\n        x_pos, y_pos = torch.randint(0, 42-28, (2,))\n        new_img[y_pos:y_pos+28, x_pos:x_pos+28] = img\n        return new_img\n\n    transform = transforms.Compose([\n        transforms.RandomAffine(degrees=(-45, 45),\n                                scale=(0.7, 1.2)),\n        transforms.ToTensor(),\n        transforms.Lambda(lambda img: place_digit_randomly(img)),\n        transforms.Lambda(lambda img: img.unsqueeze(0))\n    ])\n    train_dataset = datasets.MNIST('./data', transform=transform,\n                                   train=True, download=True)\n    test_dataset = datasets.MNIST('./data', transform=transform,\n                                   train=True, download=True)\n    return train_dataset, test_dataset\n\n\ntrain_dataset, test_dataset = load_data()\n\n\n\n\nModel Implementation\nThe model implementation can be divided into three tasks:\n\nNetwork Architectures: The network architectures are based upon the description in appendix A.4 of Jaderberg et al. (2015). Note that there is only one ST at the beginning of the network such that the resulting transformation is only applied over one channel (input channel). For the sake of simplicity, we only implement an affine transformation matrix. Clearly, including an ST increases the networks capacity due to the number of added trainable parameters. To allow for a fair comparison, we therefore increase the capacity of the convolutional and linear layers in the standard CNN.\nThe code below creates both architectures and counts their trainable parameters.\n\n\nCode\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\n\n\ndef get_number_of_trainable_parameters(model):\n  \"\"\"taken from\n  discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325\n  \"\"\"\n  model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n  params = sum([np.prod(p.size()) for p in model_parameters])\n  return params\n\n\nclass CNN(nn.Module):\n\n  def __init__(self, img_size=42, include_ST=False):\n      super(CNN, self).__init__()\n      self.ST = include_ST\n      self.name = 'ST-CNN Affine' if include_ST else 'CNN'\n      c_dim = 32 if include_ST else 36\n      self.convs = nn.Sequential(\n          nn.Conv2d(1, c_dim, kernel_size=9, stride=1, padding=0),\n          nn.MaxPool2d(kernel_size=(2,2), stride=2),\n          nn.ReLU(True),\n          nn.Conv2d(c_dim, c_dim, kernel_size=7, stride=1, padding=0),\n          nn.MaxPool2d(kernel_size=(2,2), stride=2),\n          nn.ReLU(True),\n      )\n      out_conv = int((int((img_size - 8)/2) - 6)/2)\n      self.classification = nn.Sequential(\n          nn.Linear(out_conv**2*c_dim, 50),\n          nn.ReLU(True),\n          nn.Linear(50, 10),\n          nn.LogSoftmax(dim=1),\n      )\n      if include_ST:\n          loc_conv_out_dim = int((int(img_size/2) - 4)/2) - 4\n          loc_regression_layer = nn.Linear(20, 6)\n          # initalize final regression layer to identity transform\n          loc_regression_layer.weight.data.fill_(0)\n          loc_regression_layer.bias = nn.Parameter(\n              torch.tensor([1., 0., 0., 0., 1., 0.]))\n          self.localisation_net = nn.Sequential(\n              nn.Conv2d(1, 20, kernel_size=5, stride=1, padding=0),\n              nn.MaxPool2d(kernel_size=(2,2), stride=2),\n              nn.ReLU(True),\n              nn.Conv2d(20, 20, kernel_size=5, stride=1, padding=0),\n              nn.ReLU(True),\n              nn.Flatten(),\n              nn.Linear(loc_conv_out_dim**2*20, 20),\n              nn.ReLU(True),\n              loc_regression_layer\n          )\n      return\n\n  def forward(self, img):\n      batch_size = img.shape[0]\n      if self.ST:\n          out_ST = self.ST_module(img)\n          img = out_ST\n      out_conv = self.convs(img)\n      out_classification = self.classification(out_conv.view(batch_size, -1))\n      return out_classification\n\n  def ST_module(self, inp):\n      # act on twice downsampled inp\n      down_inp = F.interpolate(inp, scale_factor=0.5, mode='bilinear',\n                                recompute_scale_factor=False, align_corners=False)\n      theta_vector = self.localisation_net(down_inp)\n      # affine transformation\n      theta_matrix = theta_vector.view(-1, 2, 3)\n      # grid generator\n      grid = F.affine_grid(theta_matrix, inp.size(), align_corners=False)\n      # sampler\n      out = F.grid_sample(inp, grid, align_corners=False)\n      return out\n\n  def get_attention_rectangle(self, inp):\n      assert inp.shape[0] == 1, 'batch size has to be one'\n      # act on twice downsampled inp\n      down_inp = F.interpolate(inp, scale_factor=0.5, mode='bilinear',\n                               recompute_scale_factor=False, align_corners=False)\n      theta_vector = self.localisation_net(down_inp)\n      # affine transformation matrix\n      theta_matrix = theta_vector.view(2, 3).detach()\n      # create normalized target rectangle input image\n      target_rectangle = torch.tensor([\n          [-1., -1., 1., 1., -1.],\n          [-1., 1., 1., -1, -1.],\n          [1., 1., 1., 1., 1.]]\n      ).to(inp.device)\n      # get source rectangle by transformation\n      source_rectangle = torch.matmul(theta_matrix, target_rectangle)\n      return source_rectangle\n\n\n# instantiate models\ncnn = CNN(img_size=42, include_ST=False)\nst_cnn = CNN(img_size=42, include_ST=True)\n# print trainable parameters\nfor model in [cnn, st_cnn]:\n  num_trainable_params = get_number_of_trainable_parameters(model)\n  print(f'{model.name} has {num_trainable_params} trainable parameters')\n\n\n\n\n\nTrainable Parameters\n\n\nTraining Procedure: As described in appendix A.4 of Jaderberg et al. (2015), the networks are trained with standard SGD, batch size of \\(256\\) and base learning rate of \\(0.01\\). To reduce computation time, the number of epochs is limited to \\(50\\).\nThe loss function is the multinomial cross entropy loss, i.e.,\n\\[\n  \\text{Loss} = - \\sum_{i=1}^N \\sum_{k=1}^C p_i^{(k)} \\cdot \\log\n  \\left( \\widehat{p}_i^{(k)} \\right),\n\\]\nwhere \\(k\\) enumerates the number of classes, \\(i\\) enumerates the number of images, \\(p_i^{(k)} \\in \\{0, 1\\}\\) denotes the true probability of image \\(i\\) and class \\(k\\) and \\(\\widehat{p}_i^{(k)} \\in [0, 1]\\) is the probability predicted by the network. Note that the true probability distribution is categorical (hard labels), i.e.,\n\\[\n  p_i^{(k)} = 1_{k = y_i} = \\begin{cases}1 & \\text{if } k = y_i \\\\ 0\n  & \\text{else}\\end{cases}\n\\]\nwhere \\(y_i \\in \\{0, 1, \\cdots, 9 \\}\\) is the label assigned to the \\(i\\)-th image \\(\\textbf{x}_i\\). Thus, we can rewrite the loss as follows\n\\[\n  \\text{Loss} = - \\sum_{i=1}^N \\log \\left( \\widehat{p}_{i, y_i}\n  \\right),\n\\]\nwhich is the definition of the negative log likelihood loss (NLLLoss) in Pytorch, when the logarithmized predictions \\(\\log \\left(\n\\widehat{p}_{i, y_i} \\right)\\) (matrix of size \\(N\\times C\\)) and class labels \\(y_i\\) (vector of size \\(N\\)) are given as input.\nThe code below summarizes the whole training procedure.\n\n\nCode\nfrom livelossplot import PlotLosses\nfrom torch.utils.data import DataLoader\n\n\ndef train(model, dataset):\n    # fix hyperparameters\n    epochs = 50\n    learning_rate = 0.01\n    batch_size = 256\n    step_size_scheduler = 50000\n    gamma_scheduler = 0.1\n    # set device\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    print(f'Device: {device}')\n\n    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True,\n                            num_workers=4)\n\n    model.to(device)\n    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=gamma_scheduler,\n                                                step_size=step_size_scheduler)\n\n    losses_plot = PlotLosses()\n    print(f'Start training with {model.name}')\n    for epoch in range(1, epochs+1):\n        avg_loss = 0\n        for data, label in data_loader:\n            model.zero_grad()\n\n            log_prop_pred = model(data.to(device))\n            # multinomial cross entropy loss\n            loss = F.nll_loss(log_prop_pred, label.to(device))\n\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            avg_loss += loss.item() / len(data_loader)\n\n        losses_plot.update({'log loss': np.log(avg_loss)})\n        losses_plot.send()\n    trained_model = model\n    return trained_model\n\n\nTest Procedure: A very simple test procedure to evaluate both models is shown below. It is basically the same as in the pytorch tutorial.\n\n\nCode\ndef test(trained_model, test_dataset):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=True,\n                            num_workers=4)\n    with torch.no_grad():\n        trained_model.eval()\n        test_loss = 0\n        correct = 0\n        for data, label in test_loader:\n            data, label = data.to(device), label.to(device)\n\n            log_prop_pred = trained_model(data)\n            class_pred = log_prop_pred.max(1, keepdim=True)[1]\n\n            test_loss += F.nll_loss(log_prop_pred, label).item()/len(test_loader)\n            correct += class_pred.eq(label.view_as(class_pred)).sum().item()\n\n        print(f'{trained_model.name}: avg loss: {np.round(test_loss, 2)},  ' +\n              f'avg acc {np.round(100*correct/len(test_dataset), 2)}%')\n    return\n\n\n\n\n\nResults\nLastly, the results can also divided into three sections:\n\nTraining Results: Firstly, we train our models on the training dataset and compare the logarithmized losses:\n\n\nCode\ntrained_cnn = train(cnn, train_dataset)\n\n\n\n\n\nTraining Results CNN\n\n\n\n\nCode\ntrained_st_cnn = train(st_cnn, train_dataset)\n\n\n\n\n\nTraining Results ST-CNN\n\n\nThe logarithmized losses already indicate that the ST-CNN performs better than the standard CNN (at least, it decreases the loss faster). However, it can also be noted that training the ST-CNN seems less stable.\nTest Performance: While the performance on the training dataset may be a good indicator, test set performance is much more meaningful. Let’s compare the losses and accuracies between both trained models:\n\n\nCode\nfor trained_model in [trained_cnn, trained_st_cnn]:\n    test(trained_model, test_dataset)\n\n\n\n\n\nTest Results\n\n\nClearly, the ST-CNN performs much better than the standard CNN. Note that training for more epochs would probably result in even better accuracies in both models.\nVisualization of Learned Transformations: Lastly, it might be interesting to see what the ST module actually does after training:\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import ConnectionPatch\n\n\ndef visualize_learned_transformations(trained_st_cnn, test_dataset, digit_class=8):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    trained_st_cnn.to(device)\n    n_samples = 5\n\n    data_loader = DataLoader(test_dataset, batch_size=256, shuffle=True)\n    batch_img, batch_label = next(iter(data_loader))\n    i_samples = np.where(batch_label.numpy() == digit_class)[0][0:n_samples]\n\n    fig = plt.figure(figsize=(n_samples*2.5, 2.5*4))\n    for counter, i_sample in enumerate(i_samples):\n        img = batch_img[i_sample]\n        label = batch_label[i_sample]\n\n        # input image\n        ax1 = plt.subplot(4, n_samples, 1 + counter)\n        plt.imshow(transforms.ToPILImage()(img), cmap='gray')\n        plt.axis('off')\n        if counter == 0:\n            ax1.annotate('Input', xy=(-0.3, 0.5), xycoords='axes fraction',\n                        fontsize=14, va='center', ha='right')\n\n        # image including border of affine transformation\n        img_inp = img.unsqueeze(0).to(device)\n        source_normalized = trained_st_cnn.get_attention_rectangle(img_inp)\n        # remap into absolute values\n        source_absolute = 0 + 20.5*(source_normalized.cpu() + 1)\n        ax2 = plt.subplot(4, n_samples, 1 + counter + n_samples)\n        x = np.arange(42)\n        y = np.arange(42)\n        X, Y = np.meshgrid(x, y)\n        plt.pcolor(X, Y, img.squeeze(0), cmap='gray')\n        plt.plot(source_absolute[0], source_absolute[1], color='red')\n        plt.axis('off')\n        ax2.axes.set_aspect('equal')\n        ax2.set_ylim(41, 0)\n        ax2.set_xlim(0, 41)\n        if counter == 0:\n            ax2.annotate('ST', xy=(-0.3, 0.5), xycoords='axes fraction',\n                        fontsize=14, va='center', ha='right')\n        # add arrow between\n        con = ConnectionPatch(xyA=(21, 41), xyB=(21, 0), coordsA='data',\n                              coordsB='data', axesA=ax1, axesB=ax2,\n                              arrowstyle=\"-|&gt;\", shrinkB=5)\n        ax2.add_artist(con)\n\n        # ST module output\n        st_img = trained_st_cnn.ST_module(img.unsqueeze(0).to(device))\n\n        ax3 = plt.subplot(4, n_samples, 1 + counter + 2*n_samples)\n        plt.imshow(transforms.ToPILImage()(st_img.squeeze(0).cpu()), cmap='gray')\n        plt.axis('off')\n        if counter == 0:\n            ax3.annotate('ST Output', xy=(-0.3, 0.5), xycoords='axes fraction',\n                        fontsize=14, va='center', ha='right')\n        # add arrow between\n        con = ConnectionPatch(xyA=(21, 41), xyB=(21, 0), coordsA='data',\n                              coordsB='data', axesA=ax2, axesB=ax3,\n                              arrowstyle=\"-|&gt;\", shrinkB=5)\n        ax3.add_artist(con)\n\n        # predicted label\n        log_pred = trained_st_cnn(img.unsqueeze(0).to(device))\n        pred_label = log_pred.max(1)[1].item()\n\n        ax4 = plt.subplot(4, n_samples, 1 + counter + 3*n_samples)\n        plt.text(0.45, 0.43, str(pred_label), fontsize=22)\n        plt.axis('off')\n        #plt.title(f'Ground Truth {label.item()}', y=-0.1, fontsize=14)\n        if counter == 0:\n            ax4.annotate('Prediction', xy=(-0.3, 0.5), xycoords='axes fraction',\n                        fontsize=14, va='center', ha='right')\n        # add arrow between\n        con = ConnectionPatch(xyA=(21, 41), xyB=(0.5, 0.65), coordsA='data',\n                              coordsB='data', axesA=ax3, axesB=ax4,\n                              arrowstyle=\"-|&gt;\", shrinkB=5)\n        ax4.add_artist(con)\n    return\n\n\nvisualize_learned_transformations(st_cnn, test_dataset, 2)\n\n\n\n\n\nTransformation Visualization\n\n\nClearly, the ST module attends to the digits such that the ST output has much less variation in terms of rotation, translation and scale making the classification task for the follow up CNN easier.\nPretty cool, hugh?"
  },
  {
    "objectID": "paper_summaries/spatial_transformer/index.html#footnotes",
    "href": "paper_summaries/spatial_transformer/index.html#footnotes",
    "title": "Spatial Transformer Networks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nClearly, convolutional layers are not rotation or scale invariant. Even the translation-equivariance property does not necessarily make CNNs translation-invariant as typically some fully connected layers are added at the end. Max-pooling layers can introduce some translation invariance, however are limited by their size such that often large translation are not captured.↩︎\nJaderberg et al. (2015) define the transformation with normalized coordinates, i.e., \\(-1\n\\le x_i^s, y_i^s \\le 1\\). However, in the sampling kernel equations it seems more likely that they assume unnormalized/absolute coordinates, e.g., in equation 4 of the paper normalized coordinates would be nonsensical.↩︎"
  },
  {
    "objectID": "paper_summaries/interaction_network/index.html",
    "href": "paper_summaries/interaction_network/index.html",
    "title": "Interaction Networks for Learning about Objects, Relations and Physics",
    "section": "",
    "text": "Battaglia et al. (2016) introduce the Interaction Network (IN) as the first general-purpose learnable physics engine capable of zero-shot generalization in terms of varying configurations of objects and relations. The IN leverages object- and relation-based reasoning by defining a message passing scheme on a graph-structured representation of objects as nodes and relations as edges. As a proof of concept, they show that their model successfully learned to predict physical trajectories in gravitational systems, bouncing ball domains and mass string systems, and that it could also learn to estimate abstract properties such as the potential energy. Although its formulation is based on dynamical physical systems, it might also be applicable to other domains that can be abstracted into a graph-structured representation of objects and relations such as model-based reinforcment learning."
  },
  {
    "objectID": "paper_summaries/interaction_network/index.html#model-description",
    "href": "paper_summaries/interaction_network/index.html#model-description",
    "title": "Interaction Networks for Learning about Objects, Relations and Physics",
    "section": "Model Description",
    "text": "Model Description\nIn essence, the IN model can be understood as a graph-based simulator (i.e., state is represented as a graph) that predicts a future state (i.e., altered graph) using a message-passing scheme. Battaglia et al. (2016) used a handcrafted scene encoder/decoder to convert the physical scene into the corresponding graph structure and vice versa.\nDefinition: Let \\(G=\\langle O, R \\rangle\\) be an attributed, directed multigraph in which the set of nodes \\(O=\\\\{o_j\\\\}_{j=1 \\dots N_O}\\) represents objects and the set of edges \\(R = \\\\{ \\langle i, j , r_k\n\\rangle_k \\\\}\\_{1 \\dots N_R}\\) represents the relations between the objects, i.e., the triplet \\(\\langle i,j, r_k \\rangle_k\\) defines the \\(k^{\\text{th}}\\) relation from sender \\(o_i\\) to receiver \\(o_j\\) with relation attribute \\(r_k\\). Each object \\(o_i\\) may have several attributes1, an object state \\(o_i^{(t)}\\) at time \\(t\\) can be understood as a value assignment to all of its attributes. Additionally, let \\(X=\\\\{ x_j \\\\}\\_{1 \\dots N_O}\\) denote external effects (e.g., active control or gravitation) which are applied to each object separately.\nIntuition: The ultimate goal of the IN is to predict all future object states \\(o_i^{(t+1)}\\) based on the graph \\(G\\), the current external effects per object \\(x_i^{(t)}\\) and all current object states \\(o_i^{(t)}\\). A message passing scheme is defined to achieve this goal in which first effects resulting from interactions are computed (relational reasoning), then these effects (messages) together with the external effects are aggregated towards the objects, lastly the aggregated information is used to update the object states (object reasoning).\nFormally, the basic IN is defined as follows\n\\[\n\\begin{align}\n&\\text{IN}(G) = \\phi_O \\Bigg( a\\Big( G, X, \\phi_R \\big( m (G)\\big) \\Big)\\Bigg)\n\\end{align}\n\\]\n\\[\n\\begin{align}\n  \\begin{aligned}\n    & m(G) = B = \\{ b_k\\}_{k=1\\dots N_R} \\\\\n    & f_{R} (b_k) = e_k \\\\\n    & \\phi_{R} (B) = E = \\{e_k\\}_{k=1 \\dots N_R}\n  \\end{aligned}\n    &&\n       \\begin{aligned}\n         & a(G, X, E) = C = \\{c_j\\}_{j=1\\dots N_O} \\\\\n         & f_O (c_j) = p_j \\\\\n         & \\phi_O (C) = P = \\{p_j\\}_{j=1\\dots N_o}\n       \\end{aligned}\n\\end{align}\n\\]\nIn this definition \\(m\\) denotes the marshalling function which rearranges objects and relations into interaction terms \\(b_k= \\langle o_i, o_j, r_k \\rangle \\in B\\) on which the relational function \\(\\phi_R\\) can operate (element-wise by applying \\(f_R\\) on each interaction term) to predict the effects of each interaction \\(e_k\\in E\\). The aggregation function \\(a\\) builds a a set of of object model inputs \\(c_j \\in C\\) (one per object) by collecting and merging all incoming effects per object and combining the result with the object state \\(o_{j}^{(t)}\\) and the external effects for that object \\(x_j^{(t)}\\). Lastly, the object model \\(\\phi_O\\) predicts for all objects their result \\(p_j\\in P\\), i.e., future object states \\(o_{j}^{(t+1)}\\), by applying \\(f_O\\) to each \\(c_j\\). The figure below represents the described procedure of an (exemplary) IN.\n\n\n\n\n\n\n\n\nSchematic of the IN’s update procedure for an exemplary IN2:Firstly, the marshalling function \\(m\\) rearranges objects \\(o_i\\) based on the relations \\(r_j\\) into interaction terms \\(b_k = \\langle o_i, o_j, r_k \\rangle\\). Secondly, the function \\(f_R\\) is applied on each interaction term to compute the corresponding (directed) effects.Thirdly, the aggregation function \\(a\\) uses the graph structure to collect and merge the incoming effects, and to add the corresponding object state and external effects into a new object term \\(c_k = \\langle o_k, x_k, \\hat{e}_k \\rangle\\) (\\(\\hat{e}_k\\) denotes aggregated effect). Lastly, this representation is used to predict the results \\(p_k\\), i.e., future object states, by applying \\(f_O\\) to each \\(c_k\\).\n\n\n\nIntuition: Computing the trajectories of planets in a solar system may be a good example to motivate and understand the IN definition. Objects in the graph shall be the planets and relations the pairwise gravitational forces on each other, i.e., each object has an arrow pointing to all other objects. Object attributes could be the mass, acceleration, velocity and position. As external effects we could define the step size (necessary for approximate integration). Relational attributes are not needed then. The physics approach to compute the approximated trajacetories would be to first compute all gravitational forces per object which corresponds to computing the effects in the IN. Then, the net force would be computed as the sum of all forces per object, i.e., aggregation in the IN. Lastly, the object attributes would be updated (except mass) using the calculated net force, current object state and step size which corresponds to the object-centric update in the IN."
  },
  {
    "objectID": "paper_summaries/interaction_network/index.html#learning-the-model",
    "href": "paper_summaries/interaction_network/index.html#learning-the-model",
    "title": "Interaction Networks for Learning about Objects, Relations and Physics",
    "section": "Learning the Model",
    "text": "Learning the Model\nIn the IN defintion, there are no limitations whatsoever for the functions and how they operate over their inputs (e.g., objects itself could also be graphs). Thus, learning the underlying dynamics given the graph structured representation and the scene encoder/decoder (and assuming that there is some IN that can simulate the dynamics) without further assumptions remains an intractable quest. To overcome this problem, Battaglia et al. (2016) present a learnable implementation of the IN which uses deep neural networks as function approximators and a specific object and relation representation using matrices. Then, learning the model comes down to training from data using the standard deep learning framework.\nImplementation: Let each object \\(o_i^{(t)}\\in \\mathbb{R}^{D_s}\\) be represented by a \\(D_s\\)-dimensional vector where each entry corresponds to an attribute, i.e., attributes have a predefined order which is fix over all objects. Then, \\(O\\) is defined as a \\(D_S \\times N_O\\) matrix where each column represents an object. Similarly, \\(X\\) is defined as a \\(D_X \\times N_O\\) matrix where each \\(D_x\\)-dimensional column represents the external effects that correspond to the object defined in the same column of \\(O\\). Let each relation be formalized into a triple of three vectors \\(r_k = \\langle\nr_r, r_s, r_a \\rangle\\) where \\(r_a \\in \\mathbb{R}^{D_R}\\) represents the (ordered) relational attributes and \\(r_r, r_s \\in \\{0, 1\\}^{N_O}\\) are one-hot encodings of the receiver and sender object, respectively. Then, all relations can be represented by the triplet \\(R = \\langle R_r, R_s, R_a \\rangle\\) where the matrices \\(R_r, R_s \\in \\{0,1\\}^{N_O \\times N_R}\\) and \\(R_a \\in \\mathbb{R}^{D_R \\times N_R}\\) are generated by stacking the relations column-wise.\nIt follows that the interaction terms \\(b_k\\) can be vectorized by concatenation of the receiver and sender object attributes and the relational attributes into a \\((2 D_s + D_R)\\)-length vector. The marshalling function \\(m\\) can be stated as follows\n\\[\n\\begin{align}\n  m(G)\n  = \\begin{bmatrix}  O R_r \\\\ O R_s \\\\ R_a  \\end{bmatrix} = \\begin{bmatrix} b_1  &\n    \\dots & b_{N_R} \\end{bmatrix} = B.\n\\end{align}\n\\]\n\\(B\\) is the input to the relational model \\(\\phi_R\\) which is defined through the application of \\(f_R\\) on each column of \\(B\\) (each interaction term), i.e.,\n\\[\n\\begin{align}\n  \\phi_R (B) = \\begin{bmatrix} f_R \\big(b_1\\big) & \\dots & f_R \\big(b_{N_R}\\big) \\end{bmatrix}\n             = \\begin{bmatrix} e_1 & \\dots e_{N_R}\\end{bmatrix}  = E.\n\\end{align}\n\\]\n\\(f_R\\) shall be approximated by a neural network to estimate a \\(D_E\\)-length vector \\(e_k\\) that encodes the resulting effect. Similar to the marshalling function, the aggregation function \\(a\\) constructs vectorized object terms \\(c_k\n= \\begin{bmatrix} o_k^{(t)} & x_k^{(t)} & \\hat{e}_k^{(t)} \\end{bmatrix}^{\\text{T}}\\) by concatenation of the object attributes, the external effects and the aggregated effect (summation of all incoming effects per object):\n\\[\n\\begin{align}\n  a(O, R, X, E) = \\begin{bmatrix} O & X & E R_r^{\\text{T}}\n  \\end{bmatrix}^{\\text{T}} =\n  \\begin{bmatrix} c_1 & \\dots & c_{N_O} \\end{bmatrix} = C.\n\\end{align}\n\\]\nLastly, \\(C\\) is used as the input to the object model \\(\\phi_O\\) which is defined through the application of \\(f_O\\) on each column of \\(C\\), i.e.,\n\\[\n  \\phi_O (C) =\n  \\begin{bmatrix} f_O \\big( c_1 \\big) & \\dots & f_O \\big( c_{N_O} \\big) \\end{bmatrix}=\n                                                \\begin{bmatrix} o_1^{(t+1)} & \\dots & o_{N_O}^{(t+1)} \\end{bmatrix}.\n\\]\nThe result can be used to update the graph structured representation. The figure below summarizes the implementation of the IN.\n\n\n\n\n\n\n\n\nOne step roll out of the IN implementation. The physical scene is encoded (decoded) into (from) a graph structured representation using a handcrafted scene encoder (decoder). Battaglia et al. (2016) present a learnable implementation by using neural networks (blue boxes) as function approximators for the relational model (\\(\\phi_R\\)) and the object model (\\(\\phi_O\\))."
  },
  {
    "objectID": "paper_summaries/interaction_network/index.html#drawbacks-of-paper",
    "href": "paper_summaries/interaction_network/index.html#drawbacks-of-paper",
    "title": "Interaction Networks for Learning about Objects, Relations and Physics",
    "section": "Drawbacks of Paper",
    "text": "Drawbacks of Paper\n\nhandcrafted scene encoder/decoder \\(\\Rightarrow\\) not end-to-end\nobject states could blow up since all objects share the same attributes"
  },
  {
    "objectID": "paper_summaries/interaction_network/index.html#footnotes",
    "href": "paper_summaries/interaction_network/index.html#footnotes",
    "title": "Interaction Networks for Learning about Objects, Relations and Physics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn their implementation, Battaglia et al. (2016) assume that all objects share the same attributes, i.e., are instances from the same class. 2: Schematic is taken from the original paper of Battaglia et al. (2016).↩︎"
  },
  {
    "objectID": "paper_summaries/auto-encoding_variational_bayes/index.html",
    "href": "paper_summaries/auto-encoding_variational_bayes/index.html",
    "title": "Auto-Encoding Variational Bayes",
    "section": "",
    "text": "Kingma and Welling (2013) introduced the Variational Auto-Encoder (VAE) to showcase how their Auto-Encoding Variational Bayes (AEVB) algorithm can be used in practice. Assuming i.i.d. datasets and continuous latent variables, the AEVB algorithm learns an approximate probabilistic encoder \\(q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x})\\) jointly with the probabilisitc decoder \\(p_{\\boldsymbol{\\theta}}\n(\\textbf{x}|\\textbf{z})\\) (where \\(\\boldsymbol{\\phi},\n\\boldsymbol{\\theta}\\) parametrize the corresponding distributions) by learning the optimal model parameters \\(\\boldsymbol{\\phi},\n\\boldsymbol{\\theta}\\) through optimizing an objective function with standard gradient ascent methods. In summary, a VAE is probabilistic autoencoder which uses variational inference to regularize the coding space. Furthermore, a VAE is a deep generative model as sampling from the coding space is possible, i.e., new observations can be generated."
  },
  {
    "objectID": "paper_summaries/auto-encoding_variational_bayes/index.html#model-description",
    "href": "paper_summaries/auto-encoding_variational_bayes/index.html#model-description",
    "title": "Auto-Encoding Variational Bayes",
    "section": "Model Description",
    "text": "Model Description\nThe AEVB algorithm basically assumes a generative process, introduces a variational approximation (see figure below) and optimizes the model parameters by maximizing an objective function. The objective function consists of the (reparametrized) variational lower bound of each datapoint. Reparametrization is necessary to allow the explicit formulation of gradients with respect to the model parameters.\n\n\n\n\n\n\n\n\n\n\n\nThe directed graphical models represent the assumed generative process (a) and the variational approximation of the intractable posterior (b) in the AEVB algorithm.\n\n\n\nObjective Function Derivation: Let \\(\\textbf{X}=\\{\\textbf{x}^{(i)}\\}_{i=1}^{N}\\) denote the dataset consisting of \\(N\\) i.i.d. samples and let \\(\\textbf{z}\\) denote the unobserved continuous random variable (i.e., hidden or code variable). Kingma and Welling (2013) assume that each observed sample \\(\\textbf{x}^{(i)}\\) comes from a generative process in which: Firstly, a hidden variable \\(\\textbf{z}^{(i)}\\) is generated from a prior distribution \\(p_{\\boldsymbol{\\theta}} (\\textbf{z})\\). Secondly, \\(\\textbf{x}^{(i)}\\) is generated from the conditional distribution \\(p_{\\boldsymbol{\\theta}}(\\textbf{x}|\\textbf{z}^{(i)})\\). Note that we do not know \\(\\boldsymbol{\\theta}\\) nor do we have information about \\(\\textbf{z}^{(i)}\\). In order to recover this generative process, they introduce \\(q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x})\\) as an approximation to the intractable true posterior1 \\(p_{\\boldsymbol{\\theta}} (\\textbf{z}|\\textbf{x})\\). The marginal log likelihood of each individual datapoint \\(\\textbf{x}^{(i)}\\) can then be stated as follows (see Eric Jang’s amazing blog post for detailed derivation)\n\\[\n  \\log p_{\\boldsymbol{\\theta}} \\left(\\textbf{x}^{(i)}\\right) =\n  \\underbrace{D_{KL} \\left(q_{\\boldsymbol{\\phi}}\\left(\\textbf{z} | \\textbf{x}^{(i)}\\right)\n  || p_{\\boldsymbol{\\theta}} \\left(\\textbf{z}|\\textbf{x}^{(i)}\\right)\\right)}_{\\ge 0} +\n  \\mathcal{L} \\left( \\boldsymbol{\\theta}, \\boldsymbol{\\phi}, \\textbf{x}^{(i)}\\right) \\ge\n  \\mathcal{L} \\left( \\boldsymbol{\\theta}, \\boldsymbol{\\phi}, \\textbf{x}^{(i)}\\right),\n\\]\nwhere \\(D_{KL}(\\cdot)\\) denotes the KL divergence of the approximate from the true posterior (this quantity remains unknown since the true posterior \\(p_{\\boldsymbol{\\theta}} (\\textbf{z}|\\textbf{x}^{(i)})\\) is intractable). \\(\\mathcal{L} \\left(\\boldsymbol{\\theta},\n\\boldsymbol{\\phi}, \\textbf{x}^{(i)}\\right)\\) is called the variational lower bound or evidence lower bound (ELBO). The goal is to optimize \\(\\boldsymbol{\\phi}, \\boldsymbol{\\theta}\\) such that variational lower bound is maximized, thereby we indirectly maximize the marginal log likelihood. The variational lower bound can rewritten such that the objective function is obtained (also derived in Eric Jang’s blog post)\n\\[\n\\begin{align}\n  \\mathcal{L} \\left(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}, \\textbf{x}^{(i)}\\right) &=\n  \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x}^{(i)})}\n  \\left[ -\\log\n  q_{\\boldsymbol{\\phi}} (\\textbf{z} | \\textbf{x}^{(i)} ) +\n  \\log p_{\\boldsymbol{\\theta}} (\\textbf{z}) + \\log\n  p_{\\boldsymbol{\\theta}}\n  (\\textbf{x}^{(i)}|\\textbf{z}) \\right] \\\\\n  &= \\underbrace{-D_{KL} \\left(  q_{\\boldsymbol{\\phi}} \\left(\n  \\textbf{z} | \\textbf{x}^{(i)} \\right), p_{\\boldsymbol{\\theta}}\n  (\\textbf{z}) \\right)}_{\\text{Regularization Term}} +\n    \\underbrace{\n    \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x}^{(i)})}\n    \\left[ \\log p_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)}| \\textbf{z} \\right) \\right]}_{\\text{Reconstruction Accuracy}}\n    .\n\\end{align}\n\\]\nThe two terms have an associated interpretation in autoencoder language:\n\nReconstruction Accuracy (opposite of Reconstruction Error): The expectation can be interpreted using Monte Carlo integration,i.e.,\n\\[\n    \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x}^{(i)})}\n    \\left[ \\log p_{\\boldsymbol{\\theta}} \\left(\n    \\textbf{x}^{(i)} | \\textbf{z}\n    \\right) \\right] \\approx \\frac {1}{N} \\sum_{k=1}^{N} \\log p_{\\boldsymbol{\\theta}}\n    \\left( \\textbf{x}^{(i)} | \\textbf{z}^{(k)} \\right) \\qquad \\textbf{z}^{(k)} \\sim\n    q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x}^{(i)}),\n  \\]\nwhich results in an unbiased estimate. Sampling \\(\\textbf{z}^{(k)}\\sim\nq_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x}^{(i)})\\) can be understood as encoding the observed input \\(\\textbf{x}^{(i)}\\) into a code \\(\\textbf{z}^{(k)}\\) using the probabilistic encoder \\(q_{\\boldsymbol{\\phi}}\\). Clearly, the expectation is maximized when the decoder \\(p_{\\boldsymbol{\\theta}}\\) maps the encoded input \\(\\textbf{z}^{(k)}\\) back the original input \\(\\textbf{x}^{(i)}\\), i.e., assigns high probability to \\(p_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)} | \\textbf{z}^{(i)} \\right)\\).\nRegularization Term: The KL divergence is non-negative and only zero if both distributions are identical. Thus, maximizing this term forces the encoder distribution \\(q_{\\boldsymbol{\\phi}}\\) to be close to the prior \\(p_{\\boldsymbol{\\theta}}(\\textbf{z})\\). In VAEs, the prior is typically set to be an isotropic normal distribution resulting in a regularized code space, i.e., encouraging a code space that is close to a normal distribution.\n\nReparametrization Trick: While the KL-divergence \\(D_{KL} \\left(\nq_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}^{(i)} \\right),\np_{\\boldsymbol{\\theta}} (\\textbf{z})\\right)\\) (i.e., the regularization term) can often be integrated analytically, the second term \\(\\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\textbf{z}|\\textbf{x}^{(i)})} \\left[ \\log\np_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)}| \\textbf{z} \\right) \\right]\\) (i.e., the reconstruction accuracy) requires sampling from \\(q_{\\boldsymbol{\\phi}}\\). There are two downsides associated wih sampling from \\(q_{\\boldsymbol{\\phi}}\\) approaches:\n\nBackpropagation does not work with a sampling operation, i.e., the implementation of VAEs would be more difficult.\nThe usual Monte Carlo gradient estimator (which relies on sampling from \\(q_{\\boldsymbol{\\phi}}\\)) w.r.t. \\(\\boldsymbol{\\phi}\\) exhibits very high variance.\n\nTo overcome these problems, Kingma and Welling (2013) use the reparametrization trick:\n\nSubstitute sampling \\(\\textbf{z} \\sim q_{\\boldsymbol{\\phi}}\\) by using a deterministic mapping \\(\\textbf{z} = g_{\\boldsymbol{\\phi}}\n(\\boldsymbol{\\epsilon},\n\\textbf{x})\\) with the differential transformation \\(g_{\\boldsymbol{\\phi}}\\) of an auxiliary noise variable \\(\\boldsymbol{\\epsilon}\\) with \\(\\boldsymbol{\\epsilon}\\sim p(\\boldsymbol{\\epsilon})\\).\n\n As a result, the reparametrized objective function can be written as follows\n\\[\n  \\mathcal{L} \\left(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\textbf{x}^{(i)}\\right) =\n  -D_{KL} \\left(  q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}^{(i)} \\right),\n  p_{\\boldsymbol{\\theta}} (\\textbf{z}) \\right) +\n  \\mathbb{E}_{p(\\boldsymbol{\\epsilon})} \\left[ \\log\n  p_{\\boldsymbol{\\theta}}\n  \\left( \\textbf{x}^{(i)}| g_{\\boldsymbol{\\phi}} \\left(\n  \\boldsymbol{\\epsilon},\n  \\textbf{x}^{(i)} \\right) \\right) \\right]\n\\]\nin which the second term can be approximated with Monte Carlo integration yielding\n\\[\n  \\widetilde{\\mathcal{L}} \\left(\\boldsymbol{\\theta},\n  \\boldsymbol{\\phi};\n  \\textbf{x}^{(i)}\\right) =\n  -D_{KL} \\left(  q_{\\boldsymbol{\\phi}} \\left( \\textbf{z} | \\textbf{x}^{(i)} \\right),\n  p_{\\boldsymbol{\\theta}} (\\textbf{z}) \\right) +\n  \\frac {1}{L} \\sum_{l=1}^{L} \\log p_{\\boldsymbol{\\theta}}\\left(\n  \\textbf{x}^{(i)}| g_{\\boldsymbol{\\phi}}\n\\left( \\boldsymbol{\\epsilon}^{(i, l)}, \\textbf{x}^{(i)} \\right)\\right)\n\\]\nwith \\(\\boldsymbol{\\epsilon} \\sim p(\\boldsymbol{\\epsilon})\\). Note that Kingma and Welling denote this estimator as the second version of the Stochastic Gradient Variational Bayes (SGVB) estimator. Assuming that the KL-divergence can be integrated analytically, the derivatives \\(\\nabla_{\\boldsymbol{\\theta},\\boldsymbol{\\phi}} \\widetilde{L}\\) can be taken (see figure below), i.e., this estimator can be optimized using standard stochastic gradient methods.\n\n\n\n\n\n\n\n\n\n\n\nThe computation graphs summarize the difference between the computation of the reconstruction accuracy in the original objective (a) and the reparametrized objective (b). Circles indicate a sampling operation through which backpropagation is not allowed.\n\n\n\nTo increase stability and performance, Kingma and Welling introduce a minibatch estimator of the lower bound:\n\\[\n  \\widetilde{\\mathcal{L}}^{M} (\\boldsymbol{\\theta},\n  \\boldsymbol{\\phi}; \\textbf{X}^{M})  =  \\frac {N}{M}\n  \\sum_{i=1}^{M}\\widetilde{\\mathcal{L}} \\left(\n  \\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\textbf{x}^{(i)}\\right),\n\\]\nwhere \\(\\textbf{X}^{M} = \\left\\{ \\textbf{x}^{(i)} \\right\\}_{i=1}^{M}\\) denotes a minibatch of \\(M\\) datapoints from the full dataset \\(\\textbf{X}\\) of \\(N\\) datapoints."
  },
  {
    "objectID": "paper_summaries/auto-encoding_variational_bayes/index.html#learning-the-model",
    "href": "paper_summaries/auto-encoding_variational_bayes/index.html#learning-the-model",
    "title": "Auto-Encoding Variational Bayes",
    "section": "Learning the Model",
    "text": "Learning the Model\nLearning the probabilistic encoder \\(q_{\\boldsymbol{\\phi}}\\) and decoder \\(p_{\\boldsymbol{\\theta}}\\) comes down to learning the optimal model parameters \\(\\boldsymbol{\\phi}, \\boldsymbol{\\theta}\\) using the AEVB algorithm which can be summarized in 5 steps:\n\nInitialize model parameters \\(\\boldsymbol{\\phi}, \\boldsymbol{\\theta}\\) randomly.\nSample random minibatch \\(\\textbf{X}^{M} = \\left\\{ \\textbf{x}^{(i)} \\right\\}_{i=1}^{M}\\).\nCompute gradients \\(\\nabla_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}}\n\\widetilde{\\mathcal{L}}^{M} \\left(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\textbf{X}^{M} \\right)\\).\nUpdate model parameters \\(\\boldsymbol{\\phi}, \\boldsymbol{\\theta}\\) by taking a gradient ascent step.\nRepeat steps 2-4 until model parameters converged"
  },
  {
    "objectID": "paper_summaries/auto-encoding_variational_bayes/index.html#vae-implementation",
    "href": "paper_summaries/auto-encoding_variational_bayes/index.html#vae-implementation",
    "title": "Auto-Encoding Variational Bayes",
    "section": "VAE Implementation",
    "text": "VAE Implementation\nA VAE simply uses deep neural networks (DNNs) as function approximators to parametrize the probabilistic encoder \\(q_{\\boldsymbol{\\phi}}\\) and decoder \\(p_{\\boldsymbol{\\theta}}\\). The optimal parameters \\(\\boldsymbol{\\phi}, \\boldsymbol{\\theta}\\) are learned jointly by training the VAE using the AEVB algorithm.\n\n\n\n\n\n\n\n\n\n\n\nSchematic of a standard VAE\n\n\n\nRegularization Term: Typically, the prior over the latent variables is set to be the centered isotropic Gaussian, i.e., \\(p_{\\boldsymbol{\\theta}} (\\textbf{z}) \\sim\n\\mathcal{N} (\\textbf{0}, \\textbf{I})\\). Note that this prior is needed to compute the regularization term in the objective function. Furthermore, it is commonly assumed that the true posterior \\(p_{\\boldsymbol{\\theta}}\\left(\\textbf{z} | \\textbf{x}^{(i)}\\right)\\) may be approximated by \\(q_{\\boldsymbol{\\phi}} \\left(\\textbf{z}  |\n\\textbf{x}^{(i)} \\right) \\sim \\mathcal{N}\\left(\\boldsymbol{\\mu}_E^{(i)},\n\\boldsymbol{\\sigma}_E^{2 (i)} \\textbf{I}  \\right)\\) (subscripts denote that these parameters come from the encoder network). As a result, the regularization term can be integrated analytically leading to a term that only depends on \\(\\boldsymbol{\\mu}_E^{(i)},\n\\boldsymbol{\\sigma}_E^{2 (i)}\\) (see Appendix B of Kingma and Welling)\n\\[\n-D_{KL} \\left(  q_{\\boldsymbol{\\phi}} \\left(\n  \\textbf{z} | \\textbf{x}^{(i)} \\right), p_{\\boldsymbol{\\theta}}\n  (\\textbf{z}) \\right) = \\frac {1}{2} \\sum_{j=1}^{J} \\left(\n  1 + \\log \\left( \\left( \\sigma_{E_j}^{(i)} \\right)^2 \\right)\n  - \\left( \\mu_{E_j}^{(i)}  \\right)^2 -  \\left( \\sigma_{E_j}^{(i)} \\right)^2\n  \\right),\n\\]\nwhere \\(J\\) denotes the latent space dimension.\nEncoder/Decoder Network: Kingma and Welling (2013) use simple neural networks with only one hidden layer to approximate the parameters of the probabilistic encoder and decoder. As stated above, the encoder network is fixed to compute the parameters \\(\\boldsymbol{\\mu}^{(i)}_E,\n\\boldsymbol{\\sigma}_E^{(i)} \\in \\mathbb{R}^{L}\\) of the Gaussian distribution \\(\\mathcal{N}\\left(\\boldsymbol{\\mu}_E^{(i)},\n\\boldsymbol{\\sigma}_E^{2 (i)} \\textbf{I}  \\right)\\). In fact, the encoder network takes a sample \\(\\textbf{x}^{(i)}\\) and outputs the mean \\(\\boldsymbol{\\mu}_E^{(i)}\\) and logarithmized variance, i.e.,\n\\[\n\\begin{bmatrix} \\boldsymbol{\\mu}_E^{(i)} & \\log\n\\boldsymbol{\\sigma}^{2(i)} \\end{bmatrix} = f_{\\boldsymbol{\\phi}} \\left( \\textbf{x}^{(i)} \\right).\n\\]\nNote that using the logarithmized version of the variance increases stability and simplifies the training2.\nIn principle, the encoder and decoder network are very similar only that the dimension of the input and output are reversed. While the encoder network is fixed to approximate a multivariate Gaussian with diagonal covariance structure, the decoder network can approximate a multivariate Gaussian (real-valued data) or Bernoulli (binary data) distribution.\nBelow is a simple Python class that can be used to instantiate the encoder or decoder network as described in appendix C of Kingma and Welling (2013).\n\n\nCode\nimport torch.nn as nn\nfrom collections import OrderedDict\n\n\nclass CoderNetwork(nn.Module):\n    r\"\"\"Encoder/Decoder for use in VAE based on Kingma and Welling\n    \n    Args:\n        input_dim: input dimension (int)\n        output_dim: output dimension (int)\n        hidden_dim: hidden layer dimension (int)\n        coder_type: encoder/decoder type can be \n                   'Gaussian'   - Gaussian with diagonal covariance structure\n                   'I-Gaussian' - Gaussian with identity as covariance matrix \n                   'Bernoulli'  - Bernoulli distribution       \n    \"\"\"\n    \n    def __init__(self, input_dim, hidden_dim, output_dim, coder_type='Gaussian'):\n        super().__init__()\n        \n        assert coder_type in  ['Gaussian', 'I-Gaussian' ,'Bernoulli'], \\\n            'unknown coder_type'\n        \n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.coder_type = coder_type\n        \n        self.coder = nn.Sequential(OrderedDict([\n            ('h', nn.Linear(input_dim, hidden_dim)),\n            ('ReLU', nn.ReLU()) # ReLU instead of Tanh proposed by K. and W.       \n        ]))\n        self.fc_mu = nn.Linear(hidden_dim, output_dim)\n        \n        if coder_type == 'Gaussian':\n            self.fc_log_var = nn.Linear(hidden_dim, output_dim)\n        elif coder_type == 'Bernoulli':\n            self.sigmoid_mu = nn.Sigmoid()\n        return\n    \n    def forward(self, inp):\n        out = self.coder(inp)\n        mu = self.fc_mu(out)\n        \n        if self.coder_type == 'Gaussian':\n            log_var = self.fc_log_var(out)\n            return [mu, log_var]\n        elif self.coder_type == 'I-Gaussian':\n            return mu\n        elif self.coder_type == 'Bernoulli':\n            return self.sigmoid_mu(mu)\n        return\n\n\nReconstruction Accuracy: Sampling from the encoder distribution is avoided by using the reparameterization trick, i.e., the latent variable \\(\\textbf{z}^{(i)}\\) is expressed as a deterministic variable\n\\[\n  \\textbf{z}^{(i, l)}=g_{\\boldsymbol{\\phi}} (\\textbf{x}^{(i)},\n  \\boldsymbol{\\epsilon}) = \\boldsymbol{\\mu}_E^{(i)} +\n  \\boldsymbol{\\sigma}_E^{(i)} \\odot \\boldsymbol{\\epsilon}^{(l)} \\quad\n  \\text{where}\n  \\quad \\boldsymbol{\\epsilon} \\sim\n  \\mathcal{N} (\\textbf{0}, \\textbf{I}),\n\\]\nand \\(\\odot\\) denotes element-wise multiplication.\nNote that we do not need to sample from the decoder distribution, since during training the reconstruction accuracy in the objective function only sums the log-likelihood of each sample \\(\\textbf{z}^{(i,\nl)}\\) and during test time we are mostly interested in the reconstructed \\(\\textbf{x}^{\\prime}\\) with highest probability, i.e., the mean.\nThe reconstruction accuracy in the reparametrized form is given by\n\\[\n\\text{Reconstruction Accuracy} =\n   \\frac {1}{L}\n\\sum_{l=1}^{L} \\log p_{\\boldsymbol{\\theta}}\\left(\n  \\textbf{x}^{(i)}| \\textbf{z}^{(i,l)}\\right),\n\\]\nwhere \\(L\\) denotes the number of samples used during the reparameterization trick. Depending on the chosen decoder distribution, the log-likelihood can be stated in terms of the estimated distribution parameters:\n\nGaussian distribution with diagonal covariance structure \\(p_{\\boldsymbol{\\theta}} \\sim \\mathcal{N} \\left( \\textbf{x}^\\prime | \\boldsymbol{\\mu}_D^{(i)} , \\text{diag} \\left( \\boldsymbol{\\sigma}_D^{2(i)} \\right) \\right)\\)\n\\[\n\\log_e p_{\\boldsymbol{\\theta}}\\left(\n\\textbf{x}^{(i)}| \\textbf{z}^{(i,l)}\\right) = \\underbrace{- \\frac {D}{2} \\ln\n2\\pi}_{\\text{const}} - \\frac {1}{2} \\ln \\left( \\prod_{k=1}^{D} \\sigma_{D_k}^{2(i)} \\right)\n- \\frac {1}{2}\\sum_{k=1}^{D} \\frac {1}{\\sigma_{D_k}^{2(i)}}\\left( x_k^{(i)}  - \\mu_{D_k}^{(i)}\\right)^2\n\\]\nwith the original observation \\(\\textbf{x}^{(i)} \\in \\mathbb{R}^{D}\\). In this form, the objective function is ill-posed since there are no limitations on the form of the normal distribution. As a result the objective function is unbounded, i.e., the VAE could learn the true mean \\(\\boldsymbol{\\mu}_D^{(i)} =\n\\textbf{x}^{(i)}\\) with arbitrary variance \\(\\boldsymbol{\\sigma}_D^{2(i)}\\) or huge variances with arbitrary means to maximize the log-likelihood (see this post). Note that in the encoder network, the prior \\(p_{\\boldsymbol{\\theta}}(\\textbf{z})\\) is used to constrain the encoder distribution (i.e., the mean and variance).\nGaussian distribution with identity as covariance variance \\(p_{\\boldsymbol{\\theta}} \\sim \\mathcal{N} \\left( \\textbf{x}^\\prime | \\boldsymbol{\\mu}_D^{(i)} , \\textbf{I} \\right)\\)\n\\[\n\\log_e p_{\\boldsymbol{\\theta}}\\left(\n\\textbf{x}^{(i)}| \\textbf{z}^{(i,l)}\\right) =\n- \\frac {1}{2}\\sum_{k=1}^{D} \\left( x_k^{(i)}  -\n\\mu_{D_k}^{(i)}\\right)^2 + \\text{const}\n\\]\nwith the original observation \\(\\textbf{x}^{(i)} \\in \\mathbb{R}^{D}\\). In this case the reconstruction accuracy is proportional to the negative mean squarred error which is typically used as the loss function in standard autoencoders.\n\nBernoulli distribution \\(p_{\\boldsymbol{\\theta}} \\sim\\text{Bern} \\left(\\textbf{x}^\\prime | \\boldsymbol{\\mu}_D^{(i)} \\right) =  \\prod_{k=1}^{D} \\left( \\mu_{D_k}^{(i)}\\right)^{x_k^\\prime} \\left( 1 - \\mu_{D_k}^{(i)}\\right)^{1 - x_k^\\prime}\\)\n\\[\n   \\log_e p_{\\boldsymbol{\\theta}}\\left(\n\\textbf{x}^{(i)}| \\textbf{z}^{(i,l)}\\right) = \\sum_{k=1}^{D} \\left(\n   x_k^{(i)} \\ln \\left( \\mu_{D_k}^{(i)} \\right) + \\left(1 - x_k^{(i)}\n   \\right) \\ln \\left( 1 - \\mu_{D_k}^{(i)} \\right) \\right)\n\\]\nwith the original observation \\(\\textbf{x}^{(i)} \\in \\{0, 1\\}^{D}\\). In this case the reconstruction accuracy equals the negative binary cross entropy loss. Note that there are plenty of VAE implementations that use the binary cross entropy loss on non-binary observations, see discussions in this thread.\n\nTo put this into practice, below is a simple VAE Python class which will be used to compare the different decoder distributions.\n\n\nCode\nimport torch\nfrom torch.distributions.multivariate_normal import MultivariateNormal\n\n\nclass VAE(nn.Module):\n    r\"\"\"A simple VAE class based on Kingma and Welling\n        \n    Args:\n        encoder_network:  instance of CoderNetwork class\n        decoder_network:  instance of CoderNetwork class\n        L:                number of samples used during reparameterization trick\n    \"\"\"\n    \n    def __init__(self, encoder_network, decoder_network, L=1):\n        super().__init__()\n        self.encoder = encoder_network\n        self.decoder = decoder_network\n        self.L = L\n        \n        latent_dim = encoder_network.output_dim\n                \n        self.normal_dist = MultivariateNormal(torch.zeros(latent_dim), \n                                              torch.eye(latent_dim))\n        return\n    \n    def forward(self, x):\n        L = self.L\n        \n        z, mu_E, log_var_E = self.encode(x, L)\n        # regularization term per batch, i.e., size: (batch_size)\n        regularization_term = (1/2) * (1 + log_var_E - mu_E**2\n                                       - torch.exp(log_var_E)).sum(axis=1)\n        \n        # upsample x and reshape\n        batch_size = x.shape[0]\n        x_ups = x.repeat(L, 1).view(batch_size, L, -1)    \n        if self.decoder.coder_type == 'Gaussian':\n            # mu_D, log_var_D have shape (batch_size, L, output_dim)\n            mu_D, log_var_D = self.decode(z)\n            # reconstruction accuracy per batch, i.e., size: (batch_size)\n            recons_acc = (1/L) * (-(0.5)*(log_var_D.sum(axis=2)).sum(axis=1)\n               -(0.5) * ((1/torch.exp(log_var_D))*((x_ups - mu_D)**2)\n                         ).sum(axis=2).sum(axis=1))\n        elif self.decoder.coder_type == 'I-Gaussian':\n            # mu_D has shape (batch_size, L, output_dim)\n            mu_D = self.decode(z)\n            # reconstruction accuracy per batch, i.e., size: (batch_size)\n            recons_acc = (1/L) * (-(0.5) * ((x_ups - mu_D)**2\n                                            ).sum(axis=2).sum(axis=1))\n        elif self.decoder.coder_type == 'Bernoulli':\n            # mu_D has shape (batch_size, L, output_dim)\n            mu_D = self.decode(z)     \n            # reconstruction accuracy per batch, i.e., size: (batch_size)\n            # corresponds to the negative binary cross entropy loss (BCELoss)\n            recons_acc = (1/L) * (x_ups * torch.log(mu_D) + \n                                  (1 - x_ups) * torch.log(1 - mu_D)\n                                  ).sum(axis=2).sum(axis=1)\n        loss = - regularization_term.sum() - recons_acc.sum()\n        return loss\n    \n    def encode(self, x, L=1):\n        # get encoder distribution parameters\n        mu_E, log_var_E = self.encoder(x)\n        # sample noise variable L times for each batch\n        batch_size = x.shape[0]\n        epsilon = self.normal_dist.sample(sample_shape=(batch_size, L, ))\n        # upsample mu_E, log_var_E and reshape\n        mu_E_ups = mu_E.repeat(L, 1).view(batch_size, L, -1) \n        log_var_E_ups = log_var_E.repeat(L, 1).view(batch_size, L, -1)\n        # get latent variable by reparametrization trick\n        z = mu_E_ups + torch.sqrt(torch.exp(log_var_E_ups)) * epsilon\n        return z, mu_E, log_var_E\n    \n    def decode(self, z):\n        # get decoder distribution parameters\n        if self.decoder.coder_type == 'Gaussian':\n            mu_D, log_var_D = self.decoder(z)\n            return mu_D, log_var_D\n        elif self.decoder.coder_type == 'I-Gaussian':\n            mu_D = self.decoder(z)\n            return mu_D\n        elif self.decoder.coder_type == 'Bernoulli':\n            mu_D = self.decoder(z)\n            return mu_D\n        return\n\n\nLet’s train the three different VAEs on the MNIST digits dataset\n\n\nCode\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n\ndef train(decoder_type, dataset, x_dim, hid_dim, z_dim, batch_size, L, epochs):\n    encoder_network = CoderNetwork(input_dim=x_dim, \n                                   hidden_dim=hid_dim, \n                                   output_dim=z_dim,\n                                   coder_type='Gaussian')\n    decoder_network = CoderNetwork(input_dim=z_dim, \n                                   hidden_dim=hid_dim, \n                                   output_dim=x_dim,\n                                   coder_type=decoder_type)\n    \n    model = VAE(encoder_network, decoder_network, L=L)\n    data_loader = DataLoader(dataset, batch_size, shuffle=True)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n    \n    print('Start training with {} decoder distribution\\n'.format(decoder_type))\n    for epoch in range(1, epochs + 1):\n        print('Epoch {}/{}'.format(epoch, epochs))\n        avg_loss = 0\n        for counter, (mini_batch_data, label) in enumerate(data_loader):\n            \n            model.zero_grad()\n            \n            loss = model(mini_batch_data.view(-1, x_dim))\n            loss.backward()\n            optimizer.step()\n            \n            avg_loss += loss.item() / len(dataset)\n            \n            if counter % 20 == 0 or (counter + 1)==len(data_loader):\n                batch_loss = loss.item() / len(mini_batch_data)\n                print('\\r[{}/{}] batch loss: {:.2f}'.format(counter + 1,\n                                                            len(data_loader),\n                                                            batch_loss),\n                      end='', flush=True)\n        print('\\nAverage loss: {:.3f}'.format(avg_loss)) \n    print('Done!\\n')\n    trained_VAE = model\n    return trained_VAE\n\ndataset = datasets.MNIST('data/', transform=transforms.ToTensor(), download=True)\nx_dim, hid_dim, z_dim = 28*28, 400, 20\nbatch_size, L, epochs = 128, 5, 3\n\nBernoulli_VAE = train('Bernoulli', dataset, x_dim, hid_dim, z_dim, \n                      batch_size, L, epochs)\nGaussian_VAE = train('Gaussian', dataset, x_dim, hid_dim, z_dim, \n                     batch_size, L, epochs)\nI_Gaussian_VAE = train('I-Gaussian', dataset, x_dim, hid_dim, z_dim, \n                       batch_size, L, epochs)\n\n\n\n\n\n\n\nLet’s look at the differences in the reconstructions:\n\n\nCode\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\ndef plot_results(trained_model, dataset, n_samples):\n    decoder_type = trained_model.decoder.coder_type\n    \n    fig = plt.figure(figsize=(14, 3))\n    fig.suptitle(decoder_type + ' Distribution: Observations (top row) and ' +\n                 'their reconstructions (bottom row)')\n    for i_sample in range(n_samples):\n        x_sample = dataset[i_sample][0].view(-1, 28*28)\n        \n        z, mu_E, log_var_E = trained_model.encode(x_sample, L=1)\n        if decoder_type in ['Bernoulli', 'I-Gaussian']:\n            x_prime = trained_model.decode(z)\n        else:\n            x_prime = trained_model.decode(z)[0]\n    \n        plt.subplot(2, n_samples, i_sample + 1)\n        plt.imshow(x_sample.view(28, 28).data.numpy())\n        plt.axis('off')\n        plt.subplot(2, n_samples, i_sample + 1 + n_samples)\n        plt.imshow(x_prime.view(28, 28).data.numpy())\n        plt.axis('off')\n    return\n\n\nn_samples = 10\n\nplot_results(Bernoulli_VAE, dataset, n_samples)\nplot_results(Gaussian_VAE, dataset, n_samples)\nplot_results(I_Gaussian_VAE, dataset, n_samples)"
  },
  {
    "objectID": "paper_summaries/auto-encoding_variational_bayes/index.html#acknowledgement",
    "href": "paper_summaries/auto-encoding_variational_bayes/index.html#acknowledgement",
    "title": "Auto-Encoding Variational Bayes",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nDaniel Daza’s blog was really helpful and the presented code is highly inspired by his summary on VAEs."
  },
  {
    "objectID": "paper_summaries/auto-encoding_variational_bayes/index.html#footnotes",
    "href": "paper_summaries/auto-encoding_variational_bayes/index.html#footnotes",
    "title": "Auto-Encoding Variational Bayes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe true posterior could be calculated via Bayes theorem \\(\\displaystyle p_{\\boldsymbol{\\theta}} (\\textbf{z}|\\textbf{x}) =\n\\frac {p_{\\boldsymbol{\\theta}} (\\textbf{x}|\\textbf{z})\np_{\\boldsymbol{\\theta}} (\\textbf{z})} {\\int\np_{\\boldsymbol{\\theta}} (\\textbf{x}|\\textbf{z})\np_{\\boldsymbol{\\theta}} (\\textbf{z}) d\\textbf{z}}\\). However, the integral in the denominator is intractable in practice.↩︎\nNote that the variance is by definition greater than zero. Furthermore, the variance is typically relatively small. Thus, using the logarithmized variance as network output increases stability and performance (see this answer for details).↩︎"
  },
  {
    "objectID": "paper_summaries/air/index.html",
    "href": "paper_summaries/air/index.html",
    "title": "Attend, Infer, Repeat: Fast Scene Understanding with Generative Models",
    "section": "",
    "text": "Eslami et al. (2016) introduce the Attend-Infer-Repeat (AIR) framework as an end-to-end trainable generative model capable of decomposing multi-object scenes into its constituent objects in an unsupervised learning setting. AIR builds upon the inductive bias that real-world scenes can be understood as a composition of (locally) self-contained objects. Therefore, AIR uses a structured probabilistic model whose parameters are obtained by inference/optimization. As the name suggests, the image decomposition process can be abstracted into three steps:\nNotably, the model can handle a variable number of objects (upper-bounded) by treating inference as an iterative process. As a proof of concept, they show that AIR could successfully learn to decompose multi-object scenes in multiple datasets (multiple MNIST, Sprites, Omniglot, 3D scenes)."
  },
  {
    "objectID": "paper_summaries/air/index.html#model-description",
    "href": "paper_summaries/air/index.html#model-description",
    "title": "Attend, Infer, Repeat: Fast Scene Understanding with Generative Models",
    "section": "Model Description",
    "text": "Model Description\nAIR is a rather sophisticated framework with some non-trivial subtleties. For the sake of clarity, the following description is organized as follows: Firstly, a high-level overview of the main ideas is given. Secondly, the transition from these ideas into a mathematical formulation (ignoring difficulties) is described. Lastly, the main difficulties are highlighted and how Eslami et al. (2016) proposed to tackle them.\n\nHigh-Level Overview\nIn essence, the model can be understood as a special VAE architecture in which an image \\(\\textbf{x}\\) is encoded to some kind of latent distribution from which we sample the latent representation \\(\\textbf{z}\\) which then can be decoded into an reconstructed image \\(\\widetilde{\\textbf{x}}\\), see image below. The main idea by Eslami et al. (2016) consists of imposing additional structure in the model using the inductive bias that real-world scenes can often be approximated as multi-object scenes, i.e., compositions of several (variable number) objects. Additionally, they assume that all of these objects live in the same domain, i.e., each object is an instantiation from the same class.\n\n\n\n\n\n\n\n\n\n\n\nStandard VAE Architecture. AIR can be understood as a modified VAE architecture.\n\n\n\nTo this end, Eslami et al. (2016) replace the encoder with an recurrent, variable-length inference network to obtain a group-structured latent representation. Each group \\(\\textbf{z}^{(i)}\\) should ideally correspond to one object where the entries can be understood as the compressed attributes of that object (e.g., type, appearance, pose). The main purpose of the inference network is to explain the whole scene by iteratively updating what remains to be explained, i.e., each step is conditioned on the image and on its knowledge of previously explained objects, see image below. Since they assume that each object lives in the same domain, the decoder is applied group-wise, i.e., each vector \\(\\textbf{z}^{(i)}\\) is fed through the same decoder network, see image below.\n\n\n\n\n\n\n\n\n\n\n\nVAE with Recurrent Inference Network. A group-structured latent representation is obtained by replacing the encoder with a recurrent, variable-length inference network. This network should ideally attend to one object at a time and is conditioned on the image \\(\\textbf{x}\\) and its knowledge of previously epxlained objects \\(\\textbf{h}\\), \\(\\textbf{z}\\).\n\n\n\nEslami et al. (2016) put additional structure to the model by dividing the latent space of each object into what, where and pres. As the names suggest, \\(\\textbf{z}^{(i)}_{\\text{what}}\\) corresponds to the objects appearance, while \\(\\textbf{z}^{(i)}_{\\text{where}}\\) gives information about the position and scale. \\(\\text{z}_{\\text{pres}}^{(i)}\\) is a binary variable describing whether an object is present, it is rather a helper variable to allow for a variable number of objects to be detected (going to be explained in the Difficulties section).\nTo disentangle whatfrom where, the inference network extracts attentions crops \\(\\textbf{x}^{(i)}_{\\text{att}}\\) of the image \\(\\textbf{x}\\) based on a three-dimensional vector \\(\\textbf{z}^{(i)}_{\\text{where}} \\left( \\textbf{h}^{(i)} \\right)\\) which specifies the affine parameters \\((s^{(i)}, t_x^{(i)}, t_y^{(i)})\\) of the attention transformation1. These attention crops are then put through a standard VAE to encode the latent what-vector \\(\\textbf{z}^{(i)}_{\\text{what}}\\). Note that each attention crop is put through the same VAE, thereby consistency between compressed object attributes is achieved (i.e., each object is an instantiation of the same class).\nOn the decoder side, the reconstructed attention crop \\(\\widetilde{\\textbf{x}}^{(i)}_{\\text{att}}\\) is transformed to \\(\\widetilde{\\textbf{x}}^{(i)}\\) using the information from \\(\\textbf{z}^{(i)}_{\\text{where}}\\). \\(\\widetilde{\\textbf{x}}^{(i)}\\) can be understood as a reconstructed image of the \\(i\\)-th object in the original image \\(\\textbf{x}\\). Note that \\(\\text{z}^{(i)}_{\\text{pres}}\\) is used to decide whether the contribution of \\(\\widetilde{\\textbf{x}}^{(i)}_{\\text{att}}\\) is added to the otherwise empty canvas \\(\\widetilde{\\textbf{x}}^{(i)}\\).\nThe schematic below summarizes the whole AIR architecture.\n\n\n\n\n\n\n\n\nSchematic of AIR\n\n\n\nCreation of Attention Crops and Inverse Transformation: As stated before, a Spatial Transformer (ST) module is used to produce the attention crops using a standard attention transformation. Remind that this means that the regular grid \\(\\textbf{G} = \\{\\begin{bmatrix}\nx_k^t & y_k^t \\end{bmatrix}^{\\text{T}} \\}\\) defined on the output is transformed into a new sampling grid \\(\\widetilde{\\textbf{G}} = \\{\\begin{bmatrix}\nx_k^s & y_k^s \\end{bmatrix}^{\\text{T}} \\}\\) defined on the input. The latent vector \\(\\textbf{z}^{(i)}_{\\text{where}}\\) can be used to build the attention transformation matrix, i.e.,\n\\[\n  \\textbf{A}^{(i)} = \\begin{bmatrix} s^{(i)} & 0 & t_x^{(i)} \\\\\n   0 & s^{(i)} & t_y^{(i)} \\\\ 0 & 0 & 1\\end{bmatrix}, \\quad \\quad \\quad\n  \\begin{bmatrix} x_k^s \\\\ y_k^s \\\\ 1 \\end{bmatrix} = \\textbf{A}^{(i)}\n  \\begin{bmatrix} x_k^t \\\\ y_k^t \\\\ 1\\end{bmatrix}\n\\]\nThis is nothing new, but how do we map the reconstructed attention crop \\(\\tilde{\\textbf{x}}^{(i)}_{\\text{att}}\\) back to the original image space, i.e., how can we produce \\(\\widetilde{\\textbf{x}}^{(i)}\\) from \\(\\widetilde{\\textbf{x}}^{(i)}_{\\text{att}}\\) and \\(\\textbf{z}^{(i)}_{\\text{where}}\\)? The answer is pretty simple, we use the (pseudo)inverse2 of the formerly defined attention transformation matrix, i.e.,\n\\[\n\\begin{bmatrix} x_k^s \\\\ y_k^s \\\\ 1 \\end{bmatrix} = \\left(\\textbf{A}^{(i)}\\right)^{+}\n  \\begin{bmatrix} x_k^t \\\\ y_k^t \\\\ 1\\end{bmatrix} \\stackrel{s\\neq\n  0}{=} \\begin{bmatrix} \\frac {1}{s^{(i)}} & 0 & - \\frac{t_x^{(i)}}{s} \\\\\n   0 & \\frac {1}{s^{(i)}} & -\\frac {t_y^{(i)}}{s} \\\\ 0 & 0 &\n   1\\end{bmatrix}\\begin{bmatrix} x_k^t \\\\ y_k^t \\\\ 1\\end{bmatrix},\n\\]\nwhere \\(\\left(\\textbf{A}^{(i)}\\right)^{+}\\) denotes the Moore-Penrose inverse of \\(\\textbf{A}^{(i)}\\), and the regular grid \\(\\textbf{G} = \\{\\begin{bmatrix}\nx_k^t & y_k^t \\end{bmatrix}^{\\text{T}} \\}\\) is now defined on the original image space3. Below is a self-written interactive visualization where \\(\\widetilde{\\textbf{x}}^{(i)}_{\\text{att}} =\n\\textbf{x}^{(i)}_{\\text{att}}\\). It shows nicely that the whole process can abstractly be understood as cutting of a crop from the original image and placing the reconstructed version with the inverse scaling and shifting on an otherwise empty (black) canvas. The code and visualization can be found here.\n\n\n\n\n\n\n\n\n\n\n\nInteractive Transformation Visualization\n\n\n\n\n\nMathematical Model\nWhile the former model description gave an overview about the inner workings and ideas of AIR, the following section introduces the probabilistic model over which AIR operates. Similar to the VAE paper by Kingma and Welling (2013), Eslami et al. (2016) introduce a modeling assumption for the generative process and use a variational approximation for the true posterior of that process to allow for joint optimization of the inference (encoder) and generator (decoder) parameters.\nIn contrast to standard VAEs, the modeling assumption for the generative process is more structured in AIR, see image below. It assumes that:\n\nThe number of objects \\(n\\) is sampled from some discrete prior distribution \\(p_N\\) (e.g., geometric distribution) with maximum value \\(N\\).\nThe latent scene descriptor \\(\\textbf{z} =\n\\left(\\textbf{z}^{(1)}, \\textbf{z}^{(2)}, \\dots, \\textbf{z}^{(n)}\n\\right)\\) (length depends on sampled \\(n\\)) is sampled from a scene model \\(\\textbf{z} \\sim p_{\\boldsymbol{\\theta}}^{z} \\left( \\cdot | n\n\\right)\\), where each vector \\(\\textbf{z}^{(i)}\\) describes the attributes of one object in the scene. Furthermore, Eslami et al. (2016) assume that \\(\\textbf{z}^{(i)}\\) are independent for each possible \\(n\\), i.e., \\(p_{\\boldsymbol{\\theta}}^{z} \\left( \\textbf{z} | n \\right) =\n\\prod_{i=1}^n p_{\\boldsymbol{\\theta}}^z \\left( \\textbf{z}^{(i)}\\right)\\).\n\\(\\textbf{x}\\) is generated by sampling from the conditional distribution \\(p_{\\boldsymbol{\\theta}}^{x} \\left( \\textbf{x} |\n\\textbf{z} \\right)\\).\n\nAs a result, the marginal likelihood of an image given the generative model parameters can be stated as follows\n\\[\np_{\\boldsymbol{\\theta}} (\\textbf{x}) = \\sum_{n=1}^N p_N (n) \\int\np_{\\boldsymbol{\\theta}}^z \\left( \\textbf{z} | n \\right)\np_{\\boldsymbol{\\theta}}^x \\left( \\textbf{x} | \\textbf{z}\\right) d \\textbf{z}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nGenerative Model VAE vs AIR. Note that for a given dataset \\(\\textbf{X} = \\{ \\textbf{x}^{(i)}\\}_{i=1}^{L}\\) the marginal likelihood of the whole dataset can be computed via $p_{} ( ) = {i=1}^{L} p{} ( ^{(i)} ) $.\n\n\n\nLearning by optimizing the ELBO: Since the integral is intractable for most models, Eslami et al. (2016) introduce an amortized4 variational approximation \\(q_{\\boldsymbol{\\phi}} \\left(\\textbf{z}, n | \\textbf{x}\\right)\\) for the true posterior \\(p_{\\boldsymbol{\\theta}}\\left(\\textbf{z}, n\n|\\textbf{x}\\right)\\). From here on, the steps are very similar to the VAE paper by Kingma and Welling (2013): The objective of minimizing the KL divergence between the parameterized variational approximation (using a neural network) and the true (but unknown) posterior \\(p_{\\boldsymbol{\\theta}}\\left(\\textbf{z}, n\n|\\textbf{x}\\right)\\) is approximated by maximizing the evidence lower bound (ELBO):\n\\[\n\\mathcal{L} \\left( \\boldsymbol{\\theta}, \\boldsymbol{\\phi};\n\\textbf{x}^{(i)} \\right) = \\underbrace{- D_{KL} \\left( q_{\\boldsymbol{\\phi}}\n\\left( \\textbf{z}, n | \\textbf{x}^{(i)}\\right) || p_{\\boldsymbol{\\theta}}\n(\\textbf{z}, n)\\right)}_{\\text{Regularization Term}} + \\underbrace{\\mathbb{E}_{q_{\\boldsymbol{\\phi}} \\left(\n\\textbf{z}, n | \\textbf{x}^{(i)} \\right)} \\left[ \\log\np_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)} | \\textbf{z}, n\n\\right) \\right]}_{\\text{Reconstruction Accuracy}},\n\\]\nwhere \\(p_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)} | \\textbf{z},\nn \\right)\\) is a parameterized probabilistic decoder5 (using a neural network) and \\(p_{\\boldsymbol{\\theta}} (\\textbf{z}, n) =\np_{\\boldsymbol{\\theta}} \\left(\\textbf{z} | n \\right)\np \\left( n \\right)\\) is prior on the joint probability of \\(\\textbf{z}\\) and \\(n\\) that we need to define a priori. As a result, the optimal parameters \\(\\boldsymbol{\\theta}\\), \\(\\boldsymbol{\\phi}\\) can be learnt jointly by optimizing (maximizing) the ELBO.\n\n\nDifficulties\nIn the former explanation, it was assummed that we could easily define some parameterized probabilistic encoder \\(q_{\\boldsymbol{\\phi}} \\left(\n\\textbf{z}, n | \\textbf{x}^{(i)} \\right)\\) and decoder \\(p_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)} |\n\\textbf{z}, n \\right)\\) using neural networks. However, there are some obstacles in our way:\n\nHow can we infer a variable number of objects \\(n\\)? Actually, we would need to evaluate \\(p_N \\left(n | \\textbf{x}\\right) = \\int\nq_{\\boldsymbol{\\phi}} \\left(\\textbf{z}, n | \\textbf{x} \\right)\nd \\textbf{z}\\) for all \\(n=1,\\dots, N\\) and then sample from the resulting distribution.  \nThe number of objects \\(n\\) is clearly a discrete variable. How can we backprograte if we sample from a discrete distribution?\nWhat priors should we choose? Especially, the prior for the number of objects in a scene \\(n \\sim p_N\\) is unclear.\nWhat the first or second object in a scene constitutes is somewhat arbitrary. As a result, object assigments \\(\\begin{bmatrix} \\textbf{z}^{(1)} & \\dots & \\textbf{z}^{(n)}\n\\end{bmatrix} =\\textbf{z} \\sim q_{\\boldsymbol{\\phi}} \\left(\\textbf{z} |\n\\textbf{x}^{(i)}, n \\right)\\) should be exchangeable and the decoder \\(p_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)} |\n\\textbf{z}, n \\right)\\) should be permutation invariant in terms of \\(\\textbf{z}^{(i)}\\). Thus, the latent representation needs to preserve some strong symmetries.\n\nEslami et al. (2016) tackle these challenges by defining inference as an iterative process using a recurrent neural network (RNN) that is run for \\(N\\) steps (maximum number of objects). As a result, the number of objects \\(n\\) can be encoded in the latent distribution by defining the approximated posterior as follows\n\\[\n  q_{\\boldsymbol{\\phi}} \\left( \\textbf{z}, \\textbf{z}_{\\text{pres}} |\n  \\textbf{x} \\right) = q_{\\boldsymbol{\\phi}} \\left(\n  z_{\\text{pres}}^{(n+1)} = 0 | \\textbf{z}^{(1:n)} , \\textbf{x}\\right)\n  \\prod_{i=1}^n q_{\\boldsymbol{\\phi}} \\left( \\textbf{z}^{(i)} ,\n  z_{\\text{pres}}^{(i)}=1 | \\textbf{x}, \\textbf{z}^{(1:i-1)}\\right),\n\\]\nwhere \\(z_{\\text{pres}}^{(i)}\\) is an introduced binary variable sampled from a Bernoulli distribution \\(z_{\\text{pres}}^{(i)} \\sim \\text{Bern} \\left(\np_{\\text{pres}}^{(i)} \\right)\\) whose probability \\(p_{\\text{pres}}^{(i)}\\) is predicted at each iteration step. Whenever \\(z_{\\text{pres}}^{(i)}=0\\) the inference process stops and no more objects can be described, i.e., we enforce \\(z_{\\text{pres}}^{(i+1)}=0\\) for all subsequent steps such that the vector \\(\\textbf{z}_{\\text{pres}}\\) looks as follows\n\\[\n\\textbf{z}_{\\text{pres}} = \\begin{bmatrix} \\smash[t]{\\overbrace{\\begin{matrix}1 & 1 & \\dots &\n1\\end{matrix}}^{n \\text{ times}}}   & 0 &\\dots & 0 \\end{bmatrix}\n\\]\nThus, \\(z_{\\text{pres}}^{(i)}\\) may be understood as an interruption variable. Recurrence is required to avoid explaining the same object twice.\nBackpropagation for Discrete Variables: While we can easily draw samples from a Bernoulli distribution \\(z_{\\text{pres}}^{(i)} \\sim \\text{Bern} \\left(\np_{\\text{pres}}^{(i)} \\right)\\), backpropagation turns out to be problematic. Remind that for continuous variables such as Gaussian distributions parameterized by mean and variance (e.g., \\(\\textbf{z}^{(i)}_{\\text{what}}\\), \\(\\textbf{z}^{(i)}_{\\text{where}}\\)) there is the reparameterization trick to circumvent this problem. However, any reparameterization of discrete variables includes discontinuous operations through which we cannot backprograte. Thus, Eslami et al. (2016) use a variant of the score-function estimator as a gradient estimator. More precisely, the reconstruction accuracy gradient w.r.t. \\(\\textbf{z}_{\\text{pres}}\\) is approximated by the score-function estimator, i.e.,\n\\[\n\\begin{align}\n\\nabla_{\\boldsymbol{\\phi}}\\mathbb{E}_{q_{\\boldsymbol{\\phi}} \\left(\n\\textbf{z}, n | \\textbf{x}^{(i)} \\right)} \\left[ \\log\np_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)} | \\textbf{z}, n\n\\right) \\right] &= \\mathbb{E}_{q_{\\boldsymbol{\\phi}} \\left(\n\\textbf{z}, n | \\textbf{x}^{(i)} \\right)} \\left[ \\log\np_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)} | \\textbf{z}, n\n\\right) \\nabla_{\\boldsymbol{\\phi}} q_{\\boldsymbol{\\phi}} \\left(\n\\textbf{z}, n | \\textbf{x}^{(i)} \\right) \\right] \\\\\n&\\approx \\frac {1}{N} \\sum_{k=1}^N \\log p_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)} | \\left(\\textbf{z}, n\\right)^{(k)}\n\\right) \\nabla_{\\boldsymbol{\\phi}} q_{\\boldsymbol{\\phi}} \\left(\n\\left(\\textbf{z}, n\\right)^{(k)} | \\textbf{x}^{(i)} \\right)\\\\\n&\\quad \\text{with} \\quad \\left(\\textbf{z}, n\\right)^{(k)} \\sim q_{\\boldsymbol{\\phi}} \\left(\n\\textbf{z}, n | \\textbf{x}^{(i)} \\right)\n\\end{align}\n\\]\nEslami et al. (2016) note that in this raw form the gradient estimate is likely to have high variance. To reduce variance, they use appropriately structured neural baselines citing a paper from Minh and Gregor, 2014. Without going into too much detail, appropriately structured neural baselines build upon the idea of variance reduction in score function estimators by introducing a scalar baseline \\(\\lambda\\) as follows\n\\[\n\\begin{align}\n&\\nabla_{\\boldsymbol{\\phi}} \\mathbb{E}_{q_{\\boldsymbol{\\phi}} \\left(\n\\textbf{z}_{\\text{pres}} | \\textbf{x}^{(i)} \\right)} \\left[ \\log\np_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)} | \\textbf{z}, n\n\\right) \\right] = \\mathbb{E}_{q_{\\boldsymbol{\\phi}} \\left(\n\\textbf{z}, n | \\textbf{x}^{(i)} \\right)} \\left[ \\Big(\nf_{\\boldsymbol{\\theta}} \\left( \\textbf{x}, \\textbf{z} \\right) - \\lambda  \\Big)\n\\nabla_{\\boldsymbol{\\phi}} q_{\\boldsymbol{\\phi}} \\left(\\textbf{z}, n |\n\\textbf{x}^{(i)} \\right) \\right]\\\\\n&\\text{with} \\quad f_{\\boldsymbol{\\theta}} \\left( \\textbf{x}, \\textbf{z} \\right)\n= \\log p_{\\boldsymbol{\\theta}} \\left( \\textbf{x}^{(i)} | \\textbf{z}, n \\right), \\quad\n\\text{since} \\quad\\mathbb{E}_{q_{\\boldsymbol{\\phi}} \\left(\n\\textbf{z}_{\\text{pres}} | \\textbf{x}^{(i)} \\right)} \\left[ \\nabla_{\\boldsymbol{\\phi}} q_{\\boldsymbol{\\phi}} \\left(\\textbf{z}, n |\n\\textbf{x}^{(i)} \\right) \\right] = \\textbf{0}.\n\\end{align}\n\\]\nMinh and Gregor, 2014 propose to use a data-dependent neural baseline \\(\\lambda_{\\boldsymbol{\\psi}} (\\textbf{x})\\) that is trained to match its target \\(f_{\\boldsymbol{\\theta}}\\). For further reading, pyro’s SVI part III is a good starting point.\nPrior Distributions: Before we take a closer look on the prior distribution, it will be helpful to rewrite the regularization term\n\\[\n\\begin{align}\nD_{KL} & \\left(q_{\\boldsymbol{\\phi}} \\left(\\textbf{z}, n | \\textbf{x}^{(i)}\n\\right) || p_{\\boldsymbol{\\theta}} \\left( \\textbf{z}, n\\right) \\right) = D_{KL}\n\\left( \\prod_{i=1}^n q_{\\boldsymbol{\\phi}} \\left(\\textbf{z}^{(i)}| \\textbf{x},\n\\textbf{z}^{(1:i-1)} \\right) || \\prod_{i=1}^n p_{\\boldsymbol{\\theta}} \\left(\n\\textbf{z}^{(i)} \\right) \\right)\\\\\n&\\stackrel{\\text{independent dists.}}{=} \\sum_{i=1}^n D_{KL} \\left[\n\\prod_{k=1}^{3} q_{\\boldsymbol{\\phi}} \\left(\\textbf{z}^{(i)}_k |  \\textbf{x},\n\\textbf{z}^{(1:i-1)} \\right) || \\prod_{k=1}^3 p_{\\boldsymbol{\\theta}} \\left(\n\\textbf{z}^{(i)}_k \\right)  \\right]\\\\\n&\\stackrel{\\text{independent dists.}}{=} \\sum_{i=1}^n \\sum_{k\\in \\{\\text{pres},\n\\text{where}, \\text{what}\\}} D_{KL} \\left[ q_{\\boldsymbol{\\phi}} \\left(\\textbf{z}^{(i)}_k| \\textbf{x},\n\\textbf{z}^{(1:i-1)} \\right) || p_{\\boldsymbol{\\theta}} \\left(\n\\textbf{z}^{(i)}_k \\right)  \\right]\n\\end{align}\n\\]\nNote that we assume that each \\(\\textbf{z}_k^{(i)}\\) is sampled independently from their respective distribution such that products could equally be rewritten as concatenated vectors. Clearly, there are three different prior distributions that we need to define in advance:\n\n\\(p_{\\boldsymbol{\\theta}} \\left(\\textbf{z}_{\\text{what}}^{(i)} \\right) \\sim\n\\mathcal{N} \\left(\\textbf{0}, \\textbf{I} \\right)\\): A centerd isotropic Gaussian prior is a typical choice in standard VAEs and has proven to be effective6. Remind that the what-VAE should ideally receive patches of standard MNIST digits.\n\n\n\\(p_{\\boldsymbol{\\theta}} \\left(\\textbf{z}_{\\text{where}}^{(i)}\\right) \\sim\n\\mathcal{N} \\left( \\boldsymbol{\\mu}_{\\text{w}} ,\n\\boldsymbol{\\sigma}_{\\text{w}}^2 \\textbf{I}  \\right)\\): In this distribution, we can encode prior knowledge about the objects locality, i.e., average size and location of objects and their standard deviations.\n\\(p_{\\boldsymbol{\\theta}} \\left(\\textbf{z}_{\\text{pres}}^{(i)}\\right) \\sim\n\\text{Bern} (p_{\\text{pres}})\\): Eslami et al. (2016) used an annealing geometric distribution as a prior on the number of objects7, i.e., the success probability decreases from a value close to 1 to some small value close to 0 during the course of the training. The intuitive idea behind this process is to encourage the model to explore the use of objects (in the initial phase), and then to constrain the model to use as few objects as possible (trade-off between number of objects and reconstruction accuracy).\nFor simplicity, we use a fixed Bernoulli distribution for each step as suggested in the pyro tutorial with \\(p_{\\text{pres}} = 0.01\\), i.e., we will constrain the number of objects from the beginning. To encourage the model to use objects we initialize the what-decoder to produce empty scenes such that things do not get much worse in terms of reconstruction accuracy when objects are used (also inspired by pyro)."
  },
  {
    "objectID": "paper_summaries/air/index.html#implementation",
    "href": "paper_summaries/air/index.html#implementation",
    "title": "Attend, Infer, Repeat: Fast Scene Understanding with Generative Models",
    "section": "Implementation",
    "text": "Implementation\nThe following reimplementation aims to reproduce the results of the multi-MNIST experiment, see image below. We will make some adaptations inspired by this pyro tutorial and this pytorch reimplementation from Andrea Dittadi. As a result, the following reimplementation receives a huge speed up in terms of convergence time and can be trained in less than 10 minutes on a Nvidia Tesla K80 (compared to 2 days on a Nvidia Quadro K4000 GPU by Eslami et al. (2016)).\nAs noted by Eslami et al. (2016), their model successfully learned to count the number of digits and their location in each image (i.e., appropriate attention windows) without any supervision. Furthermore, the scanning policy of the inference network (i.e., object assignment policy) converges to spatially divided regions where the direction of the spatial border seems to be random (dependent on random initialization). Lastly, the model also learned that it never needs to assign a third object (all images in the training dataset contained a maximum of two digits).  \n\n\n\n\n\n\n\n\nPaper Results of Multi-MNIST Experiment. Taken from Eslami et al. (2016).\n\n\n\nEslami et al. (2016) argue that the the structure of AIR puts an important inductive bias onto explaining multi-object scenes by using two adversaries: * AIR wants to explain the scene, i.e., the reconstruction error should be minimized. * AIR is penalized for each instantiated object due to the KL divergence. Furthermore, the what-VAE puts an additional prior of instantiating similar objects.\n\nMulti-MNIST Dataset\nThe multi-MNIST datasets consists of \\(50 \\times 50\\) gray-scale images containing zero, one or two non-overlapping random MNIST digits with equal probability, see image below. This dataset can easily be generated by taking a blank \\(50 \\times\n50\\) canvas and positioning a random number of digits (drawn uniformly from MNIST dataset) onto it. To ensure that MNIST digits (\\(28\\times28\\)) will not overlap, we scale them to \\(24\\times 24\\) and then position them such that the centers of two MNIST digits do not overlap. Note that some small overlap may occur which we simply accept. At the same time, we record the number of digits in each generated image to measure the count accuracy during training.\n\n\n\n\n\n\n\n\n\n\n\nMulti-MNIST Dataset Examples.\n\n\n\n\n\nCode\nimport torch\nimport numpy as np\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import TensorDataset\n\nCANVAS_SIZE = 50                # canvas in which 0/1/2 MNIST digits are put\nMNIST_SIZE = 24                 # size of original MNIST digits (resized)\n\ndef generate_dataset(num_images, SEED=1):\n    \"\"\"generates multiple MNIST dataset with 0, 1 or 2 non-overlaping digits\n\n    Args:\n        num_images (int): number of images inside dataset\n\n    Returns:\n        multiple_MNIST (torch dataset)\n    \"\"\"\n    data = torch.zeros([num_images, 1, CANVAS_SIZE, CANVAS_SIZE])\n\n    original_MNIST = datasets.MNIST('./data', train=True, download=True,\n        transform=transforms.Compose([\n          transforms.Resize(size=(MNIST_SIZE, MNIST_SIZE)),\n          transforms.ToTensor()]))\n    # sample random digits and positions\n    np.random.seed(SEED)\n    pos_positions = np.arange(int(MNIST_SIZE/2),CANVAS_SIZE - int(MNIST_SIZE/2))\n\n    mnist_indices = np.random.randint(len(original_MNIST), size=(num_images, 2))\n    num_digits = np.random.randint(3, size=(num_images))\n    positions_0 = np.random.choice(pos_positions, size=(num_images, 2),\n                                   replace=True)\n\n    for i_data in range(num_images):\n        if num_digits[i_data] &gt; 0:\n            # add random digit at random position\n            random_digit = original_MNIST[mnist_indices[i_data][0]][0]\n            x_0, y_0 = positions_0[i_data][0], positions_0[i_data][1]\n            x = [x_0-int(MNIST_SIZE/2), x_0+int(MNIST_SIZE/2)]\n            y = [y_0-int(MNIST_SIZE/2), y_0+int(MNIST_SIZE/2)]\n            data[i_data,:,y[0]:y[1],x[0]:x[1]] += random_digit\n            if num_digits[i_data] == 2:\n                # add second non overlaping random digit\n                random_digit = original_MNIST[mnist_indices[i_data][1]][0]\n                impos_x_pos = np.arange(x_0-int(MNIST_SIZE/2),\n                                        x_0+int(MNIST_SIZE/2))\n                impos_y_pos = np.arange(y_0-int(MNIST_SIZE/2),\n                                        y_0+int(MNIST_SIZE/2))\n                x_1 = np.random.choice(np.setdiff1d(pos_positions, impos_x_pos),\n                                       size=1)[0]\n                y_1 = np.random.choice(np.setdiff1d(pos_positions, impos_y_pos),\n                                       size=1)[0]\n                x = [x_1-int(MNIST_SIZE/2), x_1+int(MNIST_SIZE/2)]\n                y = [y_1-int(MNIST_SIZE/2), y_1+int(MNIST_SIZE/2)]\n                data[i_data,:,y[0]:y[1],x[0]:x[1]] += random_digit\n    labels = torch.from_numpy(num_digits)\n    return TensorDataset(data.type(torch.float32), labels)\n\n\n\n\nModel Implementation\nFor the sake of clarity, the model implementation is divided into its constitutive parts:\n\nwhat-VAE implementation: The what-VAE can be implemented as an independent class that receives an image patch (crop) and outputs its reconstruction as well as its latent distribution parameters. Note that we could also compute the KL divergence and reconstruction error within that class, however we will put the whole loss computation in another function to have everything in one place. As shown in a previous summary, two fully connected layers with ReLU non-linearity in between suffice for decent reconstructions of MNIST digits.\nWe have additional prior knowledge about the output distribution: It should only be between 0 and 1. It is always useful to put as much prior knowledge as possible into the architecture, but how to achieve this?\n\nClamping: The most intuitive idea would be to simply clamp the network outputs, however this is a bad idea as gradients wont propagate if the outputs are outside of the clamped region.\nNetwork Initialization: Another approach would be to simply initialize the weights and biases of the output layer to zero such that further updates push the outputs into the positive direction. However, as the reconstruction of the whole image in AIR is a sum over multiple reconstructions, this turns out to be a bad idea as well. I tried it and the what-VAE produces negative outputs which it compensates with another object that has outputs greater than 1.\nSigmoid Layer: This is a typical choice in classification problems and is commonly used in VAEs when the decoder approximates a Bernoulli distribution. However, it should be noted that using MSE loss (Gaussian decoder distribution) with a sigmoid is generally not advised due to the vanishing/saturating gradients (explained here).\nOn the other hand, using a Bernoulli distribution for the reconstruction of the whole image (sum over multiple reconstruction) comes with additional problems, e.g., numerical instabilities due to empty canvas (binary cross entropy can not be computed when probabilties are exactly 0) and due to clamping (as the sum over multiple bernoulli means could easily overshoot 1). While there might be some workarounds, I decided to go an easier path.\nOutput Distribution \\(\\mathbb{R}_{+}\\): This is motivated by the observations I made during the network initialization approach. By impeding the network to produce negative outputs, we indirectly force outputs between \\(0\\) and \\(1\\). Thereby, we do not need to get our hands dirty with a Bernoulli distribution or vanishing gradient problems. Furthermore, we use Pytorch’s default initialization8 to produce mostly empty objects. This encourages the model to try out objects in the beginning of the training.\n\n\n\n\nCode\nfrom torch import nn\n\nWINDOW_SIZE = MNIST_SIZE        # patch size (in one dimension) of what-VAE\nZ_WHAT_HIDDEN_DIM = 400         # hidden dimension of what-VAE\nZ_WHAT_DIM = 20                 # latent dimension of what-VAE\nFIXED_VAR = 0.5**2              # fixed variance of Gaussian decoder\n\n\nclass VAE(nn.Module):\n    \"\"\"simple VAE class with a Gaussian encoder (mean and diagonal variance\n    structure) and a Gaussian decoder with fixed variance\n\n    Attributes:\n        encoder (nn.Sequential): encoder network for mean and log_var\n        decoder (nn.Sequential): decoder network for mean (fixed var)\n    \"\"\"\n\n    def __init__(self):\n        super(VAE, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(WINDOW_SIZE**2, Z_WHAT_HIDDEN_DIM),\n            nn.ReLU(),\n            nn.Linear(Z_WHAT_HIDDEN_DIM, Z_WHAT_DIM*2),\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(Z_WHAT_DIM, Z_WHAT_HIDDEN_DIM),\n            nn.ReLU(),\n            nn.Linear(Z_WHAT_HIDDEN_DIM, WINDOW_SIZE**2),\n        )\n        return\n\n    def forward(self, x_att_i):\n        z_what_i, mu_E_i, log_var_E_i = self.encode(x_att_i)\n        x_tilde_att_i = self.decode(z_what_i)\n        return x_tilde_att_i, z_what_i, mu_E_i, log_var_E_i\n\n    def encode(self, x_att_i):\n        batch_size = x_att_i.shape[0]\n        # get encoder distribution parameters\n        out_encoder = self.encoder(x_att_i.view(batch_size, -1))\n        mu_E_i, log_var_E_i = torch.chunk(out_encoder, 2, dim=1)\n        # sample noise variable for each batch\n        epsilon = torch.randn_like(log_var_E_i)\n        # get latent variable by reparametrization trick\n        z_what_i = mu_E_i + torch.exp(0.5*log_var_E_i) * epsilon\n        return z_what_i, mu_E_i, log_var_E_i\n\n    def decode(self, z_what_i):\n        # get decoder distribution parameters\n        x_tilde_att_i = self.decoder(z_what_i)\n        # force output to be positive\n        x_tilde_att_i = x_tilde_att_i.abs()\n        # reshape to [1, WINDOW_SIZE, WINDOW_SIZE] (input shape)\n        x_tilde_att_i = x_tilde_att_i.view(-1, 1, WINDOW_SIZE, WINDOW_SIZE)\n        return x_tilde_att_i\n\n\n\nRecurrent Inference Network: Eslami et al. (2016) used a standard recurrent neural network (RNN) which in each step \\(i\\) computes\n\\[\n\\left(\\underbrace{p^{(i)}_{\\text{pres}}, \\boldsymbol{\\mu}_{\\text{where}},\n\\boldsymbol{\\sigma}^2_{\\text{where}}}_{\\boldsymbol{\\omega}^{(i)}},\n\\textbf{h}^{(i)} \\right) = RNN \\left(\\textbf{x},\n\\underbrace{\\text{z}_{\\text{pres}}^{(i-1)}, \\textbf{z}_{\\text{what}}^{(i-1)},\n\\textbf{z}_{\\text{where}}^{(i-1)}}_{\\textbf{z}^{(i-1)}}, \\textbf{h}^{(i-1)}\n\\right),\n\\]\ni.e., the distribution parameters of \\(\\text{z}_{\\text{pres}}^{(i)}\\sim\n\\text{Bern}\\left( p^{(i)}_{\\text{pres}} \\right)\\) and \\(\\textbf{z}_{\\text{where}}^{(i)} \\sim \\mathcal{N} \\left( \\boldsymbol{\\mu}_{\\text{where}},\n\\boldsymbol{\\sigma}^2_{\\text{where}}\\textbf{I}\\right)\\), and the next hidden state \\(\\textbf{h}^{(i)}\\). They did not provide any specifics about the network architecture, however in my experiments it turned out that a simple 3 layer (fully-connected) network suffices for this task.\nTo speed up convergence, we initialize useful distribution parameters:\n\n\\(p_{\\text{pres}}^{(i)}\\approx 0.8\\): This encourages AIR to use objects in the beginning of training.\n\\(\\boldsymbol{\\mu}_{\\text{where}} = \\begin{bmatrix} -3 & 0 &\n0\\end{bmatrix}^{\\text{T}}\\): This leads to a center crop with (approximate) size of the inserted digits.\n\\(\\boldsymbol{\\sigma}_{\\text{where}}^2 \\approx \\begin{bmatrix} 0.05 & 0.05 &\n0.05\\end{bmatrix}^{\\text{T}}\\): Start with low variance.\n\nNote: We use a very similar recurrent network architecture for the neural baseline model (to predict the negative log-likelihood), see code below.\n\n\n\nCode\nZ_PRES_DIM = 1                      # latent dimension of z_pres\nZ_WHERE_DIM = 3                     # latent dimension of z_where\nRNN_HIDDEN_STATE_DIM = 256          # hidden state dimension of RNN\nP_PRES_INIT = [2.]                  # initialization p_pres (sigmoid -&gt; 0.8)\nMU_WHERE_INIT = [3.0, 0., 0.]       # initialization z_where mean\nLOG_VAR_WHERE_INIT = [-3.,-3.,-3.]  # initialization z_where log var\nZ_DIM = Z_PRES_DIM + Z_WHERE_DIM + Z_WHAT_DIM\n\n\nclass RNN(nn.Module):\n\n    def __init__(self, baseline_net=False):\n        super(RNN, self).__init__()\n        self.baseline_net = baseline_net\n        INPUT_SIZE = (CANVAS_SIZE**2) + RNN_HIDDEN_STATE_DIM + Z_DIM\n        if baseline_net:\n            OUTPUT_SIZE = (RNN_HIDDEN_STATE_DIM + 1)\n        else:\n            OUTPUT_SIZE = (RNN_HIDDEN_STATE_DIM + Z_PRES_DIM + 2*Z_WHERE_DIM)\n        output_layer = nn.Linear(RNN_HIDDEN_STATE_DIM, OUTPUT_SIZE)\n\n        self.fc_rnn = nn.Sequential(\n            nn.Linear(INPUT_SIZE, RNN_HIDDEN_STATE_DIM),\n            nn.ReLU(),\n            nn.Linear(RNN_HIDDEN_STATE_DIM, RNN_HIDDEN_STATE_DIM),\n            nn.ReLU(),\n            output_layer\n        )\n        if not baseline_net:\n            # initialize distribution parameters\n            output_layer.weight.data[0:7] = nn.Parameter(\n                torch.zeros(Z_PRES_DIM+2*Z_WHERE_DIM, RNN_HIDDEN_STATE_DIM)\n            )\n            output_layer.bias.data[0:7] = nn.Parameter(\n                torch.tensor(P_PRES_INIT + MU_WHERE_INIT + LOG_VAR_WHERE_INIT)\n            )\n        return\n\n    def forward(self, x, z_im1, h_im1):\n        batch_size = x.shape[0]\n        rnn_input = torch.cat((x.sum(axis=1).view(batch_size, -1), z_im1, h_im1), dim=1)\n        rnn_output = self.fc_rnn(rnn_input)\n        if self.baseline_net:\n            baseline_value_i = rnn_output[:, 0:1]\n            h_i = rnn_output[:, 1::]\n            return baseline_value_i, h_i\n        else:\n            omega_i = rnn_output[:, 0:(Z_PRES_DIM+2*Z_WHERE_DIM)]\n            h_i = rnn_output[:, (Z_PRES_DIM+2*Z_WHERE_DIM)::]\n            # omega_i[:, 0] corresponds to z_pres probability\n            omega_i[:, 0] = torch.sigmoid(omega_i[:, 0])\n            return omega_i, h_i\n\n\n\nAIR Implementation: The whole AIR model is obtained by putting everything together. To better understand what’s happening, let’s take a closer look on the two main functions:\n\nforward(x): This function essentially does what is described in High-Level Overview. Its purpose is to obtain a structured latent representation \\(\\textbf{z}=\\bigg\\{ \\left[\n\\textbf{z}_{\\text{pres}}^{(i)}, \\textbf{z}_{\\text{where}}^{(i)},\n\\textbf{z}_{\\text{what}}^{(i)}\\right]_{i=1}^N \\bigg\\}\\) for a given input (batch of images) \\(\\textbf{x}\\) and to collect everything needed to compute the loss.\ncompute_loss(x): This function is only necessary for training. It computes four loss quantities:\n\nKL Divergence: As noted above the KL divergence term can be computed by summing the KL divergences of each type (pres, what, where) for each step.\nNLL: We assume a Gaussian decoder such that the negative log-likelihood can be computed as follows\n\\[\n\\text{NLL} = \\frac {1}{2 \\cdot \\sigma^2}\\sum_{i=1}^{W \\cdot H} \\left(x_i - \\widetilde{x}_i \\right)^2,\n\\]\nwhere \\(i\\) enumerates the pixel space, \\(\\textbf{x}\\) denotes the original image, \\(\\widetilde{\\textbf{x}}\\) the reconstructed image and \\(\\sigma^2\\) is a the fixed variance of the Gaussian distribution (hyperparameter).\nREINFORCE Term: Since the image reconstruction is build by sampling from a discrete distribution, backpropagation stops at the sampling operation. In order to optimize the distribution parameters \\(p_{\\text{pres}}^{(i)}\\), we use a score-function estimator with a data-dependent neural baseline.\nBaseline Loss: This loss is needed to approximately fit the neural baseline to the true NLL in order to reduce the variance of the REINFORCE estimator.\n\n\n\n\n\nCode\nimport torch.nn.functional as F\nfrom torch.distributions import Bernoulli\n\n\nN = 3                                 # number of inference steps\nEPS = 1e-32                           # numerical stability\nPRIOR_MEAN_WHERE = [3., 0., 0.]       # prior for mean of z_i_where\nPRIOR_VAR_WHERE = [0.1**2, 1., 1.]    # prior for variance of z_i_where\nPRIOR_P_PRES = [0.01]                 # prior for p_i_pres of z_i_pres\nBETA = 0.5                            # hyperparameter to scale KL div\nOMEGA_DIM = Z_PRES_DIM + 2*Z_WHERE_DIM + 2*Z_WHAT_DIM\n\n\nclass AIR(nn.Module):\n\n    PRIOR_MEAN_Z_WHERE = nn.Parameter(torch.tensor(PRIOR_MEAN_WHERE),\n                                      requires_grad=False)\n    PRIOR_VAR_Z_WHERE = nn.Parameter(torch.tensor(PRIOR_VAR_WHERE),\n                                     requires_grad=False)\n    PRIOR_P_Z_PRES = nn.Parameter(torch.tensor(PRIOR_P_PRES),\n                                  requires_grad=False)\n\n    expansion_indices = torch.LongTensor([1, 0, 2, 0, 1, 3])\n    target_rectangle = torch.tensor(\n      [[-1., -1., 1., 1., -1.],\n       [-1., 1., 1., -1, -1.],\n       [1., 1., 1., 1., 1.]]\n    ).view(1, 3, 5)\n\n    def __init__(self):\n        super(AIR, self).__init__()\n        self.vae = VAE()\n        self.rnn = RNN()\n        self.baseline = RNN(True)\n        return\n\n    def compute_loss(self, x):\n        \"\"\"compute the loss of AIR (essentially a VAE loss)\n        assuming the following prior distributions for the latent variables\n\n            z_where ~ N(PRIOR_MEAN_WHERE, PRIOR_VAR_WHERE)\n            z_what ~ N([0, 1])\n            z_pres ~ Bern(p_pres)\n\n        and a\n\n            Gaussian decoder with fixed diagonal var (FIXED_VAR)\n        \"\"\"\n        batch_size = x.shape[0]\n        results = self.forward(x, True)\n        # kl_div for z_pres (between two Bernoulli distributions)\n        q_z_pres = results['all_prob_pres']\n        P_Z_PRES = AIR.PRIOR_P_Z_PRES.expand(q_z_pres.shape).to(x.device)\n        kl_div_pres = AIR.bernoulli_kl(q_z_pres, P_Z_PRES).sum(axis=2)\n        # kl_div for z_what (standard VAE regularization term)\n        q_z_what = [results['all_mu_what'], results['all_log_var_what']]\n        P_MU_WHAT = torch.zeros_like(results['all_mu_what'])\n        P_VAR_WHAT = torch.ones_like(results['all_log_var_what'])\n        P_Z_WHAT = [P_MU_WHAT, P_VAR_WHAT]\n        kl_div_what = AIR.gaussian_kl(q_z_what, P_Z_WHAT).sum(axis=2)\n        # kl_div for z_where (between two Gaussian distributions)\n        q_z_where = [results['all_mu_where'], results['all_log_var_where']]\n        P_MU_WHERE=AIR.PRIOR_MEAN_Z_WHERE.expand(results['all_mu_where'].shape)\n        P_VAR_WHERE=AIR.PRIOR_VAR_Z_WHERE.expand(results['all_mu_where'].shape)\n        P_Z_WHERE = [P_MU_WHERE.to(x.device), P_VAR_WHERE.to(x.device)]\n        kl_div_where = AIR.gaussian_kl(q_z_where, P_Z_WHERE).sum(axis=2)\n        # sum all kl_divs and use delayed mask to zero out irrelevants\n        delayed_mask = results['mask_delay']\n        kl_div = (kl_div_pres + kl_div_where + kl_div_what) * delayed_mask\n        # negative log-likelihood for Gaussian decoder (no gradient for z_pres)\n        factor = 0.5 * (1/FIXED_VAR)\n        nll = factor * ((x - results['x_tilde'])**2).sum(axis=(1,2,3))\n        # REINFORCE estimator for nll (gradient for z_pres)\n        baseline_target = nll.unsqueeze(1)\n        reinforce_term = ((baseline_target - results['baseline_values']\n                           ).detach()\n                          *results['z_pres_likelihood']*delayed_mask).sum(1)\n\n        # baseline model loss\n        baseline_loss = ((results['baseline_values'] -\n                          baseline_target.detach())**2 * delayed_mask).sum(1)\n        loss = dict()\n        loss['kl_div'] = BETA*kl_div.sum(1).mean()\n        loss['nll'] = nll.mean()\n        loss['reinforce'] = reinforce_term.mean()\n        loss['baseline'] = baseline_loss.mean()\n        return loss, results\n\n    def forward(self, x, save_attention_rectangle=False):\n        batch_size = x.shape[0]\n        # initializations\n        all_z = torch.empty((batch_size, N, Z_DIM), device=x.device)\n        z_pres_likelihood = torch.empty((batch_size, N), device=x.device)\n        mask_delay = torch.empty((batch_size, N), device=x.device)\n        all_omega = torch.empty((batch_size, N, OMEGA_DIM), device=x.device)\n        all_x_tilde = torch.empty((batch_size, N, CANVAS_SIZE, CANVAS_SIZE),\n                                 device=x.device)\n        baseline_values = torch.empty((batch_size, N), device=x.device)\n\n        z_im1 = torch.ones((batch_size, Z_DIM)).to(x.device)\n        h_im1 = torch.zeros((batch_size, RNN_HIDDEN_STATE_DIM)).to(x.device)\n        h_im1_b = torch.zeros((batch_size, RNN_HIDDEN_STATE_DIM)).to(x.device)\n        if save_attention_rectangle:\n            attention_rects = torch.empty((batch_size, N, 2, 5)).to(x.device)\n        for i in range(N):\n            z_im1_pres = z_im1[:, 0:1]\n            # mask_delay is used to zero out all steps AFTER FIRST z_pres = 0\n            mask_delay[:, i] = z_im1_pres.squeeze(1)\n            # obtain parameters of sampling distribution and hidden state\n            omega_i, h_i = self.rnn(x, z_im1, h_im1)\n            # baseline version\n            baseline_i, h_i_b = self.baseline(x.detach(), z_im1.detach(),\n                                              h_im1_b)\n            # set baseline 0 if z_im1_pres = 0\n            baseline_value = (baseline_i * z_im1_pres).squeeze()\n            # extract sample distributions parameters from omega_i\n            prob_pres_i = omega_i[:, 0:1]\n            mu_where_i = omega_i[:, 1:4]\n            log_var_where_i = omega_i[:, 4:7]\n            # sample from distributions to obtain z_i_pres and z_i_where\n            z_i_pres_post = Bernoulli(probs=prob_pres_i)\n            z_i_pres = z_i_pres_post.sample() * z_im1_pres\n            # likelihood of sampled z_i_pres (only if z_im_pres = 1)\n            z_pres_likelihood[:, i] = (z_i_pres_post.log_prob(z_i_pres) *\n                                       z_im1_pres).squeeze(1)\n            # get z_i_where by reparametrization trick\n            epsilon_w = torch.randn_like(log_var_where_i)\n            z_i_where = mu_where_i + torch.exp(0.5*log_var_where_i)*epsilon_w\n            # use z_where and x to obtain x_att_i\n            x_att_i = AIR.image_to_window(x, z_i_where)\n            # put x_att_i through VAE\n            x_tilde_att_i, z_i_what, mu_what_i, log_var_what_i = \\\n                self.vae(x_att_i)\n            # create image reconstruction\n            x_tilde_i = AIR.window_to_image(x_tilde_att_i, z_i_where)\n            # update im1 with current versions\n            z_im1 = torch.cat((z_i_pres, z_i_where, z_i_what), 1)\n            h_im1 = h_i\n            h_im1_b = h_i_b\n            # put all distribution parameters into omega_i\n            omega_i = torch.cat((prob_pres_i, mu_where_i, log_var_where_i,\n                                 mu_what_i, log_var_what_i), 1)\n            # store intermediate results\n            all_z[:, i:i+1] = z_im1.unsqueeze(1)\n            all_omega[:, i:i+1] = omega_i.unsqueeze(1)\n            all_x_tilde[:, i:i+1] = x_tilde_i\n            baseline_values[:, i] = baseline_value\n            # for nice visualization\n            if save_attention_rectangle:\n                attention_rects[:, i] = (AIR.get_attention_rectangle(z_i_where)\n                                         *z_i_pres.unsqueeze(1))\n        # save results in dict (easy accessibility)\n        results = dict()\n        # fixes Z_PRES_DIM = 1 and Z_WHERE_DIM = 3\n        results['z_pres_likelihood'] = z_pres_likelihood\n        results['all_z_pres'] = all_z[:, :, 0:1]\n        results['mask_delay'] = mask_delay\n        results['all_prob_pres'] = all_omega[:, :, 0:1]\n        results['all_z_where'] = all_z[:, :, 1:4]\n        results['all_mu_where'] =  all_omega[:, :, 1:4]\n        results['all_log_var_where'] = all_omega[:, :, 4:7]\n        results['all_z_what'] = all_z[:, :, 4::]\n        results['all_mu_what'] =  all_omega[:, :, 7:7+Z_WHAT_DIM]\n        results['all_log_var_what'] = all_omega[:, :, 7+Z_WHAT_DIM::]\n        results['baseline_values'] = baseline_values\n        if save_attention_rectangle:\n            results['attention_rects'] = attention_rects\n        # compute reconstructed image (take only x_tilde_i with z_i_pres=1)\n        results['x_tilde_i'] = all_x_tilde\n        x_tilde = (all_z[:, :, 0:1].unsqueeze(2) * all_x_tilde).sum(axis=1,\n                                                              keepdim=True)\n        results['x_tilde'] = x_tilde\n        # compute counts as identified objects (sum z_i_pres)\n        results['counts'] = results['all_z_pres'].sum(1).to(dtype=torch.long)\n        return results\n\n    @staticmethod\n    def image_to_window(x, z_i_where):\n        grid_shape = (z_i_where.shape[0], 1, WINDOW_SIZE, WINDOW_SIZE)\n        z_i_where_inv = AIR.invert_z_where(z_i_where)\n        x_att_i = AIR.spatial_transform(x, z_i_where_inv, grid_shape)\n        return x_att_i\n\n    @staticmethod\n    def window_to_image(x_tilde_att_i, z_i_where):\n        grid_shape = (z_i_where.shape[0], 1, CANVAS_SIZE, CANVAS_SIZE)\n        x_tilde_i = AIR.spatial_transform(x_tilde_att_i, z_i_where, grid_shape)\n        return x_tilde_i\n\n    @staticmethod\n    def spatial_transform(x, z_where, grid_shape):\n        theta_matrix = AIR.z_where_to_transformation_matrix(z_where)\n        grid = F.affine_grid(theta_matrix, grid_shape, align_corners=False)\n        out = F.grid_sample(x, grid, align_corners=False)\n        return out\n\n    @staticmethod\n    def z_where_to_transformation_matrix(z_i_where):\n        \"\"\"taken from\n        https://github.com/pyro-ppl/pyro/blob/dev/examples/air/air.py\n        \"\"\"\n        batch_size = z_i_where.shape[0]\n        out = torch.cat((z_i_where.new_zeros(batch_size, 1), z_i_where), 1)\n        ix = AIR.expansion_indices\n        if z_i_where.is_cuda:\n            ix = ix.cuda()\n        out = torch.index_select(out, 1, ix)\n        theta_matrix = out.view(batch_size, 2, 3)\n        return theta_matrix\n\n    @staticmethod\n    def invert_z_where(z_where):\n        z_where_inv = torch.zeros_like(z_where)\n        scale = z_where[:, 0:1] + 1e-9\n        z_where_inv[:, 1:3] = -z_where[:, 1:3] / scale\n        z_where_inv[:, 0:1] = 1 / scale\n        return z_where_inv\n\n    @staticmethod\n    def get_attention_rectangle(z_i_where):\n        batch_size = z_i_where.shape[0]\n        z_i_where_inv = AIR.invert_z_where(z_i_where)\n        theta_matrix = AIR.z_where_to_transformation_matrix(z_i_where_inv)\n        target_rectangle = AIR.target_rectangle.expand(batch_size, 3,\n                                                       5).to(z_i_where.device)\n        source_rectangle_normalized = torch.matmul(theta_matrix,\n                                                   target_rectangle)\n        # remap into absolute values\n        source_rectangle = 0 + (CANVAS_SIZE/2)*(source_rectangle_normalized + 1)\n        return source_rectangle\n\n    @staticmethod\n    def bernoulli_kl(q_probs, p_probs):\n        # https://github.com/pytorch/pytorch/issues/15288\n        p1 = p_probs\n        p0 = 1 - p1\n        q1 = q_probs\n        q0 = 1 - q1\n\n        logq1 = (q1 + EPS).log()\n        logq0 = (q0 + EPS).log()\n        logp1 = (p1).log()\n        logp0 = (p0).log()\n\n        kl_div_1 = q1*(logq1 - logp1)\n        kl_div_0 = q0*(logq0 - logp0)\n        return kl_div_1 + kl_div_0\n\n    @staticmethod\n    def gaussian_kl(q, p):\n        # https://pytorch.org/docs/stable/_modules/torch/distributions/kl.html\n        mean_q, log_var_q = q[0], q[1]\n        mean_p, var_p = p[0], p[1]\n\n        var_ratio = log_var_q.exp()/var_p\n        t1 = (mean_q - mean_p).pow(2)/var_p\n        return -0.5 * (1 + var_ratio.log() - var_ratio - t1)\n\n\n\nTraining Procedure: Lastly, a standard training procedure is implemented. We will use two optimizers, one for the model parameters and one for the neural baseline parameters. Note that the training process is completely unsupervised, i.e., the model only receives a batch of images to compute the losses.\n\n\n\nCode\nfrom livelossplot import PlotLosses, outputs\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\n\n\nEPOCHS = 50\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-4\nBASE_LEARNING_RATE = 1e-2\nEPOCHS_TO_SAVE_MODEL = [1, 10, EPOCHS]\n\n\ndef train(air, dataset):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    print('Device: {}'.format(device))\n\n    data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True,\n                             num_workers=4)\n    optimizer = torch.optim.Adam([{'params': list(air.rnn.parameters()) +\n                                   list(air.vae.parameters()),\n                                   'lr': LEARNING_RATE,},\n                                  {'params': air.baseline.parameters(),\n                                   'lr': BASE_LEARNING_RATE}])\n    air.to(device)\n\n    # prettify livelossplot\n    def custom(ax: plt.Axes, group: str, x_label: str):\n        ax.legend()\n        if group == 'accuracy':\n            ax.set_ylim(0, 1)\n        elif group == 'loss base':\n            ax.set_ylim(0, 300)\n\n    matplot = [outputs.MatplotlibPlot(after_subplot=custom,max_cols=3)]\n    losses_plot = PlotLosses(groups={'loss model':['KL div','NLL','REINFORCE'],\n                                     'loss base': ['baseline'],\n                                     'accuracy': ['count accuracy']},\n                             outputs=matplot)\n    for epoch in range(1, EPOCHS+1):\n        avg_kl_div, avg_nll, avg_reinforce, avg_base, avg_acc = 0, 0, 0, 0, 0\n        for x, label in data_loader:\n            air.zero_grad()\n\n            losses, results = air.compute_loss(x.to(device, non_blocking=True))\n            loss  = (losses['kl_div'] + losses['nll'] + losses['reinforce']\n                     +losses['baseline'])\n            loss.backward()\n            optimizer.step()\n\n            # compute accuracy\n            label = label.unsqueeze(1).to(device)\n            acc = (results['counts']==label).sum().item()/len(results['counts'])\n            # update epoch means\n            avg_kl_div += losses['kl_div'].item() / len(data_loader)\n            avg_nll += losses['nll'].item() / len(data_loader)\n            avg_reinforce += losses['reinforce'].item() / len(data_loader)\n            avg_base += losses['baseline'].item() / len(data_loader)\n            avg_acc += acc / len(data_loader)\n\n        if epoch in EPOCHS_TO_SAVE_MODEL:  # save model\n            torch.save(air, f'./results/checkpoint_{epoch}.pth')\n        losses_plot.update({'KL div': avg_kl_div,\n                            'NLL': avg_nll,\n                            'REINFORCE': avg_reinforce,\n                            'baseline': avg_base,\n                            'count accuracy': avg_acc}, current_step=epoch)\n        losses_plot.send()\n    print(f'Accuracy after Training {avg_acc:.2f} (on training dataset)')\n    torch.save(air, f'./results/checkpoint_{epoch}.pth')\n    trained_air = air\n    return trained_air\n\n\n\n\nResults\nLet’s train our model:\n\n\nCode\nair_model = AIR()\ntrain_dataset = generate_dataset(num_images=10000, SEED=42)\ntrained_air = train(air_model, train_dataset)\n\n\n\n\n\nTraining Results\n\n\nThis looks pretty awesome! It seems that our model nearly perfectly learns to count the number of digits without even knowing what a digit is.\nLet us take a closer look and plot the results of the model at different stages of the training against a test dataset.\n\n\nCode\ndef plot_results(dataset):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    n_samples = 7\n\n    i_samples = np.random.choice(range(len(dataset)), n_samples, replace=False)\n    colors_rect = ['red', 'green', 'yellow']\n    num_rows = len(EPOCHS_TO_SAVE_MODEL) + 1\n\n    fig = plt.figure(figsize=(14, 8))\n    for counter, i_sample in enumerate(i_samples):\n        orig_img = dataset[i_sample][0]\n        # data\n        ax = plt.subplot(num_rows, n_samples, 1 + counter)\n        plt.imshow(orig_img[0].numpy(), cmap='gray', vmin=0, vmax=1)\n        plt.axis('off')\n        if counter == 0:\n            ax.annotate('Data', xy=(-0.05, 0.5), xycoords='axes fraction',\n                        fontsize=14, va='center', ha='right', rotation=90)\n        # outputs after epochs of training\n        MODELS = [, , ]\n        for j, (epoch, model) in enumerate(zip(EPOCHS_TO_SAVE_MODEL, MODELS)):\n            trained_air = torch.load(model)\n            trained_air.to(device)\n\n            results = trained_air(orig_img.unsqueeze(0).to(device), True)\n\n            attention_recs = results['attention_rects'].squeeze(0)\n            x_tilde = torch.clamp(results['x_tilde'][0], 0 , 1)\n\n            ax = plt.subplot(num_rows, n_samples, 1 + counter + n_samples*(j+1))\n            plt.imshow(x_tilde[0].cpu().detach().numpy(), cmap='gray',\n                       vmin=0, vmax=1)\n            plt.axis('off')\n            # show attention windows\n            for step_counter, step in enumerate(range(N)):\n                rect = attention_recs[step].detach().cpu().numpy()\n                if rect.sum() &gt; 0:  # valid rectangle\n                    plt.plot(rect[0], rect[1]-0.5,\n                             color=colors_rect[step_counter])\n            if counter == 0:\n                # compute accuracy\n                data_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n                avg_acc = 0\n                for batch, label in data_loader:\n                    label = label.unsqueeze(1).to(device)\n                    r = trained_air(batch.to(device))\n                    acc = (r['counts']==label).sum().item()/len(r['counts'])\n                    avg_acc += acc / len(data_loader)\n                # annotate plot\n                ax.annotate(f'Epoch {epoch}\\n Acc {avg_acc:.2f}',\n                            xy=(-0.05, 0.5), va='center',\n                            xycoords='axes fraction', fontsize=14,  ha='right',\n                            rotation=90)\n    return\n\n\ntest_dataset = generate_dataset(num_images=300, SEED=2)\nplot_results(test_dataset)\n\n\n\n\n\nTest Results\n\n\nVery neat results, indeed! Note that this looks very similar to Figure 3 in the AIR paper. The accuracy on the left denotes the count accuracy of the test dataset.\n\n\nClosing Notes\nAlright, time to step down from our high horse. Actually, it took me quite some time to tweak the hyperparameters to obtain such good results. I put a lot of prior knowledge into the model so that completely unsupervised is probably exaggerated. Using a slightly different setup might lead to entirely different results. Furthermore, even in this setup, there may be cases in which the training converges to some local maximum (depending on the random network initializations and random training dataset)."
  },
  {
    "objectID": "paper_summaries/air/index.html#acknowledgements",
    "href": "paper_summaries/air/index.html#acknowledgements",
    "title": "Attend, Infer, Repeat: Fast Scene Understanding with Generative Models",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe blog post by Adam Kosiorek, the pyro tutorial on AIR and the pytorch implementation by Andrea Dittadi are great resources and helped very much to understand the details of the paper."
  },
  {
    "objectID": "paper_summaries/air/index.html#footnotes",
    "href": "paper_summaries/air/index.html#footnotes",
    "title": "Attend, Infer, Repeat: Fast Scene Understanding with Generative Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nVisualization of a standard attention transformation, for more details refer to my Spatial Transformer description.\n\n\n\n\n\n\n\n\n\n\n\nThis transformation is more constrained with only 3-DoF. Therefore it only allows cropping, translation and isotropic scaling to be applied to the input feature map.\n\n\n\n↩︎\nUsing the pseudoinverse (or Moore-Penrose inverse) is beneficial to allow inverse mappings even if \\(\\textbf{A}^{(i)}\\) becomes non-invertible, i.e., if \\(s^{(i)} = 0\\).↩︎\nNote that we assume normalized coordinates with same resolutions such that the notation is not completely messed up.↩︎\nAmortized variational approximation means basically parameterized variational approximation, i.e., we introduce a parameterized function \\(q_{\\boldsymbol{\\phi}} \\left( \\textbf{z},\nn | \\textbf{x}\\right)\\) (e.g., neural network parameterized by \\(\\boldsymbol{\\phi}\\)) that maps from an image \\(\\textbf{x}\\) to the distribution parameters for number of objects \\(n\\) and their latent representation \\(\\textbf{z}\\), see this excellent answer on variational inference.↩︎\nThe probabilistic decoder \\(p_{\\boldsymbol{\\theta}} \\left(\n\\textbf{x}^{(i)} | \\textbf{z}, n \\right)\\) is also just an approximation to the true generative process. However, note that for each \\(\\textbf{x}^{(i)}\\) we know how the reconstruction should look like. I.e., if \\(p_{\\boldsymbol{\\theta}} \\left(\n\\textbf{x}^{(i)} | \\textbf{z}, n \\right)\\) approximates the true generative process, we can optimize it by maximizing its expectation for given \\(\\textbf{z}\\) and \\(n\\) sampled from the approximate true posterior \\(q_{\\boldsymbol{\\phi}} \\left(\n\\textbf{z}, n | \\textbf{x}^{(i)}\\right)\\).↩︎\nOn a standard MNIST dataset, Kingma and Welling (2013) successfully used the centered isotropic multivariate Gaussian as a prior distribution.↩︎\nThat Eslami et al. (2016) used an anealing geometric distribution is not mentioned in their paper, however Adam Kosiorek emailed the authors and received that information, see his blog post.↩︎\nNote that weights and biases of linear layers are defaulty initialized by drawing uniformly in the interval \\([-\\frac {1}{\\sqrt{n_o}}, \\frac\n{1}{\\sqrt{n_o}}]\\), where \\(n_o\\) denotes the number of outputs of the linear layer.↩︎"
  },
  {
    "objectID": "ml101/probability_theory/cross_entropy_loss/index.html",
    "href": "ml101/probability_theory/cross_entropy_loss/index.html",
    "title": "Why do we use the cross entropy loss in classification?",
    "section": "",
    "text": "The cross entropy loss is commonly used during classification, since the minimization of the cross entropy corresponds to the maximum likelihood estimator.\n\n\n\n\n\n\nDerivation\n\n\n\n\n\nWe assume that we are given a classification task with a data set of \\(N\\) observations \\(\\Big(\\boldsymbol{x}^{(i)}, y^{(i)} \\Big)\\), where \\(\\textbf{x}^{(i)}\\in\\mathbb{R}^d\\) denotes some \\(d\\)-dimensional feature vector and \\(y^{(i)} \\in \\{0, 1, \\dots, C\\}\\) its corresponding label.\n\nGoal\nThe goal of classification is to predict the probability of each class given a feature vector. Thus, we actually want to infer the conditional probability distribution \\(p(y|\\boldsymbol{x})\\). Note that we do not have access to this distribution, but only to samples \\(y^{(i)}\\) from it.\n\n\nGenerative Process\nWe assume that each tuple \\(\\big( \\textbf{x}^{(i)}, y^{(i)} \\big)\\) is generated in the following procedure:\n\nPrior Distribution (Features): \\(\\boldsymbol{x} \\sim p(\\boldsymbol{x})\\) with \\(\\boldsymbol{x}\n\\in \\mathbb{R}^d\\)\nA feature vector \\(\\boldsymbol{x}^{(i)}\\) is sampled from some prior distribution \\(p(\\boldsymbol{x})\\).\nConditional Distribution (Label): \\(y \\sim p(y|\\boldsymbol{x}) = \\text{Cat} \\Big(\n\\boldsymbol{\\beta}(\\textbf{x}) \\Big)\\) with \\(y \\in \\{0, 1, \\dots, C\\}\\)\nThe associated label \\(y^{(i)}\\) is obtained via sampling from the class distribution \\(p(y|\\boldsymbol{x}^{(i)})\\).\n\n\n\nFunction Approximator\nFor the following section, we assume that we have some function approximator \\(q (y|\\boldsymbol{x}; \\boldsymbol{\\theta})\\) that aims to be as close as possible to the true class distribution \\(p(y|\\boldsymbol{x})\\) and in which \\(\\boldsymbol{\\theta}\\) denote the parameters of our model (e.g., weights in a neural network).\n\n\nMaximum Likelihood Estimator\nThe idea of the maximum likelihood estimator is to find the optimal model parameters \\(\\hat{\\boldsymbol{\\theta}}\\) by maximizing a likelihood function \\(\\mathcal{L}(\\boldsymbol{\\theta} |\n\\textbf{D})\\) of the model parameters over the data set\n\\[\n\\hat{\\boldsymbol{\\theta}} = \\arg \\max_{\\boldsymbol{\\theta}} \\mathcal{L}(\\boldsymbol{\\theta} | \\textbf{D}),\n\\]\nwhere \\(\\hat{\\boldsymbol{\\theta}}\\) is commonly referred to as maxmimum likelihood estimator. The likelihood function estimates for each specific \\(\\boldsymbol{\\theta}\\) how likely it is that our data set \\(\\mathcal{D}\\) has been generated with these model parameters. As a result, the maximum likelihood estimator denotes the model parameters under which the observed data is most probable given our assumed statistical model.\nNote that the likelihood is not a probability of the model parameters, i.e,\n\\[\n\\int \\mathcal{L} (\\boldsymbol{\\theta} | \\mathcal{D}) d\n\\boldsymbol{\\theta} \\neq 1\n\\]\nand thus needs to be distinguished from the posterior of the model parameters \\(p(\\boldsymbol{\\theta} | \\mathcal{D})\\).\nWe divide the likelihood function into a product of likelihoods using the i.i.d. assumption, i.e.,\n\\[\n\\hat{\\boldsymbol{\\theta}} = \\arg \\max_{\\boldsymbol{\\theta}} \\mathcal{L}(\\boldsymbol{\\theta} |\n\\textbf{D}) \\stackrel{i.i.d.}{=} \\arg \\max_{\\boldsymbol{\\theta}} \\prod_{i=1}^N \\mathcal{l} \\left(\\boldsymbol{\\theta} | \\big(\\boldsymbol{x}^{(i)}, y^{(i)}\\big) \\right),\n\\]\nwhere i.i.d. denotes that our observations are independently and identically drawn from the same joint distribution \\(p(\\textbf{x}, y)\\).\n\n\nLikelihood as the Probability Mass Function\nNow the question arises, how do we compute each term \\(\\mathcal{l} \\left(\\boldsymbol{\\theta} |\n\\big(\\boldsymbol{x}^{(i)}, y^{(i)}\\big) \\right)\\) if we are given a specific model parameter \\(\\boldsymbol{\\theta}\\)?\nSince we assume that the true class distribution \\(p(y|\\boldsymbol{x})\\) follows a Categorical distribution which is a discrete distribution, we can simply use its probability mass function.\n\nThe probability mass function of a Categorical distribution can be stated as follows\n\\[\nf(y | \\boldsymbol{p} ) = \\prod_{i=0}^C p_i^{1_{y==i}},\n\\]\nwhere \\(\\boldsymbol{p}= \\begin{bmatrix} p_0 & \\dots & p_C \\end{bmatrix}\\), \\(p_i\\) represents the probability of seeing \\(i\\) and \\(1_{x==i}\\) denotes the indicator function.\n\nAs a result, each term can be written as follows\n\\[\n\\mathcal{l} \\left(\\boldsymbol{\\theta} | \\big(\\boldsymbol{x}^{(i)}, y^{(i)}\\big) \\right) =\n\\prod_{k=1}^C \\left(q (y=k|\\boldsymbol{x}^{(i)}; \\boldsymbol{\\theta}) \\right)^{1_{y^{(i)} == k}}\n\\]\n\n\nNegative Log Likelihood\nWe can convert the product into a sum by applying the logarithm\n\\[\n\\log \\mathcal{L}(\\boldsymbol{\\theta} | \\textbf{D}) = \\log \\left( \\prod_{i=1}^N  \\mathcal{l}\n\\left(\\boldsymbol{\\theta} | \\big(\\boldsymbol{x}^{(i)}, y^{(i)}\\big) \\right)  \\right) = \\sum_{i=1}^N\n\\log \\mathcal{l} \\left(\\boldsymbol{\\theta} | \\big(\\boldsymbol{x}^{(i)}, y^{(i)}\\big) \\right),\n\\]\nwhere \\(\\log \\mathcal{l} (\\cdot)\\) is commonly referred to as log likelihood. Note that log is a strictly concave function and thus does not change the maximum likelihood estimator.\nAdditionally, we turn the maximization problem into a minimzation problem\n\\[\n\\hat{\\boldsymbol{\\theta}} = \\arg \\max_{\\boldsymbol{\\theta}} \\sum_{i=1}^N \\log \\mathcal{l}\n\\left(\\boldsymbol{\\theta} | \\big(\\boldsymbol{x}^{(i)}, y^{(i)}\\big) \\right) =\n\\arg \\min_{\\boldsymbol{\\theta}} \\sum_{i=1}^N - \\log \\mathcal{l} \\left(\\boldsymbol{\\theta} | \\big(\\boldsymbol{x}^{(i)}, y^{(i)}\\big) \\right)\n,\n\\]\nwhere \\(-\\log \\mathcal{l} (\\cdot)\\) is commonly referred to as negative log likelihood.\n\n\nCross Entropy as Negative Log Likelihood\nLastly, we show that the negeative log likelihood equals the cross entropy\n\\[\n- \\log \\mathcal{l} \\left(\\boldsymbol{\\theta} | \\big(\\boldsymbol{x}^{(i)}, y^{(i)}\\big) \\right)\n= \\sum_{k=1}^C  1_{y^{(i)} == k} \\cdot \\log q (y=k|\\boldsymbol{x}^{(i)}; \\boldsymbol{\\theta})\n\\]\nWe can interpret/convert the true labels as one-hot vectors such that we can associate a label distribution probability \\(\\boldsymbol{h}^{(i)}\\) per label \\(y^{(i)}\\),\n\\[\nh^{(i)}_k = \\begin{cases}1 & \\text{if } i\\text{th sample belongs to }k \\text{th class} \\\\ 0\n& \\text{else.} \\end{cases}\n\\]\nThen, the negative log likelihood can be rewritten into the cross entropy \\(H\\) between the label distribution \\(\\boldsymbol{h}\\) and the estimated (from the model) probability distribution \\(\\boldsymbol{q}\\)\n\\[\n- \\log \\mathcal{l} \\left(\\boldsymbol{\\theta} | \\big(\\boldsymbol{x}^{(i)}, y^{(i)}\\big) \\right)\n= - \\sum_{k=1}^C h^{(i)}_k \\cdot \\log q_k (\\boldsymbol{x}^{(i)} ; \\boldsymbol{\\theta}) =\nH(\\boldsymbol{h}^{(i)}, \\boldsymbol{q}^{(i)}_\\boldsymbol{\\theta} ) = H^{(i)}\n\\]\nThe likelihood estimator for the whole dataset can be concisely written as follows\n\\[\n\\hat{\\boldsymbol{\\theta}} = \\arg \\max_{\\boldsymbol{\\theta}} \\mathcal{L}(\\boldsymbol{\\theta} |\n\\textbf{D}) = \\arg \\min_{\\boldsymbol{\\theta}} \\sum_{i=1}^N H^{(i)}\n.\n\\]"
  },
  {
    "objectID": "ml101/probability_theory/pearson_vs_spearman_correlation/index.html",
    "href": "ml101/probability_theory/pearson_vs_spearman_correlation/index.html",
    "title": "What is the difference between Pearson and Spearman correlation?",
    "section": "",
    "text": "Pearson’s correlation coefficient measures the linear relationship whereas Spearman’s (rank) correleation coeffient assesses the mononotic relationship between two variables. Each coefficient has its legitmation depending on the use-case.\n\\[\n\\fbox{\n$\\textbf{Pearson} \\hspace{0.5cm}\nr \\left( \\textbf{X}, \\textbf{Y} \\right)\n= \\frac {\\text{Cov} \\left( \\textbf{X}, \\textbf{Y}\\right)} {\\sqrt{\\text{Var}\n\\left(\\textbf{X}\\right)} \\sqrt{\\text{Var}\\left(\\textbf{Y}\\right)}}\n$}\n\\]\n\\[\n\\fbox{\n$\\textbf{Spearman} \\hspace{0.5cm}\nr_s\\left( \\textbf{X}, \\textbf{Y} \\right) =\n\\frac {\\text{Cov} \\big( \\text{R} (\\textbf{X}), \\text{R} (\\textbf{Y}) \\big)} {\n\\sqrt{\\text{Var} \\big( \\text{R} (\\textbf{X}) \\big)} \\sqrt{\\text{Var} \\big( \\text{R} (\\textbf{Y}) \\big)}}\n$\n}\n\\]"
  },
  {
    "objectID": "ml101/probability_theory/pearson_vs_spearman_correlation/index.html#footnotes",
    "href": "ml101/probability_theory/pearson_vs_spearman_correlation/index.html#footnotes",
    "title": "What is the difference between Pearson and Spearman correlation?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis statement is not true. I couldn’t find who and when introduced the term covariance, but it is very likely that it wasn’t in the context of a multivariate Gaussian. However, seeing it that way may help to put covariance into context (IMHO).↩︎"
  },
  {
    "objectID": "ml101/probability_theory/score_function_estimator/index.html",
    "href": "ml101/probability_theory/score_function_estimator/index.html",
    "title": "What is a score function estimator (REINFORCE estimator)?",
    "section": "",
    "text": "There are several use cases in which we are interested in estimating the gradient of an expectation for samples drawn from a parametric distribution \\(\\textbf{z} \\sim\np_{\\boldsymbol{\\theta}}\\left(\\textbf{z}\\right)\\) and evaluated at \\(f(\\textbf{z})\\), i.e.,\n\\[\n  \\nabla_{\\boldsymbol{\\theta}} \\mathbb{E}_{\\textbf{z} \\sim\n  p_{\\boldsymbol{\\theta}}\\left(\\textbf{z} \\right)} \\big[ f\n  (\\textbf{z})\\big] = \\nabla_{\\boldsymbol{\\theta}} \\int f(\\textbf{z})\n  p_{\\boldsymbol{\\theta}} (\\textbf{z}) d\\textbf{z}\n\\]\nThe score function estimator, REINFORCE estimator or usual (naïve) Monte Carlo estimator approximates this gradient as follows\n\\[\n\\begin{align}\n\\nabla_{\\boldsymbol{\\theta}} \\mathbb{E}_{\\textbf{z} \\sim\n  p_{\\boldsymbol{\\theta}}\\left(\\textbf{z} \\right)} \\big[ f\n  (\\textbf{z})\\big] &= \\mathbb{E}_{\\textbf{z} \\sim\n  p_{\\boldsymbol{\\theta}}\\left(\\textbf{z} \\right)} \\left[\n  f(\\textbf{z}) \\nabla_{\\boldsymbol{\\theta}} \\log\n  p_{\\boldsymbol{\\theta}} (\\textbf{z}) \\right] \\\\\n  &\\approx \\frac {1}{N}\n  \\sum_{i=1}^N f\\left(\\textbf{z}^{(i)}\\right) \\nabla_{\\boldsymbol{\\theta}} \\log\n  p_{\\boldsymbol{\\theta}} \\left( \\textbf{z}^{(i)}\\right) \\quad\n  \\text{with} \\quad \\textbf{z}^{(i)} \\sim p_{\\boldsymbol{\\theta}}(\\textbf{z}),\n\\end{align}\n\\]\nwhere the term \\(\\nabla_{\\boldsymbol{\\theta}}\\log p_{\\boldsymbol{\\theta}} (\\textbf{z})\\) is called the score function. This gradient estimator is unbiased, however it (typically) exhibits very high variance.\nNote that the main advantage of the estimator is that we moved the gradient inside the expectation such that we can use Monte Carlo sampling to approximate it. Furthermore, it does not make any restriction on the function \\(f\\), i.e., \\(f\\) could even be non-differentiable and we could still compute the gradient of its expected value."
  },
  {
    "objectID": "ml101/probability_theory/score_function_estimator/index.html#acknowledgements",
    "href": "ml101/probability_theory/score_function_estimator/index.html#acknowledgements",
    "title": "What is a score function estimator (REINFORCE estimator)?",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nSyed Javed’s blog post was very helpful to get things clear. ———————————————————————————-"
  },
  {
    "objectID": "ml101/engineering/remap-numbers-in-interval/index.html",
    "href": "ml101/engineering/remap-numbers-in-interval/index.html",
    "title": "How to map numbers in an interval \\([a, b]\\) onto another interval \\([c, d]\\) ?",
    "section": "",
    "text": "Use the following function for each value \\(x \\in [a, b]\\)\n\\[\n  f(x) = c + \\frac {d - c} {b -a} \\left(x - a\\right).\n\\]\nNote that \\(f(x)\\) is a bijektiv function, i.e., there is a one-to-one correspondence between both intervals (even if the ranges are different)."
  },
  {
    "objectID": "ml101/engineering/remap-numbers-in-interval/index.html#acknowledgement",
    "href": "ml101/engineering/remap-numbers-in-interval/index.html#acknowledgement",
    "title": "How to map numbers in an interval \\([a, b]\\) onto another interval \\([c, d]\\) ?",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nBased on this answer."
  },
  {
    "objectID": "ml101/engineering/attention_mechanism/index.html",
    "href": "ml101/engineering/attention_mechanism/index.html",
    "title": "How does the attention mechanism (Attention is all you need) work?",
    "section": "",
    "text": "In essence, an attention mechanism can be intuitively understood as a means to assign individual importance (or rather attention) to each entity in a collection of entities (e.g., words in a sentence or pixels in an image) using some cues as input. Mathmatically, this translates into computing a weighted average over all entities. In the attention mechansim from the Attention is all you need, the attention weights are obtained from the attention cues.\nMore abstractly, the attention mechanism can be used to answer the following questions\n\nWhat entities (e.g., pixels or words) should we attend to or focus on?\nWhat entities (e.g., pixels or words) are relevant for the task at hand?\n\nVaswani et al. (2017) call their particular attention mechanism Scaled Dot-Product Attention. Therein, the collection of entities is termed values and the attention cues are termed queries and keys. Attention to particular values (entities) is obtained by computing the weighted average over all values (entities) in which the attention weights are obtained by combining the attention cues.\nThe attention cues (queries and keys) are vectors of length \\(d_k\\) defined per value and can be seen as representations of questions (queries) and facts (keys): E.g., we could imagine a query representing the question Are you a noun? and a corresponding key representing the facts Noun, positive connotation, important, female. The alignment between the attention cues is computed via the dot-product (hence the name), additionally the alignment scores are passed through a Softmax-layer to obtain normalized attention weights. Finally, these attention weights are used to compute the weighted average.\nTo speed things up, queries, keys and values are packed into matrices \\(\\textbf{Q}, \\textbf{K}\n\\in \\mathbb{R}^{N_v \\times d_k}\\) and \\(\\textbf{V} \\in \\mathbb{R}^{N_v \\times d_v}\\), respectively. As a result, the concise formulation of Scaled Dot-Product Attention is given by\n\\[\n\\text{Attention}(\\textbf{Q}, \\textbf{K}, \\textbf{V}) =\n\\underbrace{\\text{softmax}\n    \\left(\n    %\\overbrace{\n    \\frac {\\textbf{Q} \\textbf{K}^{\\text{T}}} {\\sqrt{d_k}}\n    %}^{\\text{attention alignment }  \\textbf{L} \\in }\n    \\right)\n}_{\\text{attention weight }\\textbf{W} \\in \\mathbb{R}^{N_v \\times N_v}} \\textbf{V}\n= \\textbf{A}\n\\]\nin which \\(\\frac{1}{\\sqrt{d_k}}\\) is an additional scalar which Vaswani et al. (2017) added to counter vanishing gradients (they hypothesize that for a higher cue dimension \\(d_k\\) the dot-product might grow large in magnitude).\nThe figure below highlights how the corresponding vectors are packed into matrices and which vectors are used to obtain the first attention vector \\(\\textbf{a}_1\\).\n\n\n\n\n\n\n\n\n\n\n\nScaled Dot-Product Attention: Matrix Packing and Computation Schematic\n\n\n\nThe result matrix \\(\\textbf{A}\\) has the same dimensionality as \\(\\textbf{V}\\) and in fact each entry \\(\\textbf{a}_i\\) is basically a (normalized) linear combination of the vectors \\(\\{\\textbf{v}_j\\}_{j=1}^{N_v}\\)\n\\[\n\\textbf{a}_i = \\sum_j w_{ij} (\\textbf{q}_i, \\textbf{k}_j) \\textbf{v}_j \\quad \\text{with} \\quad \\sum_j w_{ij} (\\textbf{q}_i, \\textbf{k}_j) = 1.\n\\]\nIn this formulation, it is obvious that scaled-dot product attention means basically computing a weighted average over all entities. Furthermore, each attention vector \\(\\textbf{a}_i\\) has a fixed query vector \\(\\textbf{q}_i\\) which explains the name query.\n\n\n\n\n\n\nDifference between Attention and Fully Connected Layer\n\n\n\n\n\nThe equation above looks surprisingly similar to a fully connected layer with no bias and same dimensionality between input \\(\\textbf{v}_i \\in\\mathbb{R}^N\\) and output \\(\\textbf{a}_i \\in \\mathbb{R}^N\\). In this case, we could describe the output of a fully connected layer as\n\\[\n\\text{a}_i = \\sum_j w_{ij} \\text{v}_j.\n\\]\nIf we would pass the weight matrix \\(\\textbf{W} \\in \\mathbb{R}^{N\\times N}\\) through a softmax layer, we could even achieve the following\n\\[\n\\text{a}_i = \\sum_j w_{ij} \\text{v}_j \\quad \\text{with} \\quad \\sum_{j} w_{ij} = 1.\n\\]\nSo what’s the difference between an attention and a fully connected layer?\nIn fact, the only difference is the value dimensionality \\(d_v\\), i.e., in case of \\(d_v = 1\\) there is no difference.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Layer\nAttention Layer\n\n\n\n\n\n\n\n\n\n\n\n\nImplementation\n\n\n\n\n\n\nimport math \nimport torch  \n\n\ndef attention( \n       query_matrix: torch.Tensor,  \n       key_matrix: torch.Tensor,  \n       value_matrix: torch.Tensor \n   ) -&gt; torch.Tensor: \n   \"\"\"Simplistic implementation of scaled dot-product attention. \n\n   Args: \n       query_matrix (torch.Tensor): shape [batch_size, N_v, d_k] \n       key_matrix (torch.Tensor):   shape [batch_size, N_v, d_k] \n       value_matrix (torch.Tensor): shape [batch_size, N_v, d_v] \n\n   Returns \n       torch.Tensor:                shape [batch_size, N_v, d_v] \n   \"\"\" \n   scale_factor = 1 / math.sqrt(query_matrix.size(-1)) \n   # compute unnormalized attention weights of shape [batch_size, N_v, N_v]  \n   attn_weights = scale_factor * query_matrix @ key_matrix.transpose(-2, -1) \n   # normalize attention weights (i.e., sum over last dimension equal to one) \n   normalized_attn_weights = torch.softmax(attn_weights, dim=-1) \n   # compute result of shape [batch_size, N_v, d_v]  \n   return normalized_attn_weights @ value_matrix \n\n\n\n\n\n\n\n\n\n\nCausal / Masked Attention\n\n\n\n\n\nIn fact, the attention mechanism defined above allows access to all future values in the sequence, e.g., the first entry \\(\\textbf{a}_1\\) is a (normalized) linear combination of all vectors \\(\\{\\textbf{v}_j\\}_{j=1}^{N_v}\\). This may be problematic when the values are generated on the fly (e.g., next-token prediction).\nTo address this, we can rewrite the attention mechanism as follows\n\\[\n\\textbf{a}_i = \\sum_{j=1}^{i} w_{ij} (\\textbf{q}_i, \\textbf{k}_j) \\textbf{v}_j \\quad\n  \\text{with} \\quad \\sum_{j=1}^{i} w_{ij} (\\textbf{q}_i, \\textbf{k}_j) = 1.\n\\]\nThis can be done by using the standard attention mechansim and masking out future values\n\\[\nw_{ij} (\\textbf{q}_i, \\textbf{k}_j) = 0 \\quad \\text{for} \\quad j \\ge i.\n\\]\nThe weights can be concisely written into a weight matrix \\(\\textbf{W} \\in \\mathbb{R}^{N_v \\times N_v}\\) and the above equation basically sets the upper triangular part to zero.\nThus, it is fairly simple to make the attention mechanism causal, see the implementation below.\n\nimport math \nimport torch  \n\ndef attention( \n       query_matrix: torch.Tensor,  \n       key_matrix: torch.Tensor,  \n       value_matrix: torch.Tensor,\n       is_causal: bool = False,\n   ) -&gt; torch.Tensor: \n   \"\"\"Simplistic implementation of scaled dot-product attention allowing for causal masking. \n\n   Args: \n       query_matrix (torch.Tensor): shape [batch_size, N_v, d_k] \n       key_matrix (torch.Tensor):   shape [batch_size, N_v, d_k] \n       value_matrix (torch.Tensor): shape [batch_size, N_v, d_v] \n       is_causal (bool):            whether to mask out future values\n\n   Returns \n       torch.Tensor:                shape [batch_size, N_v, d_v] \n   \"\"\" \n   scale_factor = 1 / math.sqrt(query_matrix.size(-1)) \n   # compute unnormalized attention weights of shape [batch_size, N_v, N_v]  \n   attn_weights = scale_factor * query_matrix @ key_matrix.transpose(-2, -1) \n   # normalize attention weights (i.e., sum over last dimension equal to one) \n   normalized_attn_weights = torch.softmax(attn_weights, dim=-1) \n   if is_causal:\n      causal_mask = torch.ones_like(attn_weights).tril(diagonal=0)\n      normalized_attn_weights = causal_mask.mul(normalized_attn_weights)\n   # compute result of shape [batch_size, N_v, d_v]  \n   return normalized_attn_weights @ value_matrix"
  },
  {
    "objectID": "ml101.html",
    "href": "ml101.html",
    "title": "ML101",
    "section": "",
    "text": "This is a loose collection of questions that I stumbled over during my studies."
  },
  {
    "objectID": "ml101.html#probability-theory",
    "href": "ml101.html#probability-theory",
    "title": "ML101",
    "section": "Probability Theory",
    "text": "Probability Theory\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\nWhy do we use the cross entropy loss in classification?\n\n\n\n\n\n\n\n\n\n\nHow can the variance of the score function estimator be decreased?\n\n\n\n\n\n\n\n\n\n\nWhat is the difference between Pearson and Spearman correlation?\n\n\n\n\n\n\n\n\n\n\nHow is linear regression connected to maximum likelihood?\n\n\n\n\n\n\n\n\n\n\nWhat is a score function estimator (REINFORCE estimator)?\n\n\n\n\n\n\n\n\n\n\nWhat is the ELBO?\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "ml101.html#engineering",
    "href": "ml101.html#engineering",
    "title": "ML101",
    "section": "Engineering",
    "text": "Engineering\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\nHow to map numbers in an interval \\([a, b]\\) onto another interval \\([c, d]\\) ?\n\n\n\n\n\n\n\n\n\n\nWhy should a MSE loss be avoided after a sigmoid layer?\n\n\n\n\n\n\n\n\n\n\nHow does the attention mechanism (Attention is all you need) work?\n\n\n\n\n\n\n\n\n\n\nHow can the average loss be calculated with batch means?\n\n\n\n\n\n\n\nNo matching items"
  }
]